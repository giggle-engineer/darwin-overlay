diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,36 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_H__
+#define __SSEPLUS_H__
+
+#include "SSEPlus_base.h"
+#include "SSEPlus_CPUID.h"
+#include "SSEPlus_REF.h"
+#include "SSEPlus_SSE.h"
+#include "SSEPlus_SSE2.h"
+#include "SSEPlus_SSE3.h"
+
+#ifdef SSP_COMPILER_SUPPORTS_SSSE3
+#include "SSEPlus_SSSE3.h"
+#endif 
+
+#ifdef SSP_COMPILER_SUPPORTS_SSE4a
+#include "SSEPlus_SSE4a.h"
+#endif 
+
+#ifdef SSP_COMPILER_SUPPORTS_SSE41
+#include "SSEPlus_SSE4.1.h"
+#endif 
+
+#ifdef SSP_COMPILER_SUPPORTS_SSE42
+#include "SSEPlus_SSE4.2.h"
+#endif 
+
+#ifdef SSP_COMPILER_SUPPORTS_SSE5
+#include "SSEPlus_SSE5.h"
+#endif 
+
+
+#endif // __SSEPLUS_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_CPUID.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_CPUID.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_CPUID.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_CPUID.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,82 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+
+// This file should contain all platform specific code. 
+
+#ifndef __SSEPLUS_CPUID_H__
+#define __SSEPLUS_CPUID_H__
+
+#include "SSEPlus_base.h"
+
+/** @addtogroup CPUID   
+ *  @{ 
+ *  @name CPUID Operations
+ */
+
+typedef enum 
+{
+    SSP_REF,
+    SSP_SSE,
+    SSP_SSE2,
+    SSP_SSE3,
+    SSP_SSSE3,
+    SSP_SSE4a,
+    SSP_SSE4_1,
+    SSP_SSE4_2,
+    SSP_SSE5,
+    SSP_SSE_COUNT
+}ssp_cpu_feature;
+
+typedef struct 
+{
+    int feature[SSP_SSE_COUNT]; 
+}ssp_cpuid;
+
+
+ssp_cpuid ssp_get_cpuid()
+{
+    unsigned maxInfoType;
+    ssp_cpuid cpu;
+    int data[4], i;
+
+    for( i=0; i<SSP_SSE_COUNT; ++i )
+    {
+        cpu.feature[i] = 0;
+    }
+
+    __cpuid( data, 0 );  
+    maxInfoType = data[0] + 0x80000000;
+
+    __cpuid( data, 1 );      
+
+    cpu.feature[SSP_SSE   ] = (data[3] & 0x1000000); // || 0;  // EDX: bit 25
+    cpu.feature[SSP_SSE2  ] = (data[3] & 0x2000000); // || 0;  // EDX: bit 26
+    cpu.feature[SSP_SSE3  ] = (data[2] & 0x1      ); // || 0;  // ECX: bit 0
+    cpu.feature[SSP_SSSE3 ] = (data[2] & 0x100    ); // || 0;  // ECX: bit 9
+    cpu.feature[SSP_SSE4_1] = (data[2] & 0x40000  ); // || 0;  // ECX: bit 19
+    cpu.feature[SSP_SSE4_2] = (data[2] & 0x80000  ); // || 0;  // ECX: bit 20
+
+    if( maxInfoType >= 0x80000001 )
+    {
+        __cpuid( data, 0x80000001 );        
+        
+        cpu.feature[ SSP_SSE4a ] = (data[2] & 0x40  ); // || 0;  // ECX: bit 6
+        cpu.feature[ SSP_SSE5  ] = (data[2] & 0x800 ); // || 0;  // ECX: bit 11
+    }
+   
+    return cpu;    
+}
+
+int ssp_is_supported( ssp_cpu_feature index )
+{
+    ssp_cpuid cpu = ssp_get_cpuid();
+    return cpu.feature[ index ];
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif //__SSEPLUS_CPUID_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,15 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_REF_H__
+#define __SSEPLUS_REF_H__
+
+#include "SSEPlus_base.h"
+#include "number/SSEPlus_number_REF.h"
+#include "emulation/SSEPlus_emulation_REF.h"
+#include "arithmetic/SSEPlus_arithmetic_REF.h"
+#include "memory/SSEPlus_memory_REF.h"
+#include "convert/SSEPlus_convert_REF.h"
+
+#endif // __SSEPLUS_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,110 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE_H__
+#define __SSEPLUS_SSE_H__
+
+#include "SSEPlus_base.h"
+
+//Forward declarations
+//============================================
+// SSE Native
+//============================================
+__m128        ssp_add_ps_SSE     (__m128 _A, __m128 _B);
+__m128        ssp_add_ss_SSE     (__m128 _A, __m128 _B);
+__m128        ssp_and_ps_SSE     (__m128 _A, __m128 _B);
+__m128        ssp_andnot_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpeq_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpeq_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpge_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpge_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpgt_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpgt_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmple_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmple_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmplt_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmplt_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpneq_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpneq_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpnge_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpnge_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpngt_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpngt_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpnle_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpnle_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpnlt_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpnlt_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpord_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpord_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpunord_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cmpunord_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_comieq_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_comige_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_comigt_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_comile_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_comilt_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_comineq_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_cvt_si2ss_SSE (__m128, int);
+int           ssp_cvt_ss2si_SSE (__m128 _A);
+__m128        ssp_cvtsi64_ss_SSE (__m128 _A, ssp_s64 _B);       // 64 bit instruction extensions
+float         ssp_cvtss_f32_SSE (__m128 _A);
+ssp_s64       ssp_cvtss_si64_SSE (__m128 _A);                   // 64 bit instruction extensions
+int           ssp_cvtt_ss2si_SSE (__m128 _A);
+ssp_s64       ssp_cvttss_si64_SSE (__m128 _A);                  // 64 bit instruction extensions
+__m128        ssp_div_ps_SSE     (__m128 _A, __m128 _B);
+__m128        ssp_div_ss_SSE     (__m128 _A, __m128 _B);
+unsigned int  ssp_getcsr_SSE (void);
+__m128        ssp_load_ps_SSE     (float const*_A);
+__m128        ssp_load_ps1_SSE (float const*_A);
+__m128        ssp_load_ss_SSE (float const*_A);
+__m128        ssp_loadr_ps_SSE (float const*_A);
+__m128        ssp_loadu_ps_SSE (float const*_A);
+__m128        ssp_max_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_max_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_min_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_min_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_move_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_movehl_ps_SSE (__m128, __m128);
+__m128        ssp_movelh_ps_SSE (__m128, __m128);
+int           ssp_movemask_ps_SSE (__m128 _A);
+__m128        ssp_mul_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_mul_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_or_ps_SSE (__m128 _A, __m128 _B);
+void          ssp_prefetch_SSE (char *_A, int _Sel);
+__m128        ssp_rcp_ps_SSE (__m128 _A);
+__m128        ssp_rcp_ss_SSE (__m128 _A);
+__m128        ssp_rsqrt_ps_SSE (__m128 _A);
+__m128        ssp_rsqrt_ss_SSE (__m128 _A);
+__m128        ssp_set_ps_SSE (float _A, float _B, float _C, float _D);
+__m128        ssp_set_ps1_SSE (float _A);
+__m128        ssp_set_ss_SSE (float _A);
+void          ssp_setcsr_SSE (unsigned int);
+__m128        ssp_setr_ps_SSE (float _A, float _B, float _C, float _D);
+__m128        ssp_setzero_ps_SSE (void);
+void          ssp_sfence_SSE (void);
+__m128        ssp_shuffle_ps_SSE (__m128 _A, __m128 _B, unsigned int _Imm8);
+__m128        ssp_sqrt_ps_SSE (__m128 _A);
+__m128        ssp_sqrt_ss_SSE (__m128 _A);
+void          ssp_store_ps_SSE (float *_V, __m128 _A);
+void          ssp_store_ps1_SSE (float *_V, __m128 _A);
+void          ssp_store_ss_SSE (float *_V, __m128 _A);
+void          ssp_storer_ps_SSE (float *_V, __m128 _A);
+void          ssp_storeu_ps_SSE (float *_V, __m128 _A);
+void          ssp_stream_ps_SSE (float *, __m128);
+__m128        ssp_sub_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_sub_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_ucomieq_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_ucomige_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_ucomigt_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_ucomile_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_ucomilt_ss_SSE (__m128 _A, __m128 _B);
+int           ssp_ucomineq_ss_SSE (__m128 _A, __m128 _B);
+__m128        ssp_unpackhi_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_unpacklo_ps_SSE (__m128 _A, __m128 _B);
+__m128        ssp_xor_ps_SSE (__m128 _A, __m128 _B);
+
+
+#include "native/SSEPlus_native_SSE.h"
+
+#endif // __SSEPLUS_SSE_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,369 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE2_H__
+#define __SSEPLUS_SSE2_H__
+
+#include "SSEPlus_base.h"
+
+//Forward declarations
+//============================================
+// SSE2 Native
+//============================================
+__m128i   ssp_add_epi16_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_add_epi32_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_add_epi64_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_add_epi8_SSE2         ( __m128i _A, __m128i _B );
+__m128d   ssp_add_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128d   ssp_add_sd_SSE2           ( __m128d _A, __m128d _B );
+__m128i   ssp_adds_epi16_SSE2       ( __m128i _A, __m128i _B );
+__m128i   ssp_adds_epi8_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_adds_epu16_SSE2       ( __m128i _A, __m128i _B );
+__m128i   ssp_adds_epu8_SSE2        ( __m128i _A, __m128i _B );
+__m128d   ssp_and_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128i   ssp_and_si128_SSE2        ( __m128i _A, __m128i _B );
+__m128d   ssp_andnot_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128i   ssp_andnot_si128_SSE2     ( __m128i _A, __m128i _B );
+__m128i   ssp_avg_epu16_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_avg_epu8_SSE2         ( __m128i _A, __m128i _B );
+__m128    ssp_castpd_ps_SSE2        ( __m128d );
+__m128i   ssp_castpd_si128_SSE2     ( __m128d );
+__m128d   ssp_castps_pd_SSE2        ( __m128 );
+__m128i   ssp_castps_si128_SSE2     ( __m128 );
+__m128d   ssp_castsi128_pd_SSE2     ( __m128i );
+__m128    ssp_castsi128_ps_SSE2     ( __m128i );
+void      ssp_clflush_SSE2          ( void const*_P );
+__m128i   ssp_cmpeq_epi16_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_cmpeq_epi32_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_cmpeq_epi8_SSE2       ( __m128i _A, __m128i _B );
+__m128d   ssp_cmpeq_pd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpeq_sd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpge_pd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpge_sd_SSE2         ( __m128d _A, __m128d _B );
+__m128i   ssp_cmpgt_epi16_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_cmpgt_epi32_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_cmpgt_epi8_SSE2       ( __m128i _A, __m128i _B );
+__m128d   ssp_cmpgt_pd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpgt_sd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmple_pd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmple_sd_SSE2         ( __m128d _A, __m128d _B );
+__m128i   ssp_cmplt_epi16_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_cmplt_epi32_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_cmplt_epi8_SSE2       ( __m128i _A, __m128i _B );
+__m128d   ssp_cmplt_pd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmplt_sd_SSE2         ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpneq_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpneq_sd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpnge_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpnge_sd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpngt_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpngt_sd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpnle_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpnle_sd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpnlt_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpnlt_sd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpord_pd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpord_sd_SSE2        ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpunord_pd_SSE2      ( __m128d _A, __m128d _B );
+__m128d   ssp_cmpunord_sd_SSE2      ( __m128d _A, __m128d _B );
+int       ssp_comieq_sd_SSE2        ( __m128d _A, __m128d _B );
+int       ssp_comige_sd_SSE2        ( __m128d _A, __m128d _B );
+int       ssp_comigt_sd_SSE2        ( __m128d _A, __m128d _B );
+int       ssp_comile_sd_SSE2        ( __m128d _A, __m128d _B );
+int       ssp_comilt_sd_SSE2        ( __m128d _A, __m128d _B );
+int       ssp_comineq_sd_SSE2       ( __m128d _A, __m128d _B );
+__m128d   ssp_cvtepi32_pd_SSE2      ( __m128i _A );
+__m128    ssp_cvtepi32_ps_SSE2      ( __m128i _A );
+__m128i   ssp_cvtpd_epi32_SSE2      ( __m128d _A );
+__m128    ssp_cvtpd_ps_SSE2         ( __m128d _A );
+__m128i   ssp_cvtps_epi32_SSE2      ( __m128 _A );
+__m128d   ssp_cvtps_pd_SSE2         ( __m128 _A );
+double    ssp_cvtsd_f64_SSE2        ( __m128d _A );
+int       ssp_cvtsd_si32_SSE2       ( __m128d _A );
+ssp_s64   ssp_cvtsd_si64_SSE2       ( __m128d );
+__m128    ssp_cvtsd_ss_SSE2         ( __m128 _A, __m128d _B );
+int       ssp_cvtsi128_si32_SSE2    ( __m128i _A );
+ssp_s64   ssp_cvtsi128_si64_SSE2    ( __m128i );
+__m128d   ssp_cvtsi32_sd_SSE2       ( __m128d _A, int _B );
+__m128i   ssp_cvtsi32_si128_SSE2    ( int _A );
+__m128d   ssp_cvtsi64_sd_SSE2       ( __m128d, ssp_s64 );
+__m128i   ssp_cvtsi64_si128_SSE2    ( ssp_s64 );
+__m128d   ssp_cvtss_sd_SSE2         ( __m128d _A, __m128 _B );
+__m128i   ssp_cvttpd_epi32_SSE2     ( __m128d _A );
+__m128i   ssp_cvttps_epi32_SSE2     ( __m128 _A );
+int       ssp_cvttsd_si32_SSE2      ( __m128d _A );
+ssp_s64   ssp_cvttsd_si64_SSE2      ( __m128d );
+__m128d   ssp_div_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128d   ssp_div_sd_SSE2           ( __m128d _A, __m128d _B );
+int       ssp_extract_epi16_SSE2    ( __m128i _A, int _Imm );
+__m128i   ssp_insert_epi16_SSE2     ( __m128i _A, int _B, int _Imm );
+void      ssp_lfence_SSE2           ( void );
+__m128d   ssp_load_pd_SSE2          ( double const*_Dp );
+__m128d   ssp_load_sd_SSE2          ( double const*_Dp );
+__m128i   ssp_load_si128_SSE2       ( __m128i const*_P );
+__m128d   ssp_load1_pd_SSE2         ( double const*_Dp );
+__m128d   ssp_loadh_pd_SSE2         ( __m128d _A, double const*_Dp );
+__m128i   ssp_loadl_epi64_SSE2      ( __m128i const*_P );
+__m128d   ssp_loadl_pd_SSE2         ( __m128d _A, double const*_Dp );
+__m128d   ssp_loadr_pd_SSE2         ( double const*_Dp );
+__m128d   ssp_loadu_pd_SSE2         ( double const*_Dp );
+__m128i   ssp_loadu_si128_SSE2      ( __m128i const*_P );
+__m128i   ssp_madd_epi16_SSE2       ( __m128i _A, __m128i _B );
+void      ssp_maskmoveu_si128_SSE2  ( __m128i _D, __m128i _N, char *_P );
+__m128i   ssp_max_epi16_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_max_epu8_SSE2         ( __m128i _A, __m128i _B );
+__m128d   ssp_max_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128d   ssp_max_sd_SSE2           ( __m128d _A, __m128d _B );
+void      ssp_mfence_SSE2           ( void );
+__m128i   ssp_min_epi16_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_min_epu8_SSE2         ( __m128i _A, __m128i _B );
+__m128d   ssp_min_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128d   ssp_min_sd_SSE2           ( __m128d _A, __m128d _B );
+__m128i   ssp_move_epi64_SSE2       ( __m128i _Q );
+__m128d   ssp_move_sd_SSE2          ( __m128d _A, __m128d _B );
+int       ssp_movemask_epi8_SSE2    ( __m128i _A );
+int       ssp_movemask_pd_SSE2      ( __m128d _A );
+__m128i   ssp_mul_epu32_SSE2        ( __m128i _A, __m128i _B );
+__m128d   ssp_mul_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128d   ssp_mul_sd_SSE2           ( __m128d _A, __m128d _B );
+__m128i   ssp_mulhi_epi16_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_mulhi_epu16_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_mullo_epi16_SSE2      ( __m128i _A, __m128i _B );
+__m128d   ssp_or_pd_SSE2            ( __m128d _A, __m128d _B );
+__m128i   ssp_or_si128_SSE2         ( __m128i _A, __m128i _B );
+__m128i   ssp_packs_epi16_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_packs_epi32_SSE2      ( __m128i _A, __m128i _B );
+__m128i   ssp_packus_epi16_SSE2     ( __m128i _A, __m128i _B );
+void      ssp_pause_SSE2            ( void );
+__m128i   ssp_sad_epu8_SSE2         ( __m128i _A, __m128i _B );
+__m128i   ssp_set_epi16_SSE2        ( short _W7, short _W6, short _W5, short _W4, short _W3, short _W2, short _W1, short _W0 );
+__m128i   ssp_set_epi32_SSE2        ( int _I3, int _I2, int _I1, int _I0 );
+
+__m128i   ssp_set_epi8_SSE2         ( char _B15, char _B14, char _B13, char _B12, char _B11, char _B10, char _B9, char _B8, char _B7, char _B6, char _B5, char _B4, char _B3, char _B2, char _B1, char _B0 );
+__m128d   ssp_set_pd_SSE2           ( double _Z, double _Y );
+__m128d   ssp_set_sd_SSE2           ( double _W );
+__m128i   ssp_set1_epi16_SSE2       ( short _W );
+__m128i   ssp_set1_epi32_SSE2       ( int _I );
+
+__m128i   ssp_set1_epi8_SSE2        ( char _B );
+__m128d   ssp_set1_pd_SSE2          ( double _A );
+__m128i   ssp_setl_epi64_SSE2       ( __m128i _Q );
+__m128i   ssp_setr_epi16_SSE2       ( short _W0, short _W1, short _W2, short _W3, short _W4, short _W5, short _W6, short _W7 );
+__m128i   ssp_setr_epi32_SSE2       ( int _I0, int _I1, int _I2, int _I3 );
+
+__m128i   ssp_setr_epi8_SSE2        ( char _B15, char _B14, char _B13, char _B12, char _B11, char _B10, char _B9, char _B8, char _B7, char _B6, char _B5, char _B4, char _B3, char _B2, char _B1, char _B0 );
+__m128d   ssp_setr_pd_SSE2          ( double _Y, double _Z );
+__m128d   ssp_setzero_pd_SSE2       ( void );
+__m128i   ssp_setzero_si128_SSE2    ( void );
+__m128i   ssp_shuffle_epi32_SSE2    ( __m128i _A, int _Imm );
+__m128d   ssp_shuffle_pd_SSE2       ( __m128d _A, __m128d _B, int _I );
+__m128i   ssp_shufflehi_epi16_SSE2  ( __m128i _A, int _Imm );
+__m128i   ssp_shufflelo_epi16_SSE2  ( __m128i _A, int _Imm );
+__m128i   ssp_sll_epi16_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_sll_epi32_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_sll_epi64_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_slli_epi16_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_slli_epi32_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_slli_epi64_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_slli_si128_SSE2       ( __m128i _A, int _Imm );
+__m128d   ssp_sqrt_pd_SSE2          ( __m128d _A );
+__m128d   ssp_sqrt_sd_SSE2          ( __m128d _A, __m128d _B );
+__m128i   ssp_sra_epi16_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_sra_epi32_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_srai_epi16_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_srai_epi32_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_srl_epi16_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_srl_epi32_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_srl_epi64_SSE2        ( __m128i _A, __m128i _Count );
+__m128i   ssp_srli_epi16_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_srli_epi32_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_srli_epi64_SSE2       ( __m128i _A, int _Count );
+__m128i   ssp_srli_si128_SSE2       ( __m128i _A, int _Imm );
+void      ssp_store_pd_SSE2         ( double *_Dp, __m128d _A );
+void      ssp_store_sd_SSE2         ( double *_Dp, __m128d _A );
+void      ssp_store_si128_SSE2      ( __m128i *_P, __m128i _B );
+void      ssp_store1_pd_SSE2        ( double *_Dp, __m128d _A );
+void      ssp_storeh_pd_SSE2        ( double *_Dp, __m128d _A );
+void      ssp_storel_epi64_SSE2     ( __m128i *_P, __m128i _Q );
+void      ssp_storel_pd_SSE2        ( double *_Dp, __m128d _A );
+void      ssp_storer_pd_SSE2        ( double *_Dp, __m128d _A );
+void      ssp_storeu_pd_SSE2        ( double *_Dp, __m128d _A );
+void      ssp_storeu_si128_SSE2     ( __m128i *_P, __m128i _B );
+void      ssp_stream_pd_SSE2        ( double *_Dp, __m128d _A );
+void      ssp_stream_si128_SSE2     ( __m128i *_P, __m128i _A );
+void      ssp_stream_si32_SSE2      ( int *_P, int _I );
+__m128i   ssp_sub_epi16_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_sub_epi32_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_sub_epi64_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_sub_epi8_SSE2         ( __m128i _A, __m128i _B );
+__m128d   ssp_sub_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128d   ssp_sub_sd_SSE2           ( __m128d _A, __m128d _B );
+
+__m128i   ssp_subs_epi16_SSE2       ( __m128i _A, __m128i _B );
+__m128i   ssp_subs_epi8_SSE2        ( __m128i _A, __m128i _B );
+__m128i   ssp_subs_epu16_SSE2       ( __m128i _A, __m128i _B );
+__m128i   ssp_subs_epu8_SSE2        ( __m128i _A, __m128i _B );
+int       ssp_ucomieq_sd_SSE2       ( __m128d _A, __m128d _B );
+int       ssp_ucomige_sd_SSE2       ( __m128d _A, __m128d _B );
+int       ssp_ucomigt_sd_SSE2       ( __m128d _A, __m128d _B );
+int       ssp_ucomile_sd_SSE2       ( __m128d _A, __m128d _B );
+int       ssp_ucomilt_sd_SSE2       ( __m128d _A, __m128d _B );
+int       ssp_ucomineq_sd_SSE2      ( __m128d _A, __m128d _B );
+__m128i   ssp_unpackhi_epi16_SSE2   ( __m128i _A, __m128i _B );
+__m128i   ssp_unpackhi_epi32_SSE2   ( __m128i _A, __m128i _B );
+__m128i   ssp_unpackhi_epi64_SSE2   ( __m128i _A, __m128i _B );
+__m128i   ssp_unpackhi_epi8_SSE2    ( __m128i _A, __m128i _B );
+__m128d   ssp_unpackhi_pd_SSE2      ( __m128d _A, __m128d _B );
+__m128i   ssp_unpacklo_epi16_SSE2   ( __m128i _A, __m128i _B );
+__m128i   ssp_unpacklo_epi32_SSE2   ( __m128i _A, __m128i _B );
+__m128i   ssp_unpacklo_epi64_SSE2   ( __m128i _A, __m128i _B );
+__m128i   ssp_unpacklo_epi8_SSE2    ( __m128i _A, __m128i _B );
+__m128d   ssp_unpacklo_pd_SSE2      ( __m128d _A, __m128d _B );
+__m128d   ssp_xor_pd_SSE2           ( __m128d _A, __m128d _B );
+__m128i   ssp_xor_si128_SSE2        ( __m128i _A, __m128i _B );
+
+//============================================
+// SSE3 Emulation
+//============================================
+__m128d   ssp_addsub_pd_SSE2        ( __m128d a, __m128d b ); 
+__m128    ssp_addsub_ps_SSE2        ( __m128 a, __m128 b ); 
+__m128d   ssp_hadd_pd_SSE2          ( __m128d a, __m128d b ); 
+__m128    ssp_hadd_ps_SSE2          ( __m128 a, __m128 b ); 
+__m128d   ssp_hsub_pd_SSE2          ( __m128d a, __m128d b ); 
+__m128    ssp_hsub_ps_SSE2          ( __m128 a, __m128 b ); 
+__m128i   ssp_lddqu_si128_SSE2      ( __m128i const *p ); 
+__m128d   ssp_loaddup_pd_SSE2       ( double const * dp ); 
+__m128d   ssp_movedup_pd_SSE2       ( __m128d a ); 
+__m128    ssp_movehdup_ps_SSE2      ( __m128 a ); 
+__m128    ssp_moveldup_ps_SSE2      ( __m128 a ); 
+
+//============================================
+// SSSE3 Emulation
+//============================================
+__m128i   ssp_abs_epi16_SSE2        ( __m128i a );
+__m128i   ssp_abs_epi32_SSE2        ( __m128i a );
+__m128i   ssp_abs_epi8_SSE2         ( __m128i a );
+__m128i   ssp_alignr_epi8_SSE2      ( __m128i a, __m128i b, const int n );
+__m128i   ssp_hadd_epi16_SSE2       ( __m128i a, __m128i b );
+__m128i   ssp_hadd_epi32_SSE2       ( __m128i a, __m128i b );
+__m128i   ssp_hadds_epi16_SSE2      ( __m128i a, __m128i b );
+__m128i   ssp_hsub_epi16_SSE2       ( __m128i a, __m128i b );
+__m128i   ssp_hsub_epi32_SSE2       ( __m128i a, __m128i b );
+__m128i   ssp_hsubs_epi16_SSE2      ( __m128i a, __m128i b );
+__m128i   ssp_maddubs_epi16_SSE2    ( __m128i a, __m128i b );
+__m128i   ssp_mulhrs_epi16_SSE2     ( __m128i a, __m128i b );
+__m128i   ssp_shuffle_epi8_SSE2     ( __m128i a, __m128i b );
+__m128i   ssp_sign_epi16_SSE2       ( __m128i a, __m128i b );
+__m128i   ssp_sign_epi32_SSE2       ( __m128i a, __m128i b );
+__m128i   ssp_sign_epi8_SSE2        ( __m128i a, __m128i b );
+
+
+//============================================
+// SSE4A Emulation
+//============================================
+__m128i   ssp_extract_si64_SSE2     ( __m128i,__m128i );   
+__m128i   ssp_extracti_si64_SSE2    ( __m128i, int, int );   
+__m128i   ssp_insert_si64_SSE2      ( __m128i,__m128i );   
+__m128i   ssp_inserti_si64_SSE2     ( __m128i, __m128i, int, int );   
+void      ssp_stream_sd_SSE2        ( double*,__m128d );   
+void      ssp_stream_ss_SSE2        ( float*,__m128 );   
+
+//============================================
+// SSE4.1 Emulation
+//============================================
+__m128i   ssp_blend_epi16_SSE2      ( __m128i v1, __m128i v2, const int mask );    
+__m128d   ssp_blend_pd_SSE2         ( __m128d v1, __m128d v2, const int mask );    
+__m128    ssp_blend_ps_SSE2         ( __m128  v1, __m128  v2, const int mask );    
+__m128i   ssp_blendv_epi8_SSE2      ( __m128i v1, __m128i v2, __m128i   mask );    
+__m128d   ssp_blendv_pd_SSE2        ( __m128d v1, __m128d v2, __m128d   mask );    
+__m128    ssp_blendv_ps_SSE2        ( __m128  v1, __m128  v2, __m128    mask );    
+__m128d   ssp_ceil_pd_SSE2          ( __m128d a );    
+__m128    ssp_ceil_ps_SSE2          ( __m128  a );    
+__m128d   ssp_ceil_sd_SSE2          ( __m128d a, __m128d b );    
+__m128    ssp_ceil_ss_SSE2          ( __m128  a, __m128  b );    
+__m128i   ssp_cmpeq_epi64_SSE2      ( __m128i val1, __m128i val2 );    
+__m128i   ssp_cvtepi16_epi32_SSE2   ( __m128i shortValues );    
+__m128i   ssp_cvtepi16_epi64_SSE2   ( __m128i shortValues );    
+__m128i   ssp_cvtepi32_epi64_SSE2   ( __m128i intValues   );    
+__m128i   ssp_cvtepi8_epi16_SSE2    ( __m128i byteValues  );    
+__m128i   ssp_cvtepi8_epi32_SSE2    ( __m128i byteValues  );    
+__m128i   ssp_cvtepi8_epi64_SSE2    ( __m128i byteValues  );    
+__m128i   ssp_cvtepu16_epi32_SSE2   ( __m128i shortValues );    
+__m128i   ssp_cvtepu16_epi64_SSE2   ( __m128i shortValues );    
+__m128i   ssp_cvtepu32_epi64_SSE2   ( __m128i intValues   );    
+__m128i   ssp_cvtepu8_epi16_SSE2    ( __m128i byteValues  );    
+__m128i   ssp_cvtepu8_epi32_SSE2    ( __m128i byteValues  );    
+__m128i   ssp_cvtepu8_epi64_SSE2    ( __m128i shortValues );    
+__m128d   ssp_dp_pd_SSE2            ( __m128d val1, __m128d val2, const int mask );    
+__m128    ssp_dp_ps_SSE2            ( __m128  val1, __m128  val2, const int mask );    
+int       ssp_extract_epi32_SSE2    ( __m128i src, const int ndx );    
+ssp_s64   ssp_extract_epi64_SSE2    ( __m128i src, const int ndx );    
+int       ssp_extract_epi8_SSE2     ( __m128i src, const int ndx );    
+int       ssp_extract_ps_SSE2       ( __m128  src, const int ndx );    
+__m128d   ssp_floor_pd_SSE2         ( __m128d a );    
+__m128    ssp_floor_ps_SSE2         ( __m128  a );    
+__m128d   ssp_floor_sd_SSE2         ( __m128d a, __m128d b );    
+__m128    ssp_floor_ss_SSE2         ( __m128  a, __m128  b );    
+__m128i   ssp_insert_epi32_SSE2     ( __m128i dst,      int s, const int ndx );    
+__m128i   ssp_insert_epi64_SSE2     ( __m128i dst, ssp_s64  s, const int ndx );    
+__m128i   ssp_insert_epi8_SSE2      ( __m128i dst,      int s, const int ndx );    
+__m128    ssp_insert_ps_SSE2        ( __m128  dst, __m128 src, const int ndx );    
+__m128i   ssp_max_epi32_SSE2        ( __m128i val1, __m128i val2 );    
+__m128i   ssp_max_epi8_SSE2         ( __m128i val1, __m128i val2 );    
+__m128i   ssp_max_epu16_SSE2        ( __m128i val1, __m128i val2 );    
+__m128i   ssp_max_epu32_SSE2        ( __m128i val1, __m128i val2 );    
+__m128i   ssp_min_epi32_SSE2        ( __m128i val1, __m128i val2 );    
+__m128i   ssp_min_epi8_SSE2         ( __m128i val1, __m128i val2 );    
+__m128i   ssp_min_epu16_SSE2        ( __m128i val1, __m128i val2 );    
+__m128i   ssp_min_epu32_SSE2        ( __m128i val1, __m128i val2 );    
+__m128i   ssp_minpos_epu16_SSE2     ( __m128i shortValues );    
+__m128i   ssp_mpsadbw_epu8_SSE2     ( __m128i s1,   __m128i s2,   const int msk  );    
+__m128i   ssp_mul_epi32_SSE2        ( __m128i a,    __m128i b );   
+__m128i   ssp_packus_epi32_SSE2     ( __m128i val1, __m128i val2 );    
+__m128d   ssp_round_pd_SSE2         ( __m128d val, int iRoundMode );    
+
+SSP_FORCEINLINE
+__m128    ssp_round_ps_SSE2         ( __m128  val, int iRoundMode );    
+__m128d   ssp_round_sd_SSE2         ( __m128d dst, __m128d val, int iRoundMode );    
+__m128    ssp_round_ss_SSE2         ( __m128  dst, __m128  val, int iRoundMode );    
+__m128i   ssp_stream_load_si128_SSE2( __m128i* v1 );    
+int       ssp_testc_si128_SSE2      ( __m128i mask, __m128i val );    
+int       ssp_testnzc_si128_SSE2    ( __m128i mask, __m128i s2  ); 
+
+SSP_FORCEINLINE
+int       ssp_testz_si128_SSE2      ( __m128i mask, __m128i val );  
+
+//============================================
+// SSE4.2 Emulation
+//============================================
+int       ssp_cmpestra_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+int       ssp_cmpestrc_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+int       ssp_cmpestri_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+__m128i   ssp_cmpestrm_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+int       ssp_cmpestro_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+int       ssp_cmpestrs_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+int       ssp_cmpestrz_SSE2         ( __m128i a, int la, __m128i b, int lb, const int mode );
+__m128i   ssp_cmpgt_epi64_SSE2      ( __m128i a, __m128i b );
+int       ssp_cmpistra_SSE2         ( __m128i a, __m128i b, const int mode );
+int       ssp_cmpistrc_SSE2         ( __m128i a, __m128i b, const int mode );
+int       ssp_cmpistri_SSE2         ( __m128i a, __m128i b, const int mode );
+__m128i   ssp_cmpistrm_SSE2         ( __m128i a, __m128i b, const int mode );
+int       ssp_cmpistro_SSE2         ( __m128i a, __m128i b, const int mode );
+int       ssp_cmpistrs_SSE2         ( __m128i a, __m128i b, const int mode );
+int       ssp_cmpistrz_SSE2         ( __m128i a, __m128i b, const int mode );
+unsigned int     ssp_crc32_u16_SSE2 ( unsigned int crc, unsigned short   v );
+unsigned int     ssp_crc32_u32_SSE2 ( unsigned int crc, unsigned int     v );
+ssp_u64   ssp_crc32_u64_SSE2        ( unsigned int crc,          ssp_u64 v );
+unsigned int     ssp_crc32_u8_SSE2  ( unsigned int crc, unsigned char    v );
+int       ssp_popcnt_u32_SSE2       ( unsigned int a     );
+int       ssp_popcnt_u64_SSE2       ( ssp_u64 a );
+
+#include "native/SSEPlus_native_SSE2.h" 
+#include "emulation/SSEPlus_emulation_SSE2.h" 
+#include "arithmetic/SSEPlus_arithmetic_SSE2.h"
+#include "logical/SSEPlus_logical_SSE2.h"
+#include "memory/SSEPlus_memory_SSE2.h"
+#include "convert/SSEPlus_convert_SSE2.h"
+
+#endif // __SSEPLUS_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,162 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE3_H__
+#define __SSEPLUS_SSE3_H__
+
+#include "SSEPlus_base.h"
+
+//============================================
+// SSE3 Native
+//============================================
+__m128d   ssp_addsub_pd_SSE3        ( __m128d a, __m128d b ); 
+__m128    ssp_addsub_ps_SSE3        ( __m128 a, __m128 b ); 
+__m128d   ssp_hadd_pd_SSE3          ( __m128d a, __m128d b ); 
+__m128    ssp_hadd_ps_SSE3          ( __m128 a, __m128 b ); 
+__m128d   ssp_hsub_pd_SSE3          ( __m128d a, __m128d b ); 
+__m128    ssp_hsub_ps_SSE3          ( __m128 a, __m128 b ); 
+__m128i   ssp_lddqu_si128_SSE3      ( __m128i const *p ); 
+__m128d   ssp_loaddup_pd_SSE3       ( double const * dp ); 
+__m128d   ssp_movedup_pd_SSE3       ( __m128d a ); 
+__m128    ssp_movehdup_ps_SSE3      ( __m128 a ); 
+__m128    ssp_moveldup_ps_SSE3      ( __m128 a ); 
+
+////============================================
+//// SSSE3 Emulation
+////============================================
+//__m128i   ssp_abs_epi16_SSE3        ( __m128i a );
+//__m128i   ssp_abs_epi32_SSE3        ( __m128i a );
+//__m128i   ssp_abs_epi8_SSE3         ( __m128i a );
+//__m64     ssp_abs_pi16_SSE3         ( __m64 a );
+//__m64     ssp_abs_pi32_SSE3         ( __m64 a );
+//__m64     ssp_abs_pi8_SSE3          ( __m64 a );
+//__m128i   ssp_alignr_epi8_SSE3      ( __m128i a, __m128i b, int n );
+//__m64     ssp_alignr_pi8_SSE3       ( __m64 a, __m64 b, int n );
+//__m128i   ssp_hadd_epi16_SSE3       ( __m128i a, __m128i b );
+//__m128i   ssp_hadd_epi32_SSE3       ( __m128i a, __m128i b );
+//__m64     ssp_hadd_pi16_SSE3        ( __m64 a, __m64 b );
+//__m64     ssp_hadd_pi32_SSE3        ( __m64 a, __m64 b );
+//__m128i   ssp_hadds_epi16_SSE3      ( __m128i a, __m128i b );
+//__m64     ssp_hadds_pi16_SSE3       ( __m64 a, __m64 b );
+//__m128i   ssp_hsub_epi16_SSE3       ( __m128i a, __m128i b );
+//__m128i   ssp_hsub_epi32_SSE3       ( __m128i a, __m128i b );
+//__m64     ssp_hsub_pi16_SSE3        ( __m64 a, __m64 b );
+//__m64     ssp_hsub_pi32_SSE3        ( __m64 a, __m64 b );
+//__m128i   ssp_hsubs_epi16_SSE3      ( __m128i a, __m128i b );
+//__m64     ssp_hsubs_pi16_SSE3       ( __m64 a, __m64 b );
+//__m128i   ssp_maddubs_epi16_SSE3    ( __m128i a, __m128i b );
+//__m64     ssp_maddubs_pi16_SSE3     ( __m64 a, __m64 b );
+//__m128i   ssp_mulhrs_epi16_SSE3     ( __m128i a, __m128i b );
+//__m64     ssp_mulhrs_pi16_SSE3      ( __m64 a, __m64 b );
+//__m128i   ssp_shuffle_epi8_SSE3     ( __m128i a, __m128i b );
+//__m64     ssp_shuffle_pi8_SSE3      ( __m64 a, __m64 b );
+//__m128i   ssp_sign_epi16_SSE3       ( __m128i a, __m128i b );
+//__m128i   ssp_sign_epi32_SSE3       ( __m128i a, __m128i b );
+//__m128i   ssp_sign_epi8_SSE3        ( __m128i a, __m128i b );
+//__m64     ssp_sign_pi16_SSE3        ( __m64 a, __m64 b );
+//__m64     ssp_sign_pi32_SSE3        ( __m64 a, __m64 b );
+//__m64     ssp_sign_pi8_SSE3         ( __m64 a, __m64 b );
+//
+////============================================
+//// SSE4A Emulation
+////============================================
+//__m128i   ssp_extract_si64_SSE3     ( __m128i,__m128i );   
+//__m128i   ssp_extracti_si64_SSE3    ( __m128i, int, int );   
+//__m128i   ssp_insert_si64_SSE3      ( __m128i,__m128i );   
+//__m128i   ssp_inserti_si64_SSE3     ( __m128i, __m128i, int, int );   
+//void      ssp_stream_sd_SSE3        ( double*,__m128d );   
+//void      ssp_stream_ss_SSE3        ( float*,__m128 );   
+//
+////============================================
+//// SSE4.1 Emulation
+////============================================
+//__m128i   ssp_blend_epi16_SSE3      ( __m128i v1, __m128i v2, const int mask );    
+//__m128d   ssp_blend_pd_SSE3         ( __m128d v1, __m128d v2, const int mask );    
+//__m128    ssp_blend_ps_SSE3         ( __m128  v1, __m128  v2, const int mask );    
+//__m128i   ssp_blendv_epi8_SSE3      ( __m128i v1, __m128i v2, __m128i   mask );    
+//__m128d   ssp_blendv_pd_SSE3        ( __m128d v1, __m128d v2, __m128d   mask );    
+//__m128    ssp_blendv_ps_SSE3        ( __m128  v1, __m128  v2, __m128    mask );    
+//__m128d   ssp_ceil_pd_SSE3          ( __m128d a );    
+//__m128    ssp_ceil_ps_SSE3          ( __m128  a );    
+//__m128d   ssp_ceil_sd_SSE3          ( __m128d a, __m128d b );    
+//__m128    ssp_ceil_ss_SSE3          ( __m128  a, __m128  b );    
+//__m128i   ssp_cmpeq_epi64_SSE3      ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_cvtepi16_epi32_SSE3   ( __m128i shortValues );    
+//__m128i   ssp_cvtepi16_epi64_SSE3   ( __m128i shortValues );    
+//__m128i   ssp_cvtepi32_epi64_SSE3   ( __m128i intValues   );    
+//__m128i   ssp_cvtepi8_epi16_SSE3    ( __m128i byteValues  );    
+//__m128i   ssp_cvtepi8_epi32_SSE3    ( __m128i byteValues  );    
+//__m128i   ssp_cvtepi8_epi64_SSE3    ( __m128i byteValues  );    
+//__m128i   ssp_cvtepu16_epi32_SSE3   ( __m128i shortValues );    
+//__m128i   ssp_cvtepu16_epi64_SSE3   ( __m128i shortValues );    
+//__m128i   ssp_cvtepu32_epi64_SSE3   ( __m128i intValues   );    
+//__m128i   ssp_cvtepu8_epi16_SSE3    ( __m128i byteValues  );    
+//__m128i   ssp_cvtepu8_epi32_SSE3    ( __m128i byteValues  );    
+//__m128i   ssp_cvtepu8_epi64_SSE3    ( __m128i shortValues );    
+//__m128d   ssp_dp_pd_SSE3            ( __m128d val1, __m128d val2, const int mask );    
+//__m128    ssp_dp_ps_SSE3            ( __m128  val1, __m128  val2, const int mask );    
+//int       ssp_extract_epi32_SSE3    ( __m128i src, const int ndx );    
+//__int64   ssp_extract_epi64_SSE3    ( __m128i src, const int ndx );    
+//int       ssp_extract_epi8_SSE3     ( __m128i src, const int ndx );    
+//int       ssp_extract_ps_SSE3       ( __m128  src, const int ndx );    
+//__m128d   ssp_floor_pd_SSE3         ( __m128d a );    
+//__m128    ssp_floor_ps_SSE3         ( __m128  a );    
+//__m128d   ssp_floor_sd_SSE3         ( __m128d a, __m128d b );    
+//__m128    ssp_floor_ss_SSE3         ( __m128  a, __m128  b );    
+//__m128i   ssp_insert_epi32_SSE3     ( __m128i dst,      int s, const int ndx );    
+//__m128i   ssp_insert_epi64_SSE3     ( __m128i dst, __int64  s, const int ndx );    
+//__m128i   ssp_insert_epi8_SSE3      ( __m128i dst,      int s, const int ndx );    
+//__m128    ssp_insert_ps_SSE3        ( __m128  dst, __m128 src, const int ndx );    
+//__m128i   ssp_max_epi32_SSE3        ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_max_epi8_SSE3         ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_max_epu16_SSE3        ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_max_epu32_SSE3        ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_min_epi32_SSE3        ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_min_epi8_SSE3         ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_min_epu16_SSE3        ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_min_epu32_SSE3        ( __m128i val1, __m128i val2 );    
+//__m128i   ssp_minpos_epu16_SSE3     ( __m128i shortValues );    
+//__m128i   ssp_mpsadbw_epu8_SSE3     ( __m128i s1,   __m128i s2,   const int msk  );    
+//__m128i   ssp_mul_epi32_SSE3        ( __m128i a,    __m128i b );    
+//__m128i   ssp_mullo_epi32_SSE3      ( __m128i a,    __m128i b );    
+//__m128i   ssp_packus_epi32_SSE3     ( __m128i val1, __m128i val2 );    
+//__m128d   ssp_round_pd_SSE3         ( __m128d val, int iRoundMode );    
+//__m128    ssp_round_ps_SSE3         ( __m128  val, int iRoundMode );    
+//__m128d   ssp_round_sd_SSE3         ( __m128d dst, __m128d val, int iRoundMode );    
+//__m128    ssp_round_ss_SSE3         ( __m128  dst, __m128  val, int iRoundMode );    
+//__m128i   ssp_stream_load_si128_SSE3( __m128i* v1 );    
+//int       ssp_testc_si128_SSE3      ( __m128i mask, __m128i val );    
+//int       ssp_testnzc_si128_SSE3    ( __m128i mask, __m128i s2  );    
+//int       ssp_testz_si128_SSE3      ( __m128i mask, __m128i val );  
+//
+////============================================
+//// SSE4.2 Emulation
+////============================================
+//int       ssp_cmpestra_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//int       ssp_cmpestrc_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//int       ssp_cmpestri_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//__m128i   ssp_cmpestrm_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//int       ssp_cmpestro_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//int       ssp_cmpestrs_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//int       ssp_cmpestrz_SSE3         ( __m128i a, int la, __m128i b, int lb, const int mode );
+//__m128i   ssp_cmpgt_epi64_SSE3      ( __m128i a, __m128i b );
+//int       ssp_cmpistra_SSE3         ( __m128i a, __m128i b, const int mode );
+//int       ssp_cmpistrc_SSE3         ( __m128i a, __m128i b, const int mode );
+//int       ssp_cmpistri_SSE3         ( __m128i a, __m128i b, const int mode );
+//__m128i   ssp_cmpistrm_SSE3         ( __m128i a, __m128i b, const int mode );
+//int       ssp_cmpistro_SSE3         ( __m128i a, __m128i b, const int mode );
+//int       ssp_cmpistrs_SSE3         ( __m128i a, __m128i b, const int mode );
+//int       ssp_cmpistrz_SSE3         ( __m128i a, __m128i b, const int mode );
+//unsigned int     ssp_crc32_u16_SSE3 ( unsigned int crc, unsigned short   v );
+//unsigned int     ssp_crc32_u32_SSE3 ( unsigned int crc, unsigned int     v );
+//unsigned __int64 ssp_crc32_u64_SSE3 ( unsigned int crc, unsigned __int64 v );
+//unsigned int     ssp_crc32_u8_SSE3  ( unsigned int crc, unsigned char    v );
+//int       ssp_popcnt_u32_SSE3       ( unsigned int a     );
+//int       ssp_popcnt_u64_SSE3       ( unsigned __int64 a );
+
+#include "native/SSEPlus_native_SSE3.h" 
+#include "emulation/SSEPlus_emulation_SSE3.h" 
+#include "arithmetic/SSEPlus_arithmetic_SSE3.h"
+
+#endif // __SSEPLUS_SSE3_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.1.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.1.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.1.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.1.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE4_1_H__
+#define __SSEPLUS_SSE4_1_H__
+
+#include "SSEPlus_base.h"
+#include "native/SSEPlus_native_SSE4.1.h"
+
+#endif // __SSEPLUS_SSE4_1_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4.2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE4_2_H__
+#define __SSEPLUS_SSE4_2_H__
+
+#include "SSEPlus_base.h"
+//#include "native/SSEPlus_native_SSE4.2.h"
+
+#endif // __SSEPLUS_SSE4_2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4a.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4a.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4a.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE4a.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE4A_H__
+#define __SSEPLUS_SSE4A_H__
+
+#include "SSEPlus_base.h"
+#include "native/SSEPlus_native_SSE4a.h"
+
+#endif // __SSEPLUS_SSE4A_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE5.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE5.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE5.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSE5.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSE5_H__
+#define __SSEPLUS_SSE5_H__
+
+#include "SSEPlus_base.h"
+#include "native/SSEPlus_native_SSE5.h"
+
+#endif // __SSEPLUS_SSE5_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_SSSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_SSSE3_H__
+#define __SSEPLUS_SSSE3_H__
+
+#include "SSEPlus_base.h"
+#include "native/SSEPlus_native_SSSE3.h"
+
+#endif // __SSEPLUS_SSSE3_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_base.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_base.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_base.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_base.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,664 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __BASE_H__
+#define __BASE_H__
+
+#define __SSEPLUS_LOGICAL_SSE2_H__ 1
+#define __SSEPLUS_MEMORY_SSE2_H__ 1
+#define __SSEPLUS_CONVERT_SSE2_H__ 1
+
+#include "SSEPlus_platform.h"
+
+#if 0
+#include <xmmintrin.h>  // SSE  (Required to use the __m128, and __m128d type)
+#include <emmintrin.h>  // SSE2 (Required to use the __m128i type)
+#else
+typedef struct {
+    float f[4];
+} __m128;
+
+typedef struct {
+    double d[2];
+} __m128d;
+
+#ifdef __LP64__
+typedef struct {
+    unsigned long i[2];
+} __m128i;
+#else
+typedef struct {
+    unsigned long long i[2];
+} __m128i;
+#endif
+
+typedef struct {
+    float m64[2];
+} __m64;
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_setzero_si128(void)
+{
+    return (__m128i){ 0LL, 0LL };
+}
+#endif
+
+//---------------------------------------
+// Type Definitions
+//---------------------------------------
+
+typedef   signed char      ssp_s8;
+typedef unsigned char      ssp_u8;
+
+typedef   signed short     ssp_s16;
+typedef unsigned short     ssp_u16;
+
+typedef   signed int       ssp_s32;
+typedef unsigned int       ssp_u32;
+
+typedef float              ssp_f32;
+typedef double             ssp_f64;
+
+typedef   signed long long ssp_s64;
+typedef unsigned long long ssp_u64;
+
+typedef union 
+{
+    __m128  f;
+    __m128d d;
+    __m128i i;
+    __uint128_t ui;
+    __m64	m64[ 2];
+    ssp_u64 u64[ 2];  
+    ssp_s64 s64[ 2];
+    ssp_f64 f64[ 2]; 
+    ssp_u32 u32[ 4];
+    ssp_s32 s32[ 4];    
+    ssp_f32 f32[ 4]; 
+    ssp_u16 u16[ 8];  
+    ssp_s16 s16[ 8];    
+    ssp_u8  u8 [16];
+    ssp_s8  s8 [16];    
+} ssp_m128;
+
+typedef union 
+{
+    __m64	m64;
+    ssp_u64 u64;  
+    ssp_s64 s64;
+    ssp_u32 u32[ 2];
+    ssp_s32 s32[ 2];    
+    ssp_u16 u16[ 4];  
+    ssp_s16 s16[ 4];    
+    ssp_u8  u8 [ 8];
+    ssp_s8  s8 [ 8];    
+} ssp_m64;
+
+ssp_u16 MAX_U16 = 65535;
+
+//---------------------------------------
+// Rounding mode macros
+//---------------------------------------
+#define SSP_FROUND_TO_NEAREST_INT    0x00
+#define SSP_FROUND_TO_NEG_INF        0x01
+#define SSP_FROUND_TO_POS_INF        0x02
+#define SSP_FROUND_TO_ZERO           0x03
+#define SSP_FROUND_CUR_DIRECTION     0x04
+
+#define SSP_FROUND_RAISE_EXC         0x00
+#define SSP_FROUND_NO_EXC            0x08
+
+//---------------------------------------
+// Floating point precision requirements
+//---------------------------------------
+const static float  SSP_F32_ALLOWANCE =  0.0001f;
+const static double SSP_F64_ALLOWANCE =  0.0001;
+
+
+//---------------------------------------
+// Warning macros
+//---------------------------------------
+#define STRING2(x) #x
+#define STRING(x) STRING2(x)
+#define WARN( ) __FILE__"("STRING(__LINE__)") : NOTE " 
+
+//#define _SSP_SHUFFLE(w,x,y,z) (((w)<<3) | ((x)<<2) | ((y)<<1) | (z))
+
+//---------------------------------------
+// Common Case Statements 
+//    for native intrinsics with an immediate input
+//---------------------------------------
+#define CASE_2( fn, ... )                   \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+default:  return fn( __VA_ARGS__, 0x1 );
+
+#define CASE_4( fn, ... )                   \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+default: return fn( __VA_ARGS__, 0x3 );
+
+#define CASE_4_NR( fn, ... )         \
+case 0x0: fn( __VA_ARGS__, 0x0 );    \
+case 0x1: fn( __VA_ARGS__, 0x1 );    \
+case 0x2: fn( __VA_ARGS__, 0x2 );    \
+default: fn( __VA_ARGS__, 0x3 );
+
+#define CASE_8( fn, ... )                   \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+case 0x3: return fn( __VA_ARGS__, 0x3 );    \
+case 0x4: return fn( __VA_ARGS__, 0x4 );    \
+case 0x5: return fn( __VA_ARGS__, 0x5 );    \
+case 0x6: return fn( __VA_ARGS__, 0x6 );    \
+default: return fn( __VA_ARGS__, 0x7 );
+
+#define CASE_16( fn, ... )                  \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+case 0x3: return fn( __VA_ARGS__, 0x3 );    \
+case 0x4: return fn( __VA_ARGS__, 0x4 );    \
+case 0x5: return fn( __VA_ARGS__, 0x5 );    \
+case 0x6: return fn( __VA_ARGS__, 0x6 );    \
+case 0x7: return fn( __VA_ARGS__, 0x7 );    \
+case 0x8: return fn( __VA_ARGS__, 0x8 );    \
+case 0x9: return fn( __VA_ARGS__, 0x9 );    \
+case 0xA: return fn( __VA_ARGS__, 0xA );    \
+case 0xB: return fn( __VA_ARGS__, 0xB );    \
+case 0xC: return fn( __VA_ARGS__, 0xC );    \
+case 0xD: return fn( __VA_ARGS__, 0xD );    \
+case 0xE: return fn( __VA_ARGS__, 0xE );    \
+default: return fn( __VA_ARGS__, 0xF );
+
+#define CASE_32( fn, ... )                  \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+case 0x3: return fn( __VA_ARGS__, 0x3 );    \
+case 0x4: return fn( __VA_ARGS__, 0x4 );    \
+case 0x5: return fn( __VA_ARGS__, 0x5 );    \
+case 0x6: return fn( __VA_ARGS__, 0x6 );    \
+case 0x7: return fn( __VA_ARGS__, 0x7 );    \
+case 0x8: return fn( __VA_ARGS__, 0x8 );    \
+case 0x9: return fn( __VA_ARGS__, 0x9 );    \
+case 0xA: return fn( __VA_ARGS__, 0xA );    \
+case 0xB: return fn( __VA_ARGS__, 0xB );    \
+case 0xC: return fn( __VA_ARGS__, 0xC );    \
+case 0xD: return fn( __VA_ARGS__, 0xD );    \
+case 0xE: return fn( __VA_ARGS__, 0xE );    \
+case 0xF: return fn( __VA_ARGS__, 0xF );    \
+case 0x10: return fn( __VA_ARGS__, 0x10 );    \
+case 0x11: return fn( __VA_ARGS__, 0x11 );    \
+case 0x12: return fn( __VA_ARGS__, 0x12 );    \
+case 0x13: return fn( __VA_ARGS__, 0x13 );    \
+case 0x14: return fn( __VA_ARGS__, 0x14 );    \
+case 0x15: return fn( __VA_ARGS__, 0x15 );    \
+case 0x16: return fn( __VA_ARGS__, 0x16 );    \
+case 0x17: return fn( __VA_ARGS__, 0x17 );    \
+case 0x18: return fn( __VA_ARGS__, 0x18 );    \
+case 0x19: return fn( __VA_ARGS__, 0x19 );    \
+case 0x1A: return fn( __VA_ARGS__, 0x1A );    \
+case 0x1B: return fn( __VA_ARGS__, 0x1B );    \
+case 0x1C: return fn( __VA_ARGS__, 0x1C );    \
+case 0x1D: return fn( __VA_ARGS__, 0x1D );    \
+case 0x1E: return fn( __VA_ARGS__, 0x1E );    \
+default:   return fn( __VA_ARGS__, 0x1F );    
+
+#define CASE_128( fn, ... )                 \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+case 0x3: return fn( __VA_ARGS__, 0x3 );    \
+case 0x4: return fn( __VA_ARGS__, 0x4 );    \
+case 0x5: return fn( __VA_ARGS__, 0x5 );    \
+case 0x6: return fn( __VA_ARGS__, 0x6 );    \
+case 0x7: return fn( __VA_ARGS__, 0x7 );    \
+case 0x8: return fn( __VA_ARGS__, 0x8 );    \
+case 0x9: return fn( __VA_ARGS__, 0x9 );    \
+case 0xA: return fn( __VA_ARGS__, 0xA );    \
+case 0xB: return fn( __VA_ARGS__, 0xB );    \
+case 0xC: return fn( __VA_ARGS__, 0xC );    \
+case 0xD: return fn( __VA_ARGS__, 0xD );    \
+case 0xE: return fn( __VA_ARGS__, 0xE );    \
+case 0xF: return fn( __VA_ARGS__, 0xF );    \
+case 0x10: return fn( __VA_ARGS__, 0x10 );    \
+case 0x11: return fn( __VA_ARGS__, 0x11 );    \
+case 0x12: return fn( __VA_ARGS__, 0x12 );    \
+case 0x13: return fn( __VA_ARGS__, 0x13 );    \
+case 0x14: return fn( __VA_ARGS__, 0x14 );    \
+case 0x15: return fn( __VA_ARGS__, 0x15 );    \
+case 0x16: return fn( __VA_ARGS__, 0x16 );    \
+case 0x17: return fn( __VA_ARGS__, 0x17 );    \
+case 0x18: return fn( __VA_ARGS__, 0x18 );    \
+case 0x19: return fn( __VA_ARGS__, 0x19 );    \
+case 0x1A: return fn( __VA_ARGS__, 0x1A );    \
+case 0x1B: return fn( __VA_ARGS__, 0x1B );    \
+case 0x1C: return fn( __VA_ARGS__, 0x1C );    \
+case 0x1D: return fn( __VA_ARGS__, 0x1D );    \
+case 0x1E: return fn( __VA_ARGS__, 0x1E );    \
+case 0x1F: return fn( __VA_ARGS__, 0x1F );    \
+case 0x20: return fn( __VA_ARGS__, 0x20 );    \
+case 0x21: return fn( __VA_ARGS__, 0x21 );    \
+case 0x22: return fn( __VA_ARGS__, 0x22 );    \
+case 0x23: return fn( __VA_ARGS__, 0x23 );    \
+case 0x24: return fn( __VA_ARGS__, 0x24 );    \
+case 0x25: return fn( __VA_ARGS__, 0x25 );    \
+case 0x26: return fn( __VA_ARGS__, 0x26 );    \
+case 0x27: return fn( __VA_ARGS__, 0x27 );    \
+case 0x28: return fn( __VA_ARGS__, 0x28 );    \
+case 0x29: return fn( __VA_ARGS__, 0x29 );    \
+case 0x2A: return fn( __VA_ARGS__, 0x2A );    \
+case 0x2B: return fn( __VA_ARGS__, 0x2B );    \
+case 0x2C: return fn( __VA_ARGS__, 0x2C );    \
+case 0x2D: return fn( __VA_ARGS__, 0x2D );    \
+case 0x2E: return fn( __VA_ARGS__, 0x2E );    \
+case 0x2F: return fn( __VA_ARGS__, 0x2F );    \
+case 0x30: return fn( __VA_ARGS__, 0x30 );    \
+case 0x31: return fn( __VA_ARGS__, 0x31 );    \
+case 0x32: return fn( __VA_ARGS__, 0x32 );    \
+case 0x33: return fn( __VA_ARGS__, 0x33 );    \
+case 0x34: return fn( __VA_ARGS__, 0x34 );    \
+case 0x35: return fn( __VA_ARGS__, 0x35 );    \
+case 0x36: return fn( __VA_ARGS__, 0x36 );    \
+case 0x37: return fn( __VA_ARGS__, 0x37 );    \
+case 0x38: return fn( __VA_ARGS__, 0x38 );    \
+case 0x39: return fn( __VA_ARGS__, 0x39 );    \
+case 0x3A: return fn( __VA_ARGS__, 0x3A );    \
+case 0x3B: return fn( __VA_ARGS__, 0x3B );    \
+case 0x3C: return fn( __VA_ARGS__, 0x3C );    \
+case 0x3D: return fn( __VA_ARGS__, 0x3D );    \
+case 0x3E: return fn( __VA_ARGS__, 0x3E );    \
+case 0x3F: return fn( __VA_ARGS__, 0x3F );    \
+case 0x40: return fn( __VA_ARGS__, 0x40 );    \
+case 0x41: return fn( __VA_ARGS__, 0x41 );    \
+case 0x42: return fn( __VA_ARGS__, 0x42 );    \
+case 0x43: return fn( __VA_ARGS__, 0x43 );    \
+case 0x44: return fn( __VA_ARGS__, 0x44 );    \
+case 0x45: return fn( __VA_ARGS__, 0x45 );    \
+case 0x46: return fn( __VA_ARGS__, 0x46 );    \
+case 0x47: return fn( __VA_ARGS__, 0x47 );    \
+case 0x48: return fn( __VA_ARGS__, 0x48 );    \
+case 0x49: return fn( __VA_ARGS__, 0x49 );    \
+case 0x4A: return fn( __VA_ARGS__, 0x4A );    \
+case 0x4B: return fn( __VA_ARGS__, 0x4B );    \
+case 0x4C: return fn( __VA_ARGS__, 0x4C );    \
+case 0x4D: return fn( __VA_ARGS__, 0x4D );    \
+case 0x4E: return fn( __VA_ARGS__, 0x4E );    \
+case 0x4F: return fn( __VA_ARGS__, 0x4F );    \
+case 0x50: return fn( __VA_ARGS__, 0x50 );    \
+case 0x51: return fn( __VA_ARGS__, 0x51 );    \
+case 0x52: return fn( __VA_ARGS__, 0x52 );    \
+case 0x53: return fn( __VA_ARGS__, 0x53 );    \
+case 0x54: return fn( __VA_ARGS__, 0x54 );    \
+case 0x55: return fn( __VA_ARGS__, 0x55 );    \
+case 0x56: return fn( __VA_ARGS__, 0x56 );    \
+case 0x57: return fn( __VA_ARGS__, 0x57 );    \
+case 0x58: return fn( __VA_ARGS__, 0x58 );    \
+case 0x59: return fn( __VA_ARGS__, 0x59 );    \
+case 0x5A: return fn( __VA_ARGS__, 0x5A );    \
+case 0x5B: return fn( __VA_ARGS__, 0x5B );    \
+case 0x5C: return fn( __VA_ARGS__, 0x5C );    \
+case 0x5D: return fn( __VA_ARGS__, 0x5D );    \
+case 0x5E: return fn( __VA_ARGS__, 0x5E );    \
+case 0x5F: return fn( __VA_ARGS__, 0x5F );    \
+case 0x60: return fn( __VA_ARGS__, 0x60 );    \
+case 0x61: return fn( __VA_ARGS__, 0x61 );    \
+case 0x62: return fn( __VA_ARGS__, 0x62 );    \
+case 0x63: return fn( __VA_ARGS__, 0x63 );    \
+case 0x64: return fn( __VA_ARGS__, 0x64 );    \
+case 0x65: return fn( __VA_ARGS__, 0x65 );    \
+case 0x66: return fn( __VA_ARGS__, 0x66 );    \
+case 0x67: return fn( __VA_ARGS__, 0x67 );    \
+case 0x68: return fn( __VA_ARGS__, 0x68 );    \
+case 0x69: return fn( __VA_ARGS__, 0x69 );    \
+case 0x6A: return fn( __VA_ARGS__, 0x6A );    \
+case 0x6B: return fn( __VA_ARGS__, 0x6B );    \
+case 0x6C: return fn( __VA_ARGS__, 0x6C );    \
+case 0x6D: return fn( __VA_ARGS__, 0x6D );    \
+case 0x6E: return fn( __VA_ARGS__, 0x6E );    \
+case 0x6F: return fn( __VA_ARGS__, 0x6F );    \
+case 0x70: return fn( __VA_ARGS__, 0x70 );    \
+case 0x71: return fn( __VA_ARGS__, 0x71 );    \
+case 0x72: return fn( __VA_ARGS__, 0x72 );    \
+case 0x73: return fn( __VA_ARGS__, 0x73 );    \
+case 0x74: return fn( __VA_ARGS__, 0x74 );    \
+case 0x75: return fn( __VA_ARGS__, 0x75 );    \
+case 0x76: return fn( __VA_ARGS__, 0x76 );    \
+case 0x77: return fn( __VA_ARGS__, 0x77 );    \
+case 0x78: return fn( __VA_ARGS__, 0x78 );    \
+case 0x79: return fn( __VA_ARGS__, 0x79 );    \
+case 0x7A: return fn( __VA_ARGS__, 0x7A );    \
+case 0x7B: return fn( __VA_ARGS__, 0x7B );    \
+case 0x7C: return fn( __VA_ARGS__, 0x7C );    \
+case 0x7D: return fn( __VA_ARGS__, 0x7D );    \
+case 0x7E: return fn( __VA_ARGS__, 0x7E );    \
+default: return fn( __VA_ARGS__, 0x7F );
+
+#define CASE_64( fn, ... )					\
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+case 0x3: return fn( __VA_ARGS__, 0x3 );    \
+case 0x4: return fn( __VA_ARGS__, 0x4 );    \
+case 0x5: return fn( __VA_ARGS__, 0x5 );    \
+case 0x6: return fn( __VA_ARGS__, 0x6 );    \
+case 0x7: return fn( __VA_ARGS__, 0x7 );    \
+case 0x8: return fn( __VA_ARGS__, 0x8 );    \
+case 0x9: return fn( __VA_ARGS__, 0x9 );    \
+case 0xA: return fn( __VA_ARGS__, 0xA );    \
+case 0xB: return fn( __VA_ARGS__, 0xB );    \
+case 0xC: return fn( __VA_ARGS__, 0xC );    \
+case 0xD: return fn( __VA_ARGS__, 0xD );    \
+case 0xE: return fn( __VA_ARGS__, 0xE );    \
+case 0xF: return fn( __VA_ARGS__, 0xF );    \
+case 0x10: return fn( __VA_ARGS__, 0x10 );    \
+case 0x11: return fn( __VA_ARGS__, 0x11 );    \
+case 0x12: return fn( __VA_ARGS__, 0x12 );    \
+case 0x13: return fn( __VA_ARGS__, 0x13 );    \
+case 0x14: return fn( __VA_ARGS__, 0x14 );    \
+case 0x15: return fn( __VA_ARGS__, 0x15 );    \
+case 0x16: return fn( __VA_ARGS__, 0x16 );    \
+case 0x17: return fn( __VA_ARGS__, 0x17 );    \
+case 0x18: return fn( __VA_ARGS__, 0x18 );    \
+case 0x19: return fn( __VA_ARGS__, 0x19 );    \
+case 0x1A: return fn( __VA_ARGS__, 0x1A );    \
+case 0x1B: return fn( __VA_ARGS__, 0x1B );    \
+case 0x1C: return fn( __VA_ARGS__, 0x1C );    \
+case 0x1D: return fn( __VA_ARGS__, 0x1D );    \
+case 0x1E: return fn( __VA_ARGS__, 0x1E );    \
+case 0x1F: return fn( __VA_ARGS__, 0x1F );    \
+case 0x20: return fn( __VA_ARGS__, 0x20 );    \
+case 0x21: return fn( __VA_ARGS__, 0x21 );    \
+case 0x22: return fn( __VA_ARGS__, 0x22 );    \
+case 0x23: return fn( __VA_ARGS__, 0x23 );    \
+case 0x24: return fn( __VA_ARGS__, 0x24 );    \
+case 0x25: return fn( __VA_ARGS__, 0x25 );    \
+case 0x26: return fn( __VA_ARGS__, 0x26 );    \
+case 0x27: return fn( __VA_ARGS__, 0x27 );    \
+case 0x28: return fn( __VA_ARGS__, 0x28 );    \
+case 0x29: return fn( __VA_ARGS__, 0x29 );    \
+case 0x2A: return fn( __VA_ARGS__, 0x2A );    \
+case 0x2B: return fn( __VA_ARGS__, 0x2B );    \
+case 0x2C: return fn( __VA_ARGS__, 0x2C );    \
+case 0x2D: return fn( __VA_ARGS__, 0x2D );    \
+case 0x2E: return fn( __VA_ARGS__, 0x2E );    \
+case 0x2F: return fn( __VA_ARGS__, 0x2F );    \
+case 0x30: return fn( __VA_ARGS__, 0x30 );    \
+case 0x31: return fn( __VA_ARGS__, 0x31 );    \
+case 0x32: return fn( __VA_ARGS__, 0x32 );    \
+case 0x33: return fn( __VA_ARGS__, 0x33 );    \
+case 0x34: return fn( __VA_ARGS__, 0x34 );    \
+case 0x35: return fn( __VA_ARGS__, 0x35 );    \
+case 0x36: return fn( __VA_ARGS__, 0x36 );    \
+case 0x37: return fn( __VA_ARGS__, 0x37 );    \
+case 0x38: return fn( __VA_ARGS__, 0x38 );    \
+case 0x39: return fn( __VA_ARGS__, 0x39 );    \
+case 0x3A: return fn( __VA_ARGS__, 0x3A );    \
+case 0x3B: return fn( __VA_ARGS__, 0x3B );    \
+case 0x3C: return fn( __VA_ARGS__, 0x3C );    \
+case 0x3D: return fn( __VA_ARGS__, 0x3D );    \
+case 0x3E: return fn( __VA_ARGS__, 0x3E );    \
+default: return fn( __VA_ARGS__, 0x3F );
+
+#define CASE_256( fn, ... )                 \
+case 0x0: return fn( __VA_ARGS__, 0x0 );    \
+case 0x1: return fn( __VA_ARGS__, 0x1 );    \
+case 0x2: return fn( __VA_ARGS__, 0x2 );    \
+case 0x3: return fn( __VA_ARGS__, 0x3 );    \
+case 0x4: return fn( __VA_ARGS__, 0x4 );    \
+case 0x5: return fn( __VA_ARGS__, 0x5 );    \
+case 0x6: return fn( __VA_ARGS__, 0x6 );    \
+case 0x7: return fn( __VA_ARGS__, 0x7 );    \
+case 0x8: return fn( __VA_ARGS__, 0x8 );    \
+case 0x9: return fn( __VA_ARGS__, 0x9 );    \
+case 0xA: return fn( __VA_ARGS__, 0xA );    \
+case 0xB: return fn( __VA_ARGS__, 0xB );    \
+case 0xC: return fn( __VA_ARGS__, 0xC );    \
+case 0xD: return fn( __VA_ARGS__, 0xD );    \
+case 0xE: return fn( __VA_ARGS__, 0xE );    \
+case 0xF: return fn( __VA_ARGS__, 0xF );    \
+case 0x10: return fn( __VA_ARGS__, 0x10 );    \
+case 0x11: return fn( __VA_ARGS__, 0x11 );    \
+case 0x12: return fn( __VA_ARGS__, 0x12 );    \
+case 0x13: return fn( __VA_ARGS__, 0x13 );    \
+case 0x14: return fn( __VA_ARGS__, 0x14 );    \
+case 0x15: return fn( __VA_ARGS__, 0x15 );    \
+case 0x16: return fn( __VA_ARGS__, 0x16 );    \
+case 0x17: return fn( __VA_ARGS__, 0x17 );    \
+case 0x18: return fn( __VA_ARGS__, 0x18 );    \
+case 0x19: return fn( __VA_ARGS__, 0x19 );    \
+case 0x1A: return fn( __VA_ARGS__, 0x1A );    \
+case 0x1B: return fn( __VA_ARGS__, 0x1B );    \
+case 0x1C: return fn( __VA_ARGS__, 0x1C );    \
+case 0x1D: return fn( __VA_ARGS__, 0x1D );    \
+case 0x1E: return fn( __VA_ARGS__, 0x1E );    \
+case 0x1F: return fn( __VA_ARGS__, 0x1F );    \
+case 0x20: return fn( __VA_ARGS__, 0x20 );    \
+case 0x21: return fn( __VA_ARGS__, 0x21 );    \
+case 0x22: return fn( __VA_ARGS__, 0x22 );    \
+case 0x23: return fn( __VA_ARGS__, 0x23 );    \
+case 0x24: return fn( __VA_ARGS__, 0x24 );    \
+case 0x25: return fn( __VA_ARGS__, 0x25 );    \
+case 0x26: return fn( __VA_ARGS__, 0x26 );    \
+case 0x27: return fn( __VA_ARGS__, 0x27 );    \
+case 0x28: return fn( __VA_ARGS__, 0x28 );    \
+case 0x29: return fn( __VA_ARGS__, 0x29 );    \
+case 0x2A: return fn( __VA_ARGS__, 0x2A );    \
+case 0x2B: return fn( __VA_ARGS__, 0x2B );    \
+case 0x2C: return fn( __VA_ARGS__, 0x2C );    \
+case 0x2D: return fn( __VA_ARGS__, 0x2D );    \
+case 0x2E: return fn( __VA_ARGS__, 0x2E );    \
+case 0x2F: return fn( __VA_ARGS__, 0x2F );    \
+case 0x30: return fn( __VA_ARGS__, 0x30 );    \
+case 0x31: return fn( __VA_ARGS__, 0x31 );    \
+case 0x32: return fn( __VA_ARGS__, 0x32 );    \
+case 0x33: return fn( __VA_ARGS__, 0x33 );    \
+case 0x34: return fn( __VA_ARGS__, 0x34 );    \
+case 0x35: return fn( __VA_ARGS__, 0x35 );    \
+case 0x36: return fn( __VA_ARGS__, 0x36 );    \
+case 0x37: return fn( __VA_ARGS__, 0x37 );    \
+case 0x38: return fn( __VA_ARGS__, 0x38 );    \
+case 0x39: return fn( __VA_ARGS__, 0x39 );    \
+case 0x3A: return fn( __VA_ARGS__, 0x3A );    \
+case 0x3B: return fn( __VA_ARGS__, 0x3B );    \
+case 0x3C: return fn( __VA_ARGS__, 0x3C );    \
+case 0x3D: return fn( __VA_ARGS__, 0x3D );    \
+case 0x3E: return fn( __VA_ARGS__, 0x3E );    \
+case 0x3F: return fn( __VA_ARGS__, 0x3F );    \
+case 0x40: return fn( __VA_ARGS__, 0x40 );    \
+case 0x41: return fn( __VA_ARGS__, 0x41 );    \
+case 0x42: return fn( __VA_ARGS__, 0x42 );    \
+case 0x43: return fn( __VA_ARGS__, 0x43 );    \
+case 0x44: return fn( __VA_ARGS__, 0x44 );    \
+case 0x45: return fn( __VA_ARGS__, 0x45 );    \
+case 0x46: return fn( __VA_ARGS__, 0x46 );    \
+case 0x47: return fn( __VA_ARGS__, 0x47 );    \
+case 0x48: return fn( __VA_ARGS__, 0x48 );    \
+case 0x49: return fn( __VA_ARGS__, 0x49 );    \
+case 0x4A: return fn( __VA_ARGS__, 0x4A );    \
+case 0x4B: return fn( __VA_ARGS__, 0x4B );    \
+case 0x4C: return fn( __VA_ARGS__, 0x4C );    \
+case 0x4D: return fn( __VA_ARGS__, 0x4D );    \
+case 0x4E: return fn( __VA_ARGS__, 0x4E );    \
+case 0x4F: return fn( __VA_ARGS__, 0x4F );    \
+case 0x50: return fn( __VA_ARGS__, 0x50 );    \
+case 0x51: return fn( __VA_ARGS__, 0x51 );    \
+case 0x52: return fn( __VA_ARGS__, 0x52 );    \
+case 0x53: return fn( __VA_ARGS__, 0x53 );    \
+case 0x54: return fn( __VA_ARGS__, 0x54 );    \
+case 0x55: return fn( __VA_ARGS__, 0x55 );    \
+case 0x56: return fn( __VA_ARGS__, 0x56 );    \
+case 0x57: return fn( __VA_ARGS__, 0x57 );    \
+case 0x58: return fn( __VA_ARGS__, 0x58 );    \
+case 0x59: return fn( __VA_ARGS__, 0x59 );    \
+case 0x5A: return fn( __VA_ARGS__, 0x5A );    \
+case 0x5B: return fn( __VA_ARGS__, 0x5B );    \
+case 0x5C: return fn( __VA_ARGS__, 0x5C );    \
+case 0x5D: return fn( __VA_ARGS__, 0x5D );    \
+case 0x5E: return fn( __VA_ARGS__, 0x5E );    \
+case 0x5F: return fn( __VA_ARGS__, 0x5F );    \
+case 0x60: return fn( __VA_ARGS__, 0x60 );    \
+case 0x61: return fn( __VA_ARGS__, 0x61 );    \
+case 0x62: return fn( __VA_ARGS__, 0x62 );    \
+case 0x63: return fn( __VA_ARGS__, 0x63 );    \
+case 0x64: return fn( __VA_ARGS__, 0x64 );    \
+case 0x65: return fn( __VA_ARGS__, 0x65 );    \
+case 0x66: return fn( __VA_ARGS__, 0x66 );    \
+case 0x67: return fn( __VA_ARGS__, 0x67 );    \
+case 0x68: return fn( __VA_ARGS__, 0x68 );    \
+case 0x69: return fn( __VA_ARGS__, 0x69 );    \
+case 0x6A: return fn( __VA_ARGS__, 0x6A );    \
+case 0x6B: return fn( __VA_ARGS__, 0x6B );    \
+case 0x6C: return fn( __VA_ARGS__, 0x6C );    \
+case 0x6D: return fn( __VA_ARGS__, 0x6D );    \
+case 0x6E: return fn( __VA_ARGS__, 0x6E );    \
+case 0x6F: return fn( __VA_ARGS__, 0x6F );    \
+case 0x70: return fn( __VA_ARGS__, 0x70 );    \
+case 0x71: return fn( __VA_ARGS__, 0x71 );    \
+case 0x72: return fn( __VA_ARGS__, 0x72 );    \
+case 0x73: return fn( __VA_ARGS__, 0x73 );    \
+case 0x74: return fn( __VA_ARGS__, 0x74 );    \
+case 0x75: return fn( __VA_ARGS__, 0x75 );    \
+case 0x76: return fn( __VA_ARGS__, 0x76 );    \
+case 0x77: return fn( __VA_ARGS__, 0x77 );    \
+case 0x78: return fn( __VA_ARGS__, 0x78 );    \
+case 0x79: return fn( __VA_ARGS__, 0x79 );    \
+case 0x7A: return fn( __VA_ARGS__, 0x7A );    \
+case 0x7B: return fn( __VA_ARGS__, 0x7B );    \
+case 0x7C: return fn( __VA_ARGS__, 0x7C );    \
+case 0x7D: return fn( __VA_ARGS__, 0x7D );    \
+case 0x7E: return fn( __VA_ARGS__, 0x7E );    \
+case 0x7F: return fn( __VA_ARGS__, 0x7F );    \
+case 0x80: return fn( __VA_ARGS__, 0x80 );    \
+case 0x81: return fn( __VA_ARGS__, 0x81 );    \
+case 0x82: return fn( __VA_ARGS__, 0x82 );    \
+case 0x83: return fn( __VA_ARGS__, 0x83 );    \
+case 0x84: return fn( __VA_ARGS__, 0x84 );    \
+case 0x85: return fn( __VA_ARGS__, 0x85 );    \
+case 0x86: return fn( __VA_ARGS__, 0x86 );    \
+case 0x87: return fn( __VA_ARGS__, 0x87 );    \
+case 0x88: return fn( __VA_ARGS__, 0x88 );    \
+case 0x89: return fn( __VA_ARGS__, 0x89 );    \
+case 0x8A: return fn( __VA_ARGS__, 0x8A );    \
+case 0x8B: return fn( __VA_ARGS__, 0x8B );    \
+case 0x8C: return fn( __VA_ARGS__, 0x8C );    \
+case 0x8D: return fn( __VA_ARGS__, 0x8D );    \
+case 0x8E: return fn( __VA_ARGS__, 0x8E );    \
+case 0x8F: return fn( __VA_ARGS__, 0x8F );    \
+case 0x90: return fn( __VA_ARGS__, 0x90 );    \
+case 0x91: return fn( __VA_ARGS__, 0x91 );    \
+case 0x92: return fn( __VA_ARGS__, 0x92 );    \
+case 0x93: return fn( __VA_ARGS__, 0x93 );    \
+case 0x94: return fn( __VA_ARGS__, 0x94 );    \
+case 0x95: return fn( __VA_ARGS__, 0x95 );    \
+case 0x96: return fn( __VA_ARGS__, 0x96 );    \
+case 0x97: return fn( __VA_ARGS__, 0x97 );    \
+case 0x98: return fn( __VA_ARGS__, 0x98 );    \
+case 0x99: return fn( __VA_ARGS__, 0x99 );    \
+case 0x9A: return fn( __VA_ARGS__, 0x9A );    \
+case 0x9B: return fn( __VA_ARGS__, 0x9B );    \
+case 0x9C: return fn( __VA_ARGS__, 0x9C );    \
+case 0x9D: return fn( __VA_ARGS__, 0x9D );    \
+case 0x9E: return fn( __VA_ARGS__, 0x9E );    \
+case 0x9F: return fn( __VA_ARGS__, 0x9F );    \
+case 0xA0: return fn( __VA_ARGS__, 0xA0 );    \
+case 0xA1: return fn( __VA_ARGS__, 0xA1 );    \
+case 0xA2: return fn( __VA_ARGS__, 0xA2 );    \
+case 0xA3: return fn( __VA_ARGS__, 0xA3 );    \
+case 0xA4: return fn( __VA_ARGS__, 0xA4 );    \
+case 0xA5: return fn( __VA_ARGS__, 0xA5 );    \
+case 0xA6: return fn( __VA_ARGS__, 0xA6 );    \
+case 0xA7: return fn( __VA_ARGS__, 0xA7 );    \
+case 0xA8: return fn( __VA_ARGS__, 0xA8 );    \
+case 0xA9: return fn( __VA_ARGS__, 0xA9 );    \
+case 0xAA: return fn( __VA_ARGS__, 0xAA );    \
+case 0xAB: return fn( __VA_ARGS__, 0xAB );    \
+case 0xAC: return fn( __VA_ARGS__, 0xAC );    \
+case 0xAD: return fn( __VA_ARGS__, 0xAD );    \
+case 0xAE: return fn( __VA_ARGS__, 0xAE );    \
+case 0xAF: return fn( __VA_ARGS__, 0xAF );    \
+case 0xB0: return fn( __VA_ARGS__, 0xB0 );    \
+case 0xB1: return fn( __VA_ARGS__, 0xB1 );    \
+case 0xB2: return fn( __VA_ARGS__, 0xB2 );    \
+case 0xB3: return fn( __VA_ARGS__, 0xB3 );    \
+case 0xB4: return fn( __VA_ARGS__, 0xB4 );    \
+case 0xB5: return fn( __VA_ARGS__, 0xB5 );    \
+case 0xB6: return fn( __VA_ARGS__, 0xB6 );    \
+case 0xB7: return fn( __VA_ARGS__, 0xB7 );    \
+case 0xB8: return fn( __VA_ARGS__, 0xB8 );    \
+case 0xB9: return fn( __VA_ARGS__, 0xB9 );    \
+case 0xBA: return fn( __VA_ARGS__, 0xBA );    \
+case 0xBB: return fn( __VA_ARGS__, 0xBB );    \
+case 0xBC: return fn( __VA_ARGS__, 0xBC );    \
+case 0xBD: return fn( __VA_ARGS__, 0xBD );    \
+case 0xBE: return fn( __VA_ARGS__, 0xBE );    \
+case 0xBF: return fn( __VA_ARGS__, 0xBF );    \
+case 0xC0: return fn( __VA_ARGS__, 0xC0 );    \
+case 0xC1: return fn( __VA_ARGS__, 0xC1 );    \
+case 0xC2: return fn( __VA_ARGS__, 0xC2 );    \
+case 0xC3: return fn( __VA_ARGS__, 0xC3 );    \
+case 0xC4: return fn( __VA_ARGS__, 0xC4 );    \
+case 0xC5: return fn( __VA_ARGS__, 0xC5 );    \
+case 0xC6: return fn( __VA_ARGS__, 0xC6 );    \
+case 0xC7: return fn( __VA_ARGS__, 0xC7 );    \
+case 0xC8: return fn( __VA_ARGS__, 0xC8 );    \
+case 0xC9: return fn( __VA_ARGS__, 0xC9 );    \
+case 0xCA: return fn( __VA_ARGS__, 0xCA );    \
+case 0xCB: return fn( __VA_ARGS__, 0xCB );    \
+case 0xCC: return fn( __VA_ARGS__, 0xCC );    \
+case 0xCD: return fn( __VA_ARGS__, 0xCD );    \
+case 0xCE: return fn( __VA_ARGS__, 0xCE );    \
+case 0xCF: return fn( __VA_ARGS__, 0xCF );    \
+case 0xD0: return fn( __VA_ARGS__, 0xD0 );    \
+case 0xD1: return fn( __VA_ARGS__, 0xD1 );    \
+case 0xD2: return fn( __VA_ARGS__, 0xD2 );    \
+case 0xD3: return fn( __VA_ARGS__, 0xD3 );    \
+case 0xD4: return fn( __VA_ARGS__, 0xD4 );    \
+case 0xD5: return fn( __VA_ARGS__, 0xD5 );    \
+case 0xD6: return fn( __VA_ARGS__, 0xD6 );    \
+case 0xD7: return fn( __VA_ARGS__, 0xD7 );    \
+case 0xD8: return fn( __VA_ARGS__, 0xD8 );    \
+case 0xD9: return fn( __VA_ARGS__, 0xD9 );    \
+case 0xDA: return fn( __VA_ARGS__, 0xDA );    \
+case 0xDB: return fn( __VA_ARGS__, 0xDB );    \
+case 0xDC: return fn( __VA_ARGS__, 0xDC );    \
+case 0xDD: return fn( __VA_ARGS__, 0xDD );    \
+case 0xDE: return fn( __VA_ARGS__, 0xDE );    \
+case 0xDF: return fn( __VA_ARGS__, 0xDF );    \
+case 0xE0: return fn( __VA_ARGS__, 0xE0 );    \
+case 0xE1: return fn( __VA_ARGS__, 0xE1 );    \
+case 0xE2: return fn( __VA_ARGS__, 0xE2 );    \
+case 0xE3: return fn( __VA_ARGS__, 0xE3 );    \
+case 0xE4: return fn( __VA_ARGS__, 0xE4 );    \
+case 0xE5: return fn( __VA_ARGS__, 0xE5 );    \
+case 0xE6: return fn( __VA_ARGS__, 0xE6 );    \
+case 0xE7: return fn( __VA_ARGS__, 0xE7 );    \
+case 0xE8: return fn( __VA_ARGS__, 0xE8 );    \
+case 0xE9: return fn( __VA_ARGS__, 0xE9 );    \
+case 0xEA: return fn( __VA_ARGS__, 0xEA );    \
+case 0xEB: return fn( __VA_ARGS__, 0xEB );    \
+case 0xEC: return fn( __VA_ARGS__, 0xEC );    \
+case 0xED: return fn( __VA_ARGS__, 0xED );    \
+case 0xEE: return fn( __VA_ARGS__, 0xEE );    \
+case 0xEF: return fn( __VA_ARGS__, 0xEF );    \
+case 0xF0: return fn( __VA_ARGS__, 0xF0 );    \
+case 0xF1: return fn( __VA_ARGS__, 0xF1 );    \
+case 0xF2: return fn( __VA_ARGS__, 0xF2 );    \
+case 0xF3: return fn( __VA_ARGS__, 0xF3 );    \
+case 0xF4: return fn( __VA_ARGS__, 0xF4 );    \
+case 0xF5: return fn( __VA_ARGS__, 0xF5 );    \
+case 0xF6: return fn( __VA_ARGS__, 0xF6 );    \
+case 0xF7: return fn( __VA_ARGS__, 0xF7 );    \
+case 0xF8: return fn( __VA_ARGS__, 0xF8 );    \
+case 0xF9: return fn( __VA_ARGS__, 0xF9 );    \
+case 0xFA: return fn( __VA_ARGS__, 0xFA );    \
+case 0xFB: return fn( __VA_ARGS__, 0xFB );    \
+case 0xFC: return fn( __VA_ARGS__, 0xFC );    \
+case 0xFD: return fn( __VA_ARGS__, 0xFD );    \
+case 0xFE: return fn( __VA_ARGS__, 0xFE );    \
+default: return fn( __VA_ARGS__, 0xFF );
+
+#endif // __BASE_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_platform.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_platform.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/SSEPlus_platform.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/SSEPlus_platform.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,295 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+
+// This file should contain all platform specific code. 
+
+#ifndef __SSEPLUS_PLATFORM_H__
+#define __SSEPLUS_PLATFORM_H__
+
+//TODO: Detect 32/64
+
+
+
+
+//---------------------------------------
+// Microsoft Visual Studio
+//---------------------------------------
+#if defined( _MSC_VER )
+#define SSP_MSVC
+
+#if( _MSC_VER >= 1500 ) 
+#define SSP_COMPILER_SUPPORTS_SSSE3
+#define SSP_COMPILER_SUPPORTS_SSE4a
+#define SSP_COMPILER_SUPPORTS_SSE41
+#define SSP_COMPILER_SUPPORTS_SSE42
+#endif
+
+#define SSP_FORCEINLINE                 __forceinline
+#define SSP_INCLUDE_FILE_SSE3           <intrin.h>          // All intrinsics, including SSE3
+#define SSP_INCLUDE_FILE_SSE4a          <intrin.h>          // All intrinsics, including SSE4a
+#define SSP_INCLUDE_FILE_SSE5           "SSEPlus_NoSSE5.h"  // SSE5
+#define SSP_INCLUDE_FILE_SSE4_1_SSE5    <smmintrin.h>       // Functions common to SSE4.1 and SSE5
+
+#include <intrin.h> // CPUID
+
+//---------------------------------------
+// GCC
+//---------------------------------------
+#elif defined( __GNUC__ )
+#define SSP_GNUC
+
+#if( __GNUC__       >= 4 )
+#if( __GNUC_MINOR__ >= 3 )
+#define SSP_COMPILER_SUPPORTS_SSSE3
+#define SSP_COMPILER_SUPPORTS_SSE4a
+#define SSP_COMPILER_SUPPORTS_SSE41
+#define SSP_COMPILER_SUPPORTS_SSE42
+#define SSP_COMPILER_SUPPORTS_SSE5
+#endif
+#endif
+
+#define SSP_FORCEINLINE                 static inline
+#define SSP_INCLUDE_FILE_SSE3           <pmmintrin.h>           // SSE3
+#define SSP_INCLUDE_FILE_SSE4a          <ammintrin.h>           // All intrinsics, including SSE4a
+#define SSP_INCLUDE_FILE_SSE5           <bmmintrin.h>           // SSE5
+#define SSP_INCLUDE_FILE_SSE4_1_SSE5    <mmintrin-common.h>     // Functions common to SSE4.1 and SSE5
+
+// CPUID
+#if defined( SYS64 )
+    #define __cpuid(CPUInfo, InfoType)    __asm__ __volatile__("    pushq %%rbx;                      \
+                                                                xorq %%rax, %%rax;                    \
+                                                                movl %%esi, %%eax;                    \
+                                                                cpuid;                                \
+                                                                movl %%eax, 0x0(%%rdi);               \
+                                                                movl %%ebx, 0x4(%%rdi);               \
+                                                                movl %%ecx, 0x8(%%rdi);               \
+                                                                movl %%edx, 0xc(%%rdi);               \
+                                                                popq %%rbx;"                          \
+                                                                : : "D" (CPUInfo), "S" (InfoType)     \
+                                                                : "%rax", "%rcx", "%rdx" )
+
+#elif defined( SYS32 )
+    #define __cpuid(CPUInfo, InfoType)    __asm__ __volatile__("    pushl %%ebx;                      \
+                                                                xorl %%eax, %%eax;                    \
+                                                                movl %%esi, %%eax;                    \
+                                                                cpuid;                                \
+                                                                movl %%eax, 0x0(%%edi);               \
+                                                                movl %%ebx, 0x4(%%edi);               \
+                                                                movl %%ecx, 0x8(%%edi);               \
+                                                                movl %%edx, 0xc(%%edi);               \
+                                                                popl %%ebx;"                          \
+                                                                : : "D" (CPUInfo), "S" (InfoType)     \
+                                                                : "%eax", "%ecx", "%edx" )
+#endif
+#endif 
+
+//---------------------------------------
+// Microsoft Visual Studio Initialization
+//---------------------------------------
+
+#define SSP_ALL_SET_32I 0xFFFFFFFF
+#define SSP_ALL_SET_64I 0xFFFFFFFFFFFFFFFF
+
+
+#if defined(SSP_MSVC)
+//#undef SSP_MSVC	
+
+#define SSP_CONST_SETR_8I( a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p ) \
+    { (unsigned __int8)(a), (unsigned __int8)(b), (unsigned __int8)(c), (unsigned __int8)(d), \
+      (unsigned __int8)(e), (unsigned __int8)(f), (unsigned __int8)(g), (unsigned __int8)(h), \
+      (unsigned __int8)(i), (unsigned __int8)(j), (unsigned __int8)(k), (unsigned __int8)(l), \
+      (unsigned __int8)(m), (unsigned __int8)(n), (unsigned __int8)(o), (unsigned __int8)(p) }
+
+#define SSP_CONST_SET_8I( a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p ) \
+    SSP_CONST_SETR_8I( (p), (o), (n), (m), (l), (k), (j), (i), (h), (g), (f), (e), (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_8I( x ) \
+    SSP_CONST_SET_8I( (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_8I() \
+    SSP_CONST_SET1_8I( 0 )
+
+#define __CNST16I28I_( x ) \
+    ((unsigned __int8)((x) & 0xFF)), ((unsigned __int8)(((x) >> 8) & 0xFF))
+
+#define SSP_CONST_SETR_16I( a, b, c, d, e, f, g, h ) \
+    { __CNST16I28I_((a)), __CNST16I28I_((b)), __CNST16I28I_((c)), __CNST16I28I_((d)), __CNST16I28I_((e)), __CNST16I28I_((f)), __CNST16I28I_((g)), __CNST16I28I_((h)) }
+
+#define SSP_CONST_SET_16I( a, b, c, d, e, f, g, h ) \
+    SSP_CONST_SETR_16I( (h), (g), (f), (e), (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_16I( x ) \
+    SSP_CONST_SET_16I( (x), (x), (x), (x), (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_16I() \
+    SSP_CONST_SETZERO_8I()
+
+#define __CNST32I28I_( x ) \
+    ((unsigned __int8)((x) & 0xFF)), ((unsigned __int8)(((x) >> 8) & 0xFF)), ((unsigned __int8)(((x) >> 16) & 0xFF)), ((unsigned __int8)(((x) >> 24) & 0xFF))
+
+#define SSP_CONST_SETR_32I( a, b, c, d ) \
+    { __CNST32I28I_((a)), __CNST32I28I_((b)), __CNST32I28I_((c)), __CNST32I28I_((d)) }
+
+#define SSP_CONST_SET_32I( a, b, c, d ) \
+    SSP_CONST_SETR_32I( (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_32I( x ) \
+    SSP_CONST_SET_32I( (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_32I() \
+    SSP_CONST_SETZERO_8I()
+
+#define __CNST64I28I_( x ) \
+    ((unsigned __int8)((x) & 0xFF)), ((unsigned __int8)(((x) >> 8) & 0xFF)), ((unsigned __int8)(((x) >> 16) & 0xFF)), ((unsigned __int8)(((x) >> 24) & 0xFF)), ((unsigned __int8)(((x) >> 32) & 0xFF)), ((unsigned __int8)(((x) >> 40) & 0xFF)), ((unsigned __int8)(((x) >> 48) & 0xFF)), ((unsigned __int8)(((x) >> 56) & 0xFF))
+
+#define SSP_CONST_SETR_64I( a, b ) \
+    { __CNST64I28I_((a)), __CNST64I28I_((b)) }
+
+#define SSP_CONST_SET_64I( a, b ) \
+    SSP_CONST_SETR_64I( (b), (a) )
+
+#define SSP_CONST_SET1_64I( x ) \
+    SSP_CONST_SET_64I( (x), (x) )
+
+#define SSP_CONST_SETZERO_I() \
+	{ 0 }
+
+#define SSP_CONST_SETZERO_64I() \
+    SSP_CONST_SETZERO_8I()
+
+#define SSP_CONST_SETR_32F( a, b, c, d ) \
+    { (a), (b), (c), (d) }
+
+#define SSP_CONST_SET_32F( a, b, c, d ) \
+    SSP_CONST_SETR_32F( (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_32F( x ) \
+    SSP_CONST_SET_32F( (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_32F() \
+    SSP_CONST_SET1_32F( 0 )
+
+#define SSP_CONST_SETR_64F( a, b ) \
+    { (a), (b) }
+
+#define SSP_CONST_SET_64F( a, b ) \
+    SSP_CONST_SETR_64F( (b), (a) )
+
+#define SSP_CONST_SET1_64F( x ) \
+    SSP_CONST_SET_64F( (x), (x) )
+
+#define SSP_CONST_SETZERO_64F() \
+    SSP_CONST_SET1_64F( 0 )
+
+#endif // SSP_MSVC
+
+
+//---------------------------------------
+// GCC Initialization
+//---------------------------------------
+#if defined(SSP_GNUC)
+//#undef SSP_GNUC
+
+#define __CNST8TO64_( a, b, c, d, e, f, g, h ) \
+	( (((h)&0xff)<<56) | (((g)&0xff)<<48) | (((f)&0xff)<<40) | (((e)&0xff)<<32) | \
+ 	(((d)&0xff)<<24) | (((c)&0xff)<<16) | (((b)&0xff)<<8) | ((a)&0xff) )
+
+#define SSP_CONST_SETR_8I( a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p ) \
+    { __CNST8TO64_( (ssp_s64)(a), (ssp_s64)(b), (ssp_s64)(c), (ssp_s64)(d), \
+                 (ssp_s64)(e), (ssp_s64)(f), (ssp_s64)(g), (ssp_s64)(h) ), \
+      __CNST8TO64_( (ssp_s64)(i), (ssp_s64)(j), (ssp_s64)(k), (ssp_s64)(l), \
+                 (ssp_s64)(m), (ssp_s64)(n), (ssp_s64)(o), (ssp_s64)(p) ) }
+
+#define SSP_CONST_SET_8I( a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p ) \
+    SSP_CONST_SETR_8I( (p), (o), (n), (m), (l), (k), (j), (i), (h), (g), (f), (e), (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_8I( x ) \
+    SSP_CONST_SET_8I( (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_8I() \
+    { 0, 0 }
+
+// 16 bit integer types
+
+#define __CNST16TO64_( a, b, c, d ) \
+	( ((d)<<48) | (((c)&0xFFFF)<<32) | (((b)&0xFFFF)<<16) | ((a)&0xFFFF) )
+
+#define SSP_CONST_SETR_16I( a, b, c, d, e, f, g, h ) \
+    { __CNST16TO64_( (ssp_u64)(a), (ssp_u64)(b), \
+                     (ssp_u64)(c), (ssp_u64)(d) ), \
+      __CNST16TO64_( (ssp_u64)(e), (ssp_u64)(f), \
+                     (ssp_u64)(g), (ssp_u64)(h) ) }
+
+#define SSP_CONST_SET_16I( a, b, c, d, e, f, g, h ) \
+    SSP_CONST_SETR_16I( (h), (g), (f), (e), (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_16I( x ) \
+    SSP_CONST_SET_16I( (x), (x), (x), (x), (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_16I() \
+    SSP_CONST_SETZERO_8I()
+
+// 32 bit integer types
+
+#define __CNST32TO64_( a, b ) \
+	( ((b)<<32) | ((a) & 0xFFFFFFFF) )
+
+#define SSP_CONST_SETR_32I( a, b, c, d ) \
+    { __CNST32TO64_( (ssp_u64)(a), (ssp_u64)(b) ), \
+      __CNST32TO64_( (ssp_u64)(c), (ssp_u64)(d) ) }
+
+#define SSP_CONST_SET_32I( a, b, c, d ) \
+    SSP_CONST_SETR_32I( (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_32I( x ) \
+    SSP_CONST_SET_32I( (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_32I() \
+    SSP_CONST_SETZERO_8I()
+
+// 64 bit integer types
+
+#define SSP_CONST_SETR_64I( a, b ) \
+    { (a), (b) }
+
+#define SSP_CONST_SET_64I( a, b ) \
+    SSP_CONST_SETR_64I( (b), (a) )
+
+#define SSP_CONST_SET1_64I( x ) \
+    SSP_CONST_SET_64I( (x), (x) )
+
+#define SSP_CONST_SETZERO_64I() \
+    SSP_CONST_SETZERO_8I()
+
+
+// 32 bit single precision floating point types
+
+#define SSP_CONST_SETR_32F( a, b, c, d ) \
+    { (a), (b), (c), (d) }
+
+#define SSP_CONST_SET_32F( a, b, c, d ) \
+    SSP_CONST_SETR_32F( (d), (c), (b), (a) )
+
+#define SSP_CONST_SET1_32F( x ) \
+    SSP_CONST_SET_32F( (x), (x), (x), (x) )
+
+#define SSP_CONST_SETZERO_32F() \
+    SSP_CONST_SET1_32F( 0 )
+
+// 64 bit double precision floating point types
+#define SSP_CONST_SETR_64F( a, b ) \
+    { (a), (b) }
+
+#define SSP_CONST_SET_64F( a, b ) \
+    SSP_CONST_SETR_64F( (b), (a) )
+
+#define SSP_CONST_SET1_64F( x ) \
+    SSP_CONST_SET_64F( (x), (x) )
+
+#define SSP_CONST_SETZERO_64F() \
+    SSP_CONST_SET1_64F( 0 )
+
+#endif // SSP_GNUC
+#endif // __SSEPLUS_PLATFORM_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,49 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_ARITHMETIC_REF_H__
+#define __SSEPLUS_ARITHMETIC_REF_H__
+
+
+/** @addtogroup supplimental_REF   
+ *  @{ 
+ *  @name Arithmetic Operations
+ */
+
+
+/** */
+SSP_FORCEINLINE __m128 ssp_arithmetic_hadd4_dup_ps_REF( __m128 a )      // Sum all 4 values
+{
+    ssp_m128 A;
+    ssp_f32 t;
+    
+    A.f = a;
+    t   = A.f32[0] + 
+          A.f32[1] + 
+          A.f32[2] + 
+          A.f32[3];
+
+    A.f32[0] = t;
+    A.f32[1] = t;
+    A.f32[2] = t;
+    A.f32[3] = t;
+    return A.f;
+}  
+
+SSP_FORCEINLINE __m128i ssp_arithmetic_hadd4_epu16_REF( __m128i a )      // Sum 2 sets of 4 values, dest in 0, and 4
+{
+    ssp_m128 A;
+    A.i = a;
+
+    A.u16[0] = A.u16[0] + A.u16[1] + + A.u16[2] + + A.u16[3];
+    A.u16[4] = A.u16[4] + A.u16[5] + + A.u16[6] + + A.u16[7];
+
+    return A.i;
+}  
+
+//@}
+//@}
+
+
+#endif // __SSEPLUS_ARITHMETIC_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,108 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_ARITHMETIC_SSE2_H__
+#define __SSEPLUS_ARITHMETIC_SSE2_H__
+
+#include "../native/SSEPlus_native_SSE2.h"
+#include "../emulation/SSEPlus_emulation_SSE2.h"
+
+/** @addtogroup supplimental_SSE2
+ *  @{ 
+ *  @name Arithmetic Operations
+ */
+
+SSP_FORCEINLINE
+__m128 ssp_arithmetic_hadd4_dup_ps_SSE2( __m128 a )      // [18 cycles]                 // Sum all 4 values
+{
+    __m128 t;
+    t = _mm_shuffle_ps( a, a, _MM_SHUFFLE(2, 3, 0, 1) );                //TODO shuflo, shuf hi
+    a = _mm_add_ps( a, t );   
+
+    t = _mm_shuffle_ps( a, a, _MM_SHUFFLE(1, 0, 3, 2) );                //TODO shuflo, shuf hi
+    a = _mm_add_ps( a, t );
+    return a;
+} 
+
+
+/**
+    in  = a,b,c,d       | e,f,g,h,       0
+    out = x,x,x,a+b+c+d | x,x,x,e+f+g+h
+
+    in  = a,b,c,d       | e,f,g,h,       3
+    out = a+b+c+d,x,x,x,| x,x,x,e+f+g+h
+
+    offset indicates desired position of sum (0,1,2,3)
+*/
+SSP_FORCEINLINE
+__m128i ssp_arithmetic_hadd4_epi16_SSE2( __m128i a, const unsigned int offset )      // Sum 2 sets of 4 values, dest in 0, and 4
+{
+    ssp_m128 A,B;
+    A.i = a;                                           //A = a, b, c, d | e, f, g, h
+
+    if( offset >= 2 ) B.i = _mm_slli_si128( A.i, 4 );  //B = c, d, x, x | g, h, x, x
+    else              B.i = _mm_srli_si128( A.i, 4 );  //B = x, x, a, b | x, x, e, f
+
+    A.i = _mm_add_epi16 ( A.i, B.i );      
+
+    if( offset & 1 )  B.i = _mm_slli_si128( A.i, 2 );  
+    else              B.i = _mm_srli_si128( A.i, 2 ); 
+  
+    A.i = _mm_add_epi16 ( A.i, B.i );      
+    return A.i;
+}  
+
+
+
+//__m128 ssp_arithmetic_hadd4_dup_ps_SSE2_a( __m128 a_ )      // [19 cycles]                 // Sum all 4 values
+//{
+//    ssp_m128 t, a; a.f = a_;
+//
+//    t.i  = _mm_shufflehi_epi16( a.i, _MM_SHUFFLE(1, 0, 3, 2) );
+//    t.i  = _mm_shufflelo_epi16( t.i, _MM_SHUFFLE(1, 0, 3, 2) );
+//    
+//    //t.f  = _mm_shuffle_ps( a, a, _MM_SHUFFLE(2, 3, 0, 1) );           //TODO shuflo, shuf hi   
+//
+//    a.f = _mm_add_ps    ( a.f, t.f );  
+//
+//    t.f = _mm_shuffle_ps( a.f, a.f, _MM_SHUFFLE(1, 0, 3, 2) );                //TODO shuflo, shuf hi
+//
+//
+//    a.f = _mm_add_ps    ( a.f, t.f );
+//    return a.f;
+//}  
+//
+//__m128 ssp_arithmetic_hadd4_dup_ps_SSE2_b( __m128 a )      // [18 cycles]                 // Sum all 4 values
+//{
+//    __m128 t;
+//    t = _mm_shuffle_ps( a, a, _MM_SHUFFLE(2, 3, 0, 1) );                //TODO shuflo, shuf hi
+//    a = _mm_add_ps( a, t );     
+//
+//    t = _mm_movelh_ps( t, a );
+//    t = _mm_movehl_ps( t, a );
+//   
+//    a = _mm_add_ps( a, t );
+//    return a;
+//}  
+
+/** This function wraps ssp_round_ps_SSE2. It guarantees that numbers rounding to 0 from a negative will generate a negative zero. */
+SSP_FORCEINLINE
+__m128 ssp_round_ps_neg_zero_SSE2( __m128  a, int iRoundMode )
+{
+    const static __m128i SIGN_BIT = SSP_CONST_SET_32I( 0x80000000, 0x80000000, 0x80000000,0x80000000 );
+    ssp_m128 A, sign;
+    A.f = a;
+    
+    sign.i = _mm_and_si128    ( A.i, SIGN_BIT );  // Store the sign bits
+    A.f    = ssp_round_ps_SSE2( A.f, iRoundMode );   
+    A.i    = _mm_or_si128     ( A.i, sign.i );    // Restore the sign bits (preserves -0)
+   
+    return A.f;
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_ARITHMETIC_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/arithmetic/SSEPlus_arithmetic_SSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,28 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_ARITHMETIC_SSE3_H__
+#define __SSEPLUS_ARITHMETIC_SSE3_H__
+
+#include "../native/SSEPlus_native_SSE3.h"
+
+/** @addtogroup supplimental_SSE3  
+ *  @{ 
+ *  @name Arithmetic Operations
+ */
+
+SSP_FORCEINLINE
+__m128 ssp_arithmetic_hadd4_dup_ps_SSE3( __m128 a )      // [18 cycles]                 // Sum all 4 values
+{
+	a = _mm_hadd_ps( a, a );
+	a = _mm_hadd_ps( a, a );
+    return a;
+} 
+
+/** @} 
+ *  @}
+ */
+
+
+#endif // __SSEPLUS_ARITHMETIC_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,565 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_CONVERT_REF_H__
+#define __SSEPLUS_CONVERT_REF_H__
+
+
+/** @addtogroup supplimental_REF   
+ *  @{ 
+ *  @name Convert Operations
+ */
+
+
+//a: 9  6 3 0    3  2  1 0               
+//b: 10 7 4 1 -> 7  6  5 4       
+//c: 11 8 5 2    11 10 9 8
+SSP_FORCEINLINE
+void ssp_convert_reverse_transpose_REF( __m128i *a, __m128i *b, __m128i *c )
+{
+    ssp_m128 Ai, Ao, Bi, Bo, Ci, Co;
+    Ai.i = *a;   
+    Bi.i = *b;   
+    Ci.i = *c;   
+
+    Ao.u32[ 0 ] = Ai.u32[ 0 ];
+    Ao.u32[ 1 ] = Bi.u32[ 0 ];
+    Ao.u32[ 2 ] = Ci.u32[ 0 ];
+    Ao.u32[ 3 ] = Ai.u32[ 1 ];
+
+    Bo.u32[ 0 ] = Bi.u32[ 1 ];
+    Bo.u32[ 1 ] = Ci.u32[ 1 ];
+    Bo.u32[ 2 ] = Ai.u32[ 2 ];
+    Bo.u32[ 3 ] = Bi.u32[ 2 ];
+
+    Co.u32[ 0 ] = Ci.u32[ 2 ];
+    Co.u32[ 1 ] = Ai.u32[ 3 ];
+    Co.u32[ 2 ] = Bi.u32[ 3 ];
+    Co.u32[ 3 ] = Ci.u32[ 3 ];
+
+    *a = Ao.i;
+    *b = Bo.i;
+    *c = Co.i;
+}
+
+
+SSP_FORCEINLINE 
+void ssp_convert_3c_3p_epi8_REF( __m128i *rgb1, __m128i *rgb2, __m128i *rgb3)
+{
+    ssp_m128 RGB1, RGB2, RGB3;
+    ssp_m128 R,G,B;
+
+    RGB1.i = *rgb1;
+    RGB2.i = *rgb2;
+    RGB3.i = *rgb3;
+    
+    R.u8[0 ] = RGB1.u8[0 ];  //RED
+    R.u8[1 ] = RGB1.u8[3 ];
+    R.u8[2 ] = RGB1.u8[6 ];
+    R.u8[3 ] = RGB1.u8[9 ];
+    R.u8[4 ] = RGB1.u8[12];
+    R.u8[5 ] = RGB1.u8[15];
+    R.u8[6 ] = RGB2.u8[2 ];
+    R.u8[7 ] = RGB2.u8[5 ];
+    R.u8[8 ] = RGB2.u8[8 ];
+    R.u8[9 ] = RGB2.u8[11];
+    R.u8[10] = RGB2.u8[14];
+    R.u8[11] = RGB3.u8[1 ];
+    R.u8[12] = RGB3.u8[4 ];
+    R.u8[13] = RGB3.u8[7 ];
+    R.u8[14] = RGB3.u8[10];
+    R.u8[15] = RGB3.u8[13];
+
+    G.u8[0 ] = RGB1.u8[1 ];  //GREEN
+    G.u8[1 ] = RGB1.u8[4 ];
+    G.u8[2 ] = RGB1.u8[7 ];
+    G.u8[3 ] = RGB1.u8[10];
+    G.u8[4 ] = RGB1.u8[13];
+    G.u8[5 ] = RGB2.u8[0 ];
+    G.u8[6 ] = RGB2.u8[3 ];
+    G.u8[7 ] = RGB2.u8[6 ];
+    G.u8[8 ] = RGB2.u8[9 ];
+    G.u8[9 ] = RGB2.u8[12];
+    G.u8[10] = RGB2.u8[15];
+    G.u8[11] = RGB3.u8[2 ];
+    G.u8[12] = RGB3.u8[5 ];
+    G.u8[13] = RGB3.u8[8 ];
+    G.u8[14] = RGB3.u8[11];
+    G.u8[15] = RGB3.u8[14];
+
+    B.u8[0 ] = RGB1.u8[2 ];  //BLUE
+    B.u8[1 ] = RGB1.u8[5 ];
+    B.u8[2 ] = RGB1.u8[8 ];
+    B.u8[3 ] = RGB1.u8[11];
+    B.u8[4 ] = RGB1.u8[14];
+    B.u8[5 ] = RGB2.u8[1 ];
+    B.u8[6 ] = RGB2.u8[4 ];
+    B.u8[7 ] = RGB2.u8[7 ];
+    B.u8[8 ] = RGB2.u8[10];
+    B.u8[9 ] = RGB2.u8[13];
+    B.u8[10] = RGB3.u8[0 ];
+    B.u8[11] = RGB3.u8[3 ];
+    B.u8[12] = RGB3.u8[6 ];
+    B.u8[13] = RGB3.u8[9 ];
+    B.u8[14] = RGB3.u8[12];
+    B.u8[15] = RGB3.u8[15];
+   
+
+    *rgb1 = R.i;
+    *rgb2 = G.i;
+    *rgb3 = B.i;   
+}
+
+
+SSP_FORCEINLINE
+void ssp_convert_3p_3c_epi8_REF( __m128i *r, __m128i *g, __m128i *b )
+{
+    ssp_m128 R,G,B;
+    ssp_m128 RGB1, RGB2, RGB3;
+    
+    R.i = *r;
+    G.i = *g;
+    B.i = *b;
+
+    RGB1.u8[0 ] = R.u8[0 ];  
+    RGB1.u8[1 ] = G.u8[0 ];
+    RGB1.u8[2 ] = B.u8[0 ];
+    RGB1.u8[3 ] = R.u8[1 ];
+    RGB1.u8[4 ] = G.u8[1 ];
+    RGB1.u8[5 ] = B.u8[1 ];
+    RGB1.u8[6 ] = R.u8[2 ];
+    RGB1.u8[7 ] = G.u8[2 ];
+    RGB1.u8[8 ] = B.u8[2 ];
+    RGB1.u8[9 ] = R.u8[3 ];
+    RGB1.u8[10] = G.u8[3 ];
+    RGB1.u8[11] = B.u8[3 ];
+    RGB1.u8[12] = R.u8[4 ];
+    RGB1.u8[13] = G.u8[4 ];
+    RGB1.u8[14] = B.u8[4 ];
+    RGB1.u8[15] = R.u8[5 ];
+
+    RGB2.u8[0 ] = G.u8[5 ];  
+    RGB2.u8[1 ] = B.u8[5 ];
+    RGB2.u8[2 ] = R.u8[6 ];
+    RGB2.u8[3 ] = G.u8[6 ];
+    RGB2.u8[4 ] = B.u8[6 ];
+    RGB2.u8[5 ] = R.u8[7 ];
+    RGB2.u8[6 ] = G.u8[7 ];
+    RGB2.u8[7 ] = B.u8[7 ];
+    RGB2.u8[8 ] = R.u8[8 ];
+    RGB2.u8[9 ] = G.u8[8 ];
+    RGB2.u8[10] = B.u8[8 ];
+    RGB2.u8[11] = R.u8[9 ];
+    RGB2.u8[12] = G.u8[9 ];
+    RGB2.u8[13] = B.u8[9 ];
+    RGB2.u8[14] = R.u8[10];
+    RGB2.u8[15] = G.u8[10];
+
+    RGB3.u8[0 ] = B.u8[10];  
+    RGB3.u8[1 ] = R.u8[11];
+    RGB3.u8[2 ] = G.u8[11];
+    RGB3.u8[3 ] = B.u8[11];
+    RGB3.u8[4 ] = R.u8[12];
+    RGB3.u8[5 ] = G.u8[12];
+    RGB3.u8[6 ] = B.u8[12];
+    RGB3.u8[7 ] = R.u8[13];
+    RGB3.u8[8 ] = G.u8[13];
+    RGB3.u8[9 ] = B.u8[13];
+    RGB3.u8[10] = R.u8[14];
+    RGB3.u8[11] = G.u8[14];
+    RGB3.u8[12] = B.u8[14];
+    RGB3.u8[13] = R.u8[15];
+    RGB3.u8[14] = G.u8[15];
+    RGB3.u8[15] = B.u8[15];
+
+    *r = RGB1.i;
+    *g = RGB2.i;
+    *b = RGB3.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3c_3p_epi16_REF(__m128i *rgb1,__m128i *rgb2,__m128i *rgb3)
+{
+    ssp_m128 trgb1, trgb2, trgb3;
+    ssp_m128 r, g, b;
+    trgb1.i = *rgb1;
+    trgb2.i = *rgb2;
+    trgb3.i = *rgb3;
+
+    r.s16[0] = trgb1.s16[0];
+    r.s16[1] = trgb1.s16[3];
+    r.s16[2] = trgb1.s16[6];
+    r.s16[3] = trgb2.s16[1];
+    r.s16[4] = trgb2.s16[4];
+    r.s16[5] = trgb2.s16[7];
+    r.s16[6] = trgb3.s16[2];
+    r.s16[7] = trgb3.s16[5];
+
+    g.s16[0] = trgb1.s16[1];
+    g.s16[1] = trgb1.s16[4];
+    g.s16[2] = trgb1.s16[7];
+    g.s16[3] = trgb2.s16[2];
+    g.s16[4] = trgb2.s16[5];
+    g.s16[5] = trgb3.s16[0];
+    g.s16[6] = trgb3.s16[3];
+    g.s16[7] = trgb3.s16[6];
+
+    b.s16[0] = trgb1.s16[2];
+    b.s16[1] = trgb1.s16[5];
+    b.s16[2] = trgb2.s16[0];
+    b.s16[3] = trgb2.s16[3];
+    b.s16[4] = trgb2.s16[6];
+    b.s16[5] = trgb3.s16[1];
+    b.s16[6] = trgb3.s16[4];
+    b.s16[7] = trgb3.s16[7];
+
+	*rgb1 = r.i;
+	*rgb2 = g.i;
+	*rgb3 = b.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3p_3c_epi16_REF(__m128i *r,__m128i *g,__m128i *b)
+{
+    ssp_m128 rgb1, rgb2, rgb3;
+    ssp_m128 tr, tg, tb;
+    tr.i = *r;
+    tg.i = *g;
+    tb.i = *b;
+
+    rgb1.s16[0] = tr.s16[0];
+    rgb1.s16[3] = tr.s16[1];
+    rgb1.s16[6] = tr.s16[2];
+    rgb2.s16[1] = tr.s16[3];
+    rgb2.s16[4] = tr.s16[4];
+    rgb2.s16[7] = tr.s16[5];
+    rgb3.s16[2] = tr.s16[6];
+    rgb3.s16[5] = tr.s16[7];
+
+    rgb1.s16[1] = tg.s16[0];
+    rgb1.s16[4] = tg.s16[1];
+    rgb1.s16[7] = tg.s16[2];
+    rgb2.s16[2] = tg.s16[3];
+    rgb2.s16[5] = tg.s16[4];
+    rgb3.s16[0] = tg.s16[5];
+    rgb3.s16[3] = tg.s16[6];
+    rgb3.s16[6] = tg.s16[7];
+
+    rgb1.s16[2] = tb.s16[0];
+    rgb1.s16[5] = tb.s16[1];
+    rgb2.s16[0] = tb.s16[2];
+    rgb2.s16[3] = tb.s16[3];
+    rgb2.s16[6] = tb.s16[4];
+    rgb3.s16[1] = tb.s16[5];
+    rgb3.s16[4] = tb.s16[6];
+    rgb3.s16[7] = tb.s16[7];
+
+	*r = rgb1.i;
+	*g = rgb2.i;
+	*b = rgb3.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3c_3p_epi32_REF(__m128i *rgb1,__m128i *rgb2,__m128i *rgb3)
+{
+    ssp_m128 trgb1, trgb2, trgb3;
+    ssp_m128 r, g, b;
+	trgb1.i = *rgb1;
+	trgb2.i = *rgb2;
+	trgb3.i = *rgb3;
+
+    r.s32[0] = trgb1.s32[0];
+    r.s32[1] = trgb1.s32[3];
+    r.s32[2] = trgb2.s32[2];
+    r.s32[3] = trgb3.s32[1];
+
+    g.s32[0] = trgb1.s32[1];
+    g.s32[1] = trgb2.s32[0];
+    g.s32[2] = trgb2.s32[3];
+    g.s32[3] = trgb3.s32[2];
+
+    b.s32[0] = trgb1.s32[2];
+    b.s32[1] = trgb2.s32[1];
+    b.s32[2] = trgb3.s32[0];
+    b.s32[3] = trgb3.s32[3];
+
+	*rgb1 = r.i;
+	*rgb2 = g.i;
+	*rgb3 = b.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3p_3c_epi32_REF(__m128i *r,__m128i *g,__m128i *b)
+{
+    ssp_m128 tr, tg, tb;
+    ssp_m128 rgb1, rgb2, rgb3;
+
+	tr.i = *r;
+	tg.i = *g;
+	tb.i = *b;
+
+    rgb1.s32[0] = tr.s32[0];
+    rgb1.s32[3] = tr.s32[1];
+    rgb2.s32[2] = tr.s32[2];
+    rgb3.s32[1] = tr.s32[3];
+
+    rgb1.s32[1] = tg.s32[0];
+    rgb2.s32[0] = tg.s32[1];
+    rgb2.s32[3] = tg.s32[2];
+    rgb3.s32[2] = tg.s32[3];
+
+    rgb1.s32[2] = tb.s32[0];
+    rgb2.s32[1] = tb.s32[1];
+    rgb3.s32[0] = tb.s32[2];
+    rgb3.s32[3] = tb.s32[3];
+
+	*r = rgb1.i;
+	*g = rgb2.i;
+	*b = rgb3.i;
+}
+
+/* convert 4-channel RGBA to 4-planar format */
+SSP_FORCEINLINE 
+void ssp_convert_4c_4p_epi8_REF( __m128i *rgba1, __m128i *rgba2, __m128i *rgba3, __m128i *rgba4 )
+{
+	int n;
+	ssp_m128 trgba1, trgba2, trgba3, trgba4;
+	ssp_m128 r, g, b, a;
+
+	trgba1.i = *rgba1;
+	trgba2.i = *rgba2;
+	trgba3.i = *rgba3;
+	trgba4.i = *rgba4;
+
+    for( n = 0; n < 4; n++ )
+    {
+        r.s8[0+n] = trgba1.s8[4*n];
+        r.s8[4+n] = trgba2.s8[4*n];
+        r.s8[8+n] = trgba3.s8[4*n];
+        r.s8[12+n] = trgba4.s8[4*n];
+
+        g.s8[0+n] = trgba1.s8[4*n+1];
+        g.s8[4+n] = trgba2.s8[4*n+1];
+        g.s8[8+n] = trgba3.s8[4*n+1];
+        g.s8[12+n] = trgba4.s8[4*n+1];
+
+        b.s8[0+n] = trgba1.s8[4*n+2];
+        b.s8[4+n] = trgba2.s8[4*n+2];
+        b.s8[8+n] = trgba3.s8[4*n+2];
+        b.s8[12+n] = trgba4.s8[4*n+2];
+
+        a.s8[0+n] = trgba1.s8[4*n+3];
+        a.s8[4+n] = trgba2.s8[4*n+3];
+        a.s8[8+n] = trgba3.s8[4*n+3];
+        a.s8[12+n] = trgba4.s8[4*n+3];
+    }
+
+	*rgba1 = r.i;
+	*rgba2 = g.i;
+	*rgba3 = b.i;
+	*rgba4 = a.i;
+}
+
+/* convert 4-planar RGBA to 4-channel format */
+SSP_FORCEINLINE 
+void ssp_convert_4p_4c_epi8_REF(__m128i *r,__m128i *g,__m128i *b,__m128i *a)
+{
+	int n;
+    ssp_m128 tr, tg, tb, ta;
+    ssp_m128 rgba1, rgba2, rgba3, rgba4;
+	tr.i = *r;
+	tg.i = *g;
+	tb.i = *b;
+	ta.i = *a;
+
+    for( n = 0; n < 4; n++ )
+    {
+        rgba1.s8[4*n] = tr.s8[0+n];
+        rgba2.s8[4*n] = tr.s8[4+n];
+        rgba3.s8[4*n] = tr.s8[8+n];
+        rgba4.s8[4*n] = tr.s8[12+n];
+
+        rgba1.s8[4*n+1] = tg.s8[0+n];
+        rgba2.s8[4*n+1] = tg.s8[4+n];
+        rgba3.s8[4*n+1] = tg.s8[8+n];
+        rgba4.s8[4*n+1] = tg.s8[12+n];
+
+        rgba1.s8[4*n+2] = tb.s8[0+n];
+        rgba2.s8[4*n+2] = tb.s8[4+n];
+        rgba3.s8[4*n+2] = tb.s8[8+n];
+        rgba4.s8[4*n+2] = tb.s8[12+n];
+
+        rgba1.s8[4*n+3] = ta.s8[0+n];
+        rgba2.s8[4*n+3] = ta.s8[4+n];
+        rgba3.s8[4*n+3] = ta.s8[8+n];
+        rgba4.s8[4*n+3] = ta.s8[12+n];
+    }
+
+	*r = rgba1.i;
+	*g = rgba2.i;
+	*b = rgba3.i;
+	*a = rgba4.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4c_4p_epi16_REF(__m128i *rgba1,__m128i *rgba2,__m128i *rgba3,__m128i *rgba4)
+{
+	int n;
+	ssp_m128 trgba1, trgba2, trgba3, trgba4;
+	ssp_m128 r, g, b, a;
+
+	trgba1.i = *rgba1;
+	trgba2.i = *rgba2;
+	trgba3.i = *rgba3;
+	trgba4.i = *rgba4;
+
+    for( n = 0; n < 2; n++ )
+    {
+        r.s16[0+n] = trgba1.s16[4*n];
+        r.s16[2+n] = trgba2.s16[4*n];
+        r.s16[4+n] = trgba3.s16[4*n];
+        r.s16[6+n] = trgba4.s16[4*n];
+
+        g.s16[0+n] = trgba1.s16[4*n+1];
+        g.s16[2+n] = trgba2.s16[4*n+1];
+        g.s16[4+n] = trgba3.s16[4*n+1];
+        g.s16[6+n] = trgba4.s16[4*n+1];
+
+        b.s16[0+n] = trgba1.s16[4*n+2];
+        b.s16[2+n] = trgba2.s16[4*n+2];
+        b.s16[4+n] = trgba3.s16[4*n+2];
+        b.s16[6+n] = trgba4.s16[4*n+2];
+
+        a.s16[0+n] = trgba1.s16[4*n+3];
+        a.s16[2+n] = trgba2.s16[4*n+3];
+        a.s16[4+n] = trgba3.s16[4*n+3];
+        a.s16[6+n] = trgba4.s16[4*n+3];
+    }
+
+	*rgba1 = r.i;
+	*rgba2 = g.i;
+	*rgba3 = b.i;
+	*rgba4 = a.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4p_4c_epi16_REF(__m128i *r,__m128i *g,__m128i *b,__m128i *a)
+{
+	int n;
+    ssp_m128 tr, tg, tb, ta;
+    ssp_m128 rgba1, rgba2, rgba3, rgba4;
+	tr.i = *r;
+	tg.i = *g;
+	tb.i = *b;
+	ta.i = *a;
+
+    for( n = 0; n < 2; n++ )
+    {
+        rgba1.s16[4*n] = tr.s16[0+n];
+        rgba2.s16[4*n] = tr.s16[2+n];
+        rgba3.s16[4*n] = tr.s16[4+n];
+        rgba4.s16[4*n] = tr.s16[6+n];
+
+        rgba1.s16[4*n+1] = tg.s16[0+n];
+        rgba2.s16[4*n+1] = tg.s16[2+n];
+        rgba3.s16[4*n+1] = tg.s16[4+n];
+        rgba4.s16[4*n+1] = tg.s16[6+n];
+
+        rgba1.s16[4*n+2] = tb.s16[0+n];
+        rgba2.s16[4*n+2] = tb.s16[2+n];
+        rgba3.s16[4*n+2] = tb.s16[4+n];
+        rgba4.s16[4*n+2] = tb.s16[6+n];
+
+        rgba1.s16[4*n+3] = ta.s16[0+n];
+        rgba2.s16[4*n+3] = ta.s16[2+n];
+        rgba3.s16[4*n+3] = ta.s16[4+n];
+        rgba4.s16[4*n+3] = ta.s16[6+n];
+    }
+
+	*r = rgba1.i;
+	*g = rgba2.i;
+	*b = rgba3.i;
+	*a = rgba4.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4c_4p_epi32_REF(__m128i *rgba1,__m128i *rgba2,__m128i *rgba3, __m128i *rgba4)
+{
+	ssp_m128 trgba1, trgba2, trgba3, trgba4;
+	ssp_m128 r, g, b, a;
+
+	trgba1.i = *rgba1;
+	trgba2.i = *rgba2;
+	trgba3.i = *rgba3;
+	trgba4.i = *rgba4;
+
+    r.s32[0] = trgba1.s32[0];
+    r.s32[1] = trgba2.s32[0];
+    r.s32[2] = trgba3.s32[0];
+    r.s32[3] = trgba4.s32[0];
+
+    g.s32[0] = trgba1.s32[1];
+    g.s32[1] = trgba2.s32[1];
+    g.s32[2] = trgba3.s32[1];
+    g.s32[3] = trgba4.s32[1];
+
+    b.s32[0] = trgba1.s32[2];
+    b.s32[1] = trgba2.s32[2];
+    b.s32[2] = trgba3.s32[2];
+    b.s32[3] = trgba4.s32[2];
+
+    a.s32[0] = trgba1.s32[3];
+    a.s32[1] = trgba2.s32[3];
+    a.s32[2] = trgba3.s32[3];
+    a.s32[3] = trgba4.s32[3];
+
+	*rgba1 = r.i;
+	*rgba2 = g.i;
+	*rgba3 = b.i;
+	*rgba4 = a.i;
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4p_4c_epi32_REF(__m128i *r,__m128i *g,__m128i *b,__m128i *a)
+{
+    ssp_m128 tr, tg, tb, ta;
+    ssp_m128 rgba1, rgba2, rgba3, rgba4;
+	tr.i = *r;
+	tg.i = *g;
+	tb.i = *b;
+	ta.i = *a;
+
+    rgba1.s32[0] = tr.s32[0];
+    rgba2.s32[0] = tr.s32[1];
+    rgba3.s32[0] = tr.s32[2];
+    rgba4.s32[0] = tr.s32[3];
+
+    rgba1.s32[1] = tg.s32[0];
+    rgba2.s32[1] = tg.s32[1];
+    rgba3.s32[1] = tg.s32[2];
+    rgba4.s32[1] = tg.s32[3];
+
+    rgba1.s32[2] = tb.s32[0];
+    rgba2.s32[2] = tb.s32[1];
+    rgba3.s32[2] = tb.s32[2];
+    rgba4.s32[2] = tb.s32[3];
+
+    rgba1.s32[3] = ta.s32[0];
+    rgba2.s32[3] = ta.s32[1];
+    rgba3.s32[3] = ta.s32[2];
+    rgba4.s32[3] = ta.s32[3];
+
+	*r = rgba1.i;
+	*g = rgba2.i;
+	*b = rgba3.i;
+	*a = rgba4.i;
+}
+
+/** @} 
+ *  @}
+ */
+
+
+
+#endif // __SSEPLUS_CONVERT_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/convert/SSEPlus_convert_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,417 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_CONVERT_SSE2_H__
+#define __SSEPLUS_CONVERT_SSE2_H__
+
+#include "../native/SSEPlus_native_SSE2.h"
+
+/** @addtogroup supplimental_SSE2   
+ *  @{ 
+ *  @name Convert Operations
+ */
+
+
+/**  */
+SSP_FORCEINLINE
+void ssp_convert_odd_even_epi16_SSE2( __m128i *a, __m128i *b )
+{
+    // IN
+    // a = a7,a6,a5,a4,a3,a2,a1,a0
+    // b = b7,b6,b5,b4,b3,b2,b1,b0
+
+    // OUT
+    // a = b6,b4,b2,b0,a6,a4,a2,a0  // even
+    // b = b7,b5,b3,b1,a7,a5,a3,a1  // odd
+
+    __m128i A = *a;
+    __m128i B = *b;
+    __m128i ta, tb, odd, even;
+
+    ta   = _mm_srai_epi32 ( A, 16 );    // sign,a7,sign,a5,sign,a3,sign,a1
+    tb   = _mm_srai_epi32 ( B, 16 );    // sign,b7,sign,b5,sign,b3,sign,b1
+    odd  = _mm_packs_epi32( ta, tb );   //   b7,b5,  b3,b1,  a7,a5,  a3,a1
+
+    A    = _mm_slli_si128 ( A, 2 );     //   a6, 0,  a4, 0,  a2, 0,  a0, 0
+    B    = _mm_slli_si128 ( B, 2 );     //   b6, 0,  b4, 0,  b2, 0,  b0, 0
+    A    = _mm_srai_epi32 ( A, 16 );    // sign,a6,sign,a4,sign,a2,sign,a0
+    B    = _mm_srai_epi32 ( B, 16 );    // sign,b6,sign,b4,sign,b2,sign,b0                                        
+    even = _mm_packs_epi32( A, B );     //   b6,b4,  b2,b0,  a6,a4,  a2,a0
+
+    *a = even;
+    *b = odd;
+}
+
+
+/** */
+SSP_FORCEINLINE
+void ssp_convert_odd_even_ps_SSE2( __m128 *a, __m128 *b )
+{
+    // IN
+    // a = a3,a2,a1,a0
+    // b = b3,b2,b1,b0
+
+    // OUT
+    // a = b2,b0,a2,a0  // even
+    // b = b3,b1,a3,a1  // odd
+    
+    __m128 c, d;  
+    c = _mm_shuffle_ps( *a, *b, _MM_SHUFFLE(3,1,3,1) );
+    d = _mm_shuffle_ps( *a, *b, _MM_SHUFFLE(2,0,2,0) );
+    *a = c;
+    *b = d;     
+}
+
+/** */
+SSP_FORCEINLINE
+void ssp_convert_odd_even_epi32_SSE2( __m128i *a, __m128i *b )
+{
+    // IN
+    // a = a3,a2,a1,a0
+    // b = b3,b2,b1,b0
+
+    // OUT
+    // a = b2,b0,a2,a0  // even
+    // b = b3,b1,a3,a1  // odd
+    
+    ssp_m128 A,B;
+    A.i = *a;
+    B.i = *b;  
+
+    ssp_convert_odd_even_ps_SSE2( &A.f, &B.f );
+
+    *a = A.i;
+    *b = B.i;       
+}
+
+
+SSP_FORCEINLINE 
+void ssp_convert_3c_3p_epi8_SSE2( __m128i *rgb1, __m128i *rgb2, __m128i *rgb3)
+{
+    __m128i temp1, temp2;
+                                                            // RGB1 =         r5 , b4  g4  r4 , b3  g3  r3 , b2  g2  r2 , b1  g1  r1 , b0  g0 r0
+                                                            // RGB2 =     g10 r10, b9  g9  r9 , b8  g8  r8 , b7  g7  r7 , b6  g6  r6 , b5  g5   
+                                                            // RGB3 = b15 g15 r15, b14 g14 r14, b13 g13 r13, b12 g12 r12, b11 g11 r11, b10 
+
+
+    *rgb2 = _mm_shuffle_epi32(*rgb2, _MM_SHUFFLE(1,0,3,2));	// b7, g7, r7, b6, g6, r6, b5, g5,g10,r10, b9, g9, r9, b8, g8, r8
+    temp1 = _mm_unpacklo_epi8(*rgb1, *rgb2);				//g10, g2,r10, r2, b9, b1, g9, g1, r9, r1, b8, b0, g8, g0, r8, r0
+    temp2 = _mm_unpackhi_epi8(*rgb2, *rgb3);				//b15, b7,g15, g7,r15, r7,b14, b6,g14, g6,r14, r6,b13, b5,g13, g5
+    *rgb3 = _mm_slli_si128   (*rgb3, 8    );				//r13,b12,g12,r12,b11,g11,r11,b10,  0,  0,  0,  0,  0,  0,  0,  0
+    *rgb2 = _mm_unpackhi_epi8(*rgb1, *rgb3);				//r13, r5,b12, b4,g12, g4,r12, r4,b11, b3,g11, g3,r11, r3,b10, b2
+
+    *rgb3 = _mm_shuffle_epi32(*rgb2, _MM_SHUFFLE(1,0,3,2));	//b11, b3,g11, g3,r11, r3,b10, b2,r13, r5,b12, b4,g12, g4,r12, r4
+    *rgb1 = _mm_unpacklo_epi8(temp1, *rgb3);				//r13, r9, r5, r1,b12, b8, b4, b0,g12, g8, g4, g0,r12, r8, r4, r0
+    temp1 = _mm_srli_si128   (temp1, 8    );				//  0,  0,  0,  0,  0,  0,  0,  0,g10, g2,r10, r2, b9, b1, g9, g1
+    temp1 = _mm_unpacklo_epi8(temp1, temp2);				//g14,g10, g6, g2,r14,r10, r6, r2,b13, b9, b5, b1,g13, g9, g5, g1
+    temp2 = _mm_unpackhi_epi8(*rgb3, temp2);				//b15,b11, b7, b3,g15,g11, g7, g3,r15,r11, r7, r3,b14,b10, b6, b2
+
+    temp1 = _mm_shuffle_epi32(temp1, _MM_SHUFFLE(1,0,3,2)); //b13, b9, b5, b1,g13, g9, g5, g1,g14,g10, g6, g2,r14,r10, r6, r2
+    *rgb3 = _mm_unpackhi_epi8(temp1, temp2);				//b15,b13,b11, b9, b7, b5, b3, b1,g15,g13,g11, g9, g7, g5, g3, g1
+    temp2 = _mm_slli_si128   (temp2, 8    );				//r15,r11, r7, r3,b14,b10, b6, b2,  0,  0,  0,  0,  0,  0,  0,  0
+    temp2 = _mm_unpackhi_epi8(*rgb1, temp2);				//r15,r13,r11, r9, r7, r5, r3, r1,b14,b12,b10, b8, b6, b4, b2, b0
+    temp1 = _mm_unpacklo_epi8(*rgb1, temp1);				//g14,g12,g10, g8, g6, g4, g2, g0,r14,r12,r10, r8, r6, r4, r2, r0
+
+    temp2 = _mm_shuffle_epi32(temp2, _MM_SHUFFLE(1,0,3,2)); //b14,b12,b10, b8, b6, b4, b2, b0,r15,r13,r11, r9, r7, r5, r3, r1
+    *rgb1 = _mm_unpacklo_epi8(temp1, temp2);				//r15,r14,r13,r12,r11,r10, r9, r8, r7, r6, r5, r4, r3, r2, r1, r0	
+    temp1 = _mm_srli_si128   (temp1, 8    );				//  0,  0,  0,  0,  0,  0,  0,  0,g14,g12,g10, g8, g6, g4, g2, g0
+    *rgb2 = _mm_unpacklo_epi8(temp1, *rgb3);				//g15,g14,g13,g12,g11,g10, g9, g8, g7, g6, g5, g4, g3, g2, g1, g0	
+    *rgb3 = _mm_unpackhi_epi8(temp2, *rgb3);				//b15,b14,b13,b12,b11,b10, b9, b8, b7, b6, b5, b4, b3, b2, b1, b0	
+}
+
+
+//a: 9  6 3 0    3  2  1 0               
+//b: 10 7 4 1 -> 7  6  5 4       
+//c: 11 8 5 2    11 10 9 8
+SSP_FORCEINLINE
+void ssp_convert_reverse_transpose_SSE2( __m128i *a, __m128i *b, __m128i *c )
+{
+    ssp_m128 A, B, C, T1, T2, T3;
+    A.i = *a;   
+    B.i = *b;   
+    C.i = *c;  
+
+    T1.f = _mm_shuffle_ps( C.f,  A.f,  _MM_SHUFFLE( 3,1,2,0) ); // 9  3  8  2
+    T2.f = _mm_shuffle_ps( B.f,  A.f,  _MM_SHUFFLE( 2,0,2,0) ); // 6  0  7  1
+    T3.f = _mm_shuffle_ps( C.f,  B.f,  _MM_SHUFFLE( 3,1,3,1) ); // 10 4  11 5
+
+    A.f  = _mm_shuffle_ps( T2.f, T1.f, _MM_SHUFFLE( 2,0,0,2 ) ); //3  2  1  0  
+    B.f  = _mm_shuffle_ps( T3.f, T2.f, _MM_SHUFFLE( 1,3,0,2 ) ); //7  6  5  4  
+    C.f  = _mm_shuffle_ps( T1.f, T3.f, _MM_SHUFFLE( 1,3,3,1 ) ); //11 10 9  8   
+
+    *a = A.i;
+    *b = B.i;
+    *c = C.i; 
+}
+
+
+SSP_FORCEINLINE
+void ssp_convert_3p_3c_epi8_SSE2( __m128i *r, __m128i *g, __m128i *b )
+{
+    const static __m128i odd_8  = SSP_CONST_SET_8I(   0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0 );
+    const static __m128i even_8 = SSP_CONST_SET_8I( 0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF   );
+
+    const static __m128i odd_16  = SSP_CONST_SET_16I(   0xFFFF,0,0xFFFF,0,0xFFFF,0,0xFFFF,0 );
+    const static __m128i even_16 = SSP_CONST_SET_16I( 0,0xFFFF,0,0xFFFF,0,0xFFFF,0,0xFFFF   );    
+
+   ssp_m128 T, RG, EGB, BR, RGBR, GBRG, BRGB;
+    
+     RG.i = _mm_and_si128 (     *r, even_8  );  // Mask out the odd r bits
+      T.i = _mm_slli_epi16(     *g, 8       );  // Move the even g bits to the odd position
+     RG.i = _mm_or_si128  (   RG.i, T.i     );  // G14 R14 ... G2 R2 G0 R0
+
+     EGB.i = _mm_srli_epi16(     *g, 8       );
+      T.i = _mm_and_si128 (     *b, odd_8   );
+     EGB.i = _mm_or_si128  (   EGB.i, T.i     );
+
+     BR.i = _mm_and_si128 (     *b, even_8  );
+      T.i = _mm_and_si128 (     *r, odd_8   );
+     BR.i = _mm_or_si128  (   BR.i, T.i     );
+
+   RGBR.i = _mm_and_si128 (   RG.i, even_16 );
+      T.i = _mm_slli_epi32(   BR.i, 16      );
+   RGBR.i = _mm_or_si128  ( RGBR.i, T.i     );
+
+   GBRG.i = _mm_and_si128 (  EGB.i, even_16 );
+      T.i = _mm_and_si128 (   RG.i, odd_16  );
+   GBRG.i = _mm_or_si128  ( GBRG.i, T.i     );
+
+   BRGB.i = _mm_srli_epi32(   BR.i, 16      );
+      T.i = _mm_and_si128 (  EGB.i, odd_16  );
+   BRGB.i = _mm_or_si128  ( BRGB.i, T.i     );
+
+   ssp_convert_reverse_transpose_SSE2( &RGBR.i, &GBRG.i, &BRGB.i );
+
+   *r = RGBR.i;
+   *g = GBRG.i;
+   *b = BRGB.i; 
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3c_3p_epi16_SSE2(__m128i *rgb1,__m128i *rgb2,__m128i *rgb3)
+{
+		__m128i temp1, temp2;
+
+		*rgb2  = _mm_shuffle_epi32(*rgb2, _MM_SHUFFLE(1,0,3,2));//b3,g3,r3,b2,r5,b4,g4,r4
+		temp1 = _mm_unpacklo_epi16(*rgb1, *rgb2);				//r5,r1,b4,b0,g4,g0,r4,r0
+		temp2 = _mm_unpackhi_epi16(*rgb2, *rgb3);				//b7,b3,g7,g3,r7,r3,b6,b2
+		*rgb3  = _mm_slli_si128(*rgb3, 8);						//g6,r6,b5,g5, 0, 0, 0, 0
+		*rgb2  = _mm_unpackhi_epi16(*rgb1, *rgb3);				//g6,g2,r6,r2,b5,b1,g5,g1
+		
+		*rgb3  = _mm_shuffle_epi32(*rgb2, _MM_SHUFFLE(1,0,3,2));//b5,b1,g5,g1,g6,g2,r6,r2
+		*rgb1  = _mm_unpacklo_epi16(temp1, *rgb3);				//g6,g4,g2,g0,r6,r4,r2,r0
+		temp1 = _mm_srli_si128(temp1, 8);						// 0, 0, 0, 0,r5,r1,b4,b0
+		temp1 = _mm_unpacklo_epi16(temp1, temp2);				//r7,r5,r3,r1,b6,b4,b2,b0
+		temp2 = _mm_unpackhi_epi16(*rgb3, temp2);				//b7,b5,b3,b1,g7,g5,g3,g1
+
+		temp1 = _mm_shuffle_epi32(temp1, _MM_SHUFFLE(1,0,3,2)); //b6,b4,b2,b0,r7,r5,r3,r1
+		*rgb3  = _mm_unpackhi_epi16(temp1, temp2);				//b7,b6,b5,b4,b3,b2,b1,b0				
+		temp2 = _mm_slli_si128(temp2, 8);						//g7,g5,g3,g1, 0, 0, 0, 0
+		*rgb2  = _mm_unpackhi_epi16(*rgb1, temp2);				//g7,g6,g5,g4,g3,g2,g1,g0				
+		*rgb1  = _mm_unpacklo_epi16(*rgb1, temp1);				//r7,r6,r5,r4,r3,r2,r1,r0				
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3p_3c_epi16_SSE2(__m128i *r,__m128i *g,__m128i *b)
+{
+		__m128i temp;
+
+		temp = _mm_srli_si128(*r, 8);			// 0, 0, 0, 0,r7,r6,r5,r4
+		*r    = _mm_unpacklo_epi16(*r, temp);	//r7,r3,r6,r2,r5,r1,r4,r0
+		temp = _mm_srli_si128(*r, 8);			// 0, 0, 0, 0,r7,r3,r6,r2
+		*r    = _mm_unpacklo_epi16(*r, temp);	//r7,r5,r3,r1,r6,r4,r2,r0
+
+		temp = _mm_srli_si128(*g, 8);			//g7,g3,g6,g2,g5,g1,g4,g0
+		*g    = _mm_unpacklo_epi16(*g, temp);	// 0, 0, 0, 0,g7,g3,g6,g2
+		temp = _mm_srli_si128(*g, 8);			//g7,g5,g3,g1,g6,g4,g2,g0
+		*g    = _mm_unpacklo_epi16(*g, temp);	//g7,g5,g3,g1,g6,g4,g2,g0
+
+		temp = _mm_srli_si128(*b, 8);			//b7,b3,b6,b2,b5,b1,b4,b0
+		*b    = _mm_unpacklo_epi16(*b, temp);	// 0, 0, 0, 0,b7,b3,b6,b2
+		temp = _mm_srli_si128(*b, 8);			//b7,b5,b3,b1,b6,b4,b2,b0
+		*b    = _mm_unpacklo_epi16(*b, temp);	//b7,b5,b3,b1,b6,b4,b2,b0
+
+		temp = _mm_unpacklo_epi16(*r, *g);		//g6,r6,g4,r4,g2,r2,g0,r0
+		*r    = _mm_srli_si128(*r , 8);			// 0, 0, 0, 0,r7,r5,r3,r1
+		*r    = _mm_unpacklo_epi16(*b, *r);		//r7,b6,r5,b4,r3,b2,r1,b0
+		*g    = _mm_unpackhi_epi16(*g, *b);		//b7,g7,b5,g5,b3,g3,b1,g1
+
+		*b    = _mm_srli_si128(*r, 8);			// 0, 0, 0, 0,r7,b6,r5,b4
+		*r    = _mm_unpacklo_epi32(*r, *b);		//r7,b6,r3,b2,r5,b4,r1,b0
+		*b    = _mm_srli_si128(*g, 8);			// 0, 0, 0, 0,b7,g7,b5,g5
+		*g    = _mm_unpacklo_epi32(*g, *b);		//b7,g7,b3,g3,b5,g5,b1,g1
+		*b    = _mm_srli_si128(temp, 8);		// 0, 0, 0, 0,g6,r6,g4,r4
+		temp = _mm_unpacklo_epi32(temp, *b);	//g6,r6,g2,r2,g4,r4,g0,r0
+
+		*b    = _mm_unpacklo_epi32(temp, *g);	//b5,g5,g4,r4,b1,g1,g0,r0
+		temp = _mm_srli_si128(temp, 8);			// 0, 0, 0, 0,g6,r6,g2,r2
+		temp = _mm_unpacklo_epi32(*r, temp);	//g6,r6,r5,b4,g2,r2,r1,b0
+		*g    = _mm_unpackhi_epi32(*r, *g);		//b7,g7,r7,b6,b3,g3,r3,b2
+		
+		*r    = _mm_unpacklo_epi32(*b, temp);	//g2,r2,b1,g1,r1,b0,g0,r0
+		temp = _mm_unpackhi_epi32(*b, temp);	//g6,r6,b5,g5,r5,b4,g4,b4
+		*b    = _mm_unpackhi_epi64(temp, *g);	//b7,g7,r7,b6,g6,r6,b5,g5
+		*g    = _mm_unpacklo_epi64(*g, temp);	//r5,b4,g4,r4,b3,g3,r3,b2
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3c_3p_epi32_SSE2(__m128i *rgb1,__m128i *rgb2,__m128i *rgb3)
+{
+		__m128i temp1, temp2;
+		
+		*rgb2  = _mm_shuffle_epi32(*rgb2, _MM_SHUFFLE(1,0,3,2));//b1,g1,g2,r2
+		temp1 = _mm_unpacklo_epi32(*rgb1, *rgb2);				//g2,g0,r2,r0
+		temp2 = _mm_unpackhi_epi32(*rgb2, *rgb3);				//b3,b1,g3,g1
+		*rgb3  = _mm_slli_si128(*rgb3, 8);						//r3,b2, 0, 0
+		*rgb2  = _mm_unpackhi_epi32(*rgb1, *rgb3);				//r3,r1,b2,b0
+		
+		*rgb3  = _mm_shuffle_epi32(*rgb2, _MM_SHUFFLE(1,0,3,2));//b2,b0,r3,r1
+		*rgb1  = _mm_unpacklo_epi32(temp1, *rgb3);				//r3,r2,r1,r0
+		temp1 = _mm_srli_si128(temp1, 8);						// 0, 0,g2,g0
+		*rgb2  = _mm_unpacklo_epi32(temp1, temp2);				//g3,g2,g1,g0
+		*rgb3  = _mm_unpackhi_epi32(*rgb3, temp2);				//b3,b2,b1,b0
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_3p_3c_epi32_SSE2(__m128i *r,__m128i *g,__m128i *b)
+{
+		__m128i temp;
+
+		temp = _mm_srli_si128(*r, 8);			// 0, 0,r3,r2
+		*r    = _mm_unpacklo_epi32(*r, temp);	//r3,r1,r2,r0
+		temp = _mm_srli_si128(*g, 8);			// 0, 0,g3,g2
+		*g    = _mm_unpacklo_epi32(*g, temp);	//g3,g1,g2,g0
+		temp = _mm_srli_si128(*b, 8);			// 0, 0,b3,b2
+		*b    = _mm_unpacklo_epi32(*b, temp);	//b3,b1,b2,b0
+
+		temp = _mm_unpacklo_epi32(*r, *g);		//g2,r2,g0,r0
+		*g    = _mm_unpackhi_epi32(*g, *b);		//b3,g3,b1,g1
+		*r    = _mm_srli_si128(*r, 8);			// 0, 0,r3,r1
+		*b    = _mm_unpacklo_epi32(*b, *r);		//r3,b2,r1,b0
+
+		*r    = _mm_unpacklo_epi64(temp, *b);	//r1,b0,g0,r0
+		*b    = _mm_unpackhi_epi64(*b, *g);		//b3,g3,r3,b2
+		*g    = _mm_slli_si128(*g, 8);			//b1,g1, 0, 0
+		*g    = _mm_unpackhi_epi64(*g, temp);	//g2,r2,b1,g1
+}
+
+/* convert 4-channel RGBA to 4-planar format */
+SSP_FORCEINLINE 
+void ssp_convert_4c_4p_epi8_SSE2( __m128i *rgba1, __m128i *rgba2, __m128i *rgba3, __m128i *rgba4 )
+{
+		__m128i temp1,temp2;
+
+		temp1 = _mm_unpacklo_epi8(*rgba1, *rgba3);			// a9, a1, b9, b1, g9, g1, r9, r1, a8, a0, b8, b0, g8, g0, r8, r0
+		*rgba1 = _mm_unpackhi_epi8(*rgba1, *rgba3);			//a11, a3,b11, b3,g11, g3,r11, r3,a10, a2,b10, b2,g10, g2,r10, r2
+		*rgba3 = _mm_unpacklo_epi8(*rgba2, *rgba4);			//a13, a5,b13, b5,g13, g5,r13, r5,a12, a4,b12, b4,g12, g4,r12, r4
+		temp2 = _mm_unpackhi_epi8(*rgba2, *rgba4);			//a15, a7,b15, b7,g15, g7,r15, r7,a14, a6,b14, b6,g14, g6,r14, r6
+
+		*rgba4 = _mm_unpackhi_epi8(*rgba1, temp2);			//a15,a11, a7, a3,b15,b11, b7, b3,g15,g11, g7, g3,r15,r11, r7, r3
+		*rgba1 = _mm_unpacklo_epi8(*rgba1, temp2);			//a14,a10, a6, a2,b14,b10, b6, b2,g14,g10, g6, g2,r14,r10, r6, r2
+		*rgba2 = _mm_unpacklo_epi8(temp1, *rgba3);			//a12, a8, a4, a0,b12, b8, b4, b0,g12, g8, g4, g0,r12, r8, r4, r0
+		*rgba3 = _mm_unpackhi_epi8(temp1, *rgba3);			//a13, a9, a5, a1,b13, b9, b5, b1,g13, g9, g5, g1,r13, r9, r5, r1
+
+		temp1 = _mm_unpacklo_epi8(*rgba3, *rgba4);			//g15,g13,g11, g9, g7, g5, g3, g1,r15,r13,r11, r9, r7, r5, r3, r1
+		*rgba3 = _mm_unpackhi_epi8(*rgba3, *rgba4);			//a15,a13,a11, a9, a7, a5, a3, a1,b15,b13,b11, b9, b7, b5, b3, b1
+		temp2 = _mm_unpackhi_epi8(*rgba2, *rgba1);			//a14,a12,a10, a8, a6, a4, a2, a0,b14,b12,b10, b8, b6, b4, b2, b0
+		*rgba2 = _mm_unpacklo_epi8(*rgba2, *rgba1);			//g14,g12,g10, g8, g6, g4, g2, g0,r14,r12,r10, r8, r6, r4, r2, r0
+
+		*rgba1 = _mm_unpacklo_epi8(*rgba2, temp1);			//r15,r14,r13,r12,r11,r10, r9, r8, r7, r6, r5, r4, r3, r2, r1, r0
+		*rgba2 = _mm_unpackhi_epi8(*rgba2, temp1);			//g15,g14,g13,g12,g11,g10, g9, g8, g7, g6, g5, g4, g3, g2, g1, g0
+		*rgba4 = _mm_unpackhi_epi8(temp2, *rgba3);			//a15,a14,a13,a12,a11,a10, a9, a8, a7, a6, a5, a4, a3, a2, a1, a0
+		*rgba3 = _mm_unpacklo_epi8(temp2, *rgba3);			//b15,b14,b13,b12,b11,b10, b9, b8, b7, b6, b5, b4, b3, b2, b1, b0
+}
+
+/* convert 4-planar RGBA to 4-channel format */
+SSP_FORCEINLINE 
+void ssp_convert_4p_4c_epi8_SSE2(__m128i *r,__m128i *g,__m128i *b,__m128i *a)
+{
+		__m128i temp1, temp2;
+
+		temp1 = _mm_unpacklo_epi8(*r, *b);			// b7, r7, b6, r6, b5, r5, b4, r4, b3, r3, b2, r2, b1, r1, b0, r0
+		*r     = _mm_unpackhi_epi8(*r, *b);         //b15,r15,b14,r14,b13,r13,b12,r12,b11,r11,b10,r10, b9, r9, b8, r8
+		temp2 = _mm_unpacklo_epi8(*g, *a);			// a7, g7, a6, g6, a5, g5, a4, g4, a3, g3, a2, g2, a1, g1, a0, g0
+		*g     = _mm_unpackhi_epi8(*g, *a);			//a15,g15,a14,g14,a13,g13,a12,g12,a11,g11,a10,g10, a9, g9, a8, g8
+
+		*b     = _mm_unpacklo_epi8(*r, *g);			//a11,b11,g11,r11,a10,b10,g10,r10, a9, b9, g9, r9, a8, b8, g8, r8
+		*a     = _mm_unpackhi_epi8(*r, *g);			//a16,b16,g16,r16,a15,b15,g15,r15,a14, b1,g14,r14,a13,b13,g12,r12
+		*r     = _mm_unpacklo_epi8(temp1, temp2);	// a3, b3, g3, r3, a2, b2, g2, r2, a1, b1, g1, r1, a0, b0, g0, r0
+		*g     = _mm_unpackhi_epi8(temp1, temp2);	// a7, b7, g7, r7, a6, b6, g6, r6, a5, b5, g5, r5, a4, b4, g4, r4
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4c_4p_epi16_SSE2(__m128i *rgba1,__m128i *rgba2,__m128i *rgba3,__m128i *rgba4)
+{
+		__m128i temp1, temp2;
+
+		temp1  = _mm_unpacklo_epi16(*rgba1, *rgba3);		//a4,a0,b4,b0,g4,g0,r4,r0
+		*rgba1  = _mm_unpackhi_epi16(*rgba1, *rgba3);		//a5,a1,b5,b1,g5,g1,r5,r1
+		*rgba3  = _mm_unpacklo_epi16(*rgba2, *rgba4);		//a6,a2,b6,b2,g6,g2,r6,r2
+		*rgba2  = _mm_unpackhi_epi16(*rgba2, *rgba4);		//a7,a3,b7,b3,g7,g3,r7,r3
+
+		*rgba4  = _mm_unpackhi_epi16(*rgba1, *rgba2);		//a7,a5,a3,a1,b7,b5,b3,b1
+		*rgba1  = _mm_unpacklo_epi16(*rgba1, *rgba2);		//g7,g5,g3,g1,r7,r5,r3,r1
+		temp2  = _mm_unpacklo_epi16(temp1, *rgba3);			//g6,g4,g2,g0,r6,r4,r2,r0
+		temp1  = _mm_unpackhi_epi16(temp1, *rgba3);			//a6,a4,a2,a0,b6,b4,b2,b0
+
+		*rgba3  = _mm_unpacklo_epi16(temp1, *rgba4);		//b7,b6,b5,b4,b3,b2,b1,b0
+		*rgba4  = _mm_unpackhi_epi16(temp1, *rgba4);		//a7,a6,a5,a4,a3,a2,a1,a0
+		*rgba2  = _mm_unpackhi_epi16(temp2, *rgba1);		//g7,g6,g5,g4,g3,g2,g1,g0
+		*rgba1  = _mm_unpacklo_epi16(temp2, *rgba1);		//r7,r6,r5,r4,r3,r2,r1,r0
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4p_4c_epi16_SSE2(__m128i *r,__m128i *g,__m128i *b,__m128i *a)
+{
+		__m128i temp1, temp2;
+
+		temp1 = _mm_unpacklo_epi16(*r, *b);			//b3,r3,b2,r2,b1,r1,b0,r0
+		*r     = _mm_unpackhi_epi16(*r, *b);        //b7,r7,b6,r6,b5,r5,b4,r4
+		temp2 = _mm_unpacklo_epi16(*g, *a);			//a3,g3,a2,g2,a1,g1,a0,g0
+		*g     = _mm_unpackhi_epi16(*g, *a);		//a7,g7,a6,g6,a5,g5,a4,g4
+
+		*b     = _mm_unpacklo_epi16(*r, *g);		//a5,b5,g5,r5,a4,b4,g4,r4
+		*a     = _mm_unpackhi_epi16(*r, *g);		//a7,b7,g7,r7,a6,b6,g6,r6
+		*r     = _mm_unpacklo_epi16(temp1, temp2);	//a1,b1,g1,r1,a0,b0,g0,r0
+		*g     = _mm_unpackhi_epi16(temp1, temp2);	//a3,b3,g3,r3,a2,b2,g2,r2
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4c_4p_epi32_SSE2(__m128i *rgba1,__m128i *rgba2,__m128i *rgba3, __m128i *rgba4)
+{
+		__m128i temp1, temp2;
+
+		temp1  = _mm_unpacklo_epi32(*rgba1, *rgba3);		//g2,g0,r2,r0
+		*rgba1  = _mm_unpackhi_epi32(*rgba1, *rgba3);		//a2,a0,b2,b0
+		temp2  = _mm_unpacklo_epi32(*rgba2, *rgba4);		//g3,g1,r3,r1
+		*rgba2  = _mm_unpackhi_epi32(*rgba2, *rgba4);		//a3,a1,b3,b1
+
+		*rgba4  = _mm_unpackhi_epi32(*rgba1, *rgba2);		//a3,a2,a1,a0
+		*rgba3  = _mm_unpacklo_epi32(*rgba1, *rgba2);		//b3,b2,b1,b0
+		*rgba1  = _mm_unpacklo_epi32(temp1, temp2);			//r3,r2,r1,r0
+		*rgba2  = _mm_unpackhi_epi32(temp1, temp2);			//g3,g2,g1,g0
+}
+
+SSP_FORCEINLINE 
+void ssp_convert_4p_4c_epi32_SSE2(__m128i *r,__m128i *g,__m128i *b,__m128i *a)
+{
+		__m128i temp1, temp2;
+
+		temp1 = _mm_unpacklo_epi32(*r, *b);			//b1,r1,b0,r0
+		*r     = _mm_unpackhi_epi32(*r, *b);        //b3,r3,b2,r2
+		temp2 = _mm_unpacklo_epi32(*g, *a);			//a1,g1,a0,g0
+		*g     = _mm_unpackhi_epi32(*g, *a);		//a3,g3,a2,g2
+
+		*b     = _mm_unpacklo_epi32(*r, *g);		//a2,b2,g2,r2
+		*a     = _mm_unpackhi_epi32(*r, *g);		//a3,b3,g3,r3
+		*r     = _mm_unpacklo_epi32(temp1, temp2);	//a0,b0,g0,r0
+		*g     = _mm_unpackhi_epi32(temp1, temp2);	//a1,b1,g1,r1
+}
+
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_CONVERT_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,3308 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_EMULATION_REF_H__
+#define __SSEPLUS_EMULATION_REF_H__
+
+#include "../SSEPlus_base.h"
+#include "../number/SSEPlus_number_REF.h"
+#include "../arithmetic/SSEPlus_arithmetic_REF.h"
+#include "SSEPlus_emulation_comps_REF.h"
+
+#ifndef KERNEL
+#include <math.h>
+#endif
+
+/** @addtogroup emulated_REF   
+ *  @{ 
+ *  @name SSE[3,4A,...,5] implemented in reference
+ */
+
+
+//--------------------------------------
+// Extract Fraction
+//--------------------------------------
+/** \SSE5{Reference,_mm_frcz_pd_REF, frczpd  } */
+SSP_FORCEINLINE __m128d ssp_frcz_pd_REF(__m128d a)
+{
+	ssp_m128 A;
+	long long temp;
+
+	A.d = a;
+
+	temp = (long long) A.f64[0];
+	A.f64[0] -= temp;
+	temp = (long long) A.f64[1];
+	A.f64[1] -= temp;
+
+	return A.d;
+}
+
+/** \SSE5{Reference,_mm_frcz_ps_REF, frczps  } */
+SSP_FORCEINLINE __m128 ssp_frcz_ps_REF(__m128 a)
+{
+	ssp_m128 A;
+	int temp;
+	A.f = a;
+
+	temp = (int) A.f32[0];
+	A.f32[0] -= temp;
+	temp = (int) A.f32[1];
+	A.f32[1] -= temp;
+	temp = (int) A.f32[2];
+	A.f32[2] -= temp;
+	temp = (int) A.f32[3];
+	A.f32[3] -= temp;
+
+	return A.f;
+}
+
+/** \SSE5{Reference,_mm_frcz_sd_REF, frczsd  } */
+SSP_FORCEINLINE __m128d ssp_frcz_sd_REF(__m128d a, __m128d b)
+{
+	ssp_m128 A, B;
+	long long temp;
+
+	A.d = a;
+	B.d = b;
+
+	temp = (long long) A.f64[0];
+	B.f64[0] = A.f64[0] - temp;
+
+	return B.d;
+}
+
+/** \SSE5{Reference,_mm_frcz_ss_REF, frczss  } */
+SSP_FORCEINLINE __m128 ssp_frcz_ss_REF(__m128 a, __m128 b)
+{
+	ssp_m128 A, B;
+	int temp;
+
+	A.f = a;
+	B.f = b;
+
+	temp = (int) A.f32[0];
+	B.f32[0] = A.f32[0] - temp;
+
+	return B.f;
+}
+
+//--------------------------------------
+// Horizontal Add and Sub
+//--------------------------------------
+/** \SSE5{Reference,_mm_haddd_epi16, phaddwd  } */
+SSP_FORCEINLINE __m128i ssp_haddd_epi16_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s32[0] = A.s16[0] + A.s16[1];
+	B.s32[1] = A.s16[2] + A.s16[3];
+	B.s32[2] = A.s16[4] + A.s16[5];
+	B.s32[3] = A.s16[6] + A.s16[7];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddd_epi8, phaddbd  } */
+SSP_FORCEINLINE __m128i ssp_haddd_epi8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s32[0] = A.s8[ 0] + A.s8[ 1] + A.s8[ 2] + A.s8[ 3];
+	B.s32[1] = A.s8[ 4] + A.s8[ 5] + A.s8[ 6] + A.s8[ 7];
+	B.s32[2] = A.s8[ 8] + A.s8[ 9] + A.s8[10] + A.s8[11];
+	B.s32[3] = A.s8[12] + A.s8[13] + A.s8[14] + A.s8[15];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddd_epu16, phadduwd  } */
+SSP_FORCEINLINE __m128i ssp_haddd_epu16_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.u32[0] = A.u16[0] + A.u16[1];
+	B.u32[1] = A.u16[2] + A.u16[3];
+	B.u32[2] = A.u16[4] + A.u16[5];
+	B.u32[3] = A.u16[6] + A.u16[7];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddd_epu8, phaddubd  } */
+SSP_FORCEINLINE __m128i ssp_haddd_epu8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.u32[0] = A.u8[ 0] + A.u8[ 1] + A.u8[ 2] + A.u8[ 3];
+	B.u32[1] = A.u8[ 4] + A.u8[ 5] + A.u8[ 6] + A.u8[ 7];
+	B.u32[2] = A.u8[ 8] + A.u8[ 9] + A.u8[10] + A.u8[11];
+	B.u32[3] = A.u8[12] + A.u8[13] + A.u8[14] + A.u8[15];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddq_epi16, phaddwq  } */
+SSP_FORCEINLINE __m128i ssp_haddq_epi16_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s64[0] = A.s16[0] + A.s16[1] + A.s16[2] + A.s16[3];
+	B.s64[1] = A.s16[4] + A.s16[5] + A.s16[6] + A.s16[7];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddq_epi32, phadddq  } */
+SSP_FORCEINLINE __m128i ssp_haddq_epi32_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s64[0] = A.s32[0] + (long long)A.s32[1];
+	B.s64[1] = A.s32[2] + (long long)A.s32[3];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddq_epi8, phaddbq  } */
+SSP_FORCEINLINE __m128i ssp_haddq_epi8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s64[0] = A.s8[0] + A.s8[1] + A.s8[2] + A.s8[3] + A.s8[4] + A.s8[5] + A.s8[6] + A.s8[7];
+	B.s64[1] = A.s8[8] + A.s8[9] + A.s8[10] + A.s8[11] + A.s8[12] + A.s8[13] + A.s8[14] + A.s8[15];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddq_epu16, phadduwq  } */
+SSP_FORCEINLINE __m128i ssp_haddq_epu16_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.u64[0] = A.u16[0] + A.u16[1] + A.u16[2] + A.u16[3];
+	B.u64[1] = A.u16[4] + A.u16[5] + A.u16[6] + A.u16[7];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddq_epu32, phaddudq  } */
+SSP_FORCEINLINE __m128i ssp_haddq_epu32_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.u64[0] = A.u32[0] + (long long)A.u32[1];
+	B.u64[1] = A.u32[2] + (long long)A.u32[3];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddq_epu8, phaddubq  } */
+SSP_FORCEINLINE __m128i ssp_haddq_epu8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.u64[0] = A.u8[0] + A.u8[1] + A.u8[2] + A.u8[3] + A.u8[4] + A.u8[5] + A.u8[6] + A.u8[7];
+	B.u64[1] = A.u8[8] + A.u8[9] + A.u8[10] + A.u8[11] + A.u8[12] + A.u8[13] + A.u8[14] + A.u8[15];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddw_epi8, phaddbw  } */
+SSP_FORCEINLINE __m128i ssp_haddw_epi8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s16[0] = A.s8[0] + A.s8[1];
+	B.s16[1] = A.s8[2] + A.s8[3];
+	B.s16[2] = A.s8[4] + A.s8[5];
+	B.s16[3] = A.s8[6] + A.s8[7];
+	B.s16[4] = A.s8[8] + A.s8[9];
+	B.s16[5] = A.s8[10] + A.s8[11];
+	B.s16[6] = A.s8[12] + A.s8[13];
+	B.s16[7] = A.s8[14] + A.s8[15];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_haddw_epu8, phaddubw  } */
+SSP_FORCEINLINE __m128i ssp_haddw_epu8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.u16[0] = A.u8[0] + A.u8[1];
+	B.u16[1] = A.u8[2] + A.u8[3];
+	B.u16[2] = A.u8[4] + A.u8[5];
+	B.u16[3] = A.u8[6] + A.u8[7];
+	B.u16[4] = A.u8[8] + A.u8[9];
+	B.u16[5] = A.u8[10] + A.u8[11];
+	B.u16[6] = A.u8[12] + A.u8[13];
+	B.u16[7] = A.u8[14] + A.u8[15];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_hsubd_epi16, phsubwd  } */
+SSP_FORCEINLINE __m128i ssp_hsubd_epi16_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s32[0] = A.s16[1] - A.s16[0];
+	B.s32[1] = A.s16[3] - A.s16[2];
+	B.s32[2] = A.s16[5] - A.s16[4];
+	B.s32[3] = A.s16[7] - A.s16[6];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_hsubq_epi32, phsubdq  } */
+SSP_FORCEINLINE __m128i ssp_hsubq_epi32_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s64[0] = (long long)A.s32[1] - A.s32[0];
+	B.s64[1] = (long long)A.s32[3] - A.s32[2];
+
+	return B.i;
+}
+
+/** \SSE5{Reference,_mm_hsubw_epi8, phsubbw  } */
+SSP_FORCEINLINE __m128i ssp_hsubw_epi8_REF(__m128i a)
+{
+	ssp_m128 A, B;
+	A.i = a;
+
+	B.s16[0] = A.s8[1] - A.s8[0];
+	B.s16[1] = A.s8[3] - A.s8[2];
+	B.s16[2] = A.s8[5] - A.s8[4];
+	B.s16[3] = A.s8[7] - A.s8[6];
+	B.s16[4] = A.s8[9] - A.s8[8];
+	B.s16[5] = A.s8[11] - A.s8[10];
+	B.s16[6] = A.s8[13] - A.s8[12];
+	B.s16[7] = A.s8[15] - A.s8[14];
+
+	return B.i;
+}
+
+//--------------------------------------
+// Multiply Add
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_macc_epi16, pmacsww } */ 
+SSP_FORCEINLINE __m128i ssp_macc_epi16_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A,B,C;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    A.s16[0] = A.s16[0] * B.s16[0] + C.s16[0];
+    A.s16[1] = A.s16[1] * B.s16[1] + C.s16[1];
+    A.s16[2] = A.s16[2] * B.s16[2] + C.s16[2];
+    A.s16[3] = A.s16[3] * B.s16[3] + C.s16[3];
+    A.s16[4] = A.s16[4] * B.s16[4] + C.s16[4];
+    A.s16[5] = A.s16[5] * B.s16[5] + C.s16[5];
+    A.s16[6] = A.s16[6] * B.s16[6] + C.s16[6];
+    A.s16[7] = A.s16[7] * B.s16[7] + C.s16[7];
+
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_macc_epi32, pmacsdd } */ 
+SSP_FORCEINLINE __m128i ssp_macc_epi32_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A,B,C;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    A.s32[0] = A.s32[0] * B.s32[0] + C.s32[0];
+    A.s32[1] = A.s32[1] * B.s32[1] + C.s32[1];
+    A.s32[2] = A.s32[2] * B.s32[2] + C.s32[2];
+    A.s32[3] = A.s32[3] * B.s32[3] + C.s32[3];
+
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_macc_ps,fmaddps } */ 
+SSP_FORCEINLINE __m128 ssp_macc_ps_REF( __m128 a, __m128 b, __m128 c )
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = A.f32[0] * B.f32[0] + C.f32[0];
+    A.f32[1] = A.f32[1] * B.f32[1] + C.f32[1];
+    A.f32[2] = A.f32[2] * B.f32[2] + C.f32[2];
+    A.f32[3] = A.f32[3] * B.f32[3] + C.f32[3];
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_macc_pd,fmaddpd} */ 
+SSP_FORCEINLINE __m128d ssp_macc_pd_REF( __m128d a, __m128d b, __m128d c )
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = A.f64[0] * B.f64[0] + C.f64[0];
+    A.f64[1] = A.f64[1] * B.f64[1] + C.f64[1]; 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_macc_ss,fmaddss} */ 
+SSP_FORCEINLINE __m128 ssp_macc_ss_REF(__m128 a, __m128 b, __m128 c)   // Assuming SSE5 *_ss semantics are similar to _mm_add_ss. TODO: confirm
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = A.f32[0] * B.f32[0] + C.f32[0];   
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_macc_sd,fmaddss} */ 
+SSP_FORCEINLINE __m128d ssp_macc_sd_REF(__m128d a, __m128d b, __m128d c)   // Assuming SSE5 *_ss semantics are similar to _mm_add_ss. TODO: confirm
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = A.f64[0] * B.f64[0] + C.f64[0];   
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_maccd_epi16, pmacswd } */ 
+SSP_FORCEINLINE __m128i ssp_maccd_epi16_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    D.s32[0] = A.s16[0] * B.s16[0] + C.s32[0];
+    D.s32[1] = A.s16[2] * B.s16[2] + C.s32[1];
+    D.s32[2] = A.s16[4] * B.s16[4] + C.s32[2];
+    D.s32[3] = A.s16[6] * B.s16[6] + C.s32[3];
+
+    return D.i;
+}
+
+/** \SSE5{Reference,_mm_macchi_epi32, pmacsdqh } */ 
+SSP_FORCEINLINE __m128i ssp_macchi_epi32_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    D.s64[0] = A.s32[1] * B.s32[1] + C.s64[0];
+    D.s64[1] = A.s32[3] * B.s32[3] + C.s64[1];
+
+    return D.i;
+}
+
+/** \SSE5{Reference,_mm_macclo_epi32, pmacsdql } */ 
+SSP_FORCEINLINE __m128i ssp_macclo_epi32_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    D.s64[0] = A.s32[0] * B.s32[0] + C.s64[0];
+    D.s64[1] = A.s32[2] * B.s32[2] + C.s64[1];
+
+    return D.i;
+}
+
+#define SSP_SATURATION(a, pos_limit, neg_limit) (a>pos_limit) ? pos_limit : ((a<neg_limit)?neg_limit:a)
+
+/** \SSE5{Reference,_mm_maccs_epi16, pmacssww } */ 
+SSP_FORCEINLINE __m128i ssp_maccs_epi16_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C;
+	int temp;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+	temp = A.s16[0] * B.s16[0] + C.s16[0];
+	A.s16[0] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[1] * B.s16[1] + C.s16[1];
+    A.s16[1] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[2] * B.s16[2] + C.s16[2];
+    A.s16[2] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[3] * B.s16[3] + C.s16[3];
+    A.s16[3] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[4] * B.s16[4] + C.s16[4];
+    A.s16[4] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[5] * B.s16[5] + C.s16[5];
+    A.s16[5] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[6] * B.s16[6] + C.s16[6];
+    A.s16[6] = SSP_SATURATION(temp, 32767, -32768);
+	temp = A.s16[7] * B.s16[7] + C.s16[7];
+    A.s16[7] = SSP_SATURATION(temp, 32767, -32768);
+
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_maccs_epi32, pmacssdd } */ 
+SSP_FORCEINLINE __m128i ssp_maccs_epi32_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C;
+	long long temp;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+	temp = (long long)A.s32[0] * B.s32[0] + C.s32[0];
+	A.s32[0] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = (long long)A.s32[1] * B.s32[1] + C.s32[1];
+    A.s32[1] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = (long long)A.s32[2] * B.s32[2] + C.s32[2];
+    A.s32[2] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = (long long)A.s32[3] * B.s32[3] + C.s32[3];
+    A.s32[3] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_maccsd_epi16, pmacsswd } */ 
+SSP_FORCEINLINE __m128i ssp_maccsd_epi16_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+	long long temp;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+	//should be able to compare data to see whether overflow/underflow
+	temp = A.s16[0] * B.s16[0] + (long long)C.s32[0];
+    D.s32[0] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = A.s16[2] * B.s16[2] + (long long)C.s32[1];
+    D.s32[1] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = A.s16[4] * B.s16[4] + (long long)C.s32[2];
+    D.s32[2] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = A.s16[6] * B.s16[6] + (long long)C.s32[3];
+    D.s32[3] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+
+    return D.i;
+}
+
+/** \SSE5{Reference,_mm_maccshi_epi32, pmacssdqh } */ 
+SSP_FORCEINLINE __m128i ssp_maccshi_epi32_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+	long long temp;
+	unsigned long long signT, signC;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+	temp = (long long)A.s32[1] * B.s32[1];
+	signT = temp & 0x8000000000000000LL;
+	signC = C.s64[0] & 0x8000000000000000LL;
+	temp += C.s64[0];
+	D.s64[0] = (signT==signC) ? ((signT >0) ? ((temp > C.s64[0]) ? 0x8000000000000000LL : temp) 
+		: ((temp < C.s64[0])? 0x7FFFFFFFFFFFFFFFLL : temp)) : temp;
+	temp = (long long)A.s32[3] * B.s32[3];
+	signT = temp & 0x8000000000000000LL;
+	signC = C.s64[1] & 0x8000000000000000LL;
+	temp += C.s64[1];
+	D.s64[1] = (signT==signC) ? ((signT >0) ? ((temp > C.s64[1]) ? 0x8000000000000000LL : temp) 
+		: ((temp < C.s64[1])? 0x7FFFFFFFFFFFFFFFLL : temp)) : temp;
+
+    return D.i;
+}
+
+/** \SSE5{Reference,_mm_maccslo_epi32, pmacssdql } */ 
+SSP_FORCEINLINE __m128i ssp_maccslo_epi32_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+	long long temp;
+	unsigned long long signT, signC;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+	temp = (long long)A.s32[0] * B.s32[0];
+	signT = temp & 0x8000000000000000LL;
+	signC = C.s64[0] & 0x8000000000000000LL;
+	temp += C.s64[0];
+	D.s64[0] = (signT==signC) ? ((signT >0) ? ((temp > C.s64[0]) ? 0x8000000000000000LL : temp) 
+		: ((temp < C.s64[0])? 0x7FFFFFFFFFFFFFFFLL : temp)) : temp;
+	temp = (long long)A.s32[2] * B.s32[2];
+	signT = temp & 0x8000000000000000LL;
+	signC = C.s64[1] & 0x8000000000000000LL;
+	temp += C.s64[1];
+	D.s64[1] = (signT==signC) ? ((signT >0) ? ((temp > C.s64[1]) ? 0x8000000000000000LL : temp) 
+		: ((temp < C.s64[1])? 0x7FFFFFFFFFFFFFFFLL : temp)) : temp;
+
+    return D.i;
+}
+
+/** \SSE5{Reference,_mm_maddd_epi16, pmadcswd } */ 
+SSP_FORCEINLINE __m128i ssp_maddd_epi16_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    D.s32[0] = A.s16[0] * B.s16[0] + A.s16[1] * B.s16[1] + C.s32[0];
+    D.s32[1] = A.s16[2] * B.s16[2] + A.s16[3] * B.s16[3] + C.s32[1];
+    D.s32[2] = A.s16[4] * B.s16[4] + A.s16[5] * B.s16[5] + C.s32[2];
+    D.s32[3] = A.s16[6] * B.s16[6] + A.s16[7] * B.s16[7] + C.s32[3];
+
+    return D.i;
+}
+
+/** \SSE5{Reference,_mm_maddsd_epi16, pmadcsswd } */ 
+SSP_FORCEINLINE __m128i ssp_maddsd_epi16_REF( __m128i a, __m128i b, __m128i c )
+{
+    ssp_m128 A, B, C, D;
+	long long temp;
+
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+	temp = A.s16[0] * B.s16[0] + A.s16[1] * B.s16[1] + (long long)C.s32[0];
+    D.s32[0] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));
+	temp = A.s16[2] * B.s16[2] + A.s16[3] * B.s16[3] + (long long)C.s32[1];
+    D.s32[1] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));;
+	temp = A.s16[4] * B.s16[4] + A.s16[5] * B.s16[5] + (long long)C.s32[2];
+    D.s32[2] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));;
+	temp = A.s16[6] * B.s16[6] + A.s16[7] * B.s16[7] + (long long)C.s32[3];
+    D.s32[3] = (ssp_s32)(SSP_SATURATION(temp, 2147483647LL, -2147483648LL));;
+
+    return D.i;
+}
+
+//--------------------------------------
+// Negative Multiply Add
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_nmacc_ps,fnmaddps} */ 
+SSP_FORCEINLINE __m128 ssp_nmacc_ps_REF(__m128 a, __m128 b, __m128 c)
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = -(A.f32[0] * B.f32[0]) + C.f32[0];
+    A.f32[1] = -(A.f32[1] * B.f32[1]) + C.f32[1];
+    A.f32[2] = -(A.f32[2] * B.f32[2]) + C.f32[2];
+    A.f32[3] = -(A.f32[3] * B.f32[3]) + C.f32[3];
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_nmacc_pd,fnmaddpd} */ 
+SSP_FORCEINLINE __m128d ssp_nmacc_pd_REF(__m128d a, __m128d b, __m128d c)
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = -(A.f64[0] * B.f64[0]) + C.f64[0];
+    A.f64[1] = -(A.f64[1] * B.f64[1]) + C.f64[1]; 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_nmacc_ss,fnmaddss} */ 
+SSP_FORCEINLINE __m128 ssp_nmacc_ss_REF(__m128 a, __m128 b, __m128 c)
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = -(A.f32[0] * B.f32[0]) + C.f32[0];   
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_nmacc_sd,fnmaddsd} */ 
+SSP_FORCEINLINE __m128d ssp_nmacc_sd_REF(__m128d a, __m128d b, __m128d c)
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = -(A.f64[0] * B.f64[0]) + C.f64[0];   
+    return A.d;
+}
+
+
+//--------------------------------------
+// Multiply Subtract
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_msub_ps,fmsubps} */ 
+SSP_FORCEINLINE __m128 ssp_msub_ps_REF( __m128 a, __m128 b, __m128 c )
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = A.f32[0] * B.f32[0] - C.f32[0];
+    A.f32[1] = A.f32[1] * B.f32[1] - C.f32[1];
+    A.f32[2] = A.f32[2] * B.f32[2] - C.f32[2];
+    A.f32[3] = A.f32[3] * B.f32[3] - C.f32[3];
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_msub_pd,fmsubpd} */ 
+SSP_FORCEINLINE __m128d ssp_msub_pd_REF( __m128d a, __m128d b, __m128d c )
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = A.f64[0] * B.f64[0] - C.f64[0];
+    A.f64[1] = A.f64[1] * B.f64[1] - C.f64[1]; 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_msub_ss,fmsubss} */ 
+SSP_FORCEINLINE __m128 ssp_msub_ss_REF(__m128 a, __m128 b, __m128 c)   // Assuming SSE5 *_ss semantics are similar to _mm_add_ss. TODO: confirm
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = A.f32[0] * B.f32[0] - C.f32[0];   
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_msub_sd,fmsubss} */ 
+SSP_FORCEINLINE __m128d ssp_msub_sd_REF(__m128d a, __m128d b, __m128d c)   // Assuming SSE5 *_ss semantics are similar to _mm_add_ss. TODO: confirm
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = A.f64[0] * B.f64[0] - C.f64[0];   
+    return A.d;
+}
+
+//--------------------------------------
+// Negative Multiply Subtract
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_nmsub_ps,fnmsubps} */ 
+SSP_FORCEINLINE __m128 ssp_nmsub_ps_REF(__m128 a, __m128 b, __m128 c)
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = -(A.f32[0] * B.f32[0]) - C.f32[0];
+    A.f32[1] = -(A.f32[1] * B.f32[1]) - C.f32[1];
+    A.f32[2] = -(A.f32[2] * B.f32[2]) - C.f32[2];
+    A.f32[3] = -(A.f32[3] * B.f32[3]) - C.f32[3];
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_nmsub_pd,fnmsubpd} */ 
+SSP_FORCEINLINE __m128d ssp_nmsub_pd_REF(__m128d a, __m128d b, __m128d c)
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = -(A.f64[0] * B.f64[0]) - C.f64[0];
+    A.f64[1] = -(A.f64[1] * B.f64[1]) - C.f64[1]; 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_nmsub_ss,fnmsubss} */ 
+SSP_FORCEINLINE __m128 ssp_nmsub_ss_REF(__m128 a, __m128 b, __m128 c)
+{
+    ssp_m128 A,B,C;
+    A.f = a;
+    B.f = b;
+    C.f = c;
+
+    A.f32[0] = -(A.f32[0] * B.f32[0]) - C.f32[0];   
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_nmsub_sd,fnmsubsd} */ 
+SSP_FORCEINLINE __m128d ssp_nmsub_sd_REF(__m128d a, __m128d b, __m128d c)
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    B.d = b;
+    C.d = c;
+
+    A.f64[0] = -(A.f64[0] * B.f64[0]) - C.f64[0];   
+    return A.d;
+}
+
+
+
+//---------------------------------------
+// AddSubtract
+//---------------------------------------
+
+/** \SSE3{Reference,_mm_addsub_ps} */
+SSP_FORCEINLINE __m128 ssp_addsub_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A, B;
+    A.f = a;
+    B.f = b;
+
+    A.f32[0] -= B.f32[0];
+    A.f32[1] += B.f32[1];
+    A.f32[2] -= B.f32[2];
+    A.f32[3] += B.f32[3];
+    return A.f;
+}
+
+/** \SSE3{Reference,_mm_addsub_pd} */
+SSP_FORCEINLINE __m128d ssp_addsub_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A, B;
+    A.d = a;
+    B.d = b;
+
+    A.f64[0] -= B.f64[0];
+    A.f64[1] += B.f64[1];
+    return A.d;
+}
+
+//---------------------------------------
+//Blend
+//---------------------------------------
+
+/** \SSE4_1{Reference,_mm_blend_epi16} */
+SSP_FORCEINLINE __m128i ssp_blend_epi16_REF     ( __m128i a, __m128i b, const int mask )
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+
+    A.s16[0] = (mask & 0x01) ? B.s16[0] : A.s16[0];
+    A.s16[1] = (mask & 0x02) ? B.s16[1] : A.s16[1];
+    A.s16[2] = (mask & 0x04) ? B.s16[2] : A.s16[2];
+    A.s16[3] = (mask & 0x08) ? B.s16[3] : A.s16[3];
+    A.s16[4] = (mask & 0x10) ? B.s16[4] : A.s16[4];
+    A.s16[5] = (mask & 0x20) ? B.s16[5] : A.s16[5];
+    A.s16[6] = (mask & 0x40) ? B.s16[6] : A.s16[6];
+    A.s16[7] = (mask & 0x80) ? B.s16[7] : A.s16[7];
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_blend_pd} */
+SSP_FORCEINLINE __m128d ssp_blend_pd_REF        ( __m128d a, __m128d b, const int mask )
+{
+    ssp_m128 A, B;
+    A.d = a;
+    B.d = b;
+
+    A.f64[0] = (mask & 0x1) ? B.f64[0] : A.f64[0];
+    A.f64[1] = (mask & 0x2) ? B.f64[1] : A.f64[1];
+    return A.d;
+}
+
+/** \SSE4_1{Reference,_mm_blend_ps} */
+SSP_FORCEINLINE __m128 ssp_blend_ps_REF        ( __m128 a, __m128 b, const int mask )
+{
+    ssp_m128 A, B;
+    A.f = a;
+    B.f = b;
+
+    A.f32[0] = (mask & 0x1) ? B.f32[0] : A.f32[0];
+    A.f32[1] = (mask & 0x2) ? B.f32[1] : A.f32[1];
+    A.f32[2] = (mask & 0x4) ? B.f32[2] : A.f32[2];
+    A.f32[3] = (mask & 0x8) ? B.f32[3] : A.f32[3];
+    return A.f;
+}
+
+/** \SSE4_1{Reference,_mm_blendv_epi8} */
+SSP_FORCEINLINE __m128i ssp_blendv_epi8_REF     ( __m128i a, __m128i b, __m128i mask )
+{
+    ssp_m128 A, B, Mask;
+    A.i = a;
+    B.i = b;
+    Mask.i = mask;
+
+    A.s8[0]  = (Mask.s8[0]  & 0x80) ? B.s8[0]  : A.s8[0];
+    A.s8[1]  = (Mask.s8[1]  & 0x80) ? B.s8[1]  : A.s8[1];
+    A.s8[2]  = (Mask.s8[2]  & 0x80) ? B.s8[2]  : A.s8[2];
+    A.s8[3]  = (Mask.s8[3]  & 0x80) ? B.s8[3]  : A.s8[3];
+    A.s8[4]  = (Mask.s8[4]  & 0x80) ? B.s8[4]  : A.s8[4];
+    A.s8[5]  = (Mask.s8[5]  & 0x80) ? B.s8[5]  : A.s8[5];
+    A.s8[6]  = (Mask.s8[6]  & 0x80) ? B.s8[6]  : A.s8[6];
+    A.s8[7]  = (Mask.s8[7]  & 0x80) ? B.s8[7]  : A.s8[7];
+    A.s8[8]  = (Mask.s8[8]  & 0x80) ? B.s8[8]  : A.s8[8];
+    A.s8[9]  = (Mask.s8[9]  & 0x80) ? B.s8[9]  : A.s8[9];
+    A.s8[10] = (Mask.s8[10] & 0x80) ? B.s8[10] : A.s8[10];
+    A.s8[11] = (Mask.s8[11] & 0x80) ? B.s8[11] : A.s8[11];
+    A.s8[12] = (Mask.s8[12] & 0x80) ? B.s8[12] : A.s8[12];
+    A.s8[13] = (Mask.s8[13] & 0x80) ? B.s8[13] : A.s8[13];
+    A.s8[14] = (Mask.s8[14] & 0x80) ? B.s8[14] : A.s8[14];
+    A.s8[15] = (Mask.s8[15] & 0x80) ? B.s8[15] : A.s8[15];
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_blendv_pd} */
+SSP_FORCEINLINE __m128d ssp_blendv_pd_REF       ( __m128d a, __m128d b, __m128d mask )
+{
+    ssp_m128 A, B, Mask;
+    A.d = a;
+    B.d = b;
+    Mask.d = mask;
+
+    A.f64[0] = (Mask.u64[0] & 0x8000000000000000ll) ? B.f64[0] : A.f64[0];
+    A.f64[1] = (Mask.u64[1] & 0x8000000000000000ll) ? B.f64[1] : A.f64[1];
+    return A.d;
+}
+
+/** \SSE4_1{Reference,_mm_blendv_epi8} */
+SSP_FORCEINLINE __m128 ssp_blendv_ps_REF       ( __m128 a, __m128 b, __m128 mask )     
+{
+    ssp_m128 A, B, Mask;
+    A.f = a;
+    B.f = b;
+    Mask.f = mask;
+
+    A.f32[0] = (Mask.u32[0] & 0x80000000) ? B.f32[0] : A.f32[0];
+    A.f32[1] = (Mask.u32[1] & 0x80000000) ? B.f32[1] : A.f32[1];
+    A.f32[2] = (Mask.u32[2] & 0x80000000) ? B.f32[2] : A.f32[2];
+    A.f32[3] = (Mask.u32[3] & 0x80000000) ? B.f32[3] : A.f32[3];
+    return A.f;
+}
+
+
+//---------------------------------------
+//Compare
+//---------------------------------------
+/** \SSE4_1{Reference,_mm_cmpeq_epi64} */
+SSP_FORCEINLINE __m128i ssp_cmpeq_epi64_REF( __m128i a, __m128i b )                       
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+
+    if( A.s64[0] == B.s64[0] )
+        A.s64[0] = 0xFFFFFFFFFFFFFFFFll;
+    else
+        A.s64[0] = 0x0ll;
+
+    if( A.s64[1] == B.s64[1] )
+        A.s64[1] = 0xFFFFFFFFFFFFFFFFll;
+    else
+        A.s64[1] = 0x0ll;
+    return A.i;
+}
+
+//---------------------------------------
+// Dot Product
+//---------------------------------------
+/** \SSE4_1{Reference,_mm_dp_pd} */
+SSP_FORCEINLINE __m128d ssp_dp_pd_REF( __m128d a, __m128d b, const int mask )             
+{
+    ssp_f64 tmp[3];
+    ssp_m128 A, B;
+    A.d = a;
+    B.d = b;
+
+    tmp[0] = (mask & 0x10) ? (A.f64[0] * B.f64[0]) : 0.0;
+    tmp[1] = (mask & 0x20) ? (A.f64[1] * B.f64[1]) : 0.0;
+
+    tmp[2] = tmp[0] + tmp[1];
+
+    A.f64[0] = (mask & 0x1) ? tmp[2] : 0.0;
+    A.f64[1] = (mask & 0x2) ? tmp[2] : 0.0;
+    return A.d;
+}
+
+/** \SSE4_1{Reference,_mm_dp_ps} */
+SSP_FORCEINLINE __m128 ssp_dp_ps_REF( __m128 a, __m128 b, const int mask )                
+{
+    ssp_f32 tmp[5];
+    ssp_m128 A, B;
+    A.f = a;
+    B.f = b;
+
+    tmp[0] = (mask & 0x10) ? (A.f32[0] * B.f32[0]) : 0.0f;
+    tmp[1] = (mask & 0x20) ? (A.f32[1] * B.f32[1]) : 0.0f;
+    tmp[2] = (mask & 0x40) ? (A.f32[2] * B.f32[2]) : 0.0f;
+    tmp[3] = (mask & 0x80) ? (A.f32[3] * B.f32[3]) : 0.0f;
+
+    tmp[4] = tmp[0] + tmp[1] + tmp[2] + tmp[3];
+
+    A.f32[0] = (mask & 0x1) ? tmp[4] : 0.0f;
+    A.f32[1] = (mask & 0x2) ? tmp[4] : 0.0f;
+    A.f32[2] = (mask & 0x4) ? tmp[4] : 0.0f;
+    A.f32[3] = (mask & 0x8) ? tmp[4] : 0.0f;
+    return A.f;
+}
+
+/** \SSSE3{Reference,_mm_maddubs_epi16} */
+SSP_FORCEINLINE __m128i ssp_maddubs_epi16_REF( __m128i a,  __m128i b)
+{
+    ssp_m128 A, B, C;
+	int tmp[8];
+    A.i = a;
+    B.i = b;
+
+	// a is 8 bit unsigned integer, b is signed integer
+	tmp[0] = A.u8[0] * B.s8[0] +  A.u8[1] * B.s8[1];
+	C.s16[0] = (ssp_s16)(SSP_SATURATION(tmp[0], 32767, -32768));
+
+	tmp[1] = A.u8[2] * B.s8[2] +  A.u8[3] * B.s8[3];
+	C.s16[1] = (ssp_s16)(SSP_SATURATION(tmp[1], 32767, -32768));
+
+	tmp[2] = A.u8[4] * B.s8[4] +  A.u8[5] * B.s8[5];
+	C.s16[2] = (ssp_s16)(SSP_SATURATION(tmp[2], 32767, -32768));
+
+	tmp[3] = A.u8[6] * B.s8[6] +  A.u8[7] * B.s8[7];
+	C.s16[3] = (ssp_s16)(SSP_SATURATION(tmp[3], 32767, -32768));
+
+	tmp[4] = A.u8[8] * B.s8[8] +  A.u8[9] * B.s8[9];
+	C.s16[4] = (ssp_s16)(SSP_SATURATION(tmp[4], 32767, -32768));
+
+	tmp[5] = A.u8[10] * B.s8[10] +  A.u8[11] * B.s8[11];
+	C.s16[5] = (ssp_s16)(SSP_SATURATION(tmp[5], 32767, -32768));
+
+	tmp[6] = A.u8[12] * B.s8[12] +  A.u8[13] * B.s8[13];
+	C.s16[6] = (ssp_s16)(SSP_SATURATION(tmp[6], 32767, -32768));
+
+	tmp[7] = A.u8[14] * B.s8[14] +  A.u8[15] * B.s8[15];
+	C.s16[7] = (ssp_s16)(SSP_SATURATION(tmp[7], 32767, -32768));
+
+	return C.i;
+}
+
+/** \SSSE3{Reference,_mm_maddubs_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+ */
+//__m64 _mm_maddubs_pi16( __m64 a,  __m64 b); [SSSE3]
+SSP_FORCEINLINE __m64 ssp_maddubs_pi16_REF( __m64 a,  __m64 b)
+{
+    ssp_m64 A, B, C;
+	int tmp[4];
+    A.m64 = a;
+    B.m64 = b;
+
+	// a is 8 bit unsigned integer, b is signed integer
+	tmp[0] = A.u8[0] * B.s8[0] +  A.u8[1] * B.s8[1];
+	C.s16[0] = (ssp_s16)(SSP_SATURATION(tmp[0], 32767, -32768));
+
+	tmp[1] = A.u8[2] * B.s8[2] +  A.u8[3] * B.s8[3];
+	C.s16[1] = (ssp_s16)(SSP_SATURATION(tmp[1], 32767, -32768));
+
+	tmp[2] = A.u8[4] * B.s8[4] +  A.u8[5] * B.s8[5];
+	C.s16[2] = (ssp_s16)(SSP_SATURATION(tmp[2], 32767, -32768));
+
+	tmp[3] = A.u8[6] * B.s8[6] +  A.u8[7] * B.s8[7];
+	C.s16[3] = (ssp_s16)(SSP_SATURATION(tmp[3], 32767, -32768));
+
+	return C.m64;
+}
+
+//__m128i _mm_mulhrs_epi16( __m128i a,  __m128i b);
+/** \SSSE3{Reference,_mm_mulhrs_epi16} */
+SSP_FORCEINLINE __m128i ssp_mulhrs_epi16_REF( __m128i a, __m128i b )
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+	A.s16[0] = (ssp_s16) ((A.s16[0] * B.s16[0] + 0x4000) >> 15);
+	A.s16[1] = (ssp_s16) ((A.s16[1] * B.s16[1] + 0x4000) >> 15);
+	A.s16[2] = (ssp_s16) ((A.s16[2] * B.s16[2] + 0x4000) >> 15);
+	A.s16[3] = (ssp_s16) ((A.s16[3] * B.s16[3] + 0x4000) >> 15);
+	A.s16[4] = (ssp_s16) ((A.s16[4] * B.s16[4] + 0x4000) >> 15);
+	A.s16[5] = (ssp_s16) ((A.s16[5] * B.s16[5] + 0x4000) >> 15);
+	A.s16[6] = (ssp_s16) ((A.s16[6] * B.s16[6] + 0x4000) >> 15);
+	A.s16[7] = (ssp_s16) ((A.s16[7] * B.s16[7] + 0x4000) >> 15);
+
+    return A.i;
+}
+
+//__m64 _mm_mulhrs_epi16( __m64 a,  __m64 b);
+/** \SSSE3{Reference,_mm_mulhrs_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_mulhrs_pi16_REF( __m64 a, __m64 b )
+{
+    ssp_m64 A,B;
+    A.m64 = a;
+    B.m64 = b;
+
+	A.s16[0] = (ssp_s16) ((A.s16[0] * B.s16[0] + 0x4000) >> 15);
+	A.s16[1] = (ssp_s16) ((A.s16[1] * B.s16[1] + 0x4000) >> 15);
+	A.s16[2] = (ssp_s16) ((A.s16[2] * B.s16[2] + 0x4000) >> 15);
+	A.s16[3] = (ssp_s16) ((A.s16[3] * B.s16[3] + 0x4000) >> 15);
+
+    return A.m64;
+}
+
+//---------------------------------------
+//Extract
+//---------------------------------------
+
+// TODO PHS: Test the actual intrinsic to deterine what value is returned if he ndx/imm is a large number.
+//           ie.  for _mm_extract_epi8, what is returned if ndx = 20 [since 20=0x14 > 0x0F]?
+//           Repeat procedures for other extract functions.
+/** \SSE4_1{Reference,_mm_extract_epi8} */
+SSP_FORCEINLINE int ssp_extract_epi8_REF( __m128i a, const int ndx )                       
+{
+    ssp_m128 A;
+    A.i = a;
+    return (int)A.u8[ndx&0xF];
+}
+
+/** \SSE4_1{Reference,_mm_extract_epi32} */
+SSP_FORCEINLINE int ssp_extract_epi32_REF( __m128i a, const int imm )                            
+{
+    ssp_m128 A;
+    A.i = a;
+    return (int)A.u32[imm&0x3];
+}
+
+/** \SSE4_1{Reference,_mm_extract_epi64} */
+SSP_FORCEINLINE ssp_s64 ssp_extract_epi64_REF( __m128i a, const int ndx )                  
+{
+    ssp_m128 A;
+    A.i = a;
+    return A.s64[ndx & 0x1];
+}
+
+/** \SSE4_1{Reference,_mm_extract_ps} */
+SSP_FORCEINLINE int ssp_extract_ps_REF( __m128 a, const int ndx )                          
+{ 
+    ssp_m128 A;
+    A.f = a; 
+    return A.s32[ndx&0x3];
+}
+
+
+/**  \SSE4a{Reference,_mm_extract_si64} 
+\n  NOTE: The upper 64-bit of the destination register are undefined.
+*/
+SSP_FORCEINLINE __m128i ssp_extract_si64_REF( __m128i a ,__m128i b )        
+{
+    ssp_u32 len, ndx;
+    ssp_s64 mask;
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+    ndx = (ssp_u32)((B.u64[0] & 0x3F00) >> 8);    // Mask ndx field.
+    len = (ssp_u32)((B.u64[0] & 0x003F));         // Mask len field.
+
+    len = (len) ? len : 64;    
+    if( (ndx+len) > 64 )               // If the sum of ndx and length is greater than 64, the results are undefined.
+        return a;                      // If index = 0 and length = 0/64, extract all lower bits.
+    mask = ~(-1 << len);
+    A.u64[0] = A.u64[0] >> ndx;
+    A.u64[0] = A.u64[0] & mask;
+    return A.i;
+}
+/**  \SSE4a{Reference,_mm_extracti_si64}
+\n  NOTE: The upper 64-bits of the destination register are undefined.
+*/
+SSP_FORCEINLINE __m128i ssp_extracti_si64_REF( __m128i a, int len, int ndx )   
+{
+    ssp_s64 mask;
+    ssp_m128 A;
+    A.i = a;
+    ndx = ndx & 0x3F; // ndx % 64
+    len = len & 0x3F; // len % 64
+
+    len = (len) ? len : 64;    
+    if( (ndx+len) > 64 )               // If the sum of ndx and length is greater than 64, the results are undefined.
+        return a;                      // If index = 0 and length = 0/64, extract all lower bits.
+    mask = ~(-1 << len);
+    A.u64[0] = A.u64[0] >> ndx;
+    A.u64[0] = A.u64[0] & mask;
+    return A.i;
+}
+
+
+
+//---------------------------------------
+// Horizontal Add
+//---------------------------------------
+/** \SSSE3{Reference,_mm_hadd_epi16} */
+SSP_FORCEINLINE __m128i ssp_hadd_epi16_REF ( __m128i a, __m128i b )                       
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+
+    A.s16[0] = A.s16[0] + A.s16[1];
+    A.s16[1] = A.s16[2] + A.s16[3];
+    A.s16[2] = A.s16[4] + A.s16[5];
+    A.s16[3] = A.s16[6] + A.s16[7];
+    A.s16[4] = B.s16[0] + B.s16[1];
+    A.s16[5] = B.s16[2] + B.s16[3];
+    A.s16[6] = B.s16[4] + B.s16[5];
+    A.s16[7] = B.s16[6] + B.s16[7];
+    return A.i;
+}
+
+/** \SSSE3{Reference,_mm_hadd_epi32} */
+SSP_FORCEINLINE __m128i ssp_hadd_epi32_REF ( __m128i a, __m128i b )                        
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+
+    A.s32[0] = A.s32[0] + A.s32[1];
+    A.s32[1] = A.s32[2] + A.s32[3];
+    A.s32[2] = B.s32[0] + B.s32[1];
+    A.s32[3] = B.s32[2] + B.s32[3];
+
+    return A.i;
+}
+
+/** \SSSE3{Reference,_mm_hadd_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_hadd_pi16_REF ( __m64 a, __m64 b )                        
+{
+    ssp_m64 A, B;
+    A.m64 = a;
+    B.m64 = b;
+
+    A.s16[0] = A.s16[0] + A.s16[1];
+    A.s16[1] = A.s16[2] + A.s16[3];
+    A.s16[2] = B.s16[0] + B.s16[1];
+    A.s16[3] = B.s16[2] + B.s16[3];
+
+    return A.m64;
+}
+
+/** \SSSE3{Reference,_mm_add_pi32}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_hadd_pi32_REF ( __m64 a, __m64 b )                        
+{
+    ssp_m64 A, B;
+    A.m64 = a;
+    B.m64 = b;
+
+    A.s32[0] = A.s32[0] + A.s32[1];
+    A.s32[1] = B.s32[0] + B.s32[1];
+
+    return A.m64;
+}
+
+/** \SSSE3{Reference,_mm_hadds_epi16} */
+SSP_FORCEINLINE __m128i ssp_hadds_epi16_REF ( __m128i a, __m128i b )                         
+{
+    ssp_m128 A, B;
+	int answer[8];
+    A.i = a;
+    B.i = b;
+
+	answer[0] = A.s16[0] + A.s16[1];
+    A.s16[0]  = (ssp_s16) (SSP_SATURATION(answer[0], 32767, -32768));
+	answer[1] = A.s16[2] + A.s16[3];
+    A.s16[1]  = (ssp_s16) (SSP_SATURATION(answer[1], 32767, -32768));
+	answer[2] = A.s16[4] + A.s16[5];
+    A.s16[2]  = (ssp_s16) (SSP_SATURATION(answer[2], 32767, -32768));
+	answer[3] = A.s16[6] + A.s16[7];
+    A.s16[3]  = (ssp_s16) (SSP_SATURATION(answer[3], 32767, -32768));
+	answer[4] = B.s16[0] + B.s16[1];
+    A.s16[4]  = (ssp_s16) (SSP_SATURATION(answer[4], 32767, -32768));
+	answer[5] = B.s16[2] + B.s16[3];
+    A.s16[5]  = (ssp_s16) (SSP_SATURATION(answer[5], 32767, -32768));
+	answer[6] = B.s16[4] + B.s16[5];
+    A.s16[6]  = (ssp_s16) (SSP_SATURATION(answer[6], 32767, -32768));
+	answer[7] = B.s16[6] + B.s16[7];
+    A.s16[7]  = (ssp_s16) (SSP_SATURATION(answer[7], 32767, -32768));
+
+	return A.i;
+}
+
+/** \SSSE3{Reference,_mm_hadds_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_hadds_pi16_REF ( __m64 a, __m64 b )                         
+{
+    ssp_m64 A, B;
+	int answer[4];
+    A.m64 = a;
+    B.m64 = b;
+
+	answer[0] = A.s16[0] + A.s16[1];
+    A.s16[0]  = (ssp_s16) (SSP_SATURATION(answer[0], 32767, -32768));
+	answer[1] = A.s16[2] + A.s16[3];
+    A.s16[1]  = (ssp_s16) (SSP_SATURATION(answer[1], 32767, -32768));
+	answer[2] = B.s16[0] + B.s16[1];
+    A.s16[2]  = (ssp_s16) (SSP_SATURATION(answer[2], 32767, -32768));
+	answer[3] = B.s16[2] + B.s16[3];
+    A.s16[3]  = (ssp_s16) (SSP_SATURATION(answer[3], 32767, -32768));
+
+	return A.m64;
+}
+
+/** \SSSE3{Reference,_mm_hadd_ps} */
+SSP_FORCEINLINE __m128 ssp_hadd_ps_REF(__m128 a, __m128 b)                                 
+{
+    ssp_m128 A, B;
+    A.f = a;
+    B.f = b;
+
+    A.f32[0] = A.f32[0] + A.f32[1];
+    A.f32[1] = A.f32[2] + A.f32[3];
+    A.f32[2] = B.f32[0] + B.f32[1];
+    A.f32[3] = B.f32[2] + B.f32[3];
+    return A.f;
+}
+
+/** \SSSE3{Reference,_mm_hadd_pd} */
+SSP_FORCEINLINE __m128d ssp_hadd_pd_REF(__m128d a, __m128d b)                               
+{
+    ssp_m128 A, B;
+    A.d = a;
+    B.d = b;
+
+    A.f64[0] = A.f64[0] + A.f64[1];
+    A.f64[1] = B.f64[0] + B.f64[1];
+    return A.d;
+}
+
+
+//---------------------------------------
+// Horizontal Subtract
+//---------------------------------------
+/** \SSSE3{Reference,_mm_hsub_epi16} */
+SSP_FORCEINLINE __m128i ssp_hsub_epi16_REF ( __m128i a, __m128i b )                        
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+
+    A.s16[0] = A.s16[0] - A.s16[1];
+    A.s16[1] = A.s16[2] - A.s16[3];
+    A.s16[2] = A.s16[4] - A.s16[5];
+    A.s16[3] = A.s16[6] - A.s16[7];
+    A.s16[4] = B.s16[0] - B.s16[1];
+    A.s16[5] = B.s16[2] - B.s16[3];
+    A.s16[6] = B.s16[4] - B.s16[5];
+    A.s16[7] = B.s16[6] - B.s16[7];
+
+	return A.i;
+}
+
+/** \SSSE3{Reference,_mm_hsub_epi32} */
+SSP_FORCEINLINE __m128i ssp_hsub_epi32_REF ( __m128i a, __m128i b )                        
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+
+    A.s32[0] = A.s32[0] - A.s32[1];
+    A.s32[1] = A.s32[2] - A.s32[3];
+    A.s32[2] = B.s32[0] - B.s32[1];
+    A.s32[3] = B.s32[2] - B.s32[3];
+
+    return A.i;
+}
+
+/** \SSSE3{Reference,_mm_hsub_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_hsub_pi16_REF ( __m64 a, __m64 b )                         
+{
+    ssp_m64 A, B;
+    A.m64 = a;
+    B.m64 = b;
+
+    A.s16[0] = A.s16[0] - A.s16[1];
+    A.s16[1] = A.s16[2] - A.s16[3];
+    A.s16[2] = B.s16[0] - B.s16[1];
+    A.s16[3] = B.s16[2] - B.s16[3];
+
+	return A.m64;
+}
+
+/** \SSSE3{Reference,_mm_hsub_pi32}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_hsub_pi32_REF ( __m64 a, __m64 b )                         
+{
+    ssp_m64 A, B;
+    A.m64 = a;
+    B.m64 = b;
+
+    A.s32[0] = A.s32[0] - A.s32[1];
+    A.s32[1] = B.s32[0] - B.s32[1];
+
+    return A.m64;
+}
+
+/** \SSSE3{Reference,_mm_hsubs_epi16} */
+SSP_FORCEINLINE __m128i ssp_hsubs_epi16_REF ( __m128i a, __m128i b )                 
+{
+    ssp_m128 A, B;
+	int answer[8];
+    A.i = a;
+    B.i = b;
+
+	answer[0] = A.s16[0] - A.s16[1];
+    A.s16[0]  = (ssp_s16) (SSP_SATURATION(answer[0], 32767, -32768));
+	answer[1] = A.s16[2] - A.s16[3];
+    A.s16[1]  = (ssp_s16) (SSP_SATURATION(answer[1], 32767, -32768));
+	answer[2] = A.s16[4] - A.s16[5];
+    A.s16[2]  = (ssp_s16) (SSP_SATURATION(answer[2], 32767, -32768));
+	answer[3] = A.s16[6] - A.s16[7];
+    A.s16[3]  = (ssp_s16) (SSP_SATURATION(answer[3], 32767, -32768));
+	answer[4] = B.s16[0] - B.s16[1];
+    A.s16[4]  = (ssp_s16) (SSP_SATURATION(answer[4], 32767, -32768));
+	answer[5] = B.s16[2] - B.s16[3];
+    A.s16[5]  = (ssp_s16) (SSP_SATURATION(answer[5], 32767, -32768));
+	answer[6] = B.s16[4] - B.s16[5];
+    A.s16[6]  = (ssp_s16) (SSP_SATURATION(answer[6], 32767, -32768));
+	answer[7] = B.s16[6] - B.s16[7];
+    A.s16[7]  = (ssp_s16) (SSP_SATURATION(answer[7], 32767, -32768));
+
+	return A.i;
+}
+
+/** \SSSE3{Reference,_mm_hsubs_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_hsubs_pi16_REF ( __m64 a, __m64 b )                        
+{
+    ssp_m64 A, B;
+	int answer[4];
+    A.m64 = a;
+    B.m64 = b;
+
+	answer[0] = A.s16[0] - A.s16[1];
+    A.s16[0]  = (ssp_s16) (SSP_SATURATION(answer[0], 32767, -32768));
+	answer[1] = A.s16[2] - A.s16[3];
+    A.s16[1]  = (ssp_s16) (SSP_SATURATION(answer[1], 32767, -32768));
+	answer[2] = B.s16[0] - B.s16[1];
+    A.s16[2]  = (ssp_s16) (SSP_SATURATION(answer[2], 32767, -32768));
+	answer[3] = B.s16[2] - B.s16[3];
+    A.s16[3]  = (ssp_s16) (SSP_SATURATION(answer[3], 32767, -32768));
+
+	return A.m64;
+}
+
+/** \SSSE3{Reference,_mm_hsub_ps} */
+SSP_FORCEINLINE __m128 ssp_hsub_ps_REF(__m128 a, __m128 b)                           
+{
+    ssp_m128 A, B;
+    A.f = a;
+    B.f = b;
+
+    A.f32[0] = A.f32[0] - A.f32[1];
+    A.f32[1] = A.f32[2] - A.f32[3];
+    A.f32[2] = B.f32[0] - B.f32[1];
+    A.f32[3] = B.f32[2] - B.f32[3];
+    return A.f;
+}
+
+/** \SSSE3{Reference,_mm_hsub_pd} */
+SSP_FORCEINLINE __m128d ssp_hsub_pd_REF(__m128d a, __m128d b)                        
+{
+    ssp_m128 A, B;
+    A.d = a;
+    B.d = b;
+
+    A.f64[0] = A.f64[0] - A.f64[1];
+    A.f64[1] = B.f64[0] - B.f64[1];
+    return A.d;
+}
+
+//---------------------------------------
+//Insert
+//---------------------------------------
+/** \SSE4_1{Reference,_mm_insert_epi8} */
+SSP_FORCEINLINE __m128i ssp_insert_epi8_REF( __m128i a, int b, const int ndx )       // Verify behavior on Intel Hardware
+{
+    ssp_m128 A;
+    A.i = a;
+
+    A.s8[ndx & 0xF] = (ssp_s8)b;
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_insert_epi32} */
+SSP_FORCEINLINE __m128i ssp_insert_epi32_REF( __m128i a, int b, const int ndx )      // Verify behavior on Intel Hardware
+{
+    ssp_m128 A;
+    A.i = a;
+
+    A.s32[ndx & 0x3] = b;
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_insert_epi64} */
+SSP_FORCEINLINE __m128i ssp_insert_epi64_REF( __m128i a, ssp_s64 b, const int ndx )  // Verify behavior on Intel Hardware
+{
+    ssp_m128 A;
+    A.i = a;
+
+    A.s64[ndx & 0x1] = b;
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_insert_ps} */
+SSP_FORCEINLINE __m128 ssp_insert_ps_REF( __m128 a, __m128 b, const int sel )          // Verify behavior on Intel Hardware
+{
+    ssp_f32 tmp;
+    int count_d,zmask;
+
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+
+    tmp     = B.f32[(sel & 0xC0)>>6];   // 0xC0 = sel[7:6]
+    count_d = (sel & 0x30)>>4;          // 0x30 = sel[5:4]
+    zmask   = sel & 0x0F;               // 0x0F = sel[3:0]
+
+    A.f32[count_d] = tmp;
+
+    A.f32[0] = (zmask & 0x1) ? 0 : A.f32[0];
+    A.f32[1] = (zmask & 0x2) ? 0 : A.f32[1];
+    A.f32[2] = (zmask & 0x4) ? 0 : A.f32[2];
+    A.f32[3] = (zmask & 0x8) ? 0 : A.f32[3];
+    return A.f;
+}
+
+/** \SSE4a{Reference,_mm_insert_si64} */
+SSP_FORCEINLINE __m128i ssp_insert_si64_REF( __m128i a, __m128i b )
+{
+    ssp_u32  ndx, len;
+    ssp_s64  mask;
+    ssp_m128 A, B;
+    B.i = b;
+    ndx = (ssp_u32)((B.u64[1] & 0x3F00) >> 8);    // Mask length field.
+    len = (ssp_u32)((B.u64[1] & 0x003F));         // Mask ndx field.
+
+    if( ( (ndx + len) > 64 ) ||
+        ( (len == 0) && (ndx > 0) ) )
+        return a;
+
+    A.i = a;
+    if( (len == 0 ) && (ndx == 0) )
+    {
+        A.u64[0] = B.u64[0];
+        return A.i;
+    }
+
+    len = (len) ? len : 64;         // A value of zero for field length is interpreted as 64.
+    mask = ~(-1 << len);
+    B.u64[0]  = B.u64[0] & mask;
+    B.u64[0]  = B.u64[0] << ndx;
+    mask      = ~(mask << ndx);
+    A.u64[0]  = A.u64[0] & mask;
+    A.u64[0] |= B.u64[0];
+    return A.i;
+}
+
+/** \SSE4a{Reference,_mm_inserti_si64} */
+SSP_FORCEINLINE __m128i ssp_inserti_si64_REF( __m128i a, __m128i b, int len, int ndx )
+{
+    ssp_s64 mask;
+    ssp_m128 A, B;
+    A.i = a;
+    ndx = ndx & 0x3F; // ndx % 64
+    len = len & 0x3F; // len % 64
+
+    if( ( (ndx + len) > 64 ) ||
+        ( (len == 0) && (ndx > 0) ) )
+        return a;
+
+    B.i = b;
+    if( (len == 0 ) && (ndx == 0) )
+    {
+        A.u64[0] = B.u64[0];
+        return A.i;
+    }
+
+    len = (len) ? len : 64;         // A value of zero for field length is interpreted as 64.
+    mask = ~(-1 << len);
+    B.u64[0]  = B.u64[0] & mask;
+    B.u64[0]  = B.u64[0] << ndx;
+    mask      = ~(mask << ndx);
+    A.u64[0]  = A.u64[0] & mask;
+    A.u64[0] |= B.u64[0];
+    return A.i;
+}
+
+
+
+//---------------------------------------
+// Load
+//---------------------------------------
+/** \SSE3{Reference,_mm_loaddup_pd} */
+SSP_FORCEINLINE __m128d ssp_loaddup_pd_REF(double const * dp)                               
+{
+    ssp_m128 a;
+    a.f64[0] = *dp;
+    a.f64[1] = *dp;
+    return a.d;
+}
+
+/** \SSE3{Reference,_mm_lddqu_si128} */
+SSP_FORCEINLINE __m128i ssp_lddqu_si128_REF(__m128i const *p)                               
+{
+    return *p;
+}
+
+/** \SSE4_1{Reference,_mm_stream_load_si128} */
+SSP_FORCEINLINE __m128i ssp_stream_load_si128_REF( __m128i *p )                             
+{
+    return *p;
+}
+
+
+//---------------------------------------
+// Min / Max
+//---------------------------------------
+
+#define SSP_SET_MIN( sd, s) sd=(sd<s)?sd:s;
+#define SSP_SET_MAX( sd, s) sd=(sd>s)?sd:s;
+
+//8 bit min/max
+/** \SSE4_1{Reference,_mm_min_epi8} */
+SSP_FORCEINLINE __m128i ssp_min_epi8_REF( __m128i a, __m128i b )
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MIN( A.s8[ 0], B.s8[ 0] );
+    SSP_SET_MIN( A.s8[ 1], B.s8[ 1] );
+    SSP_SET_MIN( A.s8[ 2], B.s8[ 2] );
+    SSP_SET_MIN( A.s8[ 3], B.s8[ 3] );
+    SSP_SET_MIN( A.s8[ 4], B.s8[ 4] );
+    SSP_SET_MIN( A.s8[ 5], B.s8[ 5] );
+    SSP_SET_MIN( A.s8[ 6], B.s8[ 6] );
+    SSP_SET_MIN( A.s8[ 7], B.s8[ 7] );
+    SSP_SET_MIN( A.s8[ 8], B.s8[ 8] );
+    SSP_SET_MIN( A.s8[ 9], B.s8[ 9] );
+    SSP_SET_MIN( A.s8[10], B.s8[10] );
+    SSP_SET_MIN( A.s8[11], B.s8[11] );
+    SSP_SET_MIN( A.s8[12], B.s8[12] );
+    SSP_SET_MIN( A.s8[13], B.s8[13] );
+    SSP_SET_MIN( A.s8[14], B.s8[14] );
+    SSP_SET_MIN( A.s8[15], B.s8[15] );
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_max_epi8} */
+SSP_FORCEINLINE __m128i ssp_max_epi8_REF( __m128i a, __m128i b )
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MAX( A.s8[ 0], B.s8[ 0] );
+    SSP_SET_MAX( A.s8[ 1], B.s8[ 1] );
+    SSP_SET_MAX( A.s8[ 2], B.s8[ 2] );
+    SSP_SET_MAX( A.s8[ 3], B.s8[ 3] );
+    SSP_SET_MAX( A.s8[ 4], B.s8[ 4] );
+    SSP_SET_MAX( A.s8[ 5], B.s8[ 5] );
+    SSP_SET_MAX( A.s8[ 6], B.s8[ 6] );
+    SSP_SET_MAX( A.s8[ 7], B.s8[ 7] );
+    SSP_SET_MAX( A.s8[ 8], B.s8[ 8] );
+    SSP_SET_MAX( A.s8[ 9], B.s8[ 9] );
+    SSP_SET_MAX( A.s8[10], B.s8[10] );
+    SSP_SET_MAX( A.s8[11], B.s8[11] );
+    SSP_SET_MAX( A.s8[12], B.s8[12] );
+    SSP_SET_MAX( A.s8[13], B.s8[13] );
+    SSP_SET_MAX( A.s8[14], B.s8[14] );
+    SSP_SET_MAX( A.s8[15], B.s8[15] );
+    return A.i;
+}
+
+//16 bit min/max
+/** \SSE4_1{Reference,_mm_min_epu16} */
+SSP_FORCEINLINE __m128i ssp_min_epu16_REF ( __m128i a, __m128i b )
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MIN( A.u16[ 0], B.u16[ 0] );
+    SSP_SET_MIN( A.u16[ 1], B.u16[ 1] );
+    SSP_SET_MIN( A.u16[ 2], B.u16[ 2] );
+    SSP_SET_MIN( A.u16[ 3], B.u16[ 3] );
+    SSP_SET_MIN( A.u16[ 4], B.u16[ 4] );
+    SSP_SET_MIN( A.u16[ 5], B.u16[ 5] );
+    SSP_SET_MIN( A.u16[ 6], B.u16[ 6] );
+    SSP_SET_MIN( A.u16[ 7], B.u16[ 7] );
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_max_epu16} */
+SSP_FORCEINLINE __m128i ssp_max_epu16_REF ( __m128i a, __m128i b )
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MAX( A.u16[ 0], B.u16[ 0] );
+    SSP_SET_MAX( A.u16[ 1], B.u16[ 1] );
+    SSP_SET_MAX( A.u16[ 2], B.u16[ 2] );
+    SSP_SET_MAX( A.u16[ 3], B.u16[ 3] );
+    SSP_SET_MAX( A.u16[ 4], B.u16[ 4] );
+    SSP_SET_MAX( A.u16[ 5], B.u16[ 5] );
+    SSP_SET_MAX( A.u16[ 6], B.u16[ 6] );
+    SSP_SET_MAX( A.u16[ 7], B.u16[ 7] );
+    return A.i;
+}
+
+//32 bit min/max
+/** \SSE4_1{Reference,_mm_min_epi32} */
+SSP_FORCEINLINE __m128i ssp_min_epi32_REF( __m128i a, __m128i b )                     
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MIN( A.s32[ 0], B.s32[ 0] );
+    SSP_SET_MIN( A.s32[ 1], B.s32[ 1] );
+    SSP_SET_MIN( A.s32[ 2], B.s32[ 2] );
+    SSP_SET_MIN( A.s32[ 3], B.s32[ 3] );
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_max_epi32} */
+SSP_FORCEINLINE __m128i ssp_max_epi32_REF( __m128i a, __m128i b )                     
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MAX( A.s32[ 0], B.s32[ 0] );
+    SSP_SET_MAX( A.s32[ 1], B.s32[ 1] );
+    SSP_SET_MAX( A.s32[ 2], B.s32[ 2] );
+    SSP_SET_MAX( A.s32[ 3], B.s32[ 3] );
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_min_epu32} */
+SSP_FORCEINLINE __m128i ssp_min_epu32_REF ( __m128i a, __m128i b )                    
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MIN( A.u32[ 0], B.u32[ 0] );
+    SSP_SET_MIN( A.u32[ 1], B.u32[ 1] );
+    SSP_SET_MIN( A.u32[ 2], B.u32[ 2] );
+    SSP_SET_MIN( A.u32[ 3], B.u32[ 3] );
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_max_epu32} */
+SSP_FORCEINLINE __m128i ssp_max_epu32_REF ( __m128i a, __m128i b )                    
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    SSP_SET_MAX( A.u32[ 0], B.u32[ 0] );
+    SSP_SET_MAX( A.u32[ 1], B.u32[ 1] );
+    SSP_SET_MAX( A.u32[ 2], B.u32[ 2] );
+    SSP_SET_MAX( A.u32[ 3], B.u32[ 3] );
+    return A.i;
+}
+
+#undef SSP_SET_MIN
+#undef SSP_SET_MAX
+
+/** \SSE4_1{Reference,_mm_minpos_epu16} */
+SSP_FORCEINLINE __m128i ssp_minpos_epu16_REF( __m128i shortValues )                   
+{
+    ssp_m128 ShortValues;
+    ShortValues.i = shortValues;
+
+    if( ShortValues.u16[1] < ShortValues.u16[0] )
+    {
+        ShortValues.u16[0] = ShortValues.u16[1];
+        ShortValues.u16[1] = 1;
+    }
+    else
+        ShortValues.u16[1] = 0;
+
+
+#define FN( I )                                     \
+    if( ShortValues.u16[I] < ShortValues.u16[0] )   \
+    {                                               \
+        ShortValues.u16[0] = ShortValues.u16[I];    \
+        ShortValues.u16[1] = I;                     \
+    }
+
+    FN( 2 );
+    FN( 3 );
+    FN( 4 );
+    FN( 5 );
+    FN( 6 );
+    FN( 7 );
+
+    ShortValues.u32[1] = 0;
+    ShortValues.u64[1] = 0;
+
+#undef FN
+
+    return ShortValues.i;
+}
+
+/** \SSE4_1{Reference,_mm_minpos_epu16} */
+SSP_FORCEINLINE __m128i ssp_minpos_epu16_REFb( __m128i shortValues )                   
+{
+    ssp_m128 ShortValues;
+    ssp_u32 i;
+    ssp_u16 pos = 0;
+    ssp_u16 minVal;
+    ShortValues.i = shortValues;
+    minVal = ShortValues.u16[0];
+
+    for( i=1; i<8; ++i )
+    {
+        if( ShortValues.u16[i] < minVal )
+        {
+            minVal = ShortValues.u16[i];
+            pos    = i;
+        }
+
+        ShortValues.u16[i] = 0;
+    }
+
+    ShortValues.u16[0] = minVal;
+    ShortValues.u16[1] = pos;
+    return ShortValues.i;
+}
+
+
+//---------------------------------------
+// Move
+//---------------------------------------
+/** \SSE3{Reference,_mm_movehdup_ps} */
+SSP_FORCEINLINE __m128 ssp_movehdup_ps_REF(__m128 a)                                   
+{
+    ssp_m128 A;
+    A.f = a;
+
+    A.f32[0] = A.f32[1];
+    A.f32[2] = A.f32[3];
+    return A.f;
+}
+
+/** \SSE3{Reference,_mm_moveldup_ps} */
+SSP_FORCEINLINE __m128 ssp_moveldup_ps_REF(__m128 a)                                   
+{
+    ssp_m128 A;
+    A.f = a;
+
+    A.f32[1] = A.f32[0];
+    A.f32[3] = A.f32[2];
+    return A.f;
+}
+
+/** \SSE3{Reference,_mm_movedup_pd} */
+SSP_FORCEINLINE __m128d ssp_movedup_pd_REF(__m128d a)                                  
+{
+    ssp_m128 A;
+    A.d = a;
+
+    A.f64[1] = A.f64[0];
+    return A.d;
+}
+
+//---------------------------------------
+// Multiply
+//---------------------------------------
+/** \SSE4_1{Reference,_mm_mul_epi32} */
+SSP_FORCEINLINE __m128i ssp_mul_epi32_REF( __m128i a, __m128i b )                      
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    A.s64[0] = A.s32[0] * B.s32[0];
+    A.s64[1] = A.s32[2] * B.s32[2];
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_mullo_epi32} */
+SSP_FORCEINLINE __m128i ssp_mullo_epi32_REF( __m128i a, __m128i b )                    
+{
+    ssp_m128 t[2];
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    t[0].s64[0] = A.s32[0] * B.s32[0];
+    t[0].s64[1] = A.s32[1] * B.s32[1];
+    t[1].s64[0] = A.s32[2] * B.s32[2];
+    t[1].s64[1] = A.s32[3] * B.s32[3];    
+
+    A.s32[0] = t[0].s32[0];
+    A.s32[1] = t[0].s32[2];
+    A.s32[2] = t[1].s32[0];
+    A.s32[3] = t[1].s32[2];
+    return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_mpsadbw_epu8} */
+SSP_FORCEINLINE __m128i ssp_mpsadbw_epu8_REF ( __m128i a,   __m128i b,   const int msk  ) 
+{
+	ssp_u8 Abyte[11], Bbyte[4], tmp[4];
+	ssp_u8 Boffset, Aoffset;
+	int i;
+
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+	Boffset = (msk & 0x3) << 2; // *32/8,   for byte size count
+	Aoffset = (msk & 0x4);      // *32/8/4, for byte size count and shift msk to bit 2
+
+	for (i=0; i<11; i++)
+	{
+		Abyte[i] = A.u8[i+Aoffset];
+	}
+	
+	Bbyte[0] = B.u8[Boffset  ];
+	Bbyte[1] = B.u8[Boffset+1];
+	Bbyte[2] = B.u8[Boffset+2];
+	Bbyte[3] = B.u8[Boffset+3];
+
+	for (i=0; i<8; i++)
+	{
+		tmp[0] = (Abyte[i  ] > Bbyte[0]) ? (Abyte[i  ] - Bbyte[0]) :  (Bbyte[0] - Abyte[i  ]);        //abs diff
+		tmp[1] = (Abyte[i+1] > Bbyte[1]) ? (Abyte[i+1] - Bbyte[1]) :  (Bbyte[1] - Abyte[i+1]);
+		tmp[2] = (Abyte[i+2] > Bbyte[2]) ? (Abyte[i+2] - Bbyte[2]) :  (Bbyte[2] - Abyte[i+2]);
+		tmp[3] = (Abyte[i+3] > Bbyte[3]) ? (Abyte[i+3] - Bbyte[3]) :  (Bbyte[3] - Abyte[i+3]);
+
+		A.u16[i] = tmp[0] + tmp[1] + tmp[2] + tmp[3];
+	}
+
+	return A.i;
+}
+
+//---------------------------------------
+// Pack
+//---------------------------------------
+/** \SSE4_1{Reference,_mm_packus_epi32} */
+SSP_FORCEINLINE __m128i ssp_packus_epi32_REF( __m128i a, __m128i b )                       
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    if( A.s32[0] < 0 )
+        A.u16[0] = 0;
+    else
+        if( A.s32[0] > 0xFFFF )
+            A.u16[0] = 0xFFFF;
+        else
+            A.s16[0] = (ssp_u16)A.s32[0];
+
+    if( A.s32[1] < 0 )
+        A.u16[1] = 0;
+    else
+        if( A.s32[1] > 0xFFFF )
+            A.u16[1] = 0xFFFF;
+        else
+            A.s16[1] = (ssp_u16)A.s32[1];
+
+    if( A.s32[2] < 0 )
+        A.u16[2] = 0;
+    else
+        if( A.s32[2] > 0xFFFF )
+            A.u16[2] = 0xFFFF;
+        else
+            A.s16[2] = (ssp_u16)A.s32[2];
+
+
+    if( A.s32[3] < 0 )
+        A.u16[3] = 0;
+    else
+        if( A.s32[3] > 0xFFFF )
+            A.u16[3] = 0xFFFF;
+        else
+            A.s16[3] = (ssp_u16)A.s32[3];
+
+    if( B.s32[0] < 0 )
+        A.u16[4] = 0;
+    else
+        if( B.s32[0] > 0xFFFF )
+            A.u16[4] = 0xFFFF;
+        else
+            A.s16[4] = (ssp_u16)B.s32[0];
+
+    if( B.s32[1] < 0 )
+        A.u16[5] = 0;
+    else
+        if( B.s32[1] > 0xFFFF )
+            A.u16[5] = 0xFFFF;
+        else
+            A.s16[5] = (ssp_u16)B.s32[1];
+
+    if( B.s32[2] < 0 )
+        A.u16[6] = 0;
+    else
+        if( B.s32[2] > 0xFFFF )
+            A.u16[6] = 0xFFFF;
+        else
+            A.s16[6] = (ssp_u16)B.s32[2];
+
+
+    if( B.s32[3] < 0 )
+        A.u16[7] = 0;
+    else
+        if( B.s32[3] > 0xFFFF )
+            A.u16[7] = 0xFFFF;
+        else
+            A.s16[7] = (ssp_u16)B.s32[3];
+
+    return A.i;
+}
+
+//---------------------------------------
+// Type Conversion
+//---------------------------------------
+/** \SSE4_1{Reference,_mm_cvtepi8_epi16} */
+SSP_FORCEINLINE __m128i ssp_cvtepi8_epi16_REF ( __m128i a)                                  
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s16[7] = A.s8[7];
+	A.s16[6] = A.s8[6];
+	A.s16[5] = A.s8[5];
+	A.s16[4] = A.s8[4];
+	A.s16[3] = A.s8[3];
+	A.s16[2] = A.s8[2];
+	A.s16[1] = A.s8[1];
+	A.s16[0] = A.s8[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepi8_epi32} */
+SSP_FORCEINLINE __m128i ssp_cvtepi8_epi32_REF ( __m128i a)                                  
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s32[3] = A.s8[3];
+	A.s32[2] = A.s8[2];
+	A.s32[1] = A.s8[1];
+	A.s32[0] = A.s8[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepi8_epi64} */
+SSP_FORCEINLINE __m128i ssp_cvtepi8_epi64_REF ( __m128i a)                                  
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s64[1] = A.s8[1];
+	A.s64[0] = A.s8[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepi16_epi32} */
+SSP_FORCEINLINE __m128i ssp_cvtepi16_epi32_REF ( __m128i a)                                 
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s32[3] = A.s16[3];
+	A.s32[2] = A.s16[2];
+	A.s32[1] = A.s16[1];
+	A.s32[0] = A.s16[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepi16_epi64} */
+SSP_FORCEINLINE __m128i ssp_cvtepi16_epi64_REF ( __m128i a)                                 
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s64[1] = A.s16[1];
+	A.s64[0] = A.s16[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepi32_epi64} */
+SSP_FORCEINLINE __m128i ssp_cvtepi32_epi64_REF ( __m128i a)                                 
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s64[1] = A.s32[1];
+    A.s64[0] = A.s32[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepu8_epi16} */
+SSP_FORCEINLINE __m128i ssp_cvtepu8_epi16_REF ( __m128i a)                                  
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s16[7] = A.u8[7];
+	A.s16[6] = A.u8[6];
+	A.s16[5] = A.u8[5];
+	A.s16[4] = A.u8[4];
+	A.s16[3] = A.u8[3];
+	A.s16[2] = A.u8[2];
+	A.s16[1] = A.u8[1];
+	A.s16[0] = A.u8[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepu8_epi32} */
+SSP_FORCEINLINE __m128i ssp_cvtepu8_epi32_REF ( __m128i a)                                  
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s32[3] = A.u8[3];
+	A.s32[2] = A.u8[2];
+	A.s32[1] = A.u8[1];
+	A.s32[0] = A.u8[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepu8_epi64} */
+SSP_FORCEINLINE __m128i ssp_cvtepu8_epi64_REF ( __m128i a)                                  
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s64[1] = A.u8[1];
+	A.s64[0] = A.u8[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepu16_epi32} */
+SSP_FORCEINLINE __m128i ssp_cvtepu16_epi32_REF ( __m128i a)                                 
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s32[3] = A.u16[3];
+	A.s32[2] = A.u16[2];
+	A.s32[1] = A.u16[1];
+	A.s32[0] = A.u16[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepu16_epi64} */
+SSP_FORCEINLINE __m128i ssp_cvtepu16_epi64_REF ( __m128i a)                                 
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s64[1] = A.u16[1];
+	A.s64[0] = A.u16[0];
+	return A.i;
+}
+
+/** \SSE4_1{Reference,_mm_cvtepu32_epi64} */
+SSP_FORCEINLINE __m128i ssp_cvtepu32_epi64_REF ( __m128i a)                                 
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s64[1] = A.u32[1];
+	A.s64[0] = A.u32[0];
+	return A.i;
+}
+
+//SSSE3
+//__m128i _mm_abs_epi8(__m128i a);
+/** \SSSE3{Reference,_mm_abs_epi8} */
+SSP_FORCEINLINE __m128i ssp_abs_epi8_REF (__m128i a)
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s8[0]  = (A.s8[0] < 0) ? -A.s8[0]  : A.s8[0];
+	A.s8[1]  = (A.s8[1] < 0) ? -A.s8[1]  : A.s8[1];
+	A.s8[2]  = (A.s8[2] < 0) ? -A.s8[2]  : A.s8[2];
+	A.s8[3]  = (A.s8[3] < 0) ? -A.s8[3]  : A.s8[3];
+	A.s8[4]  = (A.s8[4] < 0) ? -A.s8[4]  : A.s8[4];
+	A.s8[5]  = (A.s8[5] < 0) ? -A.s8[5]  : A.s8[5];
+	A.s8[6]  = (A.s8[6] < 0) ? -A.s8[6]  : A.s8[6];
+	A.s8[7]  = (A.s8[7] < 0) ? -A.s8[7]  : A.s8[7];
+	A.s8[8]  = (A.s8[8] < 0) ? -A.s8[8]  : A.s8[8];
+	A.s8[9]  = (A.s8[9] < 0) ? -A.s8[9]  : A.s8[9];
+	A.s8[10] = (A.s8[10]< 0) ? -A.s8[10] : A.s8[10];
+	A.s8[11] = (A.s8[11]< 0) ? -A.s8[11] : A.s8[11];
+	A.s8[12] = (A.s8[12]< 0) ? -A.s8[12] : A.s8[12];
+	A.s8[13] = (A.s8[13]< 0) ? -A.s8[13] : A.s8[13];
+	A.s8[14] = (A.s8[14]< 0) ? -A.s8[14] : A.s8[14];
+	A.s8[15] = (A.s8[15]< 0) ? -A.s8[15] : A.s8[15];
+
+	return A.i;
+}
+
+//__m128i _mm_abs_epi16(__m128i a);
+/** \SSSE3{Reference,_mm_abs_epi16} */
+SSP_FORCEINLINE __m128i ssp_abs_epi16_REF (__m128i a)
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s16[0]  = (A.s16[0] < 0) ? -A.s16[0]  : A.s16[0];
+	A.s16[1]  = (A.s16[1] < 0) ? -A.s16[1]  : A.s16[1];
+	A.s16[2]  = (A.s16[2] < 0) ? -A.s16[2]  : A.s16[2];
+	A.s16[3]  = (A.s16[3] < 0) ? -A.s16[3]  : A.s16[3];
+	A.s16[4]  = (A.s16[4] < 0) ? -A.s16[4]  : A.s16[4];
+	A.s16[5]  = (A.s16[5] < 0) ? -A.s16[5]  : A.s16[5];
+	A.s16[6]  = (A.s16[6] < 0) ? -A.s16[6]  : A.s16[6];
+	A.s16[7]  = (A.s16[7] < 0) ? -A.s16[7]  : A.s16[7];
+
+	return A.i;
+}
+
+/**  \SSSE3{Reference,_mm_abs_epi32} */
+SSP_FORCEINLINE __m128i ssp_abs_epi32_REF (__m128i a)
+{
+    ssp_m128 A;
+    A.i = a;
+
+	A.s32[0]  = (A.s32[0] < 0) ? -A.s32[0]  : A.s32[0];
+	A.s32[1]  = (A.s32[1] < 0) ? -A.s32[1]  : A.s32[1];
+	A.s32[2]  = (A.s32[2] < 0) ? -A.s32[2]  : A.s32[2];
+	A.s32[3]  = (A.s32[3] < 0) ? -A.s32[3]  : A.s32[3];
+
+	return A.i;
+}
+
+/** \SSSE3{Reference,_mm_abs_pi8}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+ */
+SSP_FORCEINLINE __m64 ssp_abs_pi8_REF (__m64 a)
+{
+    ssp_m64 A;
+    A.m64 = a;
+
+	A.s8[0]  = (A.s8[0] < 0) ? -A.s8[0]  : A.s8[0];
+	A.s8[1]  = (A.s8[1] < 0) ? -A.s8[1]  : A.s8[1];
+	A.s8[2]  = (A.s8[2] < 0) ? -A.s8[2]  : A.s8[2];
+	A.s8[3]  = (A.s8[3] < 0) ? -A.s8[3]  : A.s8[3];
+	A.s8[4]  = (A.s8[4] < 0) ? -A.s8[4]  : A.s8[4];
+	A.s8[5]  = (A.s8[5] < 0) ? -A.s8[5]  : A.s8[5];
+	A.s8[6]  = (A.s8[6] < 0) ? -A.s8[6]  : A.s8[6];
+	A.s8[7]  = (A.s8[7] < 0) ? -A.s8[7]  : A.s8[7];
+
+	return A.m64;
+}
+
+//__m64 _mm_abs_pi16( __m64 a);
+/** \SSSE3{Reference,_mm_abs_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_abs_pi16_REF (__m64 a)
+{
+    ssp_m64 A;
+    A.m64 = a;
+
+	A.s16[0]  = (A.s16[0] < 0) ? -A.s16[0]  : A.s16[0];
+	A.s16[1]  = (A.s16[1] < 0) ? -A.s16[1]  : A.s16[1];
+	A.s16[2]  = (A.s16[2] < 0) ? -A.s16[2]  : A.s16[2];
+	A.s16[3]  = (A.s16[3] < 0) ? -A.s16[3]  : A.s16[3];
+
+	return A.m64;
+}
+
+//__m64 _mm_abs_pi32( __m64 a);
+/** \SSSE3{Reference,_mm_abs_pi32}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_abs_pi32_REF (__m64 a)
+{
+    ssp_m64 A;
+    A.m64 = a;
+
+	A.s32[0]  = (A.s32[0] < 0) ? -A.s32[0]  : A.s32[0];
+	A.s32[1]  = (A.s32[1] < 0) ? -A.s32[1]  : A.s32[1];
+
+	return A.m64;
+}
+
+// bit manipulation
+//__m128i _mm_alignr_epi8(__m128i a, __m128i b, const int ralign);
+/**  \SSSE3{Reference,_mm_alignr_epi8} */
+SSP_FORCEINLINE __m128i ssp_alignr_epi8_REF (__m128i a, __m128i b, const int ralign)
+{
+    ssp_m128 C[3];
+	ssp_s8 * tmp;
+	int i, j;
+
+	if (ralign <0) return b; //only shift to right, no negative
+	C[2].i = _mm_setzero_si128();
+	if (ralign > 32) return C[2].i;
+    C[1].i = a;
+	C[0].i = b;
+	tmp = & (C[0].s8[0]);
+
+	for (i=ralign+15, j=15; i >=ralign; i--, j--) {
+		C[2].s8[j] = tmp[i];
+	}
+
+	return C[2].i;
+}
+
+/**  \SSSE3{Reference,_mm_alignr_pi8}
+ \n \b NOTE: The user must call _mm_empty() after a call to this function.
+ */
+SSP_FORCEINLINE __m64 ssp_alignr_pi8_REF (__m64 a, __m64 b, const int ralign)
+{
+    ssp_m64 C[3];
+    ssp_s8 * tmp;
+    int i, j;
+    
+    if (ralign <0) return b; //only shift to right, no negative
+    C[2].u32[0] = 0;
+    C[2].u32[1] = 0;
+    if (ralign > 16) return C[2].m64;
+    C[1].m64 = a;
+    C[0].m64 = b;
+    tmp = & (C[0].s8[0]);
+    
+    for (i=ralign+7, j=7; i >=ralign; i--, j--) {
+        C[2].s8[j] = tmp[i];
+    }
+    
+    return C[2].m64;
+}
+
+//__m128i _mm_shuffle_epi8( __m128i a, __m128i mask);
+/**  \SSSE3{Reference,_mm_shuffle_epi8} */
+SSP_FORCEINLINE __m128i ssp_shuffle_epi8_REF (__m128i a, __m128i mask)
+{
+    ssp_m128 A, MSK, B;
+	A.i = a;
+	MSK.i = mask;
+
+	B.s8[0]  = (MSK.s8[0]  & 0x80) ? 0 : A.s8[(MSK.s8[0]  & 0xf)];
+	B.s8[1]  = (MSK.s8[1]  & 0x80) ? 0 : A.s8[(MSK.s8[1]  & 0xf)];
+	B.s8[2]  = (MSK.s8[2]  & 0x80) ? 0 : A.s8[(MSK.s8[2]  & 0xf)];
+	B.s8[3]  = (MSK.s8[3]  & 0x80) ? 0 : A.s8[(MSK.s8[3]  & 0xf)];
+	B.s8[4]  = (MSK.s8[4]  & 0x80) ? 0 : A.s8[(MSK.s8[4]  & 0xf)];
+	B.s8[5]  = (MSK.s8[5]  & 0x80) ? 0 : A.s8[(MSK.s8[5]  & 0xf)];
+	B.s8[6]  = (MSK.s8[6]  & 0x80) ? 0 : A.s8[(MSK.s8[6]  & 0xf)];
+	B.s8[7]  = (MSK.s8[7]  & 0x80) ? 0 : A.s8[(MSK.s8[7]  & 0xf)];
+	B.s8[8]  = (MSK.s8[8]  & 0x80) ? 0 : A.s8[(MSK.s8[8]  & 0xf)];
+	B.s8[9]  = (MSK.s8[9]  & 0x80) ? 0 : A.s8[(MSK.s8[9]  & 0xf)];
+	B.s8[10] = (MSK.s8[10] & 0x80) ? 0 : A.s8[(MSK.s8[10] & 0xf)];
+	B.s8[11] = (MSK.s8[11] & 0x80) ? 0 : A.s8[(MSK.s8[11] & 0xf)];
+	B.s8[12] = (MSK.s8[12] & 0x80) ? 0 : A.s8[(MSK.s8[12] & 0xf)];
+	B.s8[13] = (MSK.s8[13] & 0x80) ? 0 : A.s8[(MSK.s8[13] & 0xf)];
+	B.s8[14] = (MSK.s8[14] & 0x80) ? 0 : A.s8[(MSK.s8[14] & 0xf)];
+	B.s8[15] = (MSK.s8[15] & 0x80) ? 0 : A.s8[(MSK.s8[15] & 0xf)];
+
+	return B.i;
+}
+
+/**  \SSSE3{Reference,_mm_shuffle_pi8}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_shuffle_pi8_REF (__m64 a, __m64 mask)
+{
+    ssp_m64 A, MSK, B;
+	A.m64 = a;
+	MSK.m64 = mask;
+
+	B.s8[0]  = (MSK.s8[0]  & 0x80) ? 0 : A.s8[(MSK.s8[0]  & 0x7)];
+	B.s8[1]  = (MSK.s8[1]  & 0x80) ? 0 : A.s8[(MSK.s8[1]  & 0x7)];
+	B.s8[2]  = (MSK.s8[2]  & 0x80) ? 0 : A.s8[(MSK.s8[2]  & 0x7)];
+	B.s8[3]  = (MSK.s8[3]  & 0x80) ? 0 : A.s8[(MSK.s8[3]  & 0x7)];
+	B.s8[4]  = (MSK.s8[4]  & 0x80) ? 0 : A.s8[(MSK.s8[4]  & 0x7)];
+	B.s8[5]  = (MSK.s8[5]  & 0x80) ? 0 : A.s8[(MSK.s8[5]  & 0x7)];
+	B.s8[6]  = (MSK.s8[6]  & 0x80) ? 0 : A.s8[(MSK.s8[6]  & 0x7)];
+	B.s8[7]  = (MSK.s8[7]  & 0x80) ? 0 : A.s8[(MSK.s8[7]  & 0x7)];
+
+	return B.m64;
+}
+
+//__m64 _mm_sign_pi8( __m64 a, __m64 b);
+/**  \SSSE3{Reference,_mm_sign_pi8} */
+/** AnV - Compile fix **/
+SSP_FORCEINLINE __m64 ssp_sign_pi8_REF (__m64 a, __m64 b)
+{
+    ssp_m64 A, B;
+    A.m64 = a;
+    B.m64 = b;
+    
+    A.s8[0]  = (B.s8[0]<0)  ? (-A.s8[0])  :((B.s8[0]==0) ? 0: A.s8[0]);
+    A.s8[1]  = (B.s8[1]<0)  ? (-A.s8[1])  :((B.s8[1]==0) ? 0: A.s8[1]);
+    A.s8[2]  = (B.s8[2]<0)  ? (-A.s8[2])  :((B.s8[2]==0) ? 0: A.s8[2]);
+    A.s8[3]  = (B.s8[3]<0)  ? (-A.s8[3])  :((B.s8[3]==0) ? 0: A.s8[3]);
+    A.s8[4]  = (B.s8[4]<0)  ? (-A.s8[4])  :((B.s8[4]==0) ? 0: A.s8[4]);
+    A.s8[5]  = (B.s8[5]<0)  ? (-A.s8[5])  :((B.s8[5]==0) ? 0: A.s8[5]);
+    A.s8[6]  = (B.s8[6]<0)  ? (-A.s8[6])  :((B.s8[6]==0) ? 0: A.s8[6]);
+    A.s8[7]  = (B.s8[7]<0)  ? (-A.s8[7])  :((B.s8[7]==0) ? 0: A.s8[7]);
+    
+    return A.m64;
+}
+
+//Negate the number
+//__m128i _mm_sign_epi8( __m128i a, __m128i b);
+/**  \SSSE3{Reference,_mm_sign_epi8} */
+SSP_FORCEINLINE __m128i ssp_sign_epi8_REF (__m128i a, __m128i b)
+{
+    ssp_m128 A, B;
+	A.i = a;
+	B.i = b;
+
+	A.s8[0]  = (B.s8[0]<0)  ? (-A.s8[0])  :((B.s8[0]==0) ? 0: A.s8[0]);
+	A.s8[1]  = (B.s8[1]<0)  ? (-A.s8[1])  :((B.s8[1]==0) ? 0: A.s8[1]);
+	A.s8[2]  = (B.s8[2]<0)  ? (-A.s8[2])  :((B.s8[2]==0) ? 0: A.s8[2]);
+	A.s8[3]  = (B.s8[3]<0)  ? (-A.s8[3])  :((B.s8[3]==0) ? 0: A.s8[3]);
+	A.s8[4]  = (B.s8[4]<0)  ? (-A.s8[4])  :((B.s8[4]==0) ? 0: A.s8[4]);
+	A.s8[5]  = (B.s8[5]<0)  ? (-A.s8[5])  :((B.s8[5]==0) ? 0: A.s8[5]);
+	A.s8[6]  = (B.s8[6]<0)  ? (-A.s8[6])  :((B.s8[6]==0) ? 0: A.s8[6]);
+	A.s8[7]  = (B.s8[7]<0)  ? (-A.s8[7])  :((B.s8[7]==0) ? 0: A.s8[7]);
+	A.s8[8]  = (B.s8[8]<0)  ? (-A.s8[8])  :((B.s8[8]==0) ? 0: A.s8[8]);
+	A.s8[9]  = (B.s8[9]<0)  ? (-A.s8[9])  :((B.s8[9]==0) ? 0: A.s8[9]);
+	A.s8[10] = (B.s8[10]<0) ? (-A.s8[10]) :((B.s8[10]==0)? 0: A.s8[10]);
+	A.s8[11] = (B.s8[11]<0) ? (-A.s8[11]) :((B.s8[11]==0)? 0: A.s8[11]);
+	A.s8[12] = (B.s8[12]<0) ? (-A.s8[12]) :((B.s8[12]==0)? 0: A.s8[12]);
+	A.s8[13] = (B.s8[13]<0) ? (-A.s8[13]) :((B.s8[13]==0)? 0: A.s8[13]);
+	A.s8[14] = (B.s8[14]<0) ? (-A.s8[14]) :((B.s8[14]==0)? 0: A.s8[14]);
+	A.s8[15] = (B.s8[15]<0) ? (-A.s8[15]) :((B.s8[15]==0)? 0: A.s8[15]);
+
+	return A.i;
+}
+
+//__m128i _mm_sign_epi16( __m128i a, __m128i b);
+/**  \SSSE3{Reference,_mm_sign_epi16} */
+SSP_FORCEINLINE __m128i ssp_sign_epi16_REF (__m128i a, __m128i b)
+{
+    ssp_m128 A, B;
+	A.i = a;
+	B.i = b;
+
+	A.s16[0]  = (B.s16[0]<0)  ? (-A.s16[0])  :((B.s16[0]==0) ? 0: A.s16[0]);
+	A.s16[1]  = (B.s16[1]<0)  ? (-A.s16[1])  :((B.s16[1]==0) ? 0: A.s16[1]);
+	A.s16[2]  = (B.s16[2]<0)  ? (-A.s16[2])  :((B.s16[2]==0) ? 0: A.s16[2]);
+	A.s16[3]  = (B.s16[3]<0)  ? (-A.s16[3])  :((B.s16[3]==0) ? 0: A.s16[3]);
+	A.s16[4]  = (B.s16[4]<0)  ? (-A.s16[4])  :((B.s16[4]==0) ? 0: A.s16[4]);
+	A.s16[5]  = (B.s16[5]<0)  ? (-A.s16[5])  :((B.s16[5]==0) ? 0: A.s16[5]);
+	A.s16[6]  = (B.s16[6]<0)  ? (-A.s16[6])  :((B.s16[6]==0) ? 0: A.s16[6]);
+	A.s16[7]  = (B.s16[7]<0)  ? (-A.s16[7])  :((B.s16[7]==0) ? 0: A.s16[7]);
+
+	return A.i;
+}
+
+//__m128i _mm_sign_epi32( __m128i a, __m128i b);
+/**  \SSSE3{Reference,_mm_sign_epi32} */
+SSP_FORCEINLINE __m128i ssp_sign_epi32_REF (__m128i a, __m128i b)
+{
+    ssp_m128 A, B;
+	A.i = a;
+	B.i = b;
+
+	A.s32[0]  = (B.s32[0]<0)  ? (-A.s32[0])  :((B.s32[0]==0) ? 0: A.s32[0]);
+	A.s32[1]  = (B.s32[1]<0)  ? (-A.s32[1])  :((B.s32[1]==0) ? 0: A.s32[1]);
+	A.s32[2]  = (B.s32[2]<0)  ? (-A.s32[2])  :((B.s32[2]==0) ? 0: A.s32[2]);
+	A.s32[3]  = (B.s32[3]<0)  ? (-A.s32[3])  :((B.s32[3]==0) ? 0: A.s32[3]);
+
+	return A.i;
+}
+
+//__m64 _mm_sign_pi16( __m64 a, __m64 b);
+/**  \SSSE3{Reference,_mm_sign_pi16}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_sign_pi16_REF (__m64 a, __m64 b)
+{
+    ssp_m64 A, B;
+	A.m64 = a;
+	B.m64 = b;
+
+	A.s16[0]  = (B.s16[0]<0)  ? (-A.s16[0])  :((B.s16[0]==0) ? 0: A.s16[0]);
+	A.s16[1]  = (B.s16[1]<0)  ? (-A.s16[1])  :((B.s16[1]==0) ? 0: A.s16[1]);
+	A.s16[2]  = (B.s16[2]<0)  ? (-A.s16[2])  :((B.s16[2]==0) ? 0: A.s16[2]);
+	A.s16[3]  = (B.s16[3]<0)  ? (-A.s16[3])  :((B.s16[3]==0) ? 0: A.s16[3]);
+
+	return A.m64;
+}
+
+//__m64 _mm_sign_pi32( __m64 a, __m64 b);
+/**  \SSSE3{Reference,_mm_sign_pi32}
+\n \b NOTE: The user must call _mm_empty() after a call to this function.
+*/
+SSP_FORCEINLINE __m64 ssp_sign_pi32_REF (__m64 a, __m64 b)
+{
+    ssp_m64 A, B;
+	A.m64 = a;
+	B.m64 = b;
+
+	A.s32[0]  = (B.s32[0]<0)  ? (-A.s32[0])  :((B.s32[0]==0) ? 0: A.s32[0]);
+	A.s32[1]  = (B.s32[1]<0)  ? (-A.s32[1])  :((B.s32[1]==0) ? 0: A.s32[1]);
+
+	return A.m64;
+}
+
+/** \SSE4a{Reference,_mm_stream_sd} */ 
+SSP_FORCEINLINE void ssp_stream_sd_REF( double *dst ,__m128d src )
+{
+    ssp_m128 SRC;
+    SRC.d = src;
+    *dst = SRC.f64[0];
+}
+
+/** \SSE4a{Reference,_mm_stream_ss} */ 
+SSP_FORCEINLINE void ssp_stream_ss_REF( float *dst, __m128 src )
+{
+    ssp_m128 SRC;
+    SRC.f = src;
+    *dst = SRC.f32[0];
+}
+
+//---------------------------------------
+// Leading Zeros Count
+//---------------------------------------
+/** \SSE4a{Reference,__lzcnt16} */ 
+SSP_FORCEINLINE unsigned short ssp_lzcnt16_REF( unsigned short val )
+{
+    
+    if( !val )
+        return 16;
+    // Binary Search Tree of possible output values
+    else if( val > 0x00FF )
+    {
+        if( val > 0x0FFF )
+        {
+            if( val > 0x3FFF )
+            {
+                if( val > 0x7FFF )
+                    return 0;
+                else
+                    return 1;
+            }
+            else // val < 0x3FFF
+            {
+                if( val > 0x1FFF )
+                    return 2;
+                else
+                    return 3;
+            }
+        }
+        else // val < 0x0FFF
+        {
+            if( val > 0x03FF )
+            {
+                if( val > 0x07FF )
+                    return 4;
+                else
+                    return 5;
+            }
+            else // val < 0x03FF
+            {
+                if( val > 0x01FF )
+                    return 6;
+                else
+                    return 7;
+            }
+        }
+    }
+    else // val < 0x00FF
+    {
+        if( val > 0x000F )
+        {
+            if( val > 0x003F  )
+            {
+                if( val > 0x007F  )
+                    return 8;
+                else
+                    return 9;
+            }
+            else // val < 0x003F
+            {
+                if( val > 0x001F)
+                    return 10;
+                else
+                    return 11;
+            }
+        }
+        else // val < 0x000F
+        {
+            if( val > 0x0003  )
+            {
+                if( val > 0x0007  )
+                    return 12;
+                else
+                    return 13;
+            }
+            else // val < 0x0003
+            {
+                if( val > 0x0001)
+                    return 14;
+                else
+                    return 15;
+            }
+        }
+    }
+}
+/** \SSE4a{Reference,__lzcnt} */ 
+SSP_FORCEINLINE unsigned int ssp_lzcnt_REF( unsigned int val )
+{
+    ssp_u32 cnt;
+    cnt = ssp_lzcnt16_REF( (ssp_u16)(val>>16) );
+    if( cnt == 16 )
+        cnt += ssp_lzcnt16_REF( (ssp_u16)(val & 0x0000FFFF) );
+    return cnt;
+}
+/** \SSE4a{Reference,__lzcnt64} */ 
+SSP_FORCEINLINE ssp_u64 ssp_lzcnt64_REF( ssp_u64 val )
+{
+    ssp_u64 cnt;
+    cnt = ssp_lzcnt_REF( (ssp_u32)(val>>32) );
+    if( cnt == 32 )
+        cnt += ssp_lzcnt_REF( (ssp_u32)(val & 0x00000000FFFFFFFF) );
+    return cnt;
+}
+
+//---------------------------------------
+// Population Count
+//---------------------------------------
+/** \SSE4a{Native,__popcnt16} */ 
+SSP_FORCEINLINE unsigned short ssp_popcnt16_REF( unsigned short val )
+{
+    int i = 0;
+    ssp_u16 cnt = 0;
+
+    while ((i < 16) && (val))
+    {
+	if (val & 0x1)
+	{
+		++cnt;
+	}
+
+        ++i;
+        val >>= 1;
+    }
+    
+    return cnt;
+}
+/** \SSE4a{Native,__popcnt} */ 
+SSP_FORCEINLINE unsigned int ssp_popcnt_REF( unsigned int val )
+{
+    int i = 0;
+    ssp_u32 cnt = 0;
+
+    while ((i < 32) && (val))
+    {
+	if (val & 0x1)
+	{
+		++cnt;
+	}
+
+        ++i;
+        val >>= 1;
+    }
+
+    return cnt;
+}
+/** \SSE4a{Native,__popcnt64} */ 
+SSP_FORCEINLINE ssp_u64 ssp_popcnt64_REF( ssp_u64 val )
+{
+    int i = 0;
+    ssp_u64 cnt = 0;
+
+    while ((i < 64) && (val))
+    {
+	if (val & 0x1)
+	{
+		++cnt;
+	}
+
+        ++i;
+        val >>= 1;
+    }
+
+    return cnt;
+}
+
+//--------------------------------------
+// Packed permute
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_perm_epi8, pperm } */
+SSP_FORCEINLINE __m128i ssp_perm_epi8_REF(__m128i a, __m128i b, __m128i c)
+{
+    int n;
+    ssp_m128 A,B,C,R;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    for( n = 0; n < 16; n++ )
+    {
+        int op = C.u8[n] >> 5;
+        switch( op )
+        {
+        case 0: // source byte (no logical opeartion)
+            R.u8[n] = ( C.u8[n] & 0x10 ) ? ( B.u8[C.u8[n] & 0xF] ) : ( A.u8[C.u8[n] & 0xF] );
+            break;
+        case 1: // invert source byte
+            {
+                ssp_u8 src = ( C.u8[n] & 0x10 ) ? ( B.u8[C.u8[n] & 0xF] ) : ( A.u8[C.u8[n] & 0xF] );
+                R.u8[n] = ~src;
+            }
+            break;
+        case 2: // bit reverse of source byte
+            {
+                ssp_u8 src = ( C.u8[n] & 0x10 ) ? ( B.u8[C.u8[n] & 0xF] ) : ( A.u8[C.u8[n] & 0xF] );
+                R.u8[n] = ( (src & 0x0F) << 4 ) | ( (src & 0xF0) >> 4 );
+                R.u8[n] = ( (R.u8[n] & 0x33) << 2 ) | ( (R.u8[n] & 0xCC) >> 2 );
+                R.u8[n] = ( (R.u8[n] & 0x55) << 1 ) | ( (R.u8[n] & 0xAA) >> 1 );
+            }
+            break;
+        case 3: // bit reverse of inverted source byte
+            {
+                ssp_u8 src = ( C.u8[n] & 0x10 ) ? ( B.u8[C.u8[n] & 0xF] ) : ( A.u8[C.u8[n] & 0xF] );
+                R.u8[n] = ( (src & 0x0F) << 4 ) | ( (src & 0xF0) >> 4 );
+                R.u8[n] = ( (R.u8[n] & 0x33) << 2 ) | ( (R.u8[n] & 0xCC) >> 2 );
+                R.u8[n] = ( (R.u8[n] & 0x55) << 1 ) | ( (R.u8[n] & 0xAA) >> 1 );
+                R.u8[n] = ~R.u8[n];
+            }
+            break;
+        case 4: // 0x00
+            R.u8[n] = 0x00;
+            break;
+        case 5: // 0xFF
+            R.u8[n] = 0xFF;
+            break;
+        case 6: // most significant bit of source byte replicated in all bit positions
+            {
+                ssp_s8 src = ( C.u8[n] & 0x10 ) ? ( B.s8[C.u8[n] & 0xF] ) : ( A.s8[C.u8[n] & 0xF] );
+                R.s8[n] = src >> 7;
+            }
+            break;
+        case 7: // invert most significant bit of source byte and replicate in all bit positions
+            {
+                ssp_s8 src = ( C.u8[n] & 0x10 ) ? ( B.s8[C.u8[n] & 0xF] ) : ( A.s8[C.u8[n] & 0xF] );
+                R.s8[n] = src >> 7;
+                R.u8[n] = ~R.u8[n];
+            }
+            break;
+        }
+    }
+    return R.i;
+}
+/** \SSE5{Reference,_mm_perm_ps,		 permps } */
+SSP_FORCEINLINE __m128 ssp_perm_ps_REF(__m128 a, __m128 b, __m128i c)
+{
+    int n;
+    ssp_m128 A,B,C,R;
+    A.f = a;
+    B.f = b;
+    C.i = c;
+
+    for( n = 0; n < 4; n++ )
+    {
+        unsigned char cb = C.u8[n*4];
+        int op = (cb >> 5) & 0x7;
+        switch( op )
+        {
+        case 0: // single-precision source operand
+            R.f32[n] = ( cb & 0x04 ) ? ( B.f32[cb & 0x03] ) : ( A.f32[cb & 0x03] );
+            break;
+        case 1: // absolute value of single-precision source operand
+            {
+                ssp_f32 src = ( cb & 0x04 ) ? ( B.f32[cb & 0x03] ) : ( A.f32[cb & 0x03] );
+                R.f32[n] = ( src < 0.0f ) ? (-src) : src;
+            }
+            break;
+        case 2: // negative value of single-precision source operand
+            {
+                ssp_f32 src = ( cb & 0x04 ) ? ( B.f32[cb & 0x03] ) : ( A.f32[cb & 0x03] );
+                R.f32[n] = -src;
+            }
+            break;
+        case 3: // negative of absolute value of single-precision source operand
+            {
+                ssp_f32 src = ( cb & 0x04 ) ? ( B.f32[cb & 0x03] ) : ( A.f32[cb & 0x03] );
+                R.f32[n] = ( src < 0.0f ) ? src : (-src);
+            }
+            break;
+        case 4: // +0.0
+            R.f32[n] = 0.0f;
+            break;
+        case 5: // -1.0
+            R.f32[n] = -1.0f;
+            break;
+        case 6: // +1.0
+            R.f32[n] = 1.0f;
+            break;
+        case 7: // +0.0
+            R.u32[n] = 0x40490FDB; //(for mxcsr.rc 00 or 10 use 0x40490FDB, for 01 or 11 use 0x40490FDA)
+            break;
+        }
+    }
+    return R.f;
+}
+/** \SSE5{Reference,_mm_perm_pd,		 permpd } */
+SSP_FORCEINLINE __m128d ssp_perm_pd_REF(__m128d a, __m128d b, __m128i c)
+{
+    int n;
+    ssp_m128 A,B,C,R;
+    A.d = a;
+    B.d = b;
+    C.i = c;
+
+    for( n = 0; n < 2; n++ )
+    {
+        unsigned char cb = C.u8[n*8];
+        int op = (cb >> 5) & 0x7;
+        switch( op )
+        {
+        case 0: // single-precision source operand
+            R.f64[n] = ( cb & 0x02 ) ? ( B.f64[cb & 0x01] ) : ( A.f64[cb & 0x01] );
+            break;
+        case 1: // absolute value of single-precision source operand
+            {
+                ssp_f64 src = ( cb & 0x02 ) ? ( B.f64[cb & 0x01] ) : ( A.f64[cb & 0x01] );
+                R.f64[n] = ( src < 0.0 ) ? (-src) : src;
+            }
+            break;
+        case 2: // negative value of single-precision source operand
+            {
+                ssp_f64 src = ( cb & 0x02 ) ? ( B.f64[cb & 0x01] ) : ( A.f64[cb & 0x01] );
+                R.f64[n] = -src;
+            }
+            break;
+        case 3: // negative of absolute value of single-precision source operand
+            {
+                ssp_f64 src = ( cb & 0x02 ) ? ( B.f64[cb & 0x01] ) : ( A.f64[cb & 0x01] );
+                R.f64[n] = ( src < 0.0 ) ? src : (-src);
+            }
+            break;
+        case 4: // +0.0
+            R.f64[n] = 0.0;
+            break;
+        case 5: // -1.0
+            R.f64[n] = -1.0;
+            break;
+        case 6: // +1.0
+            R.f64[n] = 1.0;
+            break;
+        case 7: // +0.0
+            R.u64[n] = 0x400921FB54442D18; //(for mxcsr.rc 00, 01 or 11 use 0x400921FB54442D18, for 10 use 0x400921FB54442D19)
+            break;
+        }
+    }
+    return R.d;
+}
+
+//--------------------------------------
+// conditional move
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_cmov_si128, pcmov } */
+SSP_FORCEINLINE __m128i ssp_cmov_si128_REF(__m128i a, __m128i b, __m128i c)
+{
+    int n;
+    ssp_m128 A,B,C;
+    A.i = a;
+    B.i = b;
+    C.i = c;
+
+    for( n = 0; n < 4; n++ )
+    {
+        A.u32[n] &= C.u32[n];
+        C.u32[n] = ~C.u32[n];
+        B.u32[n] &= C.u32[n];
+        A.u32[n] |= B.u32[n];
+    }
+
+    return A.i;
+}
+
+//--------------------------------------
+// Packed rotates
+//--------------------------------------
+
+/** \SSE5{Reference,_mm_rot_epi8,		 protb } */
+SSP_FORCEINLINE __m128i ssp_rot_epi8_REF(__m128i a, __m128i b  )
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 16; n++ )
+    {
+      if( B.s8[n] < 0 )
+      {
+        unsigned int count = (-B.s8[n]) % 8;
+        unsigned int carry_count = (8 - count) % 8;
+        ssp_u8 carry = A.u8[n] << carry_count;
+        A.u8[n] = A.u8[n] >> count;
+        A.u8[n] = A.u8[n] | carry;
+      }
+      else
+      {
+        unsigned int count = B.s8[n] % 8;
+        unsigned int carry_count = (8 - count) % 8;
+        ssp_u8 carry = A.u8[n] >> carry_count;
+        A.u8[n] = A.u8[n] << count;
+        A.u8[n] = A.u8[n] | carry;
+      }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_rot_epi16,	 protw } */
+SSP_FORCEINLINE __m128i ssp_rot_epi16_REF(__m128i a, __m128i b  )
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 8; n++ )
+    {
+      if( B.s16[n] < 0 )
+      {
+        unsigned int count = (-B.s16[n]) % 16;
+        unsigned int carry_count = (16 - count) % 16;
+        ssp_u16 carry = A.u16[n] << carry_count;
+        A.u16[n] = A.u16[n] >> count;
+        A.u16[n] = A.u16[n] | carry;
+      }
+      else
+      {
+        unsigned int count = B.s16[n] % 8;
+        unsigned int carry_count = (16 - count) % 16;
+        ssp_u16 carry = A.u16[n] >> carry_count;
+        A.u16[n] = A.u16[n] << count;
+        A.u16[n] = A.u16[n] | carry;
+      }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_rot_epi32,	 protd } */
+SSP_FORCEINLINE __m128i ssp_rot_epi32_REF(__m128i a, __m128i b  )
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 4; n++ )
+    {
+      if( B.s32[n] < 0 )
+      {
+        unsigned int count = (-B.s32[n]) % 32;
+        unsigned int carry_count = (32 - count) % 32;
+        ssp_u32 carry = A.u32[n] << carry_count;
+        A.u32[n] = A.u32[n] >> count;
+        A.u32[n] = A.u32[n] | carry;
+      }
+      else
+      {
+        unsigned int count = B.s32[n] % 32;
+        unsigned int carry_count = (32 - count) % 32;
+        ssp_u32 carry = A.u32[n] >> carry_count;
+        A.u32[n] = A.u32[n] << count;
+        A.u32[n] = A.u32[n] | carry;
+      }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_rot_epi64,	 protq } */
+SSP_FORCEINLINE __m128i ssp_rot_epi64_REF(__m128i a, __m128i b  )
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 2; n++ )
+    {
+      if( B.s64[n] < 0 )
+      {
+        unsigned int count = (unsigned int)((-B.s64[n]) % 64);
+        unsigned int carry_count = (64 - count) % 64;
+        ssp_u64 carry = A.u64[n] << carry_count;
+        A.u64[n] = A.u64[n] >> count;
+        A.u64[n] = A.u64[n] | carry;
+      }
+      else
+      {
+        unsigned int count = (unsigned int)(B.s64[n] % 64);
+        unsigned int carry_count = (64 - count) % 64;
+        ssp_u64 carry = A.u64[n] >> carry_count;
+        A.u64[n] = A.u64[n] << count;
+        A.u64[n] = A.u64[n] | carry;
+      }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_roti_epi8, protb } */
+SSP_FORCEINLINE __m128i ssp_roti_epi8_REF(__m128i a, const int b)
+{
+    int n;
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        unsigned int count = (-b) % 8;
+        unsigned int carry_count = (8 - count) % 8;
+        for( n = 0; n < 16; n++ )
+        {
+            ssp_u8 carry = A.u8[n] << carry_count;
+            A.u8[n] = A.u8[n] >> count;
+            A.u8[n] = A.u8[n] | carry;
+        }
+    }
+    else
+    {
+        unsigned int count = b % 8;
+        unsigned int carry_count = (8 - count) % 8;
+        for( n = 0; n < 16; n++ )
+        {
+            ssp_u8 carry = A.u8[n] >> carry_count;
+            A.u8[n] = A.u8[n] << count;
+            A.u8[n] = A.u8[n] | carry;
+        }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_roti_epi16, protw } */
+SSP_FORCEINLINE __m128i ssp_roti_epi16_REF(__m128i a, const int b)
+{
+    int n;
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        unsigned int count = (-b) % 16;
+        unsigned int carry_count = (16 - count) % 16;
+        for( n = 0; n < 8; n++ )
+        {
+            ssp_u16 carry = A.u16[n] << carry_count;
+            A.u16[n] = A.u16[n] >> count;
+            A.u16[n] = A.u16[n] | carry;
+        }
+    }
+    else
+    {
+        unsigned int count = b % 16;
+        unsigned int carry_count = (16 - count) % 16;
+        for( n = 0; n < 8; n++ )
+        {
+            ssp_u16 carry = A.u16[n] >> carry_count;
+            A.u16[n] = A.u16[n] << count;
+            A.u16[n] = A.u16[n] | carry;
+        }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_roti_epi32, protd } */
+SSP_FORCEINLINE __m128i ssp_roti_epi32_REF(__m128i a, const int b)
+{
+    int n;
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        unsigned int count = (-b) % 32;
+        unsigned int carry_count = (32 - count) % 32;
+        for( n = 0; n < 4; n++ )
+        {
+            ssp_u32 carry = A.u32[n] << carry_count;
+            A.u32[n] = A.u32[n] >> count;
+            A.u32[n] = A.u32[n] | carry;
+        }
+    }
+    else
+    {
+        unsigned int count = b % 32;
+        unsigned int carry_count = (32 - count) % 32;
+        for( n = 0; n < 4; n++ )
+        {
+            ssp_u32 carry = A.u32[n] >> carry_count;
+            A.u32[n] = A.u32[n] << count;
+            A.u32[n] = A.u32[n] | carry;
+        }
+    }
+    return A.i;
+}
+/** \SSE5{Reference,_mm_roti_epi64, protq } */
+SSP_FORCEINLINE __m128i ssp_roti_epi64_REF(__m128i a, const int b)
+{
+    int n;
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        unsigned int count = (-b) % 64;
+        unsigned int carry_count = (64 - count) % 64;
+        for( n = 0; n < 2; n++ )
+        {
+            ssp_u64 carry = A.u64[n] << carry_count;
+            A.u64[n] = A.u64[n] >> count;
+            A.u64[n] = A.u64[n] | carry;
+        }
+    }
+    else
+    {
+        unsigned int count = b % 64;
+        unsigned int carry_count = (64 - count) % 64;
+        for( n = 0; n < 2; n++ )
+        {
+            ssp_u64 carry = A.u64[n] >> carry_count;
+            A.u64[n] = A.u64[n] << count;
+            A.u64[n] = A.u64[n] | carry;
+        }
+    }
+    return A.i;
+}
+
+
+//--------------------------------------
+// Packed Shift Logical (bytes, words, dwords, qwords)
+//--------------------------------------
+
+/** \SSE5{Reference,ssp_shl_epi8,pshlb } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi8_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 16; n++ )
+    {
+      if( B.s8[n] < 0 )
+      {
+        unsigned int count = (-B.s8[n]) % 8;
+        A.u8[n] = A.u8[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n] % 8;
+        A.u8[n] = A.u8[n] << count;
+      }
+    }
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_sha_epi8,pshab } */ 
+SSP_FORCEINLINE __m128i ssp_sha_epi8_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 16; n++ )
+    {
+      if( B.s8[n] < 0 )
+      {
+        unsigned int count = (-B.s8[n]) % 8;
+        A.s8[n] = A.s8[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n] % 8;
+        A.s8[n] = A.s8[n] << count;
+      }
+    }
+
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_shl_epi16,pshlw } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi16_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 8; n++ )
+    {
+      if( B.s8[n*2] < 0 )
+      {
+        unsigned int count = (-B.s8[n*2]) % 16;
+        A.u16[n] = A.u16[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n*2] % 16;
+        A.u16[n] = A.u16[n] << count;
+      }
+    }
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_sha_epi16,pshaw } */ 
+SSP_FORCEINLINE __m128i ssp_sha_epi16_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 8; n++ )
+    {
+      if( B.s8[n*2] < 0 )
+      {
+        unsigned int count = (-B.s8[n*2]) % 16;
+        A.s16[n] = A.s16[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n*2] % 16;
+        A.s16[n] = A.s16[n] << count;
+      }
+    }
+
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_shl_epi32,pshld } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi32_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 4; n++ )
+    {
+      if( B.s8[n*4] < 0 )
+      {
+        unsigned int count = (-B.s8[n*4]) % 32;
+        A.u32[n] = A.u32[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n*4] % 32;
+        A.u32[n] = A.u32[n] << count;
+      }
+    }
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_sha_epi32,pshad } */ 
+SSP_FORCEINLINE __m128i ssp_sha_epi32_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 4; n++ )
+    {
+      if( B.s8[n*4] < 0 )
+      {
+        unsigned int count = (-B.s8[n*4]) % 32;
+        A.s32[n] = A.s32[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n*4] % 32;
+        A.s32[n] = A.s32[n] << count;
+      }
+    }
+
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_shl_epi64,pshld } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi64_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 2; n++ )
+    {
+      if( B.s8[n*8] < 0 )
+      {
+        unsigned int count = (-B.s8[n*8]) % 64;
+        A.u64[n] = A.u64[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n*8] % 64;
+        A.u64[n] = A.u64[n] << count;
+      }
+    }
+    return A.i;
+}
+
+/** \SSE5{Reference,ssp_sha_epi64,pshad } */ 
+SSP_FORCEINLINE __m128i ssp_sha_epi64_REF(__m128i a, __m128i b)
+{
+    int n;
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+
+    for( n = 0; n < 2; n++ )
+    {
+      if( B.s8[n*8] < 0 )
+      {
+        unsigned int count = (-B.s8[n*8]) % 64;
+        A.s64[n] = A.s64[n] >> count;
+      }
+      else
+      {
+        unsigned int count = B.s8[n*8] % 64;
+        A.s64[n] = A.s64[n] << count;
+      }
+    }
+
+    return A.i;
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSP_EMULATION_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,2192 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_EMULATION_SSE2_H__
+#define __SSEPLUS_EMULATION_SSE2_H__
+
+#include "../SSEPlus_SSE2.h"
+#include "../native/SSEPlus_native_SSE2.h"
+#include "../logical/SSEPlus_logical_SSE2.h"
+#include "../convert/SSEPlus_convert_SSE2.h"
+#include "../arithmetic/SSEPlus_arithmetic_SSE2.h"
+#include "SSEPlus_emulation_comps_SSE2.h"
+
+
+/** @addtogroup emulated_SSE2   
+ *  @{ 
+ *  @name SSE[3,4A,...,5] implemented in SSE2
+ */
+
+//
+// Multiply Add
+//
+/** \SSE5{SSE2,_mm_macc_epi16, pmacsww } */ 
+SSP_FORCEINLINE __m128i ssp_macc_epi16_SSE2( __m128i a, __m128i b, __m128i c )
+{
+    a = _mm_mullo_epi16( a, b );
+    a = _mm_add_epi16( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_macc_epi32, pmacsdd } */ 
+SSP_FORCEINLINE __m128i ssp_macc_epi32_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	__m128i ab02, ab13, mask;
+
+	mask = _mm_set_epi32(0, 0xFFFFFFFF, 0, 0xFFFFFFFF);
+	ab02 = _mm_mul_epu32(a, b);
+	ab02 = _mm_and_si128(ab02, mask);
+	a    = _mm_srli_epi64(a, 32);
+	b    = _mm_srli_epi64(b, 32);
+	ab13 = _mm_mul_epu32(a, b);
+	ab13 = _mm_slli_epi64(ab13, 32);
+
+	a    = _mm_add_epi32(ab02, ab13);
+
+	return _mm_add_epi32(a, c);
+}
+
+/** \SSE5{SSE2,_mm_macc_pd,fmaddpd} */ 
+SSP_FORCEINLINE __m128d ssp_macc_pd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    a = _mm_mul_pd( a, b );
+    a = _mm_add_pd( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_macc_ps,fmaddps} */ 
+SSP_FORCEINLINE __m128 ssp_macc_ps_SSE2( __m128 a, __m128 b, __m128 c )
+{
+    a = _mm_mul_ps( a, b );
+    a = _mm_add_ps( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_macc_sd,fmaddsd} */ 
+SSP_FORCEINLINE __m128d ssp_macc_sd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0, 0 );
+
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    B.d = ssp_macc_pd_SSE2( A.d, B.d, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.d;
+}
+
+/** \SSE5{SSE2,_mm_macc_ss,fmaddss} */ 
+SSP_FORCEINLINE __m128 ssp_macc_ss_SSE2(__m128 a, __m128 b, __m128 c)   // Assuming SSE5 *_ss semantics are similar to _mm_add_ss. TODO: confirm
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0 );
+
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    B.f = ssp_macc_ps_SSE2( A.f, B.f, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.f;
+}
+
+/** \SSE5{SSE2,_mm_maccd_epi16, pmacswd } */ 
+SSP_FORCEINLINE __m128i ssp_maccd_epi16_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	__m128i ab_lo, ab_hi;
+	__m128i mask = _mm_set1_epi32(0xFFFF);
+
+    ab_lo = _mm_mullo_epi16(a, b);
+	ab_hi = _mm_mulhi_epi16(a, b);
+
+	ab_lo = _mm_and_si128(ab_lo, mask);
+	ab_hi = _mm_and_si128(ab_hi, mask);
+	ab_hi = _mm_slli_epi32(ab_hi, 16);
+	a = _mm_add_epi32( ab_lo, ab_hi );
+
+	return _mm_add_epi32 (a, c);
+
+	////another method ported from Framewave CBL
+	//b     = _mm_unpacklo_epi16(ab_lo, ab_hi);
+	//ab_hi = _mm_unpackhi_epi16(ab_lo, ab_hi);
+	//ab_lo = _mm_unpacklo_epi32(b,     ab_hi);
+	//ab_hi = _mm_unpackhi_epi32(b,     ab_hi);
+	//ab_lo = _mm_unpacklo_epi32(ab_lo, ab_hi);
+	//return _mm_add_epi32(ab_lo, c);
+}
+
+/** \SSE5{SSE2,_mm_macchi_epi32, pmacsdqh } */ 
+SSP_FORCEINLINE __m128i ssp_macchi_epi32_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	__m128i mask, mask_A, mask_B, mask_C, ab;
+
+	a = _mm_srli_epi64(a, 32);
+	b = _mm_srli_epi64(b, 32);
+	mask   = _mm_set_epi32(0x7FFFFFFF, 0, 0x7FFFFFFF, 0);
+
+	//abs(A)
+	mask_A = _mm_cmplt_epi32( a, mask);     //FFF...F when a < 0
+	a      = _mm_xor_si128 ( a, mask_A );   //Invert  when a < 0
+	mask_C = _mm_srli_epi32( mask_A, 31 );	// 1      when a < 0
+	a      = _mm_add_epi32( a, mask_C );    //Add 1   when a < 0
+
+	//abs(B)
+	mask_B = _mm_cmplt_epi32( b, mask);     //FFF...F when b < 0
+	b      = _mm_xor_si128 ( b, mask_B );   //Invert  when b < 0
+	mask_C = _mm_srli_epi32( mask_B, 31 );	// 1      when b < 0
+	b      = _mm_add_epi32( b, mask_C );    //Add 1   when b < 0
+
+	ab     = _mm_mul_epu32(a, b);
+
+	//correct negative cases
+	mask_A = _mm_xor_si128(mask_A, mask_B);
+	mask_C = _mm_srli_epi32(mask_A, 31 );
+	mask_B = _mm_slli_epi64(mask_A, 32);
+	mask   = _mm_add_epi32(mask_A, mask_B);
+	a      = _mm_xor_si128(ab, mask);
+	a      = _mm_add_epi64(a, mask_C);
+
+	return _mm_add_epi64(a, c);
+}
+
+/** \SSE5{SSE2,_mm_macclo_epi32, pmacsdql } */ 
+SSP_FORCEINLINE __m128i ssp_macclo_epi32_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	__m128i mask, mask_A, mask_B, mask_C, ab;
+
+	mask   = _mm_set_epi32(0x7FFFFFFF, 0, 0x7FFFFFFF, 0);
+	//abs(A)
+	mask_A = _mm_cmplt_epi32( a, mask);     //FFF...F when a < 0
+	a      = _mm_xor_si128 ( a, mask_A );   //Invert  when a < 0
+	mask_C = _mm_srli_epi32( mask_A, 31 );	// 1      when a < 0
+	a      = _mm_add_epi32( a, mask_C );    //Add 1   when a < 0
+
+	//abs(B)
+	mask_B = _mm_cmplt_epi32( b, mask);     //FFF...F when b < 0
+	b      = _mm_xor_si128 ( b, mask_B );   //Invert  when b < 0
+	mask_C = _mm_srli_epi32( mask_B, 31 );	// 1      when b < 0
+	b      = _mm_add_epi32( b, mask_C );    //Add 1   when b < 0
+
+	ab     = _mm_mul_epu32(a, b);
+
+	//correct negative cases
+	mask_A = _mm_xor_si128(mask_A, mask_B);
+	mask_C = _mm_srli_epi32(mask_A, 31 );
+	mask_B = _mm_slli_epi64(mask_A, 32);
+	mask   = _mm_add_epi32(mask_A, mask_B);
+	a      = _mm_xor_si128(ab, mask);
+	a      = _mm_add_epi64(a, mask_C);
+
+	return _mm_add_epi64(a, c);
+}
+
+/** \SSE5{SSE2,_mm_maccs_epi16, pmacssww } */ 
+SSP_FORCEINLINE __m128i ssp_maccs_epi16_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	//similar to the version in Framewave CBL
+	__m128i ablo, abhi, unlo, unhi, signC, clo, chi;
+
+	ablo  = _mm_mullo_epi16( a, b );
+	abhi  = _mm_mulhi_epi16( a, b );
+	unlo  = _mm_unpacklo_epi16( ablo, abhi );
+	unhi  = _mm_unpackhi_epi16( ablo, abhi );
+
+	//unpack and keep the sign of C
+	signC = _mm_srai_epi16 (c, 15);
+	chi   = _mm_unpackhi_epi16(c, signC);
+	clo   = _mm_unpacklo_epi16(c, signC);
+
+	chi   = _mm_add_epi32(chi, unhi);
+	clo   = _mm_add_epi32(clo, unlo);
+
+	return _mm_packs_epi32(clo, chi);
+}
+
+/** \SSE5{SSE2,_mm_maccs_epi32, pmacssdd } */ 
+SSP_FORCEINLINE __m128i ssp_maccs_epi32_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	//slightly modified from Framewave CBL
+	ssp_m128 s1lo,s1hi,s2lo,s2hi,s3lo,s3hi, sl, sh;
+	static const __m128d max_val = {(double)0x7FFFFFFFl, (double)0x7FFFFFFFl};
+	static const __m128d min_val = {(-(double)0x80000000l), (-(double)0x80000000l)};
+
+	s1lo.d = _mm_cvtepi32_pd(a);
+	s1hi.d = _mm_cvtepi32_pd(_mm_srli_si128(a, 8)); 
+
+	s2lo.d = _mm_cvtepi32_pd(b);
+	s2hi.d = _mm_cvtepi32_pd(_mm_srli_si128(b,8)); 
+
+	s1lo.d = _mm_mul_pd(s1lo.d,s2lo.d);
+	s1hi.d = _mm_mul_pd(s1hi.d,s2hi.d);
+
+	s3lo.d = _mm_cvtepi32_pd(c);
+	s3hi.d = _mm_cvtepi32_pd(_mm_srli_si128(c,8)); 
+	
+	s1lo.d = _mm_add_pd(s1lo.d,s3lo.d);
+	s1hi.d = _mm_add_pd(s1hi.d,s3hi.d);
+
+	sl.d   = _mm_min_pd(s1lo.d, max_val);
+	sl.d   = _mm_max_pd(sl.d, min_val);
+
+	sh.d   = _mm_min_pd(s1hi.d, max_val);
+	sh.d   = _mm_max_pd(sh.d, min_val);
+
+	sl.i   = _mm_cvtpd_epi32(sl.d); 
+	sh.i   = _mm_cvtpd_epi32(sh.d);
+
+	sh.i   = _mm_slli_si128(sh.i, 8); 
+	sl.i   = _mm_or_si128(sl.i, sh.i);
+
+    return sl.i;
+}
+
+/** \SSE5{SSE2,_mm_maccsd_epi16, pmacsswd } */ 
+SSP_FORCEINLINE __m128i ssp_maccsd_epi16_SSE2( __m128i a, __m128i b, __m128i c )
+{
+	__m128i ab_lo, ab_hi, apos, cpos, cneg;
+	__m128i mask = _mm_set1_epi32(0xFFFF);
+	__m128i zero = _mm_setzero_si128();
+	__m128i minV = _mm_set1_epi32(0x80000000);
+
+    ab_lo = _mm_mullo_epi16(a, b);
+	ab_hi = _mm_mulhi_epi16(a, b);
+
+	ab_lo = _mm_and_si128(ab_lo, mask);
+	ab_hi = _mm_and_si128(ab_hi, mask);
+	ab_hi = _mm_slli_epi32(ab_hi, 16);
+	a     = _mm_or_si128(ab_lo, ab_hi);
+
+	apos  = _mm_cmpgt_epi32(a, zero);
+	cpos  = _mm_cmpgt_epi32(c, zero);
+	cneg  = _mm_cmplt_epi32(c, zero);
+	apos  = _mm_and_si128(apos, cpos);    // a & c both positive
+	cneg  = _mm_andnot_si128(apos, cneg); // a & c both negative
+
+	a     = _mm_add_epi32(a, c);
+	ab_hi = _mm_cmplt_epi32(a, c);
+	ab_lo = _mm_cmpgt_epi32(a, c);
+	ab_hi = _mm_and_si128(ab_hi, apos);   //need to saturate to pos max
+	ab_lo = _mm_and_si128(ab_lo, cneg);   //need to saturate to neg min
+
+	a     = _mm_andnot_si128(ab_hi, a);
+	ab_hi = _mm_srli_epi32(ab_hi, 1);     // to 0x7FFFFFFF
+	a     = _mm_andnot_si128(ab_lo, a);
+	ab_lo = _mm_and_si128(ab_lo, minV);
+	a     = _mm_add_epi32(a, ab_hi);
+	return _mm_add_epi32(a, ab_lo);
+
+	////modifed from Framewave CBL version, slower
+	//__m128d a01, c01, a23, c23;	
+	//static const __m128d max_val = {(double)0x7FFFFFFFl, (double)0x7FFFFFFFl};
+	//static const __m128d min_val = {(-(double)0x80000000l), (-(double)0x80000000l)};
+	//a01   = _mm_cvtepi32_pd(a);
+	//c01   = _mm_cvtepi32_pd(c);
+	//ab_lo = _mm_srli_si128(a, 8);
+	//ab_hi = _mm_srli_si128(c, 8);
+	//a23   = _mm_cvtepi32_pd(ab_lo);
+	//c23   = _mm_cvtepi32_pd(ab_hi);
+
+	//a01   = _mm_add_pd(a01, c01);
+	//a23   = _mm_add_pd(a23, c23);
+
+	//c01   = _mm_min_pd(a01, max_val);
+	//c23   = _mm_max_pd(c01, min_val);
+
+	//a01   = _mm_min_pd(a23, max_val);
+	//a23   = _mm_max_pd(a01, min_val);
+
+	//ab_lo = _mm_cvtpd_epi32(c23);
+	//ab_hi = _mm_cvtpd_epi32(a23);
+	//ab_hi = _mm_slli_si128(ab_hi, 8);
+
+	//return _mm_or_si128 (ab_lo, ab_hi);
+}
+
+//
+// Negative Multiply Add
+//
+/** \SSE5{SSE2,_mm_nmacc_ps,fnmaddps} */ 
+SSP_FORCEINLINE __m128 ssp_nmacc_ps_SSE2( __m128 a, __m128 b, __m128 c )
+{
+    const static __m128 neg1 = SSP_CONST_SET_32F( -1.0f, -1.0f, -1.0f, -1.0f );
+
+    a = _mm_mul_ps( a, b    );
+    a = _mm_mul_ps( a, neg1 );
+    a = _mm_add_ps( a, c    );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_nmacc_pd,fnmaddpd} */ 
+SSP_FORCEINLINE __m128d ssp_nmacc_pd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    const static __m128d neg1 = SSP_CONST_SET_64F( -1.0, -1.0 );
+
+    a = _mm_mul_pd( a, b    );
+    a = _mm_mul_pd( a, neg1 );
+    a = _mm_add_pd( a, c    );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_nmacc_ss,fnmaddss} */ 
+SSP_FORCEINLINE __m128 ssp_nmacc_ss_SSE2(__m128 a, __m128 b, __m128 c)   // Assuming SSE5 *_ss semantics are similar to _mm_add_ss. TODO: confirm
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0 );
+
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    B.f = ssp_nmacc_ps_SSE2( A.f, B.f, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.f;
+}
+
+/** \SSE5{SSE2,_mm_nmacc_sd,fnmaddsd} */ 
+SSP_FORCEINLINE __m128d ssp_nmacc_sd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0, 0 );
+
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    B.d = ssp_nmacc_pd_SSE2( A.d, B.d, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.d;
+}
+
+//
+// Multiply Subtract
+//
+
+/** \SSE5{SSE2,_mm_msub_ps,fmsubps} */ 
+SSP_FORCEINLINE __m128 ssp_msub_ps_SSE2(__m128 a, __m128 b, __m128 c)
+{
+    a = _mm_mul_ps( a, b );
+    a = _mm_sub_ps( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_msub_pd,fmsubpd} */ 
+SSP_FORCEINLINE __m128d ssp_msub_pd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    a = _mm_mul_pd( a, b );
+    a = _mm_sub_pd( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_msub_ss,fmsubss} */ 
+SSP_FORCEINLINE __m128 ssp_msub_ss_SSE2(__m128 a, __m128 b, __m128 c)
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0 );
+
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    B.f = ssp_msub_ps_SSE2( A.f, B.f, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.f;
+}
+
+/** \SSE5{SSE2,_mm_msub_sd,fmsubsd} */ 
+SSP_FORCEINLINE __m128d ssp_msub_sd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0, 0 );
+
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    B.d = ssp_msub_pd_SSE2( A.d, B.d, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.d;
+}
+
+//
+// Negative Multiply Subtract
+//
+
+/** \SSE5{SSE2,_mm_nmsub_ps,fnmsubps} */ 
+SSP_FORCEINLINE __m128 ssp_nmsub_ps_SSE2(__m128 a, __m128 b, __m128 c)
+{
+    const static __m128 neg1 = SSP_CONST_SET_32F( -1.0f, -1.0f, -1.0f, -1.0f );
+
+    a = _mm_mul_ps( a, b    );
+    a = _mm_mul_ps( a, neg1 );
+    a = _mm_sub_ps( a, c    );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_nmsub_pd,fnmsubpd} */ 
+SSP_FORCEINLINE __m128d ssp_nmsub_pd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    const static __m128d neg1 = SSP_CONST_SET_64F( -1.0, -1.0 );
+
+    a = _mm_mul_pd( a, b    );
+    a = _mm_mul_pd( a, neg1 );
+    a = _mm_sub_pd( a, c    );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_nmsub_ss,fnmsubss} */ 
+SSP_FORCEINLINE __m128 ssp_nmsub_ss_SSE2(__m128 a, __m128 b, __m128 c)
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0 );
+
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    B.f = ssp_nmsub_ps_SSE2( A.f, B.f, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.f;
+}
+
+/** \SSE5{SSE2,_mm_nmsub_sd,fnmsubsd} */ 
+SSP_FORCEINLINE __m128d ssp_nmsub_sd_SSE2(__m128d a, __m128d b, __m128d c)
+{
+    const static __m128i mask = SSP_CONST_SET_32I( SSP_ALL_SET_32I, SSP_ALL_SET_32I, 0, 0 );
+
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    B.d = ssp_nmsub_pd_SSE2( A.d, B.d, c );
+    B.i = ssp_logical_bitwise_select_SSE2( A.i, B.i, mask ); // This was faster than using 2 shuffles
+    return B.d;
+}
+
+//
+// Abs
+//
+
+
+/** \SSSE3{SSE2,_mm_abs_epi8} */
+SSP_FORCEINLINE
+__m128i ssp_abs_epi8_SSE2 (__m128i a)
+{
+    __m128i mask = _mm_cmplt_epi8( a, _mm_setzero_si128() );  // FFFF   where a < 0
+	__m128i one  = _mm_set1_epi8(1);
+    a    = _mm_xor_si128 ( a, mask  );                        // Invert where a < 0
+    mask = _mm_and_si128 ( mask, one );                       // 0001   where a < 0
+    a    = _mm_add_epi8  ( a, mask  );                        // Add 1  where a < 0 
+    return a;
+}
+
+/** \SSSE3{SSE2,_mm_abs_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_abs_epi16_SSE2 (__m128i a)
+{
+    __m128i mask = _mm_cmplt_epi16( a, _mm_setzero_si128() ); // FFFF   where a < 0
+    a    = _mm_xor_si128 ( a, mask  );                        // Invert where a < 0
+    mask = _mm_srli_epi16( mask, 15 );                        // 0001   where a < 0
+    a    = _mm_add_epi16 ( a, mask  );                        // Add 1  where a < 0
+    return a;
+}
+
+/** \SSSE3{SSE2,_mm_abs_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_abs_epi32_SSE2 (__m128i a)
+{
+    __m128i mask = _mm_cmplt_epi32( a, _mm_setzero_si128() ); // FFFF   where a < 0
+    a    = _mm_xor_si128 ( a, mask );                         // Invert where a < 0
+    mask = _mm_srli_epi32( mask, 31 );                        // 0001   where a < 0
+    a = _mm_add_epi32( a, mask );                             // Add 1  where a < 0
+	return a;
+}
+
+
+/** \SSE3{SSE2,_mm_addsub_ps} */
+SSP_FORCEINLINE
+__m128 ssp_addsub_ps_SSE2(__m128 a, __m128 b)
+{
+    const static __m128 neg = SSP_CONST_SET_32F(  1, -1, 1, -1 );
+
+    b = _mm_mul_ps( b, neg );
+    a = _mm_add_ps( a, b   );
+    return a;
+}
+
+/** \SSE3{SSE2,_mm_addsub_ps} */
+SSP_FORCEINLINE
+__m128d ssp_addsub_pd_SSE2(__m128d a, __m128d b)
+{
+    const static __m128d const_addSub_pd_neg = SSP_CONST_SET_64F( 1, -1 );
+
+    b = _mm_mul_pd( b, const_addSub_pd_neg );
+    a = _mm_add_pd( a, b   );
+    return a;
+}
+
+//
+// Blend
+//
+
+/** \SSE4_1{SSE2,_mm_blend_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_blend_epi16_SSE2( __m128i a, __m128i b, const int mask )
+{
+    __m128i screen;
+    const static __m128i mulShiftImm = SSP_CONST_SET_16I( 0x0100, 0x0200, 0x0400, 0x0800, 0x1000, 0x2000, 0x4000, 0x8000 ); // Shift mask multiply moves all bits to left, becomes MSB
+
+    screen = _mm_set1_epi16  ( mask                );   // Load the mask into register
+    screen = _mm_mullo_epi16 ( screen, mulShiftImm );   // Shift bits to MSB
+    screen = _mm_srai_epi16  ( screen, 15          );   // Shift bits to obtain 0xFFFF or 0x0000
+    b      = _mm_and_si128   ( screen,  b          );   // Mask out the correct values from b
+    a      = _mm_andnot_si128( screen,  a          );   // Mask out the correct values from a (invert the mask)
+    a      = _mm_or_si128    (      a,  b          );   // Or the 2 results.
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_blend_pd} */
+SSP_FORCEINLINE
+__m128d ssp_blend_pd_SSE2(  __m128d a, __m128d b, const int mask )
+{
+    __m128d screen;
+    screen = _mm_set_pd(  (mask&0x2)>>1,    mask&0x1 );
+    b      = _mm_mul_pd(              b,      screen );
+    screen = _mm_set_pd( (~mask&0x2)>>1, (~mask&0x1) );
+    a      = _mm_mul_pd(              a,      screen );
+    a      = _mm_or_pd (              a,           b );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_blend_ps} */
+SSP_FORCEINLINE
+__m128  ssp_blend_ps_SSE2( __m128  a, __m128  b, const int mask )               // _mm_blend_ps [SSE4.1]
+{
+    ssp_m128 screen, A, B;
+    A.f = a;
+    B.f = b;
+    screen.i = ssp_movmask_imm8_to_epi32_SSE2( mask );
+    screen.i = ssp_logical_bitwise_select_SSE2( B.i, A.i, screen.i );
+    return screen.f;
+}
+
+/** \SSE4_1{SSE2,_mm_blendv_epi8} */
+SSP_FORCEINLINE
+__m128i ssp_blendv_epi8_SSE2( __m128i a, __m128i b, __m128i mask )
+{
+    __m128i mHi, mLo;
+    __m128i zero = _mm_setzero_si128 ();
+
+    mHi = _mm_unpacklo_epi8( zero, mask );
+    mHi = _mm_srai_epi16   (  mHi,   15 );
+    mHi = _mm_srli_epi16   (  mHi,    1 );
+
+    mLo = _mm_unpackhi_epi8( zero, mask );
+    mLo = _mm_srai_epi16   (  mLo,   15 );
+    mLo = _mm_srli_epi16   (  mLo,    1 );
+
+    mHi = _mm_packus_epi16  ( mHi,  mLo );
+
+    b   = _mm_and_si128     (    b, mHi  );
+    a   = _mm_andnot_si128  (  mHi,  a   );
+    a   = _mm_or_si128      (    a,  b   );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_blendv_pd} */
+SSP_FORCEINLINE __m128d ssp_blendv_pd_SSE2( __m128d a, __m128d b, __m128d mask )
+{
+    ssp_m128 A, B, Mask;
+    A.d = a;
+    B.d = b;
+    Mask.d = mask;
+
+    Mask.i = _mm_shuffle_epi32( Mask.i, _MM_SHUFFLE(3, 3, 1, 1) );
+    Mask.i = _mm_srai_epi32   ( Mask.i, 31                      );
+
+    B.i = _mm_and_si128( B.i, Mask.i );
+    A.i = _mm_andnot_si128( Mask.i, A.i );
+    A.i = _mm_or_si128( A.i, B.i );
+    return A.d;
+}
+/** \SSE4_1{SSE2,_mm_blendv_epi8} */
+SSP_FORCEINLINE __m128  ssp_blendv_ps_SSE2( __m128  a, __m128  b, __m128 mask )     
+{
+    ssp_m128 A, B, Mask;
+    A.f = a;
+    B.f = b;
+    Mask.f = mask;
+
+    Mask.i = _mm_srai_epi32( Mask.i, 31 );
+    B.i = _mm_and_si128( B.i, Mask.i );
+    A.i = _mm_andnot_si128( Mask.i, A.i );
+    A.i = _mm_or_si128( A.i, B.i );
+    return A.f;
+}
+
+//
+// Compare
+//
+
+/** \SSE4_1{SSE2,_mm_cmpeq_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cmpeq_epi64_SSE2( __m128i a, __m128i b )
+{
+    return ssp_comeq_epi64_SSE2( a, b );
+}
+
+
+//
+// Horizontal Operations
+//
+
+/** \SSE3{SSE2,_mm_hadd_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_hadd_epi16_SSE2( __m128i a, __m128i b )
+{
+    ssp_convert_odd_even_epi16_SSE2( &a, &b );
+    a = _mm_add_epi16( a, b );     
+    return a;
+}
+
+/** \SSSE3{SSE2,_mm_hadds_epi16} */
+SSP_FORCEINLINE __m128i ssp_hadds_epi16_SSE2 ( __m128i a, __m128i b )                     
+{
+    ssp_convert_odd_even_epi16_SSE2( &a, &b );
+    a = _mm_adds_epi16( a, b );    
+    return a;
+}
+
+
+/** \SSE3{SSE2,_mm_hsub_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_hsub_epi16_SSE2 ( __m128i a, __m128i b )
+{
+    ssp_convert_odd_even_epi16_SSE2( &a, &b ); 
+    a = _mm_sub_epi16( a, b );     
+    return a;  
+}
+
+/** \SSE3{SSE2,_mm_hsub_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_hsubs_epi16_SSE2 ( __m128i a, __m128i b )
+{
+    ssp_convert_odd_even_epi16_SSE2( &a, &b ); 
+    a = _mm_subs_epi16( a, b );     
+    return a;  
+}
+
+
+
+/** \SSSE3{SSE2,_mm_hadd_epi32} */
+SSP_FORCEINLINE __m128i ssp_hadd_epi32_SSE2( __m128i a, __m128i b )                        
+{
+   ssp_convert_odd_even_epi32_SSE2( &a, &b );
+   a = _mm_add_epi32( a, b );
+   return a; 
+}
+
+/** \SSSE3{SSE2,_mm_hsub_epi32} */
+SSP_FORCEINLINE __m128i ssp_hsub_epi32_SSE2 ( __m128i a, __m128i b )                        
+{
+   ssp_convert_odd_even_epi32_SSE2( &a, &b );
+   a = _mm_sub_epi32( b, a );
+   return a;
+}
+
+
+/** \SSE3{SSE2,_mm_hadd_ps} */
+SSP_FORCEINLINE
+__m128 ssp_hadd_ps_SSE2(__m128 a, __m128 b)
+{
+    ssp_convert_odd_even_ps_SSE2( &a, &b );
+    a = _mm_add_ps( a, b );
+    return a;
+}
+
+/** \SSE3{SSE2,_mm_hsub_ps} */
+SSP_FORCEINLINE
+__m128 ssp_hsub_ps_SSE2(__m128 a, __m128 b)
+{
+    ssp_convert_odd_even_ps_SSE2( &a, &b );
+    a = _mm_sub_ps( b, a );
+    return a;
+}
+
+
+/** \SSE3{SSE2,_mm_hadd_pd} */
+SSP_FORCEINLINE
+__m128d ssp_hadd_pd_SSE2(__m128d a, __m128d b)
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    C.d = a;
+    B.d = b;
+
+    A.f = _mm_movelh_ps( A.f, B.f );
+    B.f = _mm_movehl_ps( B.f, C.f );
+    A.d = _mm_add_pd   ( A.d, B.d );
+    return A.d;
+}
+
+/** \SSE3{SSE2,_mm_hsub_pd} */
+SSP_FORCEINLINE
+__m128d ssp_hsub_pd_SSE2(__m128d a, __m128d b)
+{
+    ssp_m128 A,B,C;
+    A.d = a;
+    C.d = a;
+    B.d = b;
+
+    A.f = _mm_movelh_ps( A.f, B.f );
+    B.f = _mm_movehl_ps( B.f, C.f );
+    A.d = _mm_sub_pd   ( A.d, B.d );
+    return A.d;
+}
+
+
+//__m128i _mm_mulhrs_epi16( __m128i a,  __m128i b);
+/** \SSSE3{SSE2,_mm_mulhrs_epi16} */
+SSP_FORCEINLINE __m128i ssp_mulhrs_epi16_SSE2( __m128i a, __m128i b )
+{
+    const static __m128i VAL = SSP_CONST_SET_32I( 0x4000, 0x4000, 0x4000, 0x4000 );
+    __m128i c,d;   
+
+    c = _mm_mullo_epi16( a, b );
+    d = _mm_mulhi_epi16( a, b );
+   
+    a = _mm_unpackhi_epi16( c, d );
+    b = _mm_unpacklo_epi16( c, d );
+
+    a = _mm_add_epi32( a, VAL );
+    b = _mm_add_epi32( b, VAL );
+
+    a = _mm_srai_epi32( a, 15 );
+    b = _mm_srai_epi32( b, 15 );
+
+    a = _mm_packs_epi32( b, a );
+    return a;
+}
+
+
+/** \SSE4_1{SSE2,_mm_insert_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_insert_epi32_SSE2( __m128i a, int b, const int ndx )            // TODO: Verify behavior on Intel Hardware
+{
+    switch( ndx & 0x3 )
+    {
+    case 0: a = _mm_insert_epi16( a, b    , 0 );
+            a = _mm_insert_epi16( a, b<<16, 1 ); break;
+    case 1: a = _mm_insert_epi16( a, b    , 2 );
+            a = _mm_insert_epi16( a, b<<16, 3 ); break;
+    case 2: a = _mm_insert_epi16( a, b    , 4 );
+            a = _mm_insert_epi16( a, b<<16, 5 ); break;
+    case 3: a = _mm_insert_epi16( a, b    , 6 );
+            a = _mm_insert_epi16( a, b<<16, 7 ); break;
+    }
+    return a;
+}
+
+//
+// Min / Max
+//
+
+/** \SSE4_1{SSE2,_mm_min_epi8} */
+SSP_FORCEINLINE
+__m128i ssp_min_epi8_SSE2( __m128i a, __m128i b )
+{
+    __m128i mask  = _mm_cmplt_epi8( a, b );                             // FFFFFFFF where a < b
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_max_epi8} */
+SSP_FORCEINLINE
+__m128i ssp_max_epi8_SSE2( __m128i a, __m128i b )
+{
+    __m128i mask  = _mm_cmpgt_epi8( a, b );                             // FFFFFFFF where a > b
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_min_epu16} */
+SSP_FORCEINLINE
+__m128i ssp_min_epu16_SSE2( __m128i a, __m128i b )
+{
+    __m128i mask = ssp_comlt_epu16_SSE2( a, b );
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_max_epu16} */
+SSP_FORCEINLINE
+__m128i ssp_max_epu16_SSE2( __m128i a, __m128i b )
+{
+    __m128i mask = ssp_comgt_epu16_SSE2( a, b );
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_min_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_min_epi32_SSE2( __m128i a, __m128i b )
+{
+    __m128i mask  = _mm_cmplt_epi32( a, b );                            // FFFFFFFF where a < b
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_max_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_max_epi32_SSE2( __m128i a, __m128i b )
+{
+    __m128i mask  = _mm_cmpgt_epi32( a, b );                            // FFFFFFFF where a > b
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_min_epu32} */
+SSP_FORCEINLINE
+__m128i ssp_min_epu32_SSE2 ( __m128i a, __m128i b )
+{
+    __m128i mask = ssp_comlt_epu32_SSE2( a, b );
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_max_epu32} */
+SSP_FORCEINLINE
+__m128i ssp_max_epu32_SSE2 ( __m128i a, __m128i b )
+{
+   __m128i mask = ssp_comgt_epu32_SSE2( a, b );
+    a = ssp_logical_bitwise_select_SSE2( a, b, mask );
+    return a;
+}
+
+
+/** \SSSE3{SSE2,_mm_maddubs_epi16} 
+
+in:  2 registers x 16 x 8 bit values (a is unsigned, b is signed)
+out: 1 register  x 8  x 16 bit values
+
+r0 := SATURATE_16((a0 * b0) + (a1 * b1))
+
+*/
+SSP_FORCEINLINE __m128i ssp_maddubs_epi16_SSE2( __m128i a,  __m128i b)
+{
+    const static __m128i EVEN_8 = SSP_CONST_SET_8I( 0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF);
+    __m128i Aodd, Aeven, Beven, Bodd;
+
+    // Convert the 8 bit inputs into 16 bits by dropping every other value
+    Aodd  = _mm_srli_epi16( a, 8 );             // A is unsigned  
+    Bodd  = _mm_srai_epi16( b, 8 );             // B is signed
+
+    Aeven = _mm_and_si128 ( a, EVEN_8 );        // A is unsigned   
+    Beven = _mm_slli_si128( b,     1  );        // B is signed
+    Beven = _mm_srai_epi16( Beven, 8  );
+
+    a = _mm_mullo_epi16( Aodd , Bodd  );        // Will always fit in lower 16
+    b = _mm_mullo_epi16( Aeven, Beven );  
+    a = _mm_adds_epi16 ( a, b );
+	return a;
+}
+
+
+
+/** \SSE4_1{SSE2,_mm_mpsadbw_epu8} */
+SSP_FORCEINLINE
+__m128i ssp_mpsadbw_epu8_SSE2 ( __m128i a, __m128i b, const int msk  ) // _mm_mpsadbw_epu8
+{
+    const static __m128i MASK_BITS04 = SSP_CONST_SET_16I( 0,0,0,0xFFFF,0,0,0,0xFFFF );
+    const static __m128i MASK_BITS15 = SSP_CONST_SET_16I( 0,0,0xFFFF,0,0,0,0xFFFF,0 );
+    const static __m128i MASK_BITS26 = SSP_CONST_SET_16I( 0,0xFFFF,0,0,0,0xFFFF,0,0 );
+    const static __m128i MASK_BITS37 = SSP_CONST_SET_16I( 0xFFFF,0,0,0,0xFFFF,0,0,0 );
+
+    ssp_m128 A,B,A16,tmp,out;
+    A.i = a;
+    B.i = b;
+
+    switch( msk & 0x4 )         // Possible values: 0, 4
+    {
+    case 4: A.i = _mm_srli_si128( A.i, 4 );
+    }
+
+    switch( (msk & 0x3) * 4 )   // Possible values: 0, 4, 8, 12
+    {
+    case 0:     B.i = _mm_shuffle_epi32( B.i, _MM_SHUFFLE(0,0,0,0) ); break;
+    case 4:     B.i = _mm_shuffle_epi32( B.i, _MM_SHUFFLE(1,1,1,1) ); break;
+    case 8:     B.i = _mm_shuffle_epi32( B.i, _MM_SHUFFLE(2,2,2,2) ); break;
+    case 12:    B.i = _mm_shuffle_epi32( B.i, _MM_SHUFFLE(3,3,3,3) ); break;
+    //default: ASSERT( false );
+    }
+
+    // out[0,4]
+    B.i   = _mm_unpacklo_epi8( B.i, _mm_setzero_si128() );          // 1 2 3 4 | 1 2 3 4
+    A16.i = _mm_unpacklo_epi8( A.i, _mm_setzero_si128() );          // a b c d | e f g h
+    tmp.i = _mm_subs_epi16                 ( A16.i, B.i );          // a-1,b-2,c-3,d-4 | e-1,f-2,g-3,h-4
+    tmp.i = ssp_abs_epi16_SSE2             ( tmp.i    );            // abs(a-1),abs(b-2),...,abs(h-4) | ...
+    tmp.i = ssp_arithmetic_hadd4_epi16_SSE2( tmp.i, 0 );            // x,x,x,abs(a-1)+abs(b-2)+abs(c-3)+abs(d-4) | ...
+    tmp.i = _mm_and_si128                  ( tmp.i, MASK_BITS04 );  // 0,0,0,abs(a-1)+abs(b-2)+abs(c-3)+abs(d-4) | ...
+    out.i = tmp.i;
+
+    // out[1,5]
+    A16.i = _mm_srli_si128   ( A.i, 1 );
+    A16.i = _mm_unpacklo_epi8( A16.i, _mm_setzero_si128() );        // b c | d e | f g | h i
+    tmp.i = _mm_subs_epi16                 ( A16.i, B.i );
+    tmp.i = ssp_abs_epi16_SSE2             ( tmp.i    );
+    tmp.i = ssp_arithmetic_hadd4_epi16_SSE2( tmp.i, 1 );
+    tmp.i = _mm_and_si128                  ( tmp.i, MASK_BITS15 );
+    out.i = _mm_or_si128                   ( out.i, tmp.i );
+
+    // out[2,6]
+    A16.i = _mm_srli_si128   ( A.i, 2 );
+    A16.i = _mm_unpacklo_epi8( A16.i, _mm_setzero_si128() );        // c d | e f | g h | i j
+    tmp.i = _mm_subs_epi16                 ( A16.i, B.i );
+    tmp.i = ssp_abs_epi16_SSE2             ( tmp.i    );
+    tmp.i = ssp_arithmetic_hadd4_epi16_SSE2( tmp.i, 2 );
+    tmp.i = _mm_and_si128                  ( tmp.i, MASK_BITS26 );
+    out.i = _mm_or_si128                   ( out.i, tmp.i );
+
+    // out[3,7]
+    A16.i = _mm_srli_si128   ( A.i, 3 );
+    A16.i = _mm_unpacklo_epi8( A16.i, _mm_setzero_si128() );        // d e | f g | h i | j k
+    tmp.i = _mm_subs_epi16                 ( A16.i, B.i );
+    tmp.i = ssp_abs_epi16_SSE2             ( tmp.i    );
+    tmp.i = ssp_arithmetic_hadd4_epi16_SSE2( tmp.i, 3 );
+    tmp.i = _mm_and_si128                  ( tmp.i, MASK_BITS37 );
+    out.i = _mm_or_si128                   ( out.i, tmp.i );
+
+    return out.i;
+}
+
+
+//---------------------------------------
+// Dot Product
+//---------------------------------------
+
+/** \SSE4_1{SSE2,_mm_dp_pd} */
+SSP_FORCEINLINE
+__m128d ssp_dp_pd_SSE2( __m128d a, __m128d b, const int mask )               // _mm_dp_pd [SSE4,  cycles]
+{
+    int smallMask = (mask & 0x33)<<16;
+    const static __m128i mulShiftImm_01 = SSP_CONST_SET_32I( 0x40000000, 0x40000000, 0x80000000, 0x80000000 );   // Shift mask multiply moves 0,1, bits to left, becomes MSB
+    const static __m128i mulShiftImm_45 = SSP_CONST_SET_32I( 0x04000000, 0x04000000, 0x08000000, 0x08000000 );   // Shift mask multiply moves 4,5, bits to left, becomes MSB
+    ssp_m128 mHi, mLo;
+
+    mLo.i = _mm_set1_epi32( smallMask );// Load the mask into register
+    mHi.i = _mm_mullo_epi16( mLo.i, mulShiftImm_01 );       // Shift the bits
+    mLo.i = _mm_mullo_epi16( mLo.i, mulShiftImm_45 );       // Shift the bits
+
+    mHi.i = _mm_cmplt_epi32( mHi.i, _mm_setzero_si128() );  // FFFFFFFF if bit set, 00000000 if not set
+    mLo.i = _mm_cmplt_epi32( mLo.i, _mm_setzero_si128() );  // FFFFFFFF if bit set, 00000000 if not set
+
+    a = _mm_and_pd( a, mHi.d );                                     // Clear input using the high bits of the mask
+    a = _mm_mul_pd( a, b );
+
+    b = _mm_shuffle_pd( a, a, _MM_SHUFFLE2(0, 1) );                 // Shuffle the values so that we b = { a[0], a[1] } and a = { a[1], a[0] }
+    a = _mm_add_pd( a, b );                                         // Horizontally add the 4 values
+    a = _mm_and_pd( a, mLo.d );                                     // Clear output using low bits of the mask
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_dp_pd} */
+SSP_FORCEINLINE
+__m128 ssp_dp_ps_SSE2( __m128 a, __m128 b, const int mask )                  // _mm_dp_ps() [SSE4, 28 cycles]
+{
+    const static __m128i mulShiftImm_0123 = SSP_CONST_SET_32I( 0x010000, 0x020000, 0x040000, 0x080000 );   // Shift mask multiply moves 0,1,2,3 bits to left, becomes MSB
+    const static __m128i mulShiftImm_4567 = SSP_CONST_SET_32I( 0x100000, 0x200000, 0x400000, 0x800000 );   // Shift mask multiply moves 4,5,6,7 bits to left, becomes MSB
+
+    // Begin mask preparation
+    ssp_m128 mHi, mLo;
+    mLo.i = _mm_set1_epi32( mask );                                 // Load the mask into register
+    mLo.i = _mm_slli_si128( mLo.i, 3 );                             // Shift into reach of the 16 bit multiply
+
+    mHi.i = _mm_mullo_epi16( mLo.i, mulShiftImm_0123 );             // Shift the bits
+    mLo.i = _mm_mullo_epi16( mLo.i, mulShiftImm_4567 );             // Shift the bits
+
+    mHi.i = _mm_cmplt_epi32( mHi.i, _mm_setzero_si128() );          // FFFFFFFF if bit set, 00000000 if not set
+    mLo.i = _mm_cmplt_epi32( mLo.i, _mm_setzero_si128() );          // FFFFFFFF if bit set, 00000000 if not set
+    // End mask preparation - Mask bits 0-3 in mLo, 4-7 in mHi
+
+    a = _mm_and_ps( a, mHi.f );                                     // Clear input using the high bits of the mask
+    a = _mm_mul_ps( a, b );
+
+    a = ssp_arithmetic_hadd4_dup_ps_SSE2( a );                      // Horizontally add the 4 values
+    a = _mm_and_ps( a, mLo.f );                                     // Clear output using low bits of the mask
+    return a;
+}
+
+/** \SSE4_1{SSE2,_mm_round_ps}\n
+\b NOTE_1: When rounding from negative numbers to zero, this function returns 0 and NOT -0. \n
+If this behavor is desired, use the slower function ssp_round_ps_neg_zero_SSE2(). \n
+\b NOTE_2: This functon should used only with input in the range (-2,147,483,648 -> 2,147,483,647) \n
+If a greater range is desired, use the slower function ssp_round_ps_REF().
+*/
+SSP_FORCEINLINE
+__m128 ssp_round_ps_SSE2( __m128  a, int iRoundMode )
+{
+#ifndef __APPLE__
+#pragma message( "" WARN() "SSEPlus SSE2 rounding functions overflow if input outside 32 bit integer range" )
+#endif /* __APPLE__ */
+
+    enum ENUM_MXCSR
+    {
+        CSR_ROUND_TO_EVEN = 0x00001F80, //
+        CSR_ROUND_DOWN    = 0x00003F80, //
+        CSR_ROUND_UP      = 0x00005F80, //
+        CSR_ROUND_TRUNC   = 0x00007F80, //(_mm_getcsr() & ~_mm_ROUND_MASK) | _mm_ROUND_TOWARD_ZERO;
+    }; 
+
+    ssp_u32 bak = _mm_getcsr();
+    ssp_m128 A, i;
+    A.f = a;
+
+    switch( iRoundMode & 0x3 )
+    {
+    case SSP_FROUND_CUR_DIRECTION:                                      break;
+    case SSP_FROUND_TO_ZERO:            _mm_setcsr( CSR_ROUND_TRUNC  ); break;
+    case SSP_FROUND_TO_POS_INF:         _mm_setcsr( CSR_ROUND_UP     ); break;
+    case SSP_FROUND_TO_NEG_INF:         _mm_setcsr( CSR_ROUND_DOWN   ); break;
+    default:                            _mm_setcsr( CSR_ROUND_TO_EVEN); break;
+    }
+    
+    i.i    = _mm_cvtps_epi32( A.f );    // Convert to integer
+    A.f    = _mm_cvtepi32_ps( i.i );    // Convert back to float
+
+    i.u32[0] = bak;                     // Workaround for a bug in the MSVC compiler. MSVC was hoisting the mxcsr restore above the converts. 
+    _mm_setcsr( i.u32[0] );
+    return A.f;
+}
+
+
+SSP_FORCEINLINE
+__m128d ssp_round_pd_SSE2( __m128d  a, int iRoundMode )
+{
+#ifndef __APPLE__
+#pragma message( "" WARN() "SSEPlus SSE2 rounding functions overflow if input outside 32 bit integer range" )
+#endif /* __APPLE__ */
+
+    enum ENUM_MXCSR
+    {
+        CSR_ROUND_TO_EVEN = 0x00001F80, //
+        CSR_ROUND_DOWN    = 0x00003F80, //
+        CSR_ROUND_UP      = 0x00005F80, //
+        CSR_ROUND_TRUNC   = 0x00007F80, //(_mm_getcsr() & ~_mm_ROUND_MASK) | _mm_ROUND_TOWARD_ZERO;
+    }; 
+
+    ssp_u32 bak = _mm_getcsr();
+    ssp_m128 A, i;
+    A.d = a;
+    
+
+    switch( iRoundMode & 0x3 )
+    {
+    case SSP_FROUND_CUR_DIRECTION:                                      break;
+    case SSP_FROUND_TO_ZERO:            _mm_setcsr( CSR_ROUND_TRUNC  ); break;
+    case SSP_FROUND_TO_POS_INF:         _mm_setcsr( CSR_ROUND_UP     ); break;
+    case SSP_FROUND_TO_NEG_INF:         _mm_setcsr( CSR_ROUND_DOWN   ); break;
+    default:                            _mm_setcsr( CSR_ROUND_TO_EVEN); break;
+    }
+    
+    i.i    = _mm_cvtpd_epi32( A.d );    // Convert to integer
+    A.d    = _mm_cvtepi32_pd( i.i );    // Convert back to float
+
+    i.u32[0] = bak;                     // Workaround for a bug in the MSVC compiler. MSVC was hoisting the mxcsr restore above the converts. 
+    _mm_setcsr( i.u32[0] );             
+    return A.d;
+}
+
+/** \SSE4_1{SSE2,_mm_round_ss} */
+SSP_FORCEINLINE
+__m128 ssp_round_ss_SSE2( __m128  a, __m128  b, int iRoundMode )
+{
+	//Commented code will generate linker error in x64 platform
+    //ssp_m128 A,B;
+    //A.f = a;
+    //B.f = ssp_round_ps_SSE2( b, iRoundMode );
+
+    //A.f = _mm_move_ss( A.f, B.f );
+
+    ////A.f32[0] = B.f32[0];
+	//return A.f;
+	b = ssp_round_ps_SSE2(b, iRoundMode);               // B contains modified values through whole vector
+	b =    _mm_shuffle_ps(b, a, _MM_SHUFFLE(1,1,0,0));  
+    return _mm_shuffle_ps(b, a, _MM_SHUFFLE(3,2,2,0)); 
+}
+
+/** \SSE4_1{SSE2,_mm_ceil_ps} */
+SSP_FORCEINLINE
+__m128 ssp_ceil_ps_SSE2( __m128 a )
+{
+    return ssp_round_ps_SSE2( a, SSP_FROUND_TO_POS_INF );
+}
+
+/** \SSE4_1{SSE2,_mm_floor_ps} */
+SSP_FORCEINLINE
+__m128 ssp_floor_ps_SSE2( __m128 a )
+{
+    return ssp_round_ps_SSE2( a, SSP_FROUND_TO_NEG_INF );
+}
+
+/** \SSE4_1{SSE2,_mm_floor_pd} */
+SSP_FORCEINLINE
+__m128d ssp_floor_pd_SSE2( __m128d a )
+{
+    return ssp_round_pd_SSE2( a, SSP_FROUND_TO_NEG_INF );
+}
+
+/** \SSE4_1{SSE2,_mm_ceil_pd} */
+SSP_FORCEINLINE
+__m128d ssp_ceil_pd_SSE2( __m128d a )
+{
+    return ssp_round_pd_SSE2( a, SSP_FROUND_TO_POS_INF );
+}
+
+/** \SSE4_1{SSE2,_mm_ceil_sd} */
+SSP_FORCEINLINE __m128d ssp_floor_sd_SSE2( __m128d a, __m128d b)                              
+{
+	b = ssp_round_pd_SSE2(b, SSP_FROUND_TO_NEG_INF );
+
+    return _mm_shuffle_pd(b, a, _MM_SHUFFLE2(1,0));
+}
+
+/** \SSE4_1{SSE2,_mm_ceil_sd} */
+SSP_FORCEINLINE __m128d ssp_ceil_sd_SSE2( __m128d a, __m128d b)                              
+{
+	b = ssp_round_pd_SSE2(b, SSP_FROUND_TO_POS_INF );
+
+    return _mm_shuffle_pd(b, a, _MM_SHUFFLE2(1,0));
+}
+
+/** \SSE4_1{SSE2,_mm_floor_ss} */
+SSP_FORCEINLINE __m128 ssp_floor_ss_SSE2( __m128 a, __m128 b)                              
+{
+	b = ssp_round_ps_SSE2(b, SSP_FROUND_TO_NEG_INF );
+	b = _mm_shuffle_ps(b, a, _MM_SHUFFLE(1,1,0,0));
+    return _mm_shuffle_ps(b, a, _MM_SHUFFLE(3,2,2,0));
+}
+
+/** \SSE4_1{SSE2,_mm_ceil_ss} */
+SSP_FORCEINLINE __m128 ssp_ceil_ss_SSE2( __m128 a, __m128 b)                              
+{
+	b = ssp_round_ps_SSE2(b, SSP_FROUND_TO_POS_INF );
+	b = _mm_shuffle_ps(b, a, _MM_SHUFFLE(1,1,0,0));
+    return _mm_shuffle_ps(b, a, _MM_SHUFFLE(3,2,2,0));
+}
+
+
+//SSP_FORCEINLINE
+//__m128i _mm_mul_epi32( __m128i a, __m128i b ) //_mm_mul_epi32
+//{
+//    //ssp_m128 sign;
+//
+//    //sign.i = ::_mm_xor_si128( a, b );
+//
+//    //__m128i t1;
+//
+//    a = _mm_min_epi32( a, b );
+//
+//    //a = _mm_slli_si128( a, 4 );
+//    //b = _mm_slli_si128( b, 4 );
+//
+//    //a = _mm_mul_epu32( a, b );
+//
+//    //a = _mm_slli_si128( a, 4 );
+//
+//    return a;
+//}
+
+
+//type conversion
+
+/** \SSE4_1{SSE2,_mm_cvtepi8_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepi8_epi16_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_setzero_si128 ();
+	__m128i c = _mm_unpacklo_epi8(a, b);
+	__m128i d = _mm_set1_epi16 (128);
+
+	b = _mm_and_si128(d, c);
+	d = _mm_set1_epi16(0x1FE);
+	b = _mm_mullo_epi16(b, d);
+
+	return _mm_add_epi16(c, b);
+
+	//Another way, slower
+	//__m128i b = _mm_set1_epi32 (-1);				//0xFFFFFFFF
+	//__m128i c = _mm_unpacklo_epi8(a, b);			//FFa0FFa1....
+	//__m128i d = _mm_set1_epi16 (128);				//0x80
+	//b = _mm_andnot_si128(c, d);					// 0x80 for positive, 0x00 for negative
+	//d = _mm_slli_epi16(b, 1);						// 0x100 for positive, 0x000 for negative
+	//return _mm_add_epi16(c, d);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepi8_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepi8_epi32_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_set1_epi32 (-1);				//0xFFFFFFFF
+	__m128i c = _mm_unpacklo_epi8(a, b);			//FFa0FFa1....
+	__m128i d = _mm_set1_epi32 (128);				//0x80
+
+	c = _mm_unpacklo_epi16(c, b);					//FFFFFFa0FFFFFFa1...
+	b = _mm_andnot_si128(c, d);						// 0x80 for positive, 0x00 for negative
+	d = _mm_slli_epi32(b, 1);						// 0x100 for positive, 0x000 for negative
+
+	return _mm_add_epi32(c, d);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepi8_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepi8_epi64_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_set1_epi32 (-1);				//0xFFFFFFFF
+	__m128i c = _mm_unpacklo_epi8(a, b);			//FFa0FFa1....
+	__m128i d = _mm_set_epi32 (0, 128, 0, 128);		//0x80
+
+	c = _mm_unpacklo_epi16(c, b);					//FFFFFFa0FFFFFFa1...
+	c = _mm_unpacklo_epi32(c, b);					//FFFFFFFFFFFFFFa0...
+	b = _mm_andnot_si128(c, d);						// 0x80 for positive, 0x00 for negative
+	d = _mm_slli_epi64(b, 1);						// 0x100 for positive, 0x000 for negative
+
+	return _mm_add_epi64(c, d);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepi16_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepi16_epi32_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_set1_epi32 (-1);				//0xFFFFFFFF
+	__m128i c = _mm_unpacklo_epi16(a, b);			//FFFFa0**FFFFa1**....
+	__m128i d = _mm_set1_epi32 (0x8000);			//0x8000
+
+	b = _mm_andnot_si128(c, d);						// 0x80 for positive, 0x00 for negative
+	d = _mm_slli_epi32(b, 1);						// 0x100 for positive, 0x000 for negative
+
+	return _mm_add_epi32(c, d);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepi16_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepi16_epi64_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_set1_epi32 (-1);				//0xFFFFFFFF
+	__m128i c = _mm_unpacklo_epi16(a, b);			//FFFFa0**FFFFa1**....
+	__m128i d = _mm_set_epi32(0,0x8000, 0,0x8000);	//0x8000
+
+	c = _mm_unpacklo_epi32(c, b);					//FFFFFFFFFFFFFFa0...
+	b = _mm_andnot_si128(c, d);						// 0x80 for positive, 0x00 for negative
+	d = _mm_slli_epi64(b, 1);						// 0x100 for positive, 0x000 for negative
+
+	return _mm_add_epi64(c, d);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepi32_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepi32_epi64_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_set1_epi32 (-1);				//0xFFFFFFFF
+	__m128i c = _mm_unpacklo_epi32(a, b);			//FFFFFFFFa0******FFFFFFFFa1******....
+	__m128i d = _mm_set_epi32(0, 0x80000000,0,0x80000000);	//0x80000000
+
+	b = _mm_andnot_si128(c, d);						// 0x80 for positive, 0x00 for negative
+	d = _mm_slli_epi64(b, 1);						// 0x100 for positive, 0x000 for negative
+
+	return _mm_add_epi64(c, d);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepu8_epi16} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepu8_epi16_SSE2 ( __m128i a)
+{
+	__m128i b =_mm_setzero_si128 ();
+
+	return _mm_unpacklo_epi8(a, b);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepu8_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepu8_epi32_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_setzero_si128 ();
+
+	a = _mm_unpacklo_epi8(a, b);
+
+	return _mm_unpacklo_epi16(a, b);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepu8_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepu8_epi64_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_setzero_si128 ();
+
+	a = _mm_unpacklo_epi8(a, b);
+
+	a = _mm_unpacklo_epi16(a, b);
+
+	return _mm_unpacklo_epi32(a, b);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepu16_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepu16_epi32_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_setzero_si128 ();
+
+	return _mm_unpacklo_epi16(a, b);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepu16_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepu16_epi64_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_setzero_si128 ();
+
+	a = _mm_unpacklo_epi16(a, b);
+
+	return _mm_unpacklo_epi32(a, b);
+}
+
+/** \SSE4_1{SSE2,_mm_cvtepu32_epi64} */
+SSP_FORCEINLINE
+__m128i ssp_cvtepu32_epi64_SSE2 ( __m128i a)
+{
+	__m128i b = _mm_setzero_si128 ();
+
+	return _mm_unpacklo_epi32(a, b);
+}
+
+/** \SSE4_1{SSE2,_mm_packus_epi32} */
+SSP_FORCEINLINE
+__m128i ssp_packus_epi32_SSE2( __m128i a, __m128i b )
+{
+    const static __m128i val_32 = SSP_CONST_SET_32I(  0x8000, 0x8000, 0x8000, 0x8000 );
+    const static __m128i val_16 = SSP_CONST_SET_16I(  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000 );
+
+    a = _mm_sub_epi32( a, val_32 );
+    b = _mm_sub_epi32( b, val_32 );
+    a = _mm_packs_epi32( a, b );
+    a = _mm_add_epi16( a, val_16 );
+    return a;
+}
+
+//SSSE3
+// bit manipulation
+//__m128i _mm_alignr_epi8(__m128i a, __m128i b, const int ralign);
+/**  \SSSE3{Reference,_mm_alignr_epi8} */
+SSP_FORCEINLINE
+__m128i ssp_alignr_epi8_SSE2 (__m128i a, __m128i b, const int ralign)
+{
+	if (ralign < 0)  return b; //only shift to right, no negative
+	//if (ralign > 32) return _mm_setzero_si128();
+	//
+	//if (ralign > 16) return _mm_srli_si128(a, ralign-16);
+
+	//b = _mm_srli_si128(b, ralign);
+	//a = _mm_slli_si128(a, 16-ralign);
+	switch (ralign) {
+        case 0: 
+                return b;                       
+        case 1:	
+                b = _mm_srli_si128(b, 1);       
+                a = _mm_slli_si128(a, 15);	    
+                return _mm_or_si128( a, b );	
+        case 2: 
+                b = _mm_srli_si128(b, 2);       
+                a = _mm_slli_si128(a, 14);	    
+                return _mm_or_si128( a, b );	
+        case 3:	
+                b = _mm_srli_si128(b, 3);       
+                a = _mm_slli_si128(a, 13);	    
+                return _mm_or_si128( a, b );	
+        case 4:				                
+                b = _mm_srli_si128(b, 4);	    
+                a = _mm_slli_si128(a, 12);	    
+                return _mm_or_si128( a, b );	
+        case 5:				                
+                b = _mm_srli_si128(b, 5);	    
+                a = _mm_slli_si128(a, 11);	    
+                return _mm_or_si128( a, b );	
+        case 6:				                
+                b = _mm_srli_si128(b, 6);	    
+                 a = _mm_slli_si128(a, 10);	    
+                return _mm_or_si128( a, b );	
+        case 7:				                
+                b = _mm_srli_si128(b, 7);	    
+                a = _mm_slli_si128(a, 9);	    
+                return _mm_or_si128( a, b );	
+        case 8:				                
+                b = _mm_srli_si128(b, 8);	    
+                a = _mm_slli_si128(a, 8);	    
+                return _mm_or_si128( a, b );	
+        case 9:				                
+                b = _mm_srli_si128(b, 9);	    
+                a = _mm_slli_si128(a, 7);	    
+                return _mm_or_si128( a, b );	
+        case 10:				            
+                b = _mm_srli_si128(b, 10);	    
+                a = _mm_slli_si128(a,  6);	    
+                return _mm_or_si128( a, b );	
+        case 11:				            
+                b = _mm_srli_si128(b, 11);      
+                a = _mm_slli_si128(a,  5);      
+                return _mm_or_si128( a, b );	
+        case 12:				            
+                b = _mm_srli_si128(b, 12);      
+                a = _mm_slli_si128(a,  4);      
+                return _mm_or_si128( a, b );	
+        case 13:				            
+                b = _mm_srli_si128(b, 13);      
+                a = _mm_slli_si128(a,  3);      
+                return _mm_or_si128( a, b );	
+        case 14:				            
+                b = _mm_srli_si128(b, 14);	    
+                a = _mm_slli_si128(a,  2);      
+                return _mm_or_si128( a, b );	
+        case 15:				            
+                b = _mm_srli_si128(b, 15);	    
+                a = _mm_slli_si128(a,  1);	    
+                return _mm_or_si128( a, b );    
+        case 16:                            
+                return a;                       
+        case 17:                            
+	            a    = _mm_slli_si128(a,  1);   
+	            return _mm_srli_si128(a,  1);   
+        case 18:                            
+	            a    = _mm_slli_si128(a,  2);   
+	            return _mm_srli_si128(a,  2);   
+        case 19:                            
+	            a    = _mm_slli_si128(a,  3);   
+	            return _mm_srli_si128(a,  3);   
+        case 20:                            
+	            a    = _mm_slli_si128(a,  4);   
+	            return _mm_srli_si128(a,  4);   
+        case 21:                            
+	            a    = _mm_slli_si128(a,  5);   
+	            return _mm_srli_si128(a,  5);   
+        case 22:                            
+	            a    = _mm_slli_si128(a,  6);   
+	            return _mm_srli_si128(a,  6);   
+        case 23:                            
+	            a    = _mm_slli_si128(a,  7);   
+	            return _mm_srli_si128(a,  7);   
+        case 24:                            
+	            a    = _mm_slli_si128(a,  8);   
+	            return _mm_srli_si128(a,  8);   
+        case 25:                            
+	            a    = _mm_slli_si128(a,  9);   
+	            return _mm_srli_si128(a,  9);   
+        case 26:                            
+	            a    = _mm_slli_si128(a, 10);   
+	            return _mm_srli_si128(a, 10);   
+        case 27:                            
+	            a    = _mm_slli_si128(a, 11);   
+	            return _mm_srli_si128(a, 11);   
+        case 28:                            
+	            a    = _mm_slli_si128(a, 12);   
+	            return _mm_srli_si128(a, 12);   
+        case 29:                            
+	            a    = _mm_slli_si128(a, 13);   
+	            return _mm_srli_si128(a, 13);   
+        case 30:                            
+	            a    = _mm_slli_si128(a, 14);   
+	            return _mm_srli_si128(a, 14);   
+        case 31:                            
+	            a    = _mm_slli_si128(a, 15);   
+	            return _mm_srli_si128(a, 15);   
+        default:                            
+	            return _mm_setzero_si128(); 
+	}
+}
+
+//---------------------------------------
+//Insert
+//---------------------------------------
+/** \SSE4_1{SSE2,_mm_insert_epi8} */
+SSP_FORCEINLINE __m128i ssp_insert_epi8_SSE2( __m128i a, int b, const int ndx )
+{
+    ssp_m128 Ahi, Alo;
+    b = b & 0xFF;                                           /* Convert to 8-bit integer */
+    Ahi.i = _mm_unpackhi_epi8( a, _mm_setzero_si128() );    /* Ahi = a_8[8:15]  Simulate 8bit integers as 16-bit integers */
+    Alo.i = _mm_unpacklo_epi8( a, _mm_setzero_si128() );    /* Alo = a_8[0:7]   Simulate 8bit integers as 16-bit integers */
+
+    /* Insert b as a 16-bit integer to upper or lower half of a */
+    switch( ndx & 0xF )
+    {
+    case 0:  Alo.i = _mm_insert_epi16( Alo.i, b, 0 ); break;
+    case 1:  Alo.i = _mm_insert_epi16( Alo.i, b, 1 ); break;
+    case 2:  Alo.i = _mm_insert_epi16( Alo.i, b, 2 ); break;
+    case 3:  Alo.i = _mm_insert_epi16( Alo.i, b, 3 ); break;
+    case 4:  Alo.i = _mm_insert_epi16( Alo.i, b, 4 ); break;
+    case 5:  Alo.i = _mm_insert_epi16( Alo.i, b, 5 ); break;
+    case 6:  Alo.i = _mm_insert_epi16( Alo.i, b, 6 ); break;
+    case 7:  Alo.i = _mm_insert_epi16( Alo.i, b, 7 ); break;
+    case 8:  Ahi.i = _mm_insert_epi16( Ahi.i, b, 0 ); break;
+    case 9:  Ahi.i = _mm_insert_epi16( Ahi.i, b, 1 ); break;
+    case 10: Ahi.i = _mm_insert_epi16( Ahi.i, b, 2 ); break;
+    case 11: Ahi.i = _mm_insert_epi16( Ahi.i, b, 3 ); break;
+    case 12: Ahi.i = _mm_insert_epi16( Ahi.i, b, 4 ); break;
+    case 13: Ahi.i = _mm_insert_epi16( Ahi.i, b, 5 ); break;
+    case 14: Ahi.i = _mm_insert_epi16( Ahi.i, b, 6 ); break;
+    default: Ahi.i = _mm_insert_epi16( Ahi.i, b, 7 );
+    }
+    return _mm_packus_epi16( Alo.i, Ahi.i ); // Pack the 16-bit integers to 8bit again.
+
+    ///* Another implementation, but slower: */
+    //ssp_m128 A, B, mask;
+    //mask.i = _mm_setzero_si128();
+    //mask.s8[ ndx & 0x0F ] = (ssp_s8)0xFF;
+    //B.i    = _mm_set1_epi8( (ssp_s8)b );
+    //A.i    = _mm_andnot_si128( mask.i, a );
+    //mask.i = _mm_and_si128( mask.i, B.i );
+    //A.i = _mm_or_si128( A.i, mask.i );
+    //return A.i;
+}
+/** \SSE4a{SSE2,_mm_inserti_si64} */
+SSP_FORCEINLINE __m128i ssp_inserti_si64_SSE2( __m128i a, __m128i b, int len, int ndx )
+{
+    const static __m128i MASK = SSP_CONST_SET_32I( 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF );
+
+    int left = ndx + len;
+    __m128i m;
+    m = _mm_slli_epi64( MASK, 64-left );    // clear the mask to the left
+    m = _mm_srli_epi64( m,    64-len  );    // clear the mask to the right
+    m = _mm_slli_epi64( m,    ndx     );    // put the mask into the proper position
+    b = _mm_slli_epi64( b,    ndx     );    // put the insert bits into the proper position
+
+    a = ssp_logical_bitwise_select_SSE2( b, a, m );
+    return a;
+}
+
+
+/** \SSE4a{SSE2,_mm_insert_si64} */
+SSP_FORCEINLINE __m128i ssp_insert_si64_SSE2( __m128i a, __m128i b )
+{
+    ssp_u32 ndx, len;
+    ssp_m128 B;
+    B.i = b;
+
+    ndx = (ssp_u32)((B.u64[1] & 0x3F00) >> 8);    // Mask length field.
+    len = (ssp_u32)((B.u64[1] & 0x003F));         // Mask ndx field.
+
+    a = ssp_inserti_si64_SSE2( a, b, len, ndx );
+    return a;
+}
+
+//---------------------------------------
+//Extract
+//---------------------------------------
+
+/** \SSE4_1{SSE2,_mm_extract_epi8} */
+SSP_FORCEINLINE int ssp_extract_epi8_SSE2( __m128i a, const int ndx )                       
+{
+    ssp_m128 mask;
+    switch( ndx & 0xF )
+    {
+    case 15:  a = _mm_srli_si128( a, 15 ); break;
+    case 14:  a = _mm_srli_si128( a, 14 ); break;
+    case 13:  a = _mm_srli_si128( a, 13 ); break;
+    case 12:  a = _mm_srli_si128( a, 12 ); break;
+    case 11:  a = _mm_srli_si128( a, 11 ); break;
+    case 10:  a = _mm_srli_si128( a, 10 ); break;
+    case 9:   a = _mm_srli_si128( a,  9 ); break;
+    case 8:   a = _mm_srli_si128( a,  8 ); break;
+    case 7:   a = _mm_srli_si128( a,  7 ); break;
+    case 6:   a = _mm_srli_si128( a,  6 ); break;
+    case 5:   a = _mm_srli_si128( a,  5 ); break;
+    case 4:   a = _mm_srli_si128( a,  4 ); break;
+    case 3:   a = _mm_srli_si128( a,  3 ); break;
+    case 2:   a = _mm_srli_si128( a,  2 ); break;
+    case 1:   a = _mm_srli_si128( a,  1 ); break;
+    }
+
+    mask.i = _mm_setr_epi8 ( -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ); 
+    // mask = { 00,00,00,00,00,00,00,00,00,00,00,00,00,00,00,FF }
+    mask.i = _mm_and_si128 ( mask.i, a   );
+    return mask.s8[0];
+}
+
+/** \SSE4_1{SSE2,_mm_extract_epi32} */
+SSP_FORCEINLINE int ssp_extract_epi32_SSE2( __m128i a, const int imm )                            
+{
+    ssp_m128 mask;
+    switch( imm & 0x3 )
+    {
+    case 3:  a = _mm_srli_si128( a, 12 ); break;
+    case 2:  a = _mm_srli_si128( a, 8  ); break;
+    case 1:  a = _mm_srli_si128( a, 4  ); break;
+    }
+
+    mask.i = _mm_set_epi32 ( 0x00000000, 0x00000000, 0x00000000, 0xFFFFFFFF );
+    mask.i = _mm_and_si128 ( mask.i, a   );
+
+    return mask.s32[0];
+}
+
+/** \SSE4_1{SSE2,_mm_extract_ps} */
+SSP_FORCEINLINE int ssp_extract_ps_SSE2( __m128 a, const int ndx )                          
+{
+    ssp_m128 A;
+    A.f = a;
+    return ssp_extract_epi32_SSE2( A.i, ndx );
+}
+
+/** \SSE4_1{SSE2,_mm_extract_epi64} */
+SSP_FORCEINLINE ssp_s64 ssp_extract_epi64_SSE2( __m128i a, const int ndx )                  
+{
+    ssp_m128 mask;
+    switch( ndx & 0x1 )
+    {
+    case 1:  a = _mm_srli_si128( a, 8  ); break;
+    }
+
+    mask.i = _mm_set_epi32 ( 0x00000000, 0x00000000, 0xFFFFFFFF, 0xFFFFFFFF );
+    mask.i = _mm_and_si128 ( mask.i, a   );
+
+    return mask.s64[0];
+}
+
+/**  \SSE4a{SSE2,_mm_extracti_si64}
+\n  NOTE: The upper 64-bits of the destination register are undefined.
+*/
+SSP_FORCEINLINE __m128i ssp_extracti_si64_SSE2( __m128i a, int len, int ndx )   
+{
+    int left = ndx + len;   
+    a = _mm_slli_epi64( a, 64-left );    // clear the mask to the left
+    a = _mm_srli_epi64( a, 64-len  );    // clear the mask to the right      
+    return a;
+}
+
+
+/**  \SSE4a{SSE2,_mm_extract_si64} 
+\n  NOTE: The upper 64-bit of the destination register are undefined.
+*/
+SSP_FORCEINLINE __m128i ssp_extract_si64_SSE2( __m128i a ,__m128i b )        
+{
+    ssp_u32 len, ndx;   
+    ssp_m128 B;
+    B.i = b;
+
+    ndx = (ssp_u32)((B.u64[0] & 0x3F00) >> 8);    // Mask ndx field.
+    len = (ssp_u32)((B.u64[0] & 0x003F));         // Mask len field.
+
+    a = ssp_extracti_si64_SSE2( a, len, ndx );   
+    return a;
+}
+
+
+/**  \SSSE3{SSE2,_mm_shuffle_epi8} */
+SSP_FORCEINLINE __m128i ssp_shuffle_epi8_SSE2 (__m128i a, __m128i mask)
+{  
+    ssp_m128 A,B, MASK, maskZero;	
+    A.i        = a;
+    maskZero.i = ssp_comge_epi8_SSE2( mask, _mm_setzero_si128()        );    
+    MASK.i     = _mm_and_si128      ( mask, _mm_set1_epi8( (char)0x0F) );
+
+    B.s8[ 0] = A.s8[ (MASK.s8[ 0]) ];
+	B.s8[ 1] = A.s8[ (MASK.s8[ 1]) ];
+	B.s8[ 2] = A.s8[ (MASK.s8[ 2]) ];
+	B.s8[ 3] = A.s8[ (MASK.s8[ 3]) ];
+	B.s8[ 4] = A.s8[ (MASK.s8[ 4]) ];
+	B.s8[ 5] = A.s8[ (MASK.s8[ 5]) ];
+	B.s8[ 6] = A.s8[ (MASK.s8[ 6]) ];
+	B.s8[ 7] = A.s8[ (MASK.s8[ 7]) ];
+	B.s8[ 8] = A.s8[ (MASK.s8[ 8]) ];
+	B.s8[ 9] = A.s8[ (MASK.s8[ 9]) ];
+	B.s8[10] = A.s8[ (MASK.s8[10]) ];
+	B.s8[11] = A.s8[ (MASK.s8[11]) ];
+	B.s8[12] = A.s8[ (MASK.s8[12]) ];
+	B.s8[13] = A.s8[ (MASK.s8[13]) ];
+	B.s8[14] = A.s8[ (MASK.s8[14]) ];
+	B.s8[15] = A.s8[ (MASK.s8[15]) ];  
+
+    B.i = _mm_and_si128( B.i, maskZero.i );
+	return B.i;
+}
+
+
+/** \SSSE3{SSE2,_mm_sign_epi8} */
+SSP_FORCEINLINE 
+__m128i ssp_sign_epi8_SSE2 (__m128i a, __m128i b)
+{
+    __m128i ap, an, c, d, zero, one;
+
+	zero=_mm_setzero_si128();
+	//Great than zero part
+	d  = _mm_cmpgt_epi8(b, zero);
+	ap = _mm_and_si128(a, d);
+
+	//Less than zero
+	c   = _mm_cmplt_epi8(b, zero);
+	one = _mm_set1_epi8(1);
+	an  = _mm_and_si128(a, c);  //get the all number which needs to be negated 
+	an  = _mm_xor_si128(an, c);
+	one = _mm_and_si128(one, c);
+	an  = _mm_add_epi8(an, one);
+
+	return _mm_or_si128(an, ap);//_mm_add_epi8(an, ap);
+}
+
+/** \SSSE3{SSE2,_mm_sign_epi16} */
+SSP_FORCEINLINE 
+__m128i ssp_sign_epi16_SSE2 (__m128i a, __m128i b)
+{
+    __m128i c, d, zero;
+
+	zero=_mm_setzero_si128();
+	d   = _mm_cmpgt_epi16(b, zero);
+	c   = _mm_cmplt_epi16(b, zero);
+	d   = _mm_srli_epi16(d, 15);
+	c   = _mm_or_si128(c, d);
+	a   = _mm_mullo_epi16(a, c);
+
+	//The following method has same performance
+	//zero=_mm_setzero_si128();
+	//d   = _mm_cmpgt_epi16(b, zero);
+	//c   = _mm_cmplt_epi16(b, zero);
+	//one = _mm_set1_epi16(1);
+	//d   = _mm_and_si128(d, one);
+	//c   = _mm_add_epi16(c, d);
+	//a   = _mm_mullo_epi16(a, c);
+
+	return a;
+}
+
+/** \SSSE3{SSE2,_mm_sign_epi32} */
+SSP_FORCEINLINE 
+__m128i ssp_sign_epi32_SSE2 (__m128i a, __m128i b)
+{
+    __m128i ap, an, c, d, zero, one;
+
+	zero=_mm_setzero_si128();
+	//Great than zero part
+	d  = _mm_cmpgt_epi32(b, zero);
+	ap = _mm_and_si128(a, d);
+
+	//Less than zero
+	c   = _mm_cmplt_epi32(b, zero);
+	one = _mm_set1_epi32(1);
+	an  = _mm_and_si128(a, c);  //get the all number which needs to be negated 
+	an  = _mm_xor_si128(an, c);
+	one = _mm_and_si128(one, c);
+	an  = _mm_add_epi8(an, one);
+
+	return _mm_or_si128(an, ap);
+}
+
+//---------------------------------------
+// Test
+//---------------------------------------
+/** \SSE4_1{SSE2,_mm_testc_si128} */
+SSP_FORCEINLINE int ssp_testc_si128_SSE2( __m128i a, __m128i b)                              
+{
+    a = _mm_xor_si128( a, b );
+    return ssp_testz_si128_SSE2( a, a );
+}
+
+/** \SSE4_1{SSE2,_mm_testz_si128} */
+SSP_FORCEINLINE 
+int ssp_testz_si128_SSE2( __m128i a, __m128i b)   // This is much faster in 64 bit                           
+{
+    ssp_m128 t;
+    t.i = _mm_and_si128  ( a, b );   
+    t.i = _mm_packs_epi32( t.i, _mm_setzero_si128() );   
+    return t.u64[0] == 0;
+}
+
+/** \SSE4_1{SSE2,_mm_testnzc_si128} */
+SSP_FORCEINLINE 
+int ssp_testnzc_si128_SSE2( __m128i a, __m128i b)                            
+{
+    ssp_m128 zf, cf;    
+
+    zf.i = _mm_and_si128  ( a, b );   
+    zf.i = _mm_packs_epi32( zf.i, _mm_setzero_si128() ); 
+ 
+    cf.i = _mm_andnot_si128( a, b );
+    cf.i = _mm_packs_epi32( cf.i, _mm_setzero_si128() );  
+
+    return ( !(zf.u64[0] == 0) && !(cf.u64[0] == 0));
+}
+
+
+//---------------------------------------
+// Move
+//---------------------------------------
+/** \SSE3{SSE2,_mm_movehdup_ps} */
+SSP_FORCEINLINE __m128 ssp_movehdup_ps_SSE2(__m128 a)                                   
+{
+    ssp_m128 A;
+    A.f = a;
+    A.i = _mm_shuffle_epi32( A.i, _MM_SHUFFLE( 3, 3, 1, 1) );
+    return A.f;
+}
+
+/** \SSE3{SSE2,_mm_moveldup_ps} */
+SSP_FORCEINLINE __m128 ssp_moveldup_ps_SSE2(__m128 a)                                   
+{
+    ssp_m128 A;
+    A.f = a;
+    A.i = _mm_shuffle_epi32( A.i, _MM_SHUFFLE( 2, 2, 0, 0) );
+    return A.f;
+}
+
+/** \SSE3{SSE2,_mm_movedup_pd} */
+SSP_FORCEINLINE __m128d ssp_movedup_pd_SSE2(__m128d a)                                  
+{
+    ssp_m128 A;
+    A.d = a;
+    return _mm_set_pd( A.f64[0], A.f64[0] );
+}
+
+/** \SSE5{SSE2,_mm_cmov_si128, pcmov } */
+SSP_FORCEINLINE __m128i ssp_cmov_si128_SSE2(__m128i a, __m128i b, __m128i c)
+{
+    a = _mm_and_si128( a, c );
+    b = _mm_andnot_si128( c, b );
+    a = _mm_or_si128( a, b );
+    return a;
+}
+
+//---------------------------------------
+// Rotates
+//---------------------------------------
+
+/** \SSE5{SSE2,_mm_roti_epi8, protb } */
+SSP_FORCEINLINE __m128i ssp_roti_epi8_SSE2(__m128i a, const int b)
+{
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        const unsigned int count = (-b) % 8;
+        const unsigned int carry_count = (8 - count) % 8;
+        __m128i t = ssp_slli_epi8_SSE2( A.i, carry_count );
+        A.i = ssp_srli_epi8_SSE2( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+    else
+    {
+        const unsigned int count = b % 8;
+        const unsigned int carry_count = (8 - count) % 8;
+        __m128i t = ssp_srli_epi8_SSE2( A.i, carry_count );
+        A.i = ssp_slli_epi8_SSE2( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+
+    return A.i;
+}
+/** \SSE5{SSE2,_mm_roti_epi16, protw } */
+SSP_FORCEINLINE __m128i ssp_roti_epi16_SSE2(__m128i a, const int b)
+{
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        const unsigned int count = (-b) % 16;
+        const unsigned int carry_count = (16 - count) % 16;
+        __m128i t = _mm_slli_epi16( A.i, carry_count );
+        A.i = _mm_srli_epi16( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+    else
+    {
+        const unsigned int count = b % 16;
+        const unsigned int carry_count = (16 - count) % 16;
+        __m128i t = _mm_srli_epi16( A.i, carry_count );
+        A.i = _mm_slli_epi16( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+
+    return A.i;
+}
+/** \SSE5{SSE2,_mm_roti_epi32, protd } */
+SSP_FORCEINLINE __m128i ssp_roti_epi32_SSE2(__m128i a, const int b)
+{
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        const unsigned int count = (-b) % 32;
+        const unsigned int carry_count = (32 - count) % 32;
+        __m128i t = _mm_slli_epi32( A.i, carry_count );
+        A.i = _mm_srli_epi32( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+    else
+    {
+        const unsigned int count = b % 32;
+        const unsigned int carry_count = (32 - count) % 32;
+        __m128i t = _mm_srli_epi32( A.i, carry_count );
+        A.i = _mm_slli_epi32( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+
+    return A.i;
+}
+/** \SSE5{SSE2,_mm_roti_epi64, protq } */
+SSP_FORCEINLINE __m128i ssp_roti_epi64_SSE2(__m128i a, const int b)
+{
+    ssp_m128 A;
+    A.i = a;
+
+    if( b < 0 )
+    {
+        const unsigned int count = (-b) % 64;
+        const unsigned int carry_count = (64 - count) % 64;
+        __m128i t = _mm_slli_epi64( A.i, carry_count );
+        A.i = _mm_srli_epi64( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+    else
+    {
+        const unsigned int count = b % 64;
+        const unsigned int carry_count = (64 - count) % 64;
+        __m128i t = _mm_srli_epi64( A.i, carry_count );
+        A.i = _mm_slli_epi64( A.i, count );
+        A.i = _mm_or_si128( A.i, t );
+    }
+
+    return A.i;
+}
+
+//--------------------------------------
+// Packed Shift Logical & Arithmetic
+//--------------------------------------
+
+/** \SSE5{SSE2,ssp_shl_epi16,pshlw } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi16_SSE2(__m128i a, __m128i b)
+{
+    __m128i v1, v2, mask, mask2, b1, b2;
+    b1 = ssp_abs_epi8_SSE2( b );
+    mask = _mm_set_epi16( 0, 0, 0, 0, 0, 0, 0, -1 );
+    mask2 = _mm_srli_epi16( mask, 12 ); // the shfit count is a 4 bit value
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ); // negative shift
+    v2 = _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+
+    mask = _mm_setzero_si128();
+    mask = _mm_cmpgt_epi8( mask, b ); // set mask to 0xFF for all negative shift counts in b
+    mask2 = _mm_slli_epi16( mask, 8 );
+    mask = _mm_or_si128( mask, mask2 );
+    v1 = _mm_and_si128( v1, mask );
+    mask = _mm_andnot_si128( mask, v2 );
+    v1 = _mm_or_si128( v1, mask );
+    return v1;
+}
+
+/** \SSE5{SSE2,ssp_sha_epi16,pshaw } */ 
+SSP_FORCEINLINE __m128i ssp_sha_epi16_SSE2(__m128i a, __m128i b)
+{
+    __m128i v1, v2, mask, mask2, b1, b2;
+    b1 = ssp_abs_epi8_SSE2( b );
+    mask = _mm_set_epi16( 0, 0, 0, 0, 0, 0, 0, -1 );
+    mask2 = _mm_srli_epi16( mask, 12 ); // the shfit count is a 4 bit value
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ); // negative shift
+    v2 = _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 2 );
+    b1 = _mm_srli_si128( b1, 2 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi16( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi16( a, b2 ), mask ) ); // positive shift
+
+    mask = _mm_setzero_si128();
+    mask = _mm_cmpgt_epi8( mask, b ); // set mask to 0xFF for all negative shift counts in b
+    mask2 = _mm_slli_epi16( mask, 8 );
+    mask = _mm_or_si128( mask, mask2 );
+    v1 = _mm_and_si128( v1, mask );
+    mask = _mm_andnot_si128( mask, v2 );
+    v1 = _mm_or_si128( v1, mask );
+    return v1;
+}
+
+/** \SSE5{SSE2,ssp_shl_epi32,pshld } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi32_SSE2(__m128i a, __m128i b)
+{
+    __m128i v1, v2, mask, mask2, b1, b2;
+    b1 = ssp_abs_epi8_SSE2( b );
+    mask = _mm_set_epi32( 0, 0, 0, -1 );
+    mask2 = _mm_srli_epi32( mask, 27 ); // the shfit count is a 5 bit value
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_and_si128( _mm_srl_epi32( a, b2 ), mask ); // negative shift
+    v2 = _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ); // positive shift
+    mask = _mm_slli_si128( mask, 4 );
+    b1 = _mm_srli_si128( b1, 4 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi32( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 4 );
+    b1 = _mm_srli_si128( b1, 4 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi32( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 4 );
+    b1 = _mm_srli_si128( b1, 4 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi32( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ) ); // positive shift
+
+    mask = _mm_setzero_si128();
+    mask = _mm_cmpgt_epi8( mask, b ); // set mask to 0xFF for all negative shift counts in b
+    mask = _mm_slli_epi32( mask, 24 );
+    mask = _mm_srai_epi32( mask, 24 );
+    v1 = _mm_and_si128( v1, mask );
+    mask = _mm_andnot_si128( mask, v2 );
+    v1 = _mm_or_si128( v1, mask );
+    return v1;
+}
+
+/** \SSE5{SSE2,ssp_sha_epi32,pshad } */ 
+SSP_FORCEINLINE __m128i ssp_sha_epi32_SSE2(__m128i a, __m128i b)
+{
+    __m128i v1, v2, mask, mask2, b1, b2;
+    b1 = ssp_abs_epi8_SSE2( b );
+    mask = _mm_set_epi32( 0, 0, 0, -1 );
+    mask2 = _mm_srli_epi32( mask, 27 ); // the shfit count is a 5 bit value
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_and_si128( _mm_sra_epi32( a, b2 ), mask ); // negative shift
+    v2 = _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ); // positive shift
+    mask = _mm_slli_si128( mask, 4 );
+    b1 = _mm_srli_si128( b1, 4 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi32( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 4 );
+    b1 = _mm_srli_si128( b1, 4 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi32( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ) ); // positive shift
+    mask = _mm_slli_si128( mask, 4 );
+    b1 = _mm_srli_si128( b1, 4 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_sra_epi32( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi32( a, b2 ), mask ) ); // positive shift
+
+    mask = _mm_setzero_si128();
+    mask = _mm_cmpgt_epi8( mask, b ); // set mask to 0xFF for all negative shift counts in b
+    mask = _mm_slli_epi32( mask, 24 );
+    mask = _mm_srai_epi32( mask, 24 );
+    v1 = _mm_and_si128( v1, mask );
+    mask = _mm_andnot_si128( mask, v2 );
+    v1 = _mm_or_si128( v1, mask );
+    return v1;
+}
+
+/** \SSE5{SSE2,ssp_shl_epi64,pshlq } */ 
+SSP_FORCEINLINE __m128i ssp_shl_epi64_SSE2(__m128i a, __m128i b)
+{
+    __m128i v1, v2, mask, mask2, b1, b2;
+    b1 = ssp_abs_epi8_SSE2( b );
+    mask = _mm_set_epi32( 0, 0, -1, -1 );
+    mask2 = _mm_srli_epi64( mask, 58 ); // the shfit count is a 6 bit value
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_and_si128( _mm_srl_epi64( a, b2 ), mask ); // negative shift
+    v2 = _mm_and_si128( _mm_sll_epi64( a, b2 ), mask ); // positive shift
+    mask = _mm_slli_si128( mask, 8 );
+    b1 = _mm_srli_si128( b1, 8 );
+
+    b2 = _mm_and_si128( b1, mask2 );
+    v1 = _mm_or_si128( v1, _mm_and_si128( _mm_srl_epi64( a, b2 ), mask ) ); // negative shift
+    v2 = _mm_or_si128( v2, _mm_and_si128( _mm_sll_epi64( a, b2 ), mask ) ); // positive shift
+
+    mask = _mm_setzero_si128();
+    mask = _mm_cmpgt_epi8( mask, b ); // set mask to 0xFF for all negative shift counts in b
+    mask = _mm_slli_epi16( mask, 8 );
+    mask = _mm_srai_epi16( mask, 8 );
+    mask = _mm_shufflelo_epi16( mask, _MM_SHUFFLE(0,0,0,0) );
+    mask = _mm_shufflehi_epi16( mask, _MM_SHUFFLE(0,0,0,0) );
+    v1 = _mm_and_si128( v1, mask );
+    mask = _mm_andnot_si128( mask, v2 );
+    v1 = _mm_or_si128( v1, mask );
+    return v1;
+}
+
+/** @} 
+ *  @}
+ */
+
+
+#endif // __SSEPLUS_EMULATION_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_SSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,49 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_EMULATION_SSE3_H__
+#define __SSEPLUS_EMULATION_SSE3_H__
+
+#include "../SSEPlus_base.h"
+#include "../native/SSEPlus_native_SSE3.h"
+#include "../arithmetic/SSEPlus_arithmetic_SSE3.h"
+#include "SSEPlus_emulation_comps_SSE3.h"
+
+/** @addtogroup emulated_SSE3   
+ *  @{ 
+ *  @name SSE[4A,...,5] implemented in SSE3
+ */
+
+/** \SSE4_1{SSE3,_mm_dp_ps} */ 
+SSP_FORCEINLINE
+__m128 ssp_dp_ps_SSE3( __m128 a, __m128 b, const int mask )                 
+{
+    const static __m128i mulShiftImm_0123 = SSP_CONST_SET_32I( 0x010000, 0x020000, 0x040000, 0x080000 );   // Shift mask multiply moves 0,1,2,3 bits to left, becomes MSB
+    const static __m128i mulShiftImm_4567 = SSP_CONST_SET_32I( 0x100000, 0x200000, 0x400000, 0x800000 );   // Shift mask multiply moves 4,5,6,7 bits to left, becomes MSB
+
+    // Begin mask preparation
+    ssp_m128 mHi, mLo;
+    mLo.i = _mm_set1_epi32( mask );                                 // Load the mask into register
+    mLo.i = _mm_slli_si128( mLo.i, 3 );                         // Shift into reach of the 16 bit multiply
+
+    mHi.i = _mm_mullo_epi16( mLo.i, mulShiftImm_0123 );   // Shift the bits
+    mLo.i = _mm_mullo_epi16( mLo.i, mulShiftImm_4567 );   // Shift the bits
+
+    mHi.i = _mm_cmplt_epi32( mHi.i, _mm_setzero_si128() );    // FFFFFFFF if bit set, 00000000 if not set
+    mLo.i = _mm_cmplt_epi32( mLo.i, _mm_setzero_si128() );    // FFFFFFFF if bit set, 00000000 if not set
+    // End mask preparation - Mask bits 0-3 in mLo, 4-7 in mHi
+
+    a = _mm_and_ps( a, mHi.f );                                       // Clear input using the high bits of the mask
+    a = _mm_mul_ps( a, b );
+
+    a = ssp_arithmetic_hadd4_dup_ps_SSE3( a );                            // Horizontally add the 4 values
+    a = _mm_and_ps( a, mLo.f );                                      // Clear output using low bits of the mask
+    return a;
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_EMULATION_SSE3_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1681 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_EMULATION_COMPS_REF_H__
+#define __SSEPLUS_EMULATION_COMPS_REF_H__
+
+#include "../SSEPlus_base.h"
+
+/** @addtogroup emulated_REF   
+ *  @{ 
+ *  @name SSE[3,4A,...,5] implemented in reference
+ */
+
+//----------------------------------------
+// COMEQ (Condition 0)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comeq_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.s16[0]==B.s16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.s16[1]==B.s16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.s16[2]==B.s16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.s16[3]==B.s16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.s16[4]==B.s16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.s16[5]==B.s16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.s16[6]==B.s16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.s16[7]==B.s16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.s32[0]==B.s32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.s32[1]==B.s32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.s32[2]==B.s32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.s32[3]==B.s32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.s64[0]==B.s64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.s64[1]==B.s64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.s8[ 0]==B.s8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.s8[ 1]==B.s8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.s8[ 2]==B.s8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.s8[ 3]==B.s8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.s8[ 4]==B.s8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.s8[ 5]==B.s8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.s8[ 6]==B.s8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.s8[ 7]==B.s8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.s8[ 8]==B.s8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.s8[ 9]==B.s8[ 9]) ? 0xFF : 0;
+    A.u8[10]= (A.s8[10]==B.s8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.s8[11]==B.s8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.s8[12]==B.s8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.s8[13]==B.s8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.s8[14]==B.s8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.s8[15]==B.s8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.u16[0]==B.u16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.u16[1]==B.u16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.u16[2]==B.u16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.u16[3]==B.u16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.u16[4]==B.u16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.u16[5]==B.u16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.u16[6]==B.u16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.u16[7]==B.u16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.u32[0]==B.u32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.u32[1]==B.u32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.u32[2]==B.u32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.u32[3]==B.u32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.u64[0]==B.u64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.u64[1]==B.u64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.u8[ 0]==B.u8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.u8[ 1]==B.u8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.u8[ 2]==B.u8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.u8[ 3]==B.u8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.u8[ 4]==B.u8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.u8[ 5]==B.u8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.u8[ 6]==B.u8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.u8[ 7]==B.u8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.u8[ 8]==B.u8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.u8[ 9]==B.u8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.u8[10]==B.u8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.u8[11]==B.u8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.u8[12]==B.u8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.u8[13]==B.u8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.u8[14]==B.u8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.u8[15]==B.u8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comeq_pd, compd }*/
+SSP_FORCEINLINE __m128d ssp_comeq_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]==B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.f64[1]==B.f64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comeq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comeq_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]==B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]==B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]==B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]==B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comeq_sd, comsd }*/
+SSP_FORCEINLINE __m128d ssp_comeq_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]==B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comeq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comeq_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]==B.f32[0]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+//----------------------------------------
+// COMLT (Condition 1)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comlt_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.s16[0]<B.s16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.s16[1]<B.s16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.s16[2]<B.s16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.s16[3]<B.s16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.s16[4]<B.s16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.s16[5]<B.s16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.s16[6]<B.s16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.s16[7]<B.s16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.s32[0]<B.s32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.s32[1]<B.s32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.s32[2]<B.s32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.s32[3]<B.s32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.s64[0]<B.s64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.s64[1]<B.s64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.s8[ 0]<B.s8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.s8[ 1]<B.s8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.s8[ 2]<B.s8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.s8[ 3]<B.s8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.s8[ 4]<B.s8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.s8[ 5]<B.s8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.s8[ 6]<B.s8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.s8[ 7]<B.s8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.s8[ 8]<B.s8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.s8[ 9]<B.s8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.s8[10]<B.s8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.s8[11]<B.s8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.s8[12]<B.s8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.s8[13]<B.s8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.s8[14]<B.s8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.s8[15]<B.s8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.u16[0]<B.u16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.u16[1]<B.u16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.u16[2]<B.u16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.u16[3]<B.u16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.u16[4]<B.u16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.u16[5]<B.u16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.u16[6]<B.u16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.u16[7]<B.u16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.u32[0]<B.u32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.u32[1]<B.u32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.u32[2]<B.u32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.u32[3]<B.u32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.u64[0]<B.u64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.u64[1]<B.u64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.u8[ 0]<B.u8[0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.u8[ 1]<B.u8[1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.u8[ 2]<B.u8[2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.u8[ 3]<B.u8[3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.u8[ 4]<B.u8[4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.u8[ 5]<B.u8[5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.u8[ 6]<B.u8[6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.u8[ 7]<B.u8[7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.u8[ 8]<B.u8[8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.u8[ 9]<B.u8[9]) ? 0xFF : 0;
+    A.u8[10] = (A.u8[10]<B.u8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.u8[11]<B.u8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.u8[12]<B.u8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.u8[13]<B.u8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.u8[14]<B.u8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.u8[15]<B.u8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comlt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comlt_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = (A.f64[0]<B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.f64[1]<B.f64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comlt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comlt_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]<B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]<B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]<B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]<B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comlt_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comlt_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = (A.f64[0]<B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comlt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comlt_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+	A.u32[0] = (A.f32[0]<B.f32[0]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+//----------------------------------------
+// COMLE (Condition 2)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comle_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comle_epi16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.s16[0]<=B.s16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.s16[1]<=B.s16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.s16[2]<=B.s16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.s16[3]<=B.s16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.s16[4]<=B.s16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.s16[5]<=B.s16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.s16[6]<=B.s16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.s16[7]<=B.s16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comle_epi32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.s32[0]<=B.s32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.s32[1]<=B.s32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.s32[2]<=B.s32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.s32[3]<=B.s32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comle_epi64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.s64[0]<=B.s64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.s64[1]<=B.s64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comle_epi8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.s8[ 0]<=B.s8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.s8[ 1]<=B.s8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.s8[ 2]<=B.s8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.s8[ 3]<=B.s8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.s8[ 4]<=B.s8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.s8[ 5]<=B.s8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.s8[ 6]<=B.s8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.s8[ 7]<=B.s8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.s8[ 8]<=B.s8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.s8[ 9]<=B.s8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.s8[10]<=B.s8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.s8[11]<=B.s8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.s8[12]<=B.s8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.s8[13]<=B.s8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.s8[14]<=B.s8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.s8[15]<=B.s8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comle_epu16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.u16[0]<=B.u16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.u16[1]<=B.u16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.u16[2]<=B.u16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.u16[3]<=B.u16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.u16[4]<=B.u16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.u16[5]<=B.u16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.u16[6]<=B.u16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.u16[7]<=B.u16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comle_epu32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.u32[0]<=B.u32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.u32[1]<=B.u32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.u32[2]<=B.u32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.u32[3]<=B.u32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comle_epu64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.u64[0]<=B.u64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.u64[1]<=B.u64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comle_epu8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.u8[ 0]<=B.u8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.u8[ 1]<=B.u8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.u8[ 2]<=B.u8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.u8[ 3]<=B.u8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.u8[ 4]<=B.u8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.u8[ 5]<=B.u8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.u8[ 6]<=B.u8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.u8[ 7]<=B.u8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.u8[ 8]<=B.u8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.u8[ 9]<=B.u8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.u8[10]<=B.u8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.u8[11]<=B.u8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.u8[12]<=B.u8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.u8[13]<=B.u8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.u8[14]<=B.u8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.u8[15]<=B.u8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comle_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comle_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+   
+	A.u64[0] = (A.f64[0]<=B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.f64[1]<=B.f64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comle_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comle_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]<=B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]<=B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]<=B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]<=B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comle_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comle_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+   
+	A.u64[0] = (A.f64[0]<=B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comle_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comle_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+	A.u32[0] = (A.f32[0]<=B.f32[0]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+//----------------------------------------
+// COMUNORD (Condition 3)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comunord_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comunord_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b; // NAN(A)              || NAN(B)         
+    A.u64[0] = ((A.f64[0]!=A.f64[0]) || (B.f64[0]!=B.f64[0])) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = ((A.f64[1]!=A.f64[1]) || (B.f64[1]!=B.f64[1])) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comunord_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comunord_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b; // NAN(A)              || NAN(B)         
+    A.u32[0] = (A.f32[0]!=A.f32[0]) || (B.f32[0]!=B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]!=A.f32[1]) || (B.f32[1]!=B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]!=A.f32[2]) || (B.f32[2]!=B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]!=A.f32[3]) || (B.f32[3]!=B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comunord_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comunord_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b; // NAN(A)              || NAN(B)         
+    A.u64[0] = ((A.f64[0]!=A.f64[0]) || (B.f64[0]!=B.f64[0])) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comunord_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comunord_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b; // NAN(A)              || NAN(B)         
+    A.u32[0] = (A.f32[0]!=A.f32[0]) || (B.f32[0]!=B.f32[0]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMNEQ (Condition 4)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comneq_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.s16[0]!=B.s16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.s16[1]!=B.s16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.s16[2]!=B.s16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.s16[3]!=B.s16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.s16[4]!=B.s16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.s16[5]!=B.s16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.s16[6]!=B.s16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.s16[7]!=B.s16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.s32[0]!=B.s32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.s32[1]!=B.s32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.s32[2]!=B.s32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.s32[3]!=B.s32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.s64[0]!=B.s64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.s64[1]!=B.s64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.s8[ 0]!=B.s8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.s8[ 1]!=B.s8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.s8[ 2]!=B.s8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.s8[ 3]!=B.s8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.s8[ 4]!=B.s8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.s8[ 5]!=B.s8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.s8[ 6]!=B.s8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.s8[ 7]!=B.s8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.s8[ 8]!=B.s8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.s8[ 9]!=B.s8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.s8[10]!=B.s8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.s8[11]!=B.s8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.s8[12]!=B.s8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.s8[13]!=B.s8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.s8[14]!=B.s8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.s8[15]!=B.s8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.u16[0]!=B.u16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.u16[1]!=B.u16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.u16[2]!=B.u16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.u16[3]!=B.u16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.u16[4]!=B.u16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.u16[5]!=B.u16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.u16[6]!=B.u16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.u16[7]!=B.u16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.u32[0]!=B.u32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.u32[1]!=B.u32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.u32[2]!=B.u32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.u32[3]!=B.u32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.u64[0]!=B.u64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.u64[1]!=B.u64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.u8[ 0]!=B.u8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.u8[ 1]!=B.u8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.u8[ 2]!=B.u8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.u8[ 3]!=B.u8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.u8[ 4]!=B.u8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.u8[ 5]!=B.u8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.u8[ 6]!=B.u8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.u8[ 7]!=B.u8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.u8[ 8]!=B.u8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.u8[ 9]!=B.u8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.u8[10]!=B.u8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.u8[11]!=B.u8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.u8[12]!=B.u8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.u8[13]!=B.u8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.u8[14]!=B.u8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.u8[15]!=B.u8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comneq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comneq_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    
+	A.u64[0] = (A.f64[0]!=B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.f64[1]!=B.f64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comneq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comneq_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]!=B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]!=B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]!=B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]!=B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comneq_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comneq_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    
+	A.u64[0] = (A.f64[0]!=B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comneq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comneq_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+	A.u32[0] = (A.f32[0]!=B.f32[0]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMNLT (Condition 5)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comnlt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comnlt_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+ 
+    A.u64[0] = (A.f64[0]<B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    A.u64[1] = (A.f64[1]<B.f64[1]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+ 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comnlt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comnlt_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]<B.f32[0]) ? 0 : 0xFFFFFFFF;
+    A.u32[1] = (A.f32[1]<B.f32[1]) ? 0 : 0xFFFFFFFF;
+    A.u32[2] = (A.f32[2]<B.f32[2]) ? 0 : 0xFFFFFFFF;
+    A.u32[3] = (A.f32[3]<B.f32[3]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comnlt_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comnlt_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+ 
+    A.u64[0] = (A.f64[0]<B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+ 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comnlt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comnlt_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]<B.f32[0]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMNLE (Condition 6)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comnle_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comnle_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]<=B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    A.u64[1] = (A.f64[1]<=B.f64[1]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+ 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comnle_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comnle_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]<=B.f32[0]) ? 0 : 0xFFFFFFFF;
+    A.u32[1] = (A.f32[1]<=B.f32[1]) ? 0 : 0xFFFFFFFF;
+    A.u32[2] = (A.f32[2]<=B.f32[2]) ? 0 : 0xFFFFFFFF;
+    A.u32[3] = (A.f32[3]<=B.f32[3]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comnle_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comnle_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]<=B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+ 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comnle_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comnle_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+	A.u32[0] = (A.f32[0]<=B.f32[0]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMORD (Condition 7)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comord_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comord_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b; // NAN(A)              || NAN(B)         
+    A.u64[0] = (A.f64[0]!=A.f64[0]) || (B.f64[0]!=B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    A.u64[1] = (A.f64[1]!=A.f64[1]) || (B.f64[1]!=B.f64[1]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comord_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comord_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b; // NAN(A)              || NAN(B)         
+    A.u32[0] = (A.f32[0]!=A.f32[0]) || (B.f32[0]!=B.f32[0]) ? 0 : 0xFFFFFFFF;
+    A.u32[1] = (A.f32[1]!=A.f32[1]) || (B.f32[1]!=B.f32[1]) ? 0 : 0xFFFFFFFF;
+    A.u32[2] = (A.f32[2]!=A.f32[2]) || (B.f32[2]!=B.f32[2]) ? 0 : 0xFFFFFFFF;
+    A.u32[3] = (A.f32[3]!=A.f32[3]) || (B.f32[3]!=B.f32[3]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comord_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comord_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b; // NAN(A)              || NAN(B)         
+    A.u64[0] = (A.f64[0]!=A.f64[0]) || (B.f64[0]!=B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comord_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comord_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b; // NAN(A)              || NAN(B)         
+    A.u32[0] = (A.f32[0]!=A.f32[0]) || (B.f32[0]!=B.f32[0]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMUEQ (Condition 8)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comueq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comueq_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = ((A.f64[0]<B.f64[0]) || (A.f64[0]>B.f64[0])) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    A.u64[1] = ((A.f64[1]<B.f64[1]) || (A.f64[1]>B.f64[1])) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comueq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comueq_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;             
+    A.u32[0] = (A.f32[0]<B.f32[0]) || (A.f32[0]>B.f32[0]) ? 0 : 0xFFFFFFFF;
+    A.u32[1] = (A.f32[1]<B.f32[1]) || (A.f32[1]>B.f32[1]) ? 0 : 0xFFFFFFFF;
+    A.u32[2] = (A.f32[2]<B.f32[2]) || (A.f32[2]>B.f32[2]) ? 0 : 0xFFFFFFFF;
+    A.u32[3] = (A.f32[3]<B.f32[3]) || (A.f32[3]>B.f32[3]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comueq_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comueq_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = ((A.f64[0]<B.f64[0]) || (A.f64[0]>B.f64[0])) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comueq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comueq_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+	A.u32[0] = (A.f32[0]<B.f32[0]) || (A.f32[0]>B.f32[0]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMNGE (Condition 9)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comnge_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comnge_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = (A.f64[0]>=B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    A.u64[1] = (A.f64[1]>=B.f64[1]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comnge_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comnge_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]>=B.f32[0]) ? 0 : 0xFFFFFFFF;
+    A.u32[1] = (A.f32[1]>=B.f32[1]) ? 0 : 0xFFFFFFFF;
+    A.u32[2] = (A.f32[2]>=B.f32[2]) ? 0 : 0xFFFFFFFF;
+    A.u32[3] = (A.f32[3]>=B.f32[3]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comnge_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comnge_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = (A.f64[0]>=B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comnge_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comnge_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]>=B.f32[0]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMNGT (Condition 10)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comngt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comngt_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = (A.f64[0]>B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    A.u64[1] = (A.f64[1]>B.f64[1]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comngt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comngt_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]>B.f32[0]) ? 0 : 0xFFFFFFFF;
+    A.u32[1] = (A.f32[1]>B.f32[1]) ? 0 : 0xFFFFFFFF;
+    A.u32[2] = (A.f32[2]>B.f32[2]) ? 0 : 0xFFFFFFFF;
+    A.u32[3] = (A.f32[3]>B.f32[3]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comngt_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comngt_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+	A.u64[0] = (A.f64[0]>B.f64[0]) ? 0 : 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comngt_ss, comps } */
+SSP_FORCEINLINE __m128 ssp_comngt_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+	A.u32[0] = (A.f32[0]>B.f32[0]) ? 0 : 0xFFFFFFFF;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMFALSE (Condition 11)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comfalse_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi16_REF(__m128i a, __m128i b)
+{
+    const static __m128i tmp = SSP_CONST_SET_32I( 0,0,0,0 );  
+    return tmp;
+}
+
+/** \SSE5{Reference,_mm_comfalse_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi32_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi64_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi8_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu16_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu32_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu64_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu8_REF(__m128i a, __m128i b)
+{
+    return ssp_comfalse_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comfalse_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comfalse_pd_REF(__m128d a, __m128d b)
+{
+    const static __m128d tmp = SSP_CONST_SET_64F( 0, 0 );      
+    return tmp;
+}
+
+/** \SSE5{Reference,_mm_comfalse_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comfalse_ps_REF(__m128 a, __m128 b)
+{
+   const static __m128 tmp = SSP_CONST_SET_32F( 0, 0, 0, 0 );      
+   return tmp;
+}
+
+/** \SSE5{Reference,_mm_comfalse_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comfalse_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A;
+    A.d = a;
+    A.u64[0] = 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comfalse_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comfalse_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A;
+    A.f = a;
+    A.u32[0] = 0;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMONEQ (Condition 12)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comoneq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comoneq_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b; 
+    A.u64[0] = ((A.f64[0]<B.f64[0]) || (A.f64[0]>B.f64[0]))  ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = ((A.f64[1]<B.f64[1]) || (A.f64[1]>B.f64[1]))  ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;   
+}
+
+/** \SSE5{Reference,_mm_comoneq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comoneq_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b; 
+    A.u32[0] = (A.f32[0]<B.f32[0]) || (A.f32[0]>B.f32[0])  ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]<B.f32[1]) || (A.f32[1]>B.f32[1])  ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]<B.f32[2]) || (A.f32[2]>B.f32[2])  ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]<B.f32[3]) || (A.f32[3]>B.f32[3])  ? 0xFFFFFFFF : 0;
+    return A.f;   
+}
+
+/** \SSE5{Reference,_mm_comoneq_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comoneq_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b; 
+    A.u64[0] = ((A.f64[0]<B.f64[0]) || (A.f64[0]>B.f64[0]))  ? 0xFFFFFFFFFFFFFFFF : 0; 
+    return A.d;   
+}
+
+/** \SSE5{Reference,_mm_comoneq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comoneq_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;  
+	A.u32[0] = (A.f32[0]<B.f32[0]) || (A.f32[0]>B.f32[0])  ? 0xFFFFFFFF : 0;
+    return A.f;   
+}
+
+
+//----------------------------------------
+// COMGE (Condition 13)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comge_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comge_epi16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.s16[0]>=B.s16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.s16[1]>=B.s16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.s16[2]>=B.s16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.s16[3]>=B.s16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.s16[4]>=B.s16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.s16[5]>=B.s16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.s16[6]>=B.s16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.s16[7]>=B.s16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comge_epi32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.s32[0]>=B.s32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.s32[1]>=B.s32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.s32[2]>=B.s32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.s32[3]>=B.s32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comge_epi64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.s64[0]>=B.s64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.s64[1]>=B.s64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comge_epi8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.s8[ 0]>=B.s8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.s8[ 1]>=B.s8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.s8[ 2]>=B.s8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.s8[ 3]>=B.s8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.s8[ 4]>=B.s8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.s8[ 5]>=B.s8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.s8[ 6]>=B.s8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.s8[ 7]>=B.s8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.s8[ 8]>=B.s8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.s8[ 9]>=B.s8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.s8[10]>=B.s8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.s8[11]>=B.s8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.s8[12]>=B.s8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.s8[13]>=B.s8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.s8[14]>=B.s8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.s8[15]>=B.s8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comge_epu16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.u16[0]>=B.u16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.u16[1]>=B.u16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.u16[2]>=B.u16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.u16[3]>=B.u16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.u16[4]>=B.u16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.u16[5]>=B.u16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.u16[6]>=B.u16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.u16[7]>=B.u16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comge_epu32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.u32[0]>=B.u32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.u32[1]>=B.u32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.u32[2]>=B.u32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.u32[3]>=B.u32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comge_epu64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.u64[0]>=B.u64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.u64[1]>=B.u64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comge_epu8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.u8[ 0]>=B.u8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.u8[ 1]>=B.u8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.u8[ 2]>=B.u8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.u8[ 3]>=B.u8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.u8[ 4]>=B.u8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.u8[ 5]>=B.u8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.u8[ 6]>=B.u8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.u8[ 7]>=B.u8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.u8[ 8]>=B.u8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.u8[ 9]>=B.u8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.u8[10]>=B.u8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.u8[11]>=B.u8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.u8[12]>=B.u8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.u8[13]>=B.u8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.u8[14]>=B.u8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.u8[15]>=B.u8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comge_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comge_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]>=B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.f64[1]>=B.f64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comge_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comge_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]>=B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]>=B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]>=B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]>=B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comge_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comge_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]>=B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0 ;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comge_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comge_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b; 
+    A.u32[0] = (A.f32[0]>=B.f32[0]) ? 0xFFFFFFFF : 0; 
+    return A.f;
+}
+
+
+
+//----------------------------------------
+// COMGT (Condition 14)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comgt_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.s16[0]>B.s16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.s16[1]>B.s16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.s16[2]>B.s16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.s16[3]>B.s16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.s16[4]>B.s16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.s16[5]>B.s16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.s16[6]>B.s16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.s16[7]>B.s16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.s32[0]>B.s32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.s32[1]>B.s32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.s32[2]>B.s32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.s32[3]>B.s32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.s64[0]>B.s64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.s64[1]>B.s64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.s8[ 0]>B.s8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.s8[ 1]>B.s8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.s8[ 2]>B.s8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.s8[ 3]>B.s8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.s8[ 4]>B.s8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.s8[ 5]>B.s8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.s8[ 6]>B.s8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.s8[ 7]>B.s8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.s8[ 8]>B.s8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.s8[ 9]>B.s8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.s8[10]>B.s8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.s8[11]>B.s8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.s8[12]>B.s8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.s8[13]>B.s8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.s8[14]>B.s8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.s8[15]>B.s8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu16_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u16[0] = (A.u16[0]>B.u16[0]) ? 0xFFFF : 0;
+    A.u16[1] = (A.u16[1]>B.u16[1]) ? 0xFFFF : 0;
+    A.u16[2] = (A.u16[2]>B.u16[2]) ? 0xFFFF : 0;
+    A.u16[3] = (A.u16[3]>B.u16[3]) ? 0xFFFF : 0;
+    A.u16[4] = (A.u16[4]>B.u16[4]) ? 0xFFFF : 0;
+    A.u16[5] = (A.u16[5]>B.u16[5]) ? 0xFFFF : 0;
+    A.u16[6] = (A.u16[6]>B.u16[6]) ? 0xFFFF : 0;
+    A.u16[7] = (A.u16[7]>B.u16[7]) ? 0xFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu32_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u32[0] = (A.u32[0]>B.u32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.u32[1]>B.u32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.u32[2]>B.u32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.u32[3]>B.u32[3]) ? 0xFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu64_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u64[0] = (A.u64[0]>B.u64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.u64[1]>B.u64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu8_REF(__m128i a, __m128i b)
+{
+    ssp_m128 A,B;
+    A.i = a;
+    B.i = b;
+    A.u8[ 0] = (A.u8[ 0]>B.u8[ 0]) ? 0xFF : 0;
+    A.u8[ 1] = (A.u8[ 1]>B.u8[ 1]) ? 0xFF : 0;
+    A.u8[ 2] = (A.u8[ 2]>B.u8[ 2]) ? 0xFF : 0;
+    A.u8[ 3] = (A.u8[ 3]>B.u8[ 3]) ? 0xFF : 0;
+    A.u8[ 4] = (A.u8[ 4]>B.u8[ 4]) ? 0xFF : 0;
+    A.u8[ 5] = (A.u8[ 5]>B.u8[ 5]) ? 0xFF : 0;
+    A.u8[ 6] = (A.u8[ 6]>B.u8[ 6]) ? 0xFF : 0;
+    A.u8[ 7] = (A.u8[ 7]>B.u8[ 7]) ? 0xFF : 0; 
+	A.u8[ 8] = (A.u8[ 8]>B.u8[ 8]) ? 0xFF : 0;
+    A.u8[ 9] = (A.u8[ 9]>B.u8[ 9]) ? 0xFF : 0;
+    A.u8[10] = (A.u8[10]>B.u8[10]) ? 0xFF : 0;
+    A.u8[11] = (A.u8[11]>B.u8[11]) ? 0xFF : 0;
+    A.u8[12] = (A.u8[12]>B.u8[12]) ? 0xFF : 0;
+    A.u8[13] = (A.u8[13]>B.u8[13]) ? 0xFF : 0;
+    A.u8[14] = (A.u8[14]>B.u8[14]) ? 0xFF : 0;
+    A.u8[15] = (A.u8[15]>B.u8[15]) ? 0xFF : 0;
+    return A.i;
+}
+
+/** \SSE5{Reference,_mm_comgt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comgt_pd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]>B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    A.u64[1] = (A.f64[1]>B.f64[1]) ? 0xFFFFFFFFFFFFFFFF : 0;
+ 
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comgt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comgt_ps_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]>B.f32[0]) ? 0xFFFFFFFF : 0;
+    A.u32[1] = (A.f32[1]>B.f32[1]) ? 0xFFFFFFFF : 0;
+    A.u32[2] = (A.f32[2]>B.f32[2]) ? 0xFFFFFFFF : 0;
+    A.u32[3] = (A.f32[3]>B.f32[3]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comgt_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comgt_sd_REF(__m128d a, __m128d b)
+{
+    ssp_m128 A,B;
+    A.d = a;
+    B.d = b;
+    A.u64[0] = (A.f64[0]>B.f64[0]) ? 0xFFFFFFFFFFFFFFFF : 0;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comgt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comgt_ss_REF(__m128 a, __m128 b)
+{
+    ssp_m128 A,B;
+    A.f = a;
+    B.f = b;
+    A.u32[0] = (A.f32[0]>B.f32[0]) ? 0xFFFFFFFF : 0;
+    return A.f;
+}
+
+
+//----------------------------------------
+// COMTRUE (Condition 15)
+//----------------------------------------
+
+/** \SSE5{Reference,_mm_comtrue_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi16_REF(__m128i a, __m128i b)
+{
+    const static __m128i tmp = SSP_CONST_SET_64I( 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF );  
+    return tmp;
+}
+
+/** \SSE5{Reference,_mm_comtrue_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi32_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi64_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi8_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_epu16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu16_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_epu32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu32_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_epu64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu64_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_epu8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu8_REF(__m128i a, __m128i b)
+{
+    return ssp_comtrue_epi16_REF(a,b);
+}
+
+/** \SSE5{Reference,_mm_comtrue_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comtrue_pd_REF(__m128d a, __m128d b)
+{   
+    const static __m128i tmp = SSP_CONST_SET_64I( 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF );
+    ssp_m128 A;  
+    A.i = tmp;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comtrue_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comtrue_ps_REF(__m128 a, __m128 b)
+{   
+    const static __m128i tmp = SSP_CONST_SET_64I( 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF );
+    ssp_m128 A;  
+    A.i = tmp;
+    return A.f;
+}
+
+/** \SSE5{Reference,_mm_comtrue_sd, comsd } */
+SSP_FORCEINLINE __m128d ssp_comtrue_sd_REF(__m128d a, __m128d b)
+{   
+    ssp_m128 A;    
+    A.d      = a;
+    A.u64[0] = 0xFFFFFFFFFFFFFFFF;
+    return A.d;
+}
+
+/** \SSE5{Reference,_mm_comtrue_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comtrue_ss_REF(__m128 a, __m128 b)
+{   
+    ssp_m128 A;
+    A.f = a;
+    A.u32[0] = 0xFFFFFFFF;
+    return A.f;
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSP_EMULATION_COMPS_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1076 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_EMULATION_COMPS_SSE2_H__
+#define __SSEPLUS_EMULATION_COMPS_SSE2_H__
+
+#include "../SSEPlus_SSE2.h"
+
+
+/** @addtogroup emulated_SSE2   
+ *  @{ 
+ *  @name SSE[3,4A,...,5] implemented in SSE2
+ */
+
+//----------------------------------------
+// COMEQ (Condition 0)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comeq_epi16, pcomw } */ 
+SSP_FORCEINLINE __m128i ssp_comeq_epi16_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpeq_epi16( a, b );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comeq_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epi32_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpeq_epi32( a, b );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comeq_epi64, pcomq } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epi64_SSE2(__m128i a, __m128i b)
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+    A.i = _mm_cmpeq_epi32( A.i, B.i );  //  A0=B0, A1=B1,  A2=B2, A3=B3
+    B.i = _mm_slli_epi64 ( A.i, 32 );   //  A1=B1,     0,  A3=B3,     0
+    A.i = _mm_and_si128  ( A.i, B.i );  //  A0=B0      x,  A2=B2,     x
+                                        //& A1=B1,        &A3=B3  
+
+    A.i = _mm_shuffle_epi32( A.i, _MM_SHUFFLE(3,3,1,1) );        
+    return A.i;
+}
+
+/** \SSE5{SSE2,_mm_comeq_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epi8_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpeq_epi8( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comeq_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epu16_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpeq_epi16( a, b );
+    return a; 
+}
+
+/** \SSE5{SSE2,_mm_comeq_epu32, pcomud } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epu32_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpeq_epi32( a, b );
+    return a; 
+}
+
+/** \SSE5{SSE2,_mm_comeq_epu64, pcomuq } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epu64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epi64_SSE2( a, b );  
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comeq_epu8, pcomub } */ 
+SSP_FORCEINLINE __m128i ssp_comeq_epu8_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpeq_epi8( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comeq_pd, compd }*/ 
+SSP_FORCEINLINE __m128d ssp_comeq_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpeq_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comeq_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comeq_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpeq_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comeq_sd, comsd }*/   
+SSP_FORCEINLINE __m128d ssp_comeq_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpeq_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comeq_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comeq_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpeq_ss( a, b );
+    return a;
+}
+
+//----------------------------------------
+// COMLT (Condition 1)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comlt_epi16, pcomw } */  
+SSP_FORCEINLINE __m128i ssp_comlt_epi16_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmplt_epi16( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comlt_epi32_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmplt_epi32( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epi64, pcomq } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comlt_epi64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comlt_epi64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comlt_epi8_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmplt_epi8( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comlt_epu16_SSE2(__m128i a, __m128i b)
+{
+    __m128i signMask, mask;
+
+    mask     = _mm_cmplt_epi16( a, b );              // FFFF where a < b (signed)
+    signMask = _mm_xor_si128  ( a, b );              // Signbit is 1 where signs differ 
+    signMask = _mm_srai_epi16 ( signMask, 15 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128  ( mask, signMask );    // Invert output where signs differed
+    return mask;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epu32, pcomud } */  
+SSP_FORCEINLINE __m128i ssp_comlt_epu32_SSE2(__m128i a, __m128i b)
+{
+    __m128i signMask, mask;
+
+    mask     = _mm_cmplt_epi32( a, b );              // FFFF where a < b (signed)
+    signMask = _mm_xor_si128  ( a, b );              // Signbit is 1 where signs differ 
+    signMask = _mm_srai_epi32 ( signMask, 31 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128  ( mask, signMask );    // Invert output where signs differed
+    return mask;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epu64, pcomuq } */  // TODO: SSE2
+SSP_FORCEINLINE __m128i ssp_comlt_epu64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comlt_epu64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_epu8, pcomub } */  
+SSP_FORCEINLINE __m128i ssp_comlt_epu8_SSE2(__m128i a, __m128i b)
+{
+    __m128i mask = _mm_cmplt_epi8( a, b );
+    a = ssp_logical_signinvert_8_SSE2( mask, a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comlt_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmplt_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comlt_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmplt_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comlt_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmplt_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comlt_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comlt_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmplt_ss( a, b );
+    return a;
+}
+
+//----------------------------------------
+// COMLE (Condition 2)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comle_epi16, pcomw } */  
+SSP_FORCEINLINE __m128i ssp_comle_epi16_SSE2(__m128i a, __m128i b)
+{
+    __m128i c;    
+    c = _mm_cmplt_epi16( a, b );
+    a = _mm_cmpeq_epi16( a, b );
+    a = _mm_or_si128   ( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comle_epi32_SSE2(__m128i a, __m128i b)
+{
+    __m128i c;    
+    c = _mm_cmplt_epi32( a, b );
+    a = _mm_cmpeq_epi32( a, b );
+    a = _mm_or_si128   ( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epi64, pcomq } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comle_epi64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comle_epi64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comle_epi8_SSE2(__m128i a, __m128i b)
+{
+    __m128i c;    
+    c = _mm_cmplt_epi8( a, b );
+    a = _mm_cmpeq_epi8( a, b );
+    a = _mm_or_si128  ( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epu16, pcomuw } */ 
+SSP_FORCEINLINE __m128i ssp_comle_epu16_SSE2(__m128i a, __m128i b)
+{
+    __m128i mask = ssp_comle_epi16_SSE2( a, b );
+    a = ssp_logical_signinvert_16_SSE2( mask, a, b );   
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epu32, pcomud } */ 
+SSP_FORCEINLINE __m128i ssp_comle_epu32_SSE2(__m128i a, __m128i b)
+{
+    __m128i mask = ssp_comle_epi32_SSE2( a, b );
+    a = ssp_logical_signinvert_32_SSE2( mask, a, b );   
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epu64, pcomuq } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comle_epu64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comle_epu64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_epu8, pcomub } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comle_epu8_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comle_epu8_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_pd, compd } */ 
+SSP_FORCEINLINE __m128d ssp_comle_pd_SSE2(__m128d a, __m128d b)
+{
+   a = _mm_cmple_pd( a, b );
+   return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comle_ps_SSE2(__m128 a, __m128 b)
+{
+   a = _mm_cmple_ps( a, b );
+   return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comle_sd_SSE2(__m128d a, __m128d b)
+{
+   a = _mm_cmple_sd( a, b );
+   return a;
+}
+
+/** \SSE5{SSE2,_mm_comle_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comle_ss_SSE2(__m128 a, __m128 b)
+{
+   a = _mm_cmple_ss( a, b );
+   return a;
+}
+
+//----------------------------------------
+// COMUNORD (Condition 3)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comunord_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comunord_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_or_pd    ( a, b );
+    a = _mm_cmpneq_pd( a, a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comunord_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comunord_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_or_ps    ( a, b );
+    a = _mm_cmpneq_ps( a, a );
+    return a;      
+}
+
+/** \SSE5{SSE2,_mm_comunord_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comunord_sd_SSE2(__m128d a, __m128d b)
+{
+    b = _mm_or_pd    ( a, b );
+    a = _mm_cmpneq_sd( a, b );
+    return a; 
+}
+
+/** \SSE5{SSE2,_mm_comunord_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comunord_ss_SSE2(__m128 a, __m128 b)
+{
+    b = _mm_or_ps    ( a, b );
+    a = _mm_cmpneq_ss( a, b );
+    return a; 
+}
+
+
+//----------------------------------------
+// COMNEQ (Condition 4)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comneq_epi16, pcomw } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epi16_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epi16_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epi32_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epi32_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epi64, pcomq } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epi64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epi64_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epi8_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epi8_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epu16_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epu16_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epu32, pcomud } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epu32_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epu32_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epu64, pcomuq } */ 
+SSP_FORCEINLINE __m128i ssp_comneq_epu64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epu64_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_epu8, pcomub } */  
+SSP_FORCEINLINE __m128i ssp_comneq_epu8_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comeq_epu8_SSE2( a, b );
+    a = ssp_logical_invert_si128_SSE2( a );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comneq_pd, compd }*/ 
+SSP_FORCEINLINE __m128d ssp_comneq_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpneq_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comneq_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comneq_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpneq_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comneq_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comneq_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpneq_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comneq_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comneq_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpneq_ss( a, b );
+    return a;
+}
+
+//----------------------------------------
+// COMNLT (Condition 5)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comnlt_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comnlt_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpnlt_pd( a, b );    
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnlt_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comnlt_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpnlt_ps( a, b );    
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnlt_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comnlt_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpnlt_sd( a, b );    
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnlt_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comnlt_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpnlt_ss( a, b );    
+    return a;
+}
+
+
+//----------------------------------------
+// COMNLE (Condition 6)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comnle_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comnle_pd_SSE2(__m128d a, __m128d b)
+{    
+    a = _mm_cmpnle_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnle_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comnle_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpnle_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnle_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comnle_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpnle_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnle_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comnle_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpnle_ss( a, b );
+    return a;
+}
+
+
+//----------------------------------------
+// COMORD (Condition 7)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comord_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comord_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpord_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comord_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comord_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpord_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comord_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comord_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpord_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comord_ss, comss } */  //TODO:SSE2
+SSP_FORCEINLINE __m128 ssp_comord_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpord_ss( a, b );
+    return a;
+}
+
+
+//----------------------------------------
+// COMUEQ (Condition 8)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comueq_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comueq_pd_SSE2(__m128d a, __m128d b)
+{
+    __m128d c;
+    c = _mm_cmpunord_pd( a, b );
+    a = _mm_cmpeq_pd   ( a, b );
+    a = _mm_or_pd      ( a, c );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comueq_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comueq_ps_SSE2(__m128 a, __m128 b)
+{
+    __m128 c;
+    c = _mm_cmpunord_ps( a, b );
+    a = _mm_cmpeq_ps   ( a, b );
+    a = _mm_or_ps      ( a, c );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comueq_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comueq_sd_SSE2(__m128d a, __m128d b)
+{
+    __m128d c;
+    c = _mm_cmpunord_sd( a, b );
+    b = _mm_cmpeq_sd   ( a, b );
+    b = _mm_or_pd      ( b, c );
+    a = _mm_move_sd    ( a, b );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comueq_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comueq_ss_SSE2(__m128 a, __m128 b)
+{
+    __m128 c;
+    c = _mm_cmpunord_ss( a, b );
+    b = _mm_cmpeq_ss   ( a, b );
+    b = _mm_or_ps      ( a, c );
+    a = _mm_move_ss    ( a, b );
+    return a;   
+}
+
+
+//----------------------------------------
+// COMNGE (Condition 9)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comnge_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comnge_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpnge_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnge_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comnge_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpnge_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnge_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comnge_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpnge_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comnge_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comnge_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpnge_ss( a, b );
+    return a;
+}
+
+
+//----------------------------------------
+// COMNGT (Condition 10)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comngt_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comngt_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpngt_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comngt_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comngt_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpngt_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comngt_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comngt_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpngt_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comngt_ss, comps } */  
+SSP_FORCEINLINE __m128 ssp_comngt_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpngt_ss( a, b );
+    return a;
+}
+
+
+//----------------------------------------
+// COMFALSE (Condition 11)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comfalse_epi16, pcomw } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epi16_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epi32_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epi64, pcomq } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epi64_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epi8_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epu16_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epu32, pcomud } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epu32_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epu64, pcomuq } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epu64_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_epu8, pcomub } */  
+SSP_FORCEINLINE __m128i ssp_comfalse_epu8_SSE2(__m128i a, __m128i b)
+{
+	return _mm_setzero_si128();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comfalse_pd_SSE2(__m128d a, __m128d b)
+{
+	return _mm_setzero_pd();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comfalse_ps_SSE2(__m128 a, __m128 b)
+{
+	return _mm_setzero_ps();
+}
+
+/** \SSE5{SSE2,_mm_comfalse_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comfalse_sd_SSE2(__m128d a, __m128d b)
+{
+	ssp_m128 B;
+	B.i = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0, 0);
+	return _mm_and_pd(a, B.d);
+}
+
+/** \SSE5{SSE2,_mm_comfalse_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comfalse_ss_SSE2(__m128 a, __m128 b)
+{
+	ssp_m128 B;
+	B.i = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0);
+	return _mm_and_ps(a, B.f);
+}
+
+
+//----------------------------------------
+// COMONEQ (Condition 12)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comoneq_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comoneq_pd_SSE2(__m128d a, __m128d b)
+{
+    __m128d c;
+    c = _mm_cmpord_pd( a, b );
+    a = _mm_cmpneq_pd( a, b );
+    a = _mm_and_pd   ( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comoneq_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comoneq_ps_SSE2(__m128 a, __m128 b)
+{
+    __m128 c;
+    c = _mm_cmpord_ps( a, b );
+    a = _mm_cmpneq_ps( a, b );
+    a = _mm_and_ps   ( a, c );
+    return a;
+}
+
+
+/** \SSE5{SSE2,_mm_comoneq_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comoneq_sd_SSE2(__m128d a, __m128d b)
+{
+    __m128d c;
+    c = _mm_cmpord_pd( a, b );
+    b = _mm_cmpneq_pd( a, b );
+    b = _mm_and_pd   ( b, c );
+    a = _mm_move_sd  ( a, b );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comoneq_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comoneq_ss_SSE2(__m128 a, __m128 b)
+{
+    __m128 c;
+    c = _mm_cmpord_ps( a, b );
+    b = _mm_cmpneq_ps( a, b );
+    b = _mm_and_ps   ( b, c );
+    a = _mm_move_ss  ( a, b );
+    return a; 
+}
+
+
+//----------------------------------------
+// COMGE (Condition 13)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comge_epi16, pcomw } */  
+SSP_FORCEINLINE __m128i ssp_comge_epi16_SSE2(__m128i a, __m128i b)
+{
+    __m128i c;
+    c = _mm_cmpgt_epi16( a, b );
+    a = _mm_cmpeq_epi16( a, b );
+    a = _mm_or_si128  ( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comge_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comge_epi32_SSE2(__m128i a, __m128i b)
+{
+    __m128i c;
+    c = _mm_cmpgt_epi32( a, b );
+    a = _mm_cmpeq_epi32( a, b );
+    a = _mm_or_si128   ( a, c );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comge_epi64, pcomq } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comge_epi64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comge_epi64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comge_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comge_epi8_SSE2(__m128i a, __m128i b)
+{
+    __m128i c;
+    c = _mm_cmpgt_epi8( a, b );
+    a = _mm_cmpeq_epi8( a, b );
+    a = _mm_or_si128  ( a, c );
+    return a;
+}
+
+
+/** \SSE5{SSE2,_mm_comge_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comge_epu16_SSE2(__m128i a, __m128i b)
+{
+    __m128i mask;
+    mask = ssp_comge_epi16_SSE2( a, b );         // FFFF where a < b (signed)
+    mask = ssp_logical_signinvert_16_SSE2( mask, a, b );
+    return mask;
+}
+
+/** \SSE5{SSE2,_mm_comge_epu32, pcomud } */  
+SSP_FORCEINLINE __m128i ssp_comge_epu32_SSE2(__m128i a, __m128i b)
+{
+    __m128i mask;
+    mask = ssp_comge_epi32_SSE2( a, b );         // FFFF where a < b (signed)
+    mask = ssp_logical_signinvert_32_SSE2( mask, a, b );
+    return mask;
+}
+
+/** \SSE5{SSE2,_mm_comge_epu64, pcomuq } */  //TODO: SSE2
+SSP_FORCEINLINE __m128i ssp_comge_epu64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comge_epu64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comge_epu8, pcomub } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comge_epu8_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comge_epu8_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comge_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comge_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpge_pd( a, b );
+    return a;    
+}
+
+/** \SSE5{SSE2,_mm_comge_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comge_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpge_ps( a, b );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comge_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comge_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpge_sd( a, b );
+    return a;   
+}
+
+/** \SSE5{SSE2,_mm_comge_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comge_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpge_ss( a, b );
+    return a;   
+}
+
+
+//----------------------------------------
+// COMGT (Condition 14)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comgt_epi16, pcomw } */ 
+SSP_FORCEINLINE __m128i ssp_comgt_epi16_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpgt_epi16( a, b );
+    return a;  
+}
+
+/** \SSE5{SSE2,_mm_comgt_epi32, pcomd } */ 
+SSP_FORCEINLINE __m128i ssp_comgt_epi32_SSE2(__m128i a, __m128i b)
+{
+    a = _mm_cmpgt_epi32( a, b );
+    return a;  
+}
+
+/** \SSE5{SSE2,_mm_comgt_epi64, pcomq } */  //TODO: SSE2
+SSP_FORCEINLINE __m128i ssp_comgt_epi64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comgt_epi64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comgt_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comgt_epi8_SSE2(__m128i a, __m128i b)
+{
+     a = _mm_cmpgt_epi8( a, b );
+    return a;  
+}
+
+/** \SSE5{SSE2,_mm_comgt_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comgt_epu16_SSE2(__m128i a, __m128i b)
+{
+    __m128i signMask, mask;
+
+    mask     = _mm_cmpgt_epi16( a, b );              // FFFF where a > b (signed)
+    signMask = _mm_xor_si128  ( a, b );              // Signbit is 1 where signs differ 
+    signMask = _mm_srai_epi16 ( signMask, 15 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128  ( mask, signMask );    // Invert output where signs differed
+    return mask;
+}
+
+/** \SSE5{SSE2,_mm_comgt_epu32, pcomud } */ 
+SSP_FORCEINLINE __m128i ssp_comgt_epu32_SSE2(__m128i a, __m128i b)
+{
+    __m128i signMask, mask;
+
+    mask     = _mm_cmpgt_epi32( a, b );              // FFFF where a < b (signed)
+    signMask = _mm_xor_si128  ( a, b );              // Signbit is 1 where signs differ 
+    signMask = _mm_srai_epi32 ( signMask, 31 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128  ( mask, signMask );    // Invert output where signs differed
+    return mask;
+}
+
+/** \SSE5{SSE2,_mm_comgt_epu64, pcomuq } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comgt_epu64_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comgt_epu64_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comgt_epu8, pcomub } */  //TODO:SSE2
+SSP_FORCEINLINE __m128i ssp_comgt_epu8_SSE2(__m128i a, __m128i b)
+{
+    a = ssp_comgt_epu8_REF( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comgt_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comgt_pd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpgt_pd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comgt_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comgt_ps_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpgt_ps( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comgt_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comgt_sd_SSE2(__m128d a, __m128d b)
+{
+    a = _mm_cmpgt_sd( a, b );
+    return a;
+}
+
+/** \SSE5{SSE2,_mm_comgt_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comgt_ss_SSE2(__m128 a, __m128 b)
+{
+    a = _mm_cmpgt_ss( a, b );
+    return a;
+}
+
+
+//----------------------------------------
+// COMTRUE (Condition 15)
+//----------------------------------------
+
+/** \SSE5{SSE2,_mm_comtrue_epi16, pcomw } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epi16_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epi32, pcomd } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epi32_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epi64, pcomq } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epi64_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epi8, pcomb } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epi8_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epu16, pcomuw } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epu16_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epu32, pcomud } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epu32_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epu64, pcomuq } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epu64_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_epu8, pcomub } */  
+SSP_FORCEINLINE __m128i ssp_comtrue_epu8_SSE2(__m128i a, __m128i b)
+{
+	return _mm_set1_epi32(0xFFFFFFFF);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_pd, compd } */  
+SSP_FORCEINLINE __m128d ssp_comtrue_pd_SSE2(__m128d a, __m128d b)
+{
+	ssp_m128 B;
+	B.i = _mm_set1_epi32(0xFFFFFFFF);
+	return B.d;
+}
+
+/** \SSE5{SSE2,_mm_comtrue_ps, comps } */  
+SSP_FORCEINLINE __m128 ssp_comtrue_ps_SSE2(__m128 a, __m128 b)
+{
+	ssp_m128 B;
+	B.i = _mm_set1_epi32(0xFFFFFFFF);
+	return B.f;
+}
+
+/** \SSE5{SSE2,_mm_comtrue_sd, comsd } */  
+SSP_FORCEINLINE __m128d ssp_comtrue_sd_SSE2(__m128d a, __m128d b)
+{
+	ssp_m128 B;
+	B.i = _mm_set_epi32(0, 0, 0xFFFFFFFF, 0xFFFFFFFF);
+	return _mm_or_pd(a, B.d);
+}
+
+/** \SSE5{SSE2,_mm_comtrue_ss, comss } */  
+SSP_FORCEINLINE __m128 ssp_comtrue_ss_SSE2(__m128 a, __m128 b)
+{
+	ssp_m128 B;
+	B.i = _mm_set_epi32(0, 0, 0, 0xFFFFFFFF);
+	return _mm_or_ps(a, B.f);
+}
+
+
+/** @} 
+ *  @}
+ */
+
+
+#endif // __SSEPLUS_EMULATION_COMPS_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/emulation/SSEPlus_emulation_comps_SSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,33 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_EMULATION_COMPS_SSE3_H__
+#define __SSEPLUS_EMULATION_COMPS_SSE3_H__
+
+#include "../SSEPlus_SSE3.h"
+
+/** @addtogroup emulated_SSE2   
+ *  @{ 
+ *  @name SSE[3,4A,...,5] implemented in SSE2
+ */
+
+/** \SSE5{SSE2,_mm_comeq_epi64, pcomq } */  
+SSP_FORCEINLINE __m128i ssp_comeq_epi64_SSE3(__m128i a, __m128i b)
+{
+    ssp_m128 A, B;
+    A.i = a;
+    B.i = b;
+    A.i = _mm_cmpeq_epi32( A.i, B.i );  // A0=B0,  A1=B1, A2=B2,  A3=B3
+    B.f = _mm_movehdup_ps( A.f );       // A1=B1,  A1=B1, A3=B3,  A3=B3
+    A.f = _mm_moveldup_ps( A.f );       // A0=B0,  A0=B0, A2=B2,  A2=B2
+    A.i = _mm_and_si128  ( A.i, B.i );  // A0=B0 & A1=B1, A2=B2 & A3=B3   
+    return A.i;
+}
+
+/** @} 
+ *  @}
+ */
+
+
+#endif // __SSEPLUS_EMULATION_COMPS_SSE3_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,63 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_LOGICAL_REF_H__
+#define __SSEPLUS_LOGICAL_REF_H__
+
+#include "../SSEPlus_base.h"
+
+
+/** @addtogroup supplimental_REF   
+ *  @{ 
+ *  @name Logical Operations
+ */
+SSP_FORCEINLINE __m128i ssp_logical_invert_si128_REF( __m128i a )
+{
+    ssp_m128 A;
+    A.i = a;
+
+    A.u32[0] = ~A.u32[0];
+    A.u32[1] = ~A.u32[1];
+    A.u32[2] = ~A.u32[2];
+    A.u32[3] = ~A.u32[3];
+    return A.i;
+}
+
+
+/**
+  r_:= a_ << b; (logical left shift)
+*/
+SSP_FORCEINLINE __m128i ssp_slli_epi8_REF(__m128i a, const int b)
+{
+    int n;
+    ssp_m128 A;
+    A.i = a;
+    for( n = 0; n < 16; n++ )
+    {
+        A.u8[n] = A.u8[n] << b;
+    }
+    return A.i;
+}
+
+/**
+  r_:= a_ >> b; (logical right shift)
+*/
+SSP_FORCEINLINE __m128i ssp_srli_epi8_REF(__m128i a, const int b)
+{                                            //  a = VfVeVdVcVbVaV9V8V7V6V5V4V3V2V1V0
+    int n;
+    ssp_m128 A;
+    A.i = a;
+    for( n = 0; n < 16; n++ )
+    {
+        A.u8[n] = A.u8[n] >> b;
+    }
+    return A.i;
+}
+
+/** @} 
+ *  @}
+ */
+
+
+#endif // __SSEPLUS_LOGICAL_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/logical/SSEPlus_logical_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,151 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_LOGICAL_SSE2_H__
+#define __SSEPLUS_LOGICAL_SSE2_H__
+
+#include "../native/SSEPlus_native_SSE2.h"
+
+/** @addtogroup supplimental_SSE2
+ *  @{ 
+ *  @name Logical Operations
+ */
+
+SSP_FORCEINLINE __m128i ssp_logical_bitwise_select_SSE2( __m128i a, __m128i b, __m128i mask )   // Bitwise (mask ? a : b) 
+{
+    a = _mm_and_si128   ( a,    mask );                                 // clear a where mask = 0
+    b = _mm_andnot_si128( mask, b    );                                 // clear b where mask = 1
+    a = _mm_or_si128    ( a,    b    );                                 // a = a OR b                         
+    return a; 
+}
+
+
+/** Arithmetic 8 bit right shift */
+SSP_FORCEINLINE __m128i ssp_logical_srai_epi8_SSE2( __m128i a, int count )
+{
+    const static __m128i MASK = SSP_CONST_SET_8I( 0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0,0xFF,0 );
+    __m128i b;         
+    b = _mm_srai_epi16 ( a, count   ); // Shift even indexes into position
+    a = _mm_slli_epi16 ( a, 8       ); // Prepare odd indexes
+    a = _mm_srai_epi16 ( a, count+8 ); // Shift even indexes into position
+    a = ssp_logical_bitwise_select_SSE2( b, a, MASK );
+    return a;
+}
+
+
+/** Invert 'mask' if 'a' and 'b' have different signs. */
+SSP_FORCEINLINE __m128i ssp_logical_signinvert_16_SSE2( __m128i mask, __m128i a, __m128i b)
+{
+    __m128i signMask;   
+    signMask = _mm_xor_si128  ( a, b );              // Signbit is 1 where signs differ 
+    signMask = _mm_srai_epi16 ( signMask, 15 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128  ( mask, signMask );    // Invert output where signs differed
+    return mask;  
+}
+
+/** Invert 'mask' if 'a' and 'b' have different signs. */
+SSP_FORCEINLINE __m128i ssp_logical_signinvert_32_SSE2( __m128i mask, __m128i a, __m128i b)
+{
+    __m128i signMask;   
+    signMask = _mm_xor_si128  ( a, b );              // Signbit is 1 where signs differ 
+    signMask = _mm_srai_epi32 ( signMask, 31 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128  ( mask, signMask );    // Invert output where signs differed
+    return mask;  
+}
+
+/** Invert 'mask' if 'a' and 'b' have different signs. */
+SSP_FORCEINLINE __m128i ssp_logical_signinvert_8_SSE2( __m128i mask, __m128i a, __m128i b)
+{
+    __m128i signMask;   
+    signMask = _mm_xor_si128              ( a, b );             // Signbit is 1 where signs differ 
+    signMask = ssp_logical_srai_epi8_SSE2 ( signMask, 7 );      // fill all fields with sign bit     
+    mask     = _mm_xor_si128              ( mask, signMask );   // Invert output where signs differed
+    return mask;  
+}
+
+
+
+SSP_FORCEINLINE __m128i ssp_logical_invert_si128_SSE2( __m128i a )
+{
+    const static __m128i mask = SSP_CONST_SET_32I( 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF );
+    a = _mm_xor_si128( a, mask );
+    return a;
+}
+
+SSP_FORCEINLINE __m128d ssp_logical_invert_sd_SSE2( __m128d a )
+{
+    const static __m128i mask = SSP_CONST_SET_32I( 0xFFFFFFFF, 0xFFFFFFFF, 0, 0 );
+    ssp_m128 A;
+    A.d    = a;
+    A.i = _mm_xor_si128( A.i, mask );
+    return A.d;
+}
+
+SSP_FORCEINLINE __m128 ssp_logical_invert_ss_SSE2( __m128 a )
+{
+    const static __m128i mask = SSP_CONST_SET_32I( 0xFFFFFFFF, 0, 0, 0 );
+    ssp_m128 A;
+    A.f    = a;
+    A.i = _mm_xor_si128( A.i, mask );
+    return A.f;
+}
+
+
+
+
+
+//SSP_FORCEINLINE
+//__m128i ssp_generate_mask_imm8_to_epi16_SSE2( int mask )
+//{
+//    __m128i screen;
+//    const static __m128i mulShiftImm = SSP_CONST_SET_16I( 0x0100, 0x0200, 0x0400, 0x0800, 0x1000, 0x2000, 0x4000, 0x8000 ); // Shift mask multiply moves all bits to left, becomes MSB
+//    screen = _mm_set1_epi16  ( mask                );   // Load the mask into register
+//    screen = _mm_mullo_epi16 ( screen, mulShiftImm );   // Shift bits to MSB
+//    screen = _mm_srai_epi16  ( screen, 15          );   // Shift bits to obtain 0xFFFF or 0x0000
+//    return screen;
+//}
+
+SSP_FORCEINLINE
+__m128i ssp_movmask_imm8_to_epi32_SSE2( int mask )
+{
+    __m128i screen;
+    const static __m128i mulShiftImm = SSP_CONST_SET_16I( 0x1000, 0x0000, 0x2000, 0x0000, 0x4000, 0x0000, 0x8000, 0x0000 ); // Shift mask multiply moves all bits to left, becomes MSB
+    screen = _mm_set1_epi16 ( mask                );   // Load the mask into register
+    screen = _mm_mullo_epi16( screen, mulShiftImm );   // Shift bits to MSB
+    screen = _mm_srai_epi32 ( screen, 31          );   // Shift bits to obtain all F's or all 0's
+    return screen;
+}
+
+
+/**
+  r_:= a_ << b; (logical left shift)
+*/
+SSP_FORCEINLINE __m128i ssp_slli_epi8_SSE2(__m128i a, const int b)
+{                                            //  a = VfVeVdVcVbVaV9V8V7V6V5V4V3V2V1V0
+    __m128i t1 = _mm_srli_epi16( a, 8 );     // t1 =   Vf  Vd  Vb  V9  V7  V5  V3  V1
+    __m128i t2 = _mm_slli_epi16( a, b + 8 ); // t2 = Re  Rc  Ra  R8  R6  R4  R2  R0
+    t1 = _mm_slli_epi16( t1, b + 8 );        // t1 = Rf  Rd  Rb  R9  R7  R5  R3  R1
+    t2 = _mm_srli_epi16( t1, 8 );            // t2 =   Re  Rc  Ra  R8  R6  R4  R2  R0
+    t1 = _mm_or_si128( t1, t2 );             // t1 = RfReRdRcRbRaR9R8R7R6R5R4R3R2R1R0
+    return t1;
+}
+
+/**
+  r_:= a_ >> b; (logical right shift)
+*/
+SSP_FORCEINLINE __m128i ssp_srli_epi8_SSE2(__m128i a, const int b)
+{                                            //  a = VfVeVdVcVbVaV9V8V7V6V5V4V3V2V1V0
+    __m128i t1 = _mm_slli_epi16( a, 8 );     // t1 = Ve  Vc  Va  V8  V6  V4  V2  V0
+    __m128i t2 = _mm_srli_epi16( a, b + 8 ); // t2 =   Rf  Rd  Rb  R9  R7  R5  R3  R1
+    t1 = _mm_srli_epi16( t1, b + 8 );        // t1 =   Re  Rc  Ra  R8  R6  R4  R2  R0
+    t2 = _mm_slli_epi16( t1, 8 );            // t2 = Rf  Rd  Rb  R9  R7  R5  R3  R1
+    t1 = _mm_or_si128( t1, t2 );             // t1 = RfReRdRcRbRaR9R8R7R6R5R4R3R2R1R0
+    return t1;
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_LOGICAL_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,483 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __MAP_SSE2__
+#define __MAP_SSE2__
+
+//#include "SSEPlus_UNMAP_ALL.h"
+
+//SSE
+#define ssp_add_ps               ssp_add_ps_SSE
+#define ssp_add_ss               ssp_add_ss_SSE
+#define ssp_and_ps               ssp_and_ps_SSE
+#define ssp_andnot_ps            ssp_andnot_ps_SSE
+#define ssp_cmpeq_ps             ssp_cmpeq_ps_SSE
+#define ssp_cmpeq_ss             ssp_cmpeq_ss_SSE
+#define ssp_cmpge_ps             ssp_cmpge_ps_SSE
+#define ssp_cmpge_ss             ssp_cmpge_ss_SSE
+#define ssp_cmpgt_ps             ssp_cmpgt_ps_SSE
+#define ssp_cmpgt_ss             ssp_cmpgt_ss_SSE
+#define ssp_cmple_ps             ssp_cmple_ps_SSE
+#define ssp_cmple_ss             ssp_cmple_ss_SSE
+#define ssp_cmplt_ps             ssp_cmplt_ps_SSE
+#define ssp_cmplt_ss             ssp_cmplt_ss_SSE
+#define ssp_cmpneq_ps            ssp_cmpneq_ps_SSE
+#define ssp_cmpneq_ss            ssp_cmpneq_ss_SSE
+#define ssp_cmpnge_ps            ssp_cmpnge_ps_SSE
+#define ssp_cmpnge_ss            ssp_cmpnge_ss_SSE
+#define ssp_cmpngt_ps            ssp_cmpngt_ps_SSE
+#define ssp_cmpngt_ss            ssp_cmpngt_ss_SSE
+#define ssp_cmpnle_ps            ssp_cmpnle_ps_SSE
+#define ssp_cmpnle_ss            ssp_cmpnle_ss_SSE
+#define ssp_cmpnlt_ps            ssp_cmpnlt_ps_SSE
+#define ssp_cmpnlt_ss            ssp_cmpnlt_ss_SSE
+#define ssp_cmpord_ps            ssp_cmpord_ps_SSE
+#define ssp_cmpord_ss            ssp_cmpord_ss_SSE
+#define ssp_cmpunord_ps          ssp_cmpunord_ps_SSE
+#define ssp_cmpunord_ss          ssp_cmpunord_ss_SSE
+#define ssp_comieq_ss            ssp_comieq_ss_SSE
+#define ssp_comige_ss            ssp_comige_ss_SSE
+#define ssp_comigt_ss            ssp_comigt_ss_SSE
+#define ssp_comile_ss            ssp_comile_ss_SSE
+#define ssp_comilt_ss            ssp_comilt_ss_SSE
+#define ssp_comineq_ss           ssp_comineq_ss_SSE
+#define ssp_cvt_pi2ps            ssp_cvt_pi2ps_SSE
+#define ssp_cvt_ps2pi            ssp_cvt_ps2pi_SSE
+#define ssp_cvt_si2ss            ssp_cvt_si2ss_SSE
+#define ssp_cvt_ss2si            ssp_cvt_ss2si_SSE
+#define ssp_cvtsi64_ss           ssp_cvtsi64_ss_SSE
+#define ssp_cvtss_f32            ssp_cvtss_f32_SSE
+#define ssp_cvtss_si64           ssp_cvtss_si64_SSE
+#define ssp_cvtt_ps2pi           ssp_cvtt_ps2pi_SSE
+#define ssp_cvtt_ss2si           ssp_cvtt_ss2si_SSE
+#define ssp_cvttss_si64          ssp_cvttss_si64_SSE
+#define ssp_div_ps               ssp_div_ps_SSE
+#define ssp_div_ss               ssp_div_ss_SSE
+#define ssp_getcsr               ssp_getcsr_SSE
+#define ssp_load_ps              ssp_load_ps_SSE
+#define ssp_load_ps1             ssp_load_ps1_SSE
+#define ssp_load_ss              ssp_load_ss_SSE
+#define ssp_loadh_pi             ssp_loadh_pi_SSE
+#define ssp_loadl_pi             ssp_loadl_pi_SSE
+#define ssp_loadr_ps             ssp_loadr_ps_SSE
+#define ssp_loadu_ps             ssp_loadu_ps_SSE
+#define ssp_max_ps               ssp_max_ps_SSE
+#define ssp_max_ss               ssp_max_ss_SSE
+#define ssp_min_ps               ssp_min_ps_SSE
+#define ssp_min_ss               ssp_min_ss_SSE
+#define ssp_move_ss              ssp_move_ss_SSE
+#define ssp_movehl_ps            ssp_movehl_ps_SSE
+#define ssp_movelh_ps            ssp_movelh_ps_SSE
+#define ssp_movemask_ps          ssp_movemask_ps_SSE
+#define ssp_mul_ps               ssp_mul_ps_SSE
+#define ssp_mul_ss               ssp_mul_ss_SSE
+#define ssp_or_ps                ssp_or_ps_SSE
+#define ssp_prefetch             ssp_prefetch_SSE
+#define ssp_rcp_ps               ssp_rcp_ps_SSE
+#define ssp_rcp_ss               ssp_rcp_ss_SSE
+#define ssp_rsqrt_ps             ssp_rsqrt_ps_SSE
+#define ssp_rsqrt_ss             ssp_rsqrt_ss_SSE
+#define ssp_set_ps               ssp_set_ps_SSE
+#define ssp_set_ps1              ssp_set_ps1_SSE
+#define ssp_set_ss               ssp_set_ss_SSE
+#define ssp_setcsr               ssp_setcsr_SSE
+#define ssp_setr_ps              ssp_setr_ps_SSE
+#define ssp_setzero_ps           ssp_setzero_ps_SSE
+#define ssp_sfence               ssp_sfence_SSE
+#define ssp_shuffle_ps           ssp_shuffle_ps_SSE
+#define ssp_sqrt_ps              ssp_sqrt_ps_SSE
+#define ssp_sqrt_ss              ssp_sqrt_ss_SSE
+#define ssp_store_ps             ssp_store_ps_SSE
+#define ssp_store_ps1            ssp_store_ps1_SSE
+#define ssp_store_ss             ssp_store_ss_SSE
+#define ssp_storeh_pi            ssp_storeh_pi_SSE
+#define ssp_storel_pi            ssp_storel_pi_SSE
+#define ssp_storer_ps            ssp_storer_ps_SSE
+#define ssp_storeu_ps            ssp_storeu_ps_SSE
+#define ssp_stream_ps            ssp_stream_ps_SSE
+#define ssp_sub_ps               ssp_sub_ps_SSE
+#define ssp_sub_ss               ssp_sub_ss_SSE
+#define ssp_ucomieq_ss           ssp_ucomieq_ss_SSE
+#define ssp_ucomige_ss           ssp_ucomige_ss_SSE
+#define ssp_ucomigt_ss           ssp_ucomigt_ss_SSE
+#define ssp_ucomile_ss           ssp_ucomile_ss_SSE
+#define ssp_ucomilt_ss           ssp_ucomilt_ss_SSE
+#define ssp_ucomineq_ss          ssp_ucomineq_ss_SSE
+#define ssp_unpackhi_ps          ssp_unpackhi_ps_SSE
+#define ssp_unpacklo_ps          ssp_unpacklo_ps_SSE
+#define ssp_xor_ps               ssp_xor_ps_SSE
+
+// SSE2 
+#define ssp_add_epi16            ssp_add_epi16_SSE2       
+#define ssp_add_epi32            ssp_add_epi32_SSE2       
+#define ssp_add_epi64            ssp_add_epi64_SSE2
+#define ssp_add_epi8             ssp_add_epi8_SSE2
+#define ssp_add_pd               ssp_add_pd_SSE2
+#define ssp_add_sd               ssp_add_sd_SSE2
+#define ssp_add_si64             ssp_add_si64_SSE2
+#define ssp_adds_epi16           ssp_adds_epi16_SSE2
+#define ssp_adds_epi8            ssp_adds_epi8_SSE2
+#define ssp_adds_epu16           ssp_adds_epu16_SSE2
+#define ssp_adds_epu8            ssp_adds_epu8_SSE2
+#define ssp_and_pd               ssp_and_pd_SSE2
+#define ssp_and_si128            ssp_and_si128_SSE2
+#define ssp_andnot_pd            ssp_andnot_pd_SSE2
+#define ssp_andnot_si128         ssp_andnot_si128_SSE2
+#define ssp_avg_epu16            ssp_avg_epu16_SSE2
+#define ssp_avg_epu8             ssp_avg_epu8_SSE2
+#define ssp_castpd_ps            ssp_castpd_ps_SSE2
+#define ssp_castpd_si128         ssp_castpd_si128_SSE2
+#define ssp_castps_pd            ssp_castps_pd_SSE2
+#define ssp_castps_si128         ssp_castps_si128_SSE2
+#define ssp_castsi128_pd         ssp_castsi128_pd_SSE2
+#define ssp_castsi128_ps         ssp_castsi128_ps_SSE2
+#define ssp_clflush              ssp_clflush_SSE2
+#define ssp_cmpeq_epi16          ssp_cmpeq_epi16_SSE2
+#define ssp_cmpeq_epi32          ssp_cmpeq_epi32_SSE2
+#define ssp_cmpeq_epi8           ssp_cmpeq_epi8_SSE2
+#define ssp_cmpeq_pd             ssp_cmpeq_pd_SSE2
+#define ssp_cmpeq_sd             ssp_cmpeq_sd_SSE2
+#define ssp_cmpge_pd             ssp_cmpge_pd_SSE2
+#define ssp_cmpge_sd             ssp_cmpge_sd_SSE2
+#define ssp_cmpgt_epi16          ssp_cmpgt_epi16_SSE2
+#define ssp_cmpgt_epi32          ssp_cmpgt_epi32_SSE2
+#define ssp_cmpgt_epi8           ssp_cmpgt_epi8_SSE2
+#define ssp_cmpgt_pd             ssp_cmpgt_pd_SSE2
+#define ssp_cmpgt_sd             ssp_cmpgt_sd_SSE2
+#define ssp_cmple_pd             ssp_cmple_pd_SSE2
+#define ssp_cmple_sd             ssp_cmple_sd_SSE2
+#define ssp_cmplt_epi16          ssp_cmplt_epi16_SSE2
+#define ssp_cmplt_epi32          ssp_cmplt_epi32_SSE2
+#define ssp_cmplt_epi8           ssp_cmplt_epi8_SSE2
+#define ssp_cmplt_pd             ssp_cmplt_pd_SSE2
+#define ssp_cmplt_sd             ssp_cmplt_sd_SSE2
+#define ssp_cmpneq_pd            ssp_cmpneq_pd_SSE2
+#define ssp_cmpneq_sd            ssp_cmpneq_sd_SSE2
+#define ssp_cmpnge_pd            ssp_cmpnge_pd_SSE2
+#define ssp_cmpnge_sd            ssp_cmpnge_sd_SSE2
+#define ssp_cmpngt_pd            ssp_cmpngt_pd_SSE2
+#define ssp_cmpngt_sd            ssp_cmpngt_sd_SSE2
+#define ssp_cmpnle_pd            ssp_cmpnle_pd_SSE2
+#define ssp_cmpnle_sd            ssp_cmpnle_sd_SSE2
+#define ssp_cmpnlt_pd            ssp_cmpnlt_pd_SSE2
+#define ssp_cmpnlt_sd            ssp_cmpnlt_sd_SSE2
+#define ssp_cmpord_pd            ssp_cmpord_pd_SSE2
+#define ssp_cmpord_sd            ssp_cmpord_sd_SSE2
+#define ssp_cmpunord_pd          ssp_cmpunord_pd_SSE2
+#define ssp_cmpunord_sd          ssp_cmpunord_sd_SSE2
+#define ssp_comieq_sd            ssp_comieq_sd_SSE2
+#define ssp_comige_sd            ssp_comige_sd_SSE2
+#define ssp_comigt_sd            ssp_comigt_sd_SSE2
+#define ssp_comile_sd            ssp_comile_sd_SSE2
+#define ssp_comilt_sd            ssp_comilt_sd_SSE2
+#define ssp_comineq_sd           ssp_comineq_sd_SSE2
+#define ssp_cvtepi32_pd          ssp_cvtepi32_pd_SSE2
+#define ssp_cvtepi32_ps          ssp_cvtepi32_ps_SSE2
+#define ssp_cvtpd_epi32          ssp_cvtpd_epi32_SSE2
+#define ssp_cvtpd_pi32           ssp_cvtpd_pi32_SSE2
+#define ssp_cvtpd_ps             ssp_cvtpd_ps_SSE2
+#define ssp_cvtpi32_pd           ssp_cvtpi32_pd_SSE2
+#define ssp_cvtps_epi32          ssp_cvtps_epi32_SSE2
+#define ssp_cvtps_pd             ssp_cvtps_pd_SSE2
+#define ssp_cvtsd_f64            ssp_cvtsd_f64_SSE2
+#define ssp_cvtsd_si32           ssp_cvtsd_si32_SSE2
+#define ssp_cvtsd_si64           ssp_cvtsd_si64_SSE2
+#define ssp_cvtsd_ss             ssp_cvtsd_ss_SSE2
+#define ssp_cvtsi128_si32        ssp_cvtsi128_si32_SSE2
+#define ssp_cvtsi128_si64        ssp_cvtsi128_si64_SSE2
+#define ssp_cvtsi32_sd           ssp_cvtsi32_sd_SSE2
+#define ssp_cvtsi32_si128        ssp_cvtsi32_si128_SSE2
+#define ssp_cvtsi64_sd           ssp_cvtsi64_sd_SSE2
+#define ssp_cvtsi64_si128        ssp_cvtsi64_si128_SSE2
+#define ssp_cvtss_sd             ssp_cvtss_sd_SSE2
+#define ssp_cvttpd_epi32         ssp_cvttpd_epi32_SSE2
+#define ssp_cvttpd_pi32          ssp_cvttpd_pi32_SSE2
+#define ssp_cvttps_epi32         ssp_cvttps_epi32_SSE2
+#define ssp_cvttsd_si32          ssp_cvttsd_si32_SSE2
+#define ssp_cvttsd_si64          ssp_cvttsd_si64_SSE2
+#define ssp_div_pd               ssp_div_pd_SSE2
+#define ssp_div_sd               ssp_div_sd_SSE2
+#define ssp_extract_epi16        ssp_extract_epi16_SSE2
+#define ssp_insert_epi16         ssp_insert_epi16_SSE2
+#define ssp_lfence               ssp_lfence_SSE2
+#define ssp_load_pd              ssp_load_pd_SSE2
+#define ssp_load_sd              ssp_load_sd_SSE2
+#define ssp_load_si128           ssp_load_si128_SSE2
+#define ssp_load1_pd             ssp_load1_pd_SSE2
+#define ssp_loadh_pd             ssp_loadh_pd_SSE2
+#define ssp_loadl_epi64          ssp_loadl_epi64_SSE2
+#define ssp_loadl_pd             ssp_loadl_pd_SSE2
+#define ssp_loadr_pd             ssp_loadr_pd_SSE2
+#define ssp_loadu_pd             ssp_loadu_pd_SSE2
+#define ssp_loadu_si128          ssp_loadu_si128_SSE2
+#define ssp_madd_epi16           ssp_madd_epi16_SSE2
+#define ssp_maskmoveu_si128      ssp_maskmoveu_si128_SSE2
+#define ssp_max_epi16            ssp_max_epi16_SSE2
+#define ssp_max_epu8             ssp_max_epu8_SSE2
+#define ssp_max_pd               ssp_max_pd_SSE2
+#define ssp_max_sd               ssp_max_sd_SSE2
+#define ssp_mfence               ssp_mfence_SSE2
+#define ssp_min_epi16            ssp_min_epi16_SSE2
+#define ssp_min_epu8             ssp_min_epu8_SSE2
+#define ssp_min_pd               ssp_min_pd_SSE2
+#define ssp_min_sd               ssp_min_sd_SSE2
+#define ssp_move_epi64           ssp_move_epi64_SSE2
+#define ssp_move_sd              ssp_move_sd_SSE2
+#define ssp_movemask_epi8        ssp_movemask_epi8_SSE2
+#define ssp_movemask_pd          ssp_movemask_pd_SSE2
+#define ssp_movepi64_pi64        ssp_movepi64_pi64_SSE2
+#define ssp_movpi64_epi64        ssp_movpi64_epi64_SSE2
+#define ssp_mul_epu32            ssp_mul_epu32_SSE2
+#define ssp_mul_pd               ssp_mul_pd_SSE2
+#define ssp_mul_sd               ssp_mul_sd_SSE2
+#define ssp_mul_su32             ssp_mul_su32_SSE2
+#define ssp_mulhi_epi16          ssp_mulhi_epi16_SSE2
+#define ssp_mulhi_epu16          ssp_mulhi_epu16_SSE2
+#define ssp_mullo_epi16          ssp_mullo_epi16_SSE2
+#define ssp_or_pd                ssp_or_pd_SSE2
+#define ssp_or_si128             ssp_or_si128_SSE2
+#define ssp_packs_epi16          ssp_packs_epi16_SSE2
+#define ssp_packs_epi32          ssp_packs_epi32_SSE2
+#define ssp_packus_epi16         ssp_packus_epi16_SSE2
+#define ssp_pause                ssp_pause_SSE2
+#define ssp_sad_epu8             ssp_sad_epu8_SSE2
+#define ssp_set_epi16            ssp_set_epi16_SSE2
+#define ssp_set_epi32            ssp_set_epi32_SSE2
+#define ssp_set_epi64            ssp_set_epi64_SSE2
+#define ssp_set_epi8             ssp_set_epi8_SSE2
+#define ssp_set_pd               ssp_set_pd_SSE2
+#define ssp_set_sd               ssp_set_sd_SSE2
+#define ssp_set1_epi16           ssp_set1_epi16_SSE2
+#define ssp_set1_epi32           ssp_set1_epi32_SSE2
+#define ssp_set1_epi64           ssp_set1_epi64_SSE2
+#define ssp_set1_epi8            ssp_set1_epi8_SSE2
+#define ssp_set1_pd              ssp_set1_pd_SSE2
+#define ssp_setl_epi64           ssp_setl_epi64_SSE2
+#define ssp_setr_epi16           ssp_setr_epi16_SSE2
+#define ssp_setr_epi32           ssp_setr_epi32_SSE2
+#define ssp_setr_epi64           ssp_setr_epi64_SSE2
+#define ssp_setr_epi8            ssp_setr_epi8_SSE2
+#define ssp_setr_pd              ssp_setr_pd_SSE2
+#define ssp_setzero_pd           ssp_setzero_pd_SSE2
+#define ssp_setzero_si128        ssp_setzero_si128_SSE2
+#define ssp_shuffle_epi32        ssp_shuffle_epi32_SSE2
+#define ssp_shuffle_pd           ssp_shuffle_pd_SSE2
+#define ssp_shufflehi_epi16      ssp_shufflehi_epi16_SSE2
+#define ssp_shufflelo_epi16      ssp_shufflelo_epi16_SSE2
+#define ssp_sll_epi16            ssp_sll_epi16_SSE2
+#define ssp_sll_epi32            ssp_sll_epi32_SSE2
+#define ssp_sll_epi64            ssp_sll_epi64_SSE2
+#define ssp_slli_epi16           ssp_slli_epi16_SSE2
+#define ssp_slli_epi32           ssp_slli_epi32_SSE2
+#define ssp_slli_epi64           ssp_slli_epi64_SSE2
+#define ssp_slli_si128           ssp_slli_si128_SSE2
+#define ssp_sqrt_pd              ssp_sqrt_pd_SSE2
+#define ssp_sqrt_sd              ssp_sqrt_sd_SSE2
+#define ssp_sra_epi16            ssp_sra_epi16_SSE2
+#define ssp_sra_epi32            ssp_sra_epi32_SSE2
+#define ssp_srai_epi16           ssp_srai_epi16_SSE2
+#define ssp_srai_epi32           ssp_srai_epi32_SSE2
+#define ssp_srl_epi16            ssp_srl_epi16_SSE2
+#define ssp_srl_epi32            ssp_srl_epi32_SSE2
+#define ssp_srl_epi64            ssp_srl_epi64_SSE2
+#define ssp_srli_epi16           ssp_srli_epi16_SSE2
+#define ssp_srli_epi32           ssp_srli_epi32_SSE2
+#define ssp_srli_epi64           ssp_srli_epi64_SSE2
+#define ssp_srli_si128           ssp_srli_si128_SSE2
+#define ssp_store_pd             ssp_store_pd_SSE2
+#define ssp_store_sd             ssp_store_sd_SSE2
+#define ssp_store_si128          ssp_store_si128_SSE2
+#define ssp_store1_pd            ssp_store1_pd_SSE2
+#define ssp_storeh_pd            ssp_storeh_pd_SSE2
+#define ssp_storel_epi64         ssp_storel_epi64_SSE2
+#define ssp_storel_pd            ssp_storel_pd_SSE2
+#define ssp_storer_pd            ssp_storer_pd_SSE2
+#define ssp_storeu_pd            ssp_storeu_pd_SSE2
+#define ssp_storeu_si128         ssp_storeu_si128_SSE2
+#define ssp_stream_pd            ssp_stream_pd_SSE2
+#define ssp_stream_si128         ssp_stream_si128_SSE2
+#define ssp_stream_si32          ssp_stream_si32_SSE2
+#define ssp_sub_epi16            ssp_sub_epi16_SSE2
+#define ssp_sub_epi32            ssp_sub_epi32_SSE2
+#define ssp_sub_epi64            ssp_sub_epi64_SSE2
+#define ssp_sub_epi8             ssp_sub_epi8_SSE2
+#define ssp_sub_pd               ssp_sub_pd_SSE2
+#define ssp_sub_sd               ssp_sub_sd_SSE2
+#define ssp_sub_si64             ssp_sub_si64_SSE2
+#define ssp_subs_epi16           ssp_subs_epi16_SSE2
+#define ssp_subs_epi8            ssp_subs_epi8_SSE2
+#define ssp_subs_epu16           ssp_subs_epu16_SSE2
+#define ssp_subs_epu8            ssp_subs_epu8_SSE2
+#define ssp_ucomieq_sd           ssp_ucomieq_sd_SSE2
+#define ssp_ucomige_sd           ssp_ucomige_sd_SSE2
+#define ssp_ucomigt_sd           ssp_ucomigt_sd_SSE2
+#define ssp_ucomile_sd           ssp_ucomile_sd_SSE2
+#define ssp_ucomilt_sd           ssp_ucomilt_sd_SSE2
+#define ssp_ucomineq_sd          ssp_ucomineq_sd_SSE2
+#define ssp_unpackhi_epi16       ssp_unpackhi_epi16_SSE2
+#define ssp_unpackhi_epi32       ssp_unpackhi_epi32_SSE2
+#define ssp_unpackhi_epi64       ssp_unpackhi_epi64_SSE2
+#define ssp_unpackhi_epi8        ssp_unpackhi_epi8_SSE2
+#define ssp_unpackhi_pd          ssp_unpackhi_pd_SSE2
+#define ssp_unpacklo_epi16       ssp_unpacklo_epi16_SSE2
+#define ssp_unpacklo_epi32       ssp_unpacklo_epi32_SSE2
+#define ssp_unpacklo_epi64       ssp_unpacklo_epi64_SSE2
+#define ssp_unpacklo_epi8        ssp_unpacklo_epi8_SSE2
+#define ssp_unpacklo_pd          ssp_unpacklo_pd_SSE2
+#define ssp_xor_pd               ssp_xor_pd_SSE2
+#define ssp_xor_si128            ssp_xor_si128_SSE2
+#define ssp_cvttss_si32          ssp_cvttss_si32_SSE2
+#define ssp_cvttps_pi32          ssp_cvttps_pi32_SSE2
+#define ssp_maskmove_si64        ssp_maskmove_si64_SSE2
+#define ssp_avg_pu8              ssp_avg_pu8_SSE2
+#define ssp_set1_ps              ssp_set1_ps_SSE2
+#define ssp_load1_ps             ssp_load1_ps_SSE2
+#define ssp_store1_ps            ssp_store1_ps_SSE2
+
+// SSE3
+#define ssp_addsub_pd            ssp_addsub_pd_SSE2        
+#define ssp_addsub_ps            ssp_addsub_ps_SSE2        
+#define ssp_hadd_pd              ssp_hadd_pd_SSE2          
+#define ssp_hadd_ps              ssp_hadd_ps_SSE2          
+#define ssp_hsub_pd2             ssp_hsub_pd_SSE2          
+#define ssp_hsub_ps              ssp_hsub_ps_SSE2          
+#define ssp_lddqu_si128          ssp_lddqu_si128_SSE2      
+#define ssp_loaddup_pd           ssp_loaddup_pd_SSE2       
+#define ssp_movedup_pd           ssp_movedup_pd_SSE2       
+#define ssp_movehdup_ps          ssp_movehdup_ps_SSE2      
+#define ssp_moveldup_ps          ssp_moveldup_ps_SSE2    
+
+// SSSE3
+#define ssp_abs_epi16            ssp_abs_epi16_SSE2        
+#define ssp_abs_epi32            ssp_abs_epi32_SSE2        
+#define ssp_abs_epi8             ssp_abs_epi8_SSE2         
+#define ssp_abs_pi16             ssp_abs_pi16_SSE2         
+#define ssp_abs_pi32             ssp_abs_pi32_SSE2         
+#define ssp_abs_pi8              ssp_abs_pi8_SSE2          
+#define ssp_alignr_epi8          ssp_alignr_epi8_SSE2      
+#define ssp_alignr_pi8           ssp_alignr_pi8_SSE2       
+#define ssp_hadd_epi16           ssp_hadd_epi16_SSE2       
+#define ssp_hadd_epi32           ssp_hadd_epi32_SSE2       
+#define ssp_hadd_pi16            ssp_hadd_pi16_SSE2        
+#define ssp_hadd_pi32            ssp_hadd_pi32_SSE2        
+#define ssp_hadds_epi16          ssp_hadds_epi16_SSE2      
+#define ssp_hadds_pi16           ssp_hadds_pi16_SSE2       
+#define ssp_hsub_epi16           ssp_hsub_epi16_SSE2       
+#define ssp_hsub_epi32           ssp_hsub_epi32_SSE2       
+#define ssp_hsub_pi16            ssp_hsub_pi16_SSE2        
+#define ssp_hsub_pi32            ssp_hsub_pi32_SSE2        
+#define ssp_hsubs_epi16          ssp_hsubs_epi16_SSE2      
+#define ssp_hsubs_pi16           ssp_hsubs_pi16_SSE2       
+#define ssp_maddubs_epi16        ssp_maddubs_epi16_SSE2    
+#define ssp_maddubs_pi16         ssp_maddubs_pi16_SSE2     
+#define ssp_mulhrs_epi16         ssp_mulhrs_epi16_SSE2     
+#define ssp_mulhrs_pi16          ssp_mulhrs_pi16_SSE2      
+#define ssp_shuffle_epi8         ssp_shuffle_epi8_SSE2     
+#define ssp_shuffle_pi8          ssp_shuffle_pi8_SSE2      
+#define ssp_sign_epi16           ssp_sign_epi16_SSE2       
+#define ssp_sign_epi32           ssp_sign_epi32_SSE2       
+#define ssp_sign_epi8            ssp_sign_epi8_SSE2        
+#define ssp_sign_pi16            ssp_sign_pi16_SSE2        
+#define ssp_sign_pi32            ssp_sign_pi32_SSE2        
+#define ssp_sign_pi8             ssp_sign_pi8_SSE2     
+
+//SSE4A
+#define ssp_extract_si64_SSE2    ssp_extract_si64_SSE2     
+#define ssp_extracti_si64_SSE2   ssp_extracti_si64_SSE2    
+#define ssp_insert_si64_SSE2     ssp_insert_si64_SSE2      
+#define ssp_inserti_si64_SSE2    ssp_inserti_si64_SSE2     
+#define ssp_stream_sd_SSE2       ssp_stream_sd_SSE2        
+#define ssp_stream_ss_SSE2       ssp_stream_ss_SSE2
+
+// SSE4.1
+#define ssp_blend_epi16          ssp_blend_epi16_SSE2      
+#define ssp_blend_pd             ssp_blend_pd_SSE2         
+#define ssp_blend_ps             ssp_blend_ps_SSE2         
+#define ssp_blendv_epi8          ssp_blendv_epi8_SSE2      
+#define ssp_blendv_pd            ssp_blendv_pd_SSE2        
+#define ssp_blendv_ps            ssp_blendv_ps_SSE2        
+#define ssp_ceil_pd              ssp_ceil_pd_SSE2          
+#define ssp_ceil_ps              ssp_ceil_ps_SSE2          
+#define ssp_ceil_sd              ssp_ceil_sd_SSE2          
+#define ssp_ceil_ss              ssp_ceil_ss_SSE2          
+#define ssp_cmpeq_epi64          ssp_cmpeq_epi64_SSE2      
+#define ssp_cvtepi16_epi32       ssp_cvtepi16_epi32_SSE2   
+#define ssp_cvtepi16_epi64       ssp_cvtepi16_epi64_SSE2   
+#define ssp_cvtepi32_epi64       ssp_cvtepi32_epi64_SSE2   
+#define ssp_cvtepi8_epi16        ssp_cvtepi8_epi16_SSE2    
+#define ssp_cvtepi8_epi32        ssp_cvtepi8_epi32_SSE2    
+#define ssp_cvtepi8_epi64        ssp_cvtepi8_epi64_SSE2    
+#define ssp_cvtepu16_epi32       ssp_cvtepu16_epi32_SSE2   
+#define ssp_cvtepu16_epi64       ssp_cvtepu16_epi64_SSE2   
+#define ssp_cvtepu32_epi64       ssp_cvtepu32_epi64_SSE2   
+#define ssp_cvtepu8_epi16        ssp_cvtepu8_epi16_SSE2    
+#define ssp_cvtepu8_epi32        ssp_cvtepu8_epi32_SSE2    
+#define ssp_cvtepu8_epi64        ssp_cvtepu8_epi64_SSE2    
+#define ssp_dp_pd                ssp_dp_pd_SSE2            
+#define ssp_dp_ps                ssp_dp_ps_SSE2            
+#define ssp_extract_epi32        ssp_extract_epi32_SSE2    
+#define ssp_extract_epi64        ssp_extract_epi64_SSE2    
+#define ssp_extract_epi8         ssp_extract_epi8_SSE2     
+#define ssp_extract_ps           ssp_extract_ps_SSE2       
+#define ssp_floor_pd             ssp_floor_pd_SSE2         
+#define ssp_floor_ps             ssp_floor_ps_SSE2         
+#define ssp_floor_sd             ssp_floor_sd_SSE2         
+#define ssp_floor_ss             ssp_floor_ss_SSE2         
+#define ssp_insert_epi32         ssp_insert_epi32_SSE2     
+#define ssp_insert_epi64         ssp_insert_epi64_SSE2     
+#define ssp_insert_epi8          ssp_insert_epi8_SSE2      
+#define ssp_insert_ps            ssp_insert_ps_SSE2        
+#define ssp_max_epi32            ssp_max_epi32_SSE2        
+#define ssp_max_epi8             ssp_max_epi8_SSE2         
+#define ssp_max_epu16            ssp_max_epu16_SSE2        
+#define ssp_max_epu32            ssp_max_epu32_SSE2        
+#define ssp_min_epi32            ssp_min_epi32_SSE2        
+#define ssp_min_epi8             ssp_min_epi8_SSE2         
+#define ssp_min_epu16            ssp_min_epu16_SSE2        
+#define ssp_min_epu32            ssp_min_epu32_SSE2        
+#define ssp_minpos_epu16         ssp_minpos_epu16_SSE2     
+#define ssp_mpsadbw_epu8         ssp_mpsadbw_epu8_SSE2     
+#define ssp_mul_epi32            ssp_mul_epi32_SSE2        
+#define ssp_mullo_epi32          ssp_mullo_epi32_SSE2      
+#define ssp_packus_epi32         ssp_packus_epi32_SSE2     
+#define ssp_round_pd             ssp_round_pd_SSE2         
+#define ssp_round_ps             ssp_round_ps_SSE2         
+#define ssp_round_sd             ssp_round_sd_SSE2         
+#define ssp_round_ss             ssp_round_ss_SSE2         
+#define ssp_stream_load_si128     ssp_stream_load_si128_SSE2
+#define ssp_testc_si128          ssp_testc_si128_SSE2      
+#define ssp_testnzc_si128        ssp_testnzc_si128_SSE2    
+#define ssp_testz_si128          ssp_testz_si128_SSE2      
+                                      
+//SSE4.2                              
+#define ssp_cmpestra             ssp_cmpestra_SSE2         
+#define ssp_cmpestrc             ssp_cmpestrc_SSE2         
+#define ssp_cmpestri             ssp_cmpestri_SSE2         
+#define ssp_cmpestrm             ssp_cmpestrm_SSE2         
+#define ssp_cmpestro             ssp_cmpestro_SSE2         
+#define ssp_cmpestrs             ssp_cmpestrs_SSE2         
+#define ssp_cmpestrz             ssp_cmpestrz_SSE2         
+#define ssp_cmpgt_epi64          ssp_cmpgt_epi64_SSE2      
+#define ssp_cmpistra             ssp_cmpistra_SSE2         
+#define ssp_cmpistrc             ssp_cmpistrc_SSE2         
+#define ssp_cmpistri             ssp_cmpistri_SSE2         
+#define ssp_cmpistrm             ssp_cmpistrm_SSE2         
+#define ssp_cmpistro             ssp_cmpistro_SSE2         
+#define ssp_cmpistrs             ssp_cmpistrs_SSE2         
+#define ssp_cmpistrz             ssp_cmpistrz_SSE2         
+#define ssp_crc32_u16            ssp_crc32_u16_SSE2 
+#define ssp_crc32_u32            ssp_crc32_u32_SSE2 
+#define ssp_crc32_u64            ssp_crc32_u64_SSE2 
+#define ssp_crc32_u8             ssp_crc32_u8_SSE2  
+#define ssp_popcnt_u32           ssp_popcnt_u32_SSE2       
+#define ssp_popcnt_u64           ssp_popcnt_u64_SSE2  
+
+// Arithmetic
+#define ssp_arithmetic_hadd4_epi16  ssp_arithmetic_hadd4_epi16_SSE2
+#define ssp_arithmetic_hadd4_dup_ps ssp_arithmetic_hadd4_dup_ps_SSE2
+
+// Logical
+#define ssp_logical_bitwise_select  ssp_logical_bitwise_select_SSE2
+
+#endif __MAP_SSE2__
\ No newline at end of file
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_MAP_SSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,43 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __MAP_SSE3__
+#define __MAP_SSE3__
+
+#include "SSEPlus_MAP_SSE2.h"
+
+//Native Instructons
+#undef ssp_addsub_pd    
+#undef ssp_addsub_ps    
+#undef ssp_hadd_pd      
+#undef ssp_hadd_ps      
+#undef ssp_hsub_pd      
+#undef ssp_hsub_ps      
+#undef ssp_lddqu_si128  
+#undef ssp_loaddup_pd   
+#undef ssp_movedup_pd   
+#undef ssp_movehdup_ps  
+#undef ssp_moveldup_ps  
+
+#define ssp_addsub_pd        ssp_addsub_pd_SSE3        
+#define ssp_addsub_ps        ssp_addsub_ps_SSE3        
+#define ssp_hadd_pd          ssp_hadd_pd_SSE3          
+#define ssp_hadd_ps          ssp_hadd_ps_SSE3          
+#define ssp_hsub_pd          ssp_hsub_pd_SSE3          
+#define ssp_hsub_ps          ssp_hsub_ps_SSE3          
+#define ssp_lddqu_si128      ssp_lddqu_si128_SSE3      
+#define ssp_loaddup_pd       ssp_loaddup_pd_SSE3       
+#define ssp_movedup_pd       ssp_movedup_pd_SSE3       
+#define ssp_movehdup_ps      ssp_movehdup_ps_SSE3      
+#define ssp_moveldup_ps      ssp_moveldup_ps_SSE3      
+
+// Emulated
+#undef  ssp_dp_ps
+#define ssp_dp_ps            ssp_dp_ps_SSE3    
+
+// Arithmetic
+#undef  ssp_arithmetic_hadd4_dup_ps
+#define ssp_arithmetic_hadd4_dup_ps ssp_arithmetic_hadd4_dup_ps_SSE3
+
+#endif __MAP_SSE3__
\ No newline at end of file
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_UNMAP_ALL.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_UNMAP_ALL.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_UNMAP_ALL.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/map/SSEPlus_UNMAP_ALL.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,484 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+//no header guard
+
+#undef __MAP_SSE2__
+#undef __MAP_SSE3__
+
+//SSE
+#undef ssp_add_ps      
+#undef ssp_add_ss      
+#undef ssp_and_ps      
+#undef ssp_andnot_ps   
+#undef ssp_cmpeq_ps    
+#undef ssp_cmpeq_ss    
+#undef ssp_cmpge_ps    
+#undef ssp_cmpge_ss    
+#undef ssp_cmpgt_ps    
+#undef ssp_cmpgt_ss    
+#undef ssp_cmple_ps    
+#undef ssp_cmple_ss    
+#undef ssp_cmplt_ps    
+#undef ssp_cmplt_ss    
+#undef ssp_cmpneq_ps   
+#undef ssp_cmpneq_ss   
+#undef ssp_cmpnge_ps   
+#undef ssp_cmpnge_ss   
+#undef ssp_cmpngt_ps   
+#undef ssp_cmpngt_ss   
+#undef ssp_cmpnle_ps   
+#undef ssp_cmpnle_ss   
+#undef ssp_cmpnlt_ps   
+#undef ssp_cmpnlt_ss   
+#undef ssp_cmpord_ps   
+#undef ssp_cmpord_ss   
+#undef ssp_cmpunord_ps 
+#undef ssp_cmpunord_ss 
+#undef ssp_comieq_ss   
+#undef ssp_comige_ss   
+#undef ssp_comigt_ss   
+#undef ssp_comile_ss   
+#undef ssp_comilt_ss   
+#undef ssp_comineq_ss  
+#undef ssp_cvt_pi2ps   
+#undef ssp_cvt_ps2pi   
+#undef ssp_cvt_si2ss   
+#undef ssp_cvt_ss2si   
+#undef ssp_cvtsi64_ss  
+#undef ssp_cvtss_f32   
+#undef ssp_cvtss_si64  
+#undef ssp_cvtt_ps2pi  
+#undef ssp_cvtt_ss2si  
+#undef ssp_cvttss_si64 
+#undef ssp_div_ps      
+#undef ssp_div_ss      
+#undef ssp_getcsr      
+#undef ssp_load_ps     
+#undef ssp_load_ps1    
+#undef ssp_load_ss     
+#undef ssp_loadh_pi    
+#undef ssp_loadl_pi    
+#undef ssp_loadr_ps    
+#undef ssp_loadu_ps    
+#undef ssp_max_ps      
+#undef ssp_max_ss      
+#undef ssp_min_ps      
+#undef ssp_min_ss      
+#undef ssp_move_ss     
+#undef ssp_movehl_ps   
+#undef ssp_movelh_ps   
+#undef ssp_movemask_ps 
+#undef ssp_mul_ps      
+#undef ssp_mul_ss      
+#undef ssp_or_ps       
+#undef ssp_prefetch    
+#undef ssp_rcp_ps      
+#undef ssp_rcp_ss      
+#undef ssp_rsqrt_ps    
+#undef ssp_rsqrt_ss    
+#undef ssp_set_ps      
+#undef ssp_set_ps1     
+#undef ssp_set_ss      
+#undef ssp_setcsr      
+#undef ssp_setr_ps     
+#undef ssp_setzero_ps  
+#undef ssp_sfence      
+#undef ssp_shuffle_ps  
+#undef ssp_sqrt_ps     
+#undef ssp_sqrt_ss     
+#undef ssp_store_ps    
+#undef ssp_store_ps1   
+#undef ssp_store_ss    
+#undef ssp_storeh_pi   
+#undef ssp_storel_pi   
+#undef ssp_storer_ps   
+#undef ssp_storeu_ps   
+#undef ssp_stream_ps   
+#undef ssp_sub_ps      
+#undef ssp_sub_ss      
+#undef ssp_ucomieq_ss  
+#undef ssp_ucomige_ss  
+#undef ssp_ucomigt_ss  
+#undef ssp_ucomile_ss  
+#undef ssp_ucomilt_ss  
+#undef ssp_ucomineq_ss 
+#undef ssp_unpackhi_ps 
+#undef ssp_unpacklo_ps 
+#undef ssp_xor_ps      
+
+//SSE2
+#undef ssp_add_epi16        
+#undef ssp_add_epi32        
+#undef ssp_add_epi64        
+#undef ssp_add_epi8         
+#undef ssp_add_pd           
+#undef ssp_add_sd           
+#undef ssp_add_si64         
+#undef ssp_adds_epi16       
+#undef ssp_adds_epi8        
+#undef ssp_adds_epu16       
+#undef ssp_adds_epu8        
+#undef ssp_and_pd           
+#undef ssp_and_si128        
+#undef ssp_andnot_pd        
+#undef ssp_andnot_si128     
+#undef ssp_avg_epu16        
+#undef ssp_avg_epu8         
+#undef ssp_castpd_ps        
+#undef ssp_castpd_si128     
+#undef ssp_castps_pd        
+#undef ssp_castps_si128     
+#undef ssp_castsi128_pd     
+#undef ssp_castsi128_ps     
+#undef ssp_clflush          
+#undef ssp_cmpeq_epi16      
+#undef ssp_cmpeq_epi32      
+#undef ssp_cmpeq_epi8       
+#undef ssp_cmpeq_pd         
+#undef ssp_cmpeq_sd         
+#undef ssp_cmpge_pd         
+#undef ssp_cmpge_sd         
+#undef ssp_cmpgt_epi16      
+#undef ssp_cmpgt_epi32      
+#undef ssp_cmpgt_epi8       
+#undef ssp_cmpgt_pd         
+#undef ssp_cmpgt_sd         
+#undef ssp_cmple_pd         
+#undef ssp_cmple_sd         
+#undef ssp_cmplt_epi16      
+#undef ssp_cmplt_epi32      
+#undef ssp_cmplt_epi8       
+#undef ssp_cmplt_pd         
+#undef ssp_cmplt_sd         
+#undef ssp_cmpneq_pd        
+#undef ssp_cmpneq_sd        
+#undef ssp_cmpnge_pd        
+#undef ssp_cmpnge_sd        
+#undef ssp_cmpngt_pd        
+#undef ssp_cmpngt_sd        
+#undef ssp_cmpnle_pd        
+#undef ssp_cmpnle_sd        
+#undef ssp_cmpnlt_pd        
+#undef ssp_cmpnlt_sd        
+#undef ssp_cmpord_pd        
+#undef ssp_cmpord_sd        
+#undef ssp_cmpunord_pd      
+#undef ssp_cmpunord_sd      
+#undef ssp_comieq_sd        
+#undef ssp_comige_sd        
+#undef ssp_comigt_sd        
+#undef ssp_comile_sd        
+#undef ssp_comilt_sd        
+#undef ssp_comineq_sd       
+#undef ssp_cvtepi32_pd      
+#undef ssp_cvtepi32_ps      
+#undef ssp_cvtpd_epi32      
+#undef ssp_cvtpd_pi32       
+#undef ssp_cvtpd_ps         
+#undef ssp_cvtpi32_pd       
+#undef ssp_cvtps_epi32      
+#undef ssp_cvtps_pd         
+#undef ssp_cvtsd_f64        
+#undef ssp_cvtsd_si32       
+#undef ssp_cvtsd_si64       
+#undef ssp_cvtsd_ss         
+#undef ssp_cvtsi128_si32    
+#undef ssp_cvtsi128_si64    
+#undef ssp_cvtsi32_sd       
+#undef ssp_cvtsi32_si128    
+#undef ssp_cvtsi64_sd       
+#undef ssp_cvtsi64_si128    
+#undef ssp_cvtss_sd         
+#undef ssp_cvttpd_epi32     
+#undef ssp_cvttpd_pi32      
+#undef ssp_cvttps_epi32     
+#undef ssp_cvttsd_si32      
+#undef ssp_cvttsd_si64      
+#undef ssp_div_pd           
+#undef ssp_div_sd           
+#undef ssp_extract_epi16    
+#undef ssp_insert_epi16     
+#undef ssp_lfence           
+#undef ssp_load_pd          
+#undef ssp_load_sd          
+#undef ssp_load_si128       
+#undef ssp_load1_pd         
+#undef ssp_loadh_pd         
+#undef ssp_loadl_epi64      
+#undef ssp_loadl_pd         
+#undef ssp_loadr_pd         
+#undef ssp_loadu_pd         
+#undef ssp_loadu_si128      
+#undef ssp_madd_epi16       
+#undef ssp_maskmoveu_si128  
+#undef ssp_max_epi16        
+#undef ssp_max_epu8         
+#undef ssp_max_pd           
+#undef ssp_max_sd           
+#undef ssp_mfence           
+#undef ssp_min_epi16        
+#undef ssp_min_epu8         
+#undef ssp_min_pd           
+#undef ssp_min_sd           
+#undef ssp_move_epi64       
+#undef ssp_move_sd          
+#undef ssp_movemask_epi8    
+#undef ssp_movemask_pd      
+#undef ssp_movepi64_pi64    
+#undef ssp_movpi64_epi64    
+#undef ssp_mul_epu32        
+#undef ssp_mul_pd           
+#undef ssp_mul_sd           
+#undef ssp_mul_su32         
+#undef ssp_mulhi_epi16      
+#undef ssp_mulhi_epu16      
+#undef ssp_mullo_epi16      
+#undef ssp_or_pd           
+#undef ssp_or_si128         
+#undef ssp_packs_epi16      
+#undef ssp_packs_epi32      
+#undef ssp_packus_epi16     
+#undef ssp_pause           
+#undef ssp_sad_epu8         
+#undef ssp_set_epi16        
+#undef ssp_set_epi32        
+#undef ssp_set_epi64        
+#undef ssp_set_epi8         
+#undef ssp_set_pd           
+#undef ssp_set_sd           
+#undef ssp_set1_epi16       
+#undef ssp_set1_epi32       
+#undef ssp_set1_epi64       
+#undef ssp_set1_epi8        
+#undef ssp_set1_pd          
+#undef ssp_setl_epi64       
+#undef ssp_setr_epi16       
+#undef ssp_setr_epi32       
+#undef ssp_setr_epi64       
+#undef ssp_setr_epi8        
+#undef ssp_setr_pd          
+#undef ssp_setzero_pd       
+#undef ssp_setzero_si128    
+#undef ssp_shuffle_epi32    
+#undef ssp_shuffle_pd       
+#undef ssp_shufflehi_epi16  
+#undef ssp_shufflelo_epi16  
+#undef ssp_sll_epi16        
+#undef ssp_sll_epi32        
+#undef ssp_sll_epi64        
+#undef ssp_slli_epi16       
+#undef ssp_slli_epi32       
+#undef ssp_slli_epi64       
+#undef ssp_slli_si128       
+#undef ssp_sqrt_pd          
+#undef ssp_sqrt_sd          
+#undef ssp_sra_epi16        
+#undef ssp_sra_epi32        
+#undef ssp_srai_epi16       
+#undef ssp_srai_epi32       
+#undef ssp_srl_epi16        
+#undef ssp_srl_epi32        
+#undef ssp_srl_epi64        
+#undef ssp_srli_epi16       
+#undef ssp_srli_epi32       
+#undef ssp_srli_epi64       
+#undef ssp_srli_si128       
+#undef ssp_store_pd         
+#undef ssp_store_sd         
+#undef ssp_store_si128      
+#undef ssp_store1_pd        
+#undef ssp_storeh_pd        
+#undef ssp_storel_epi64     
+#undef ssp_storel_pd        
+#undef ssp_storer_pd        
+#undef ssp_storeu_pd        
+#undef ssp_storeu_si128     
+#undef ssp_stream_pd        
+#undef ssp_stream_si128     
+#undef ssp_stream_si32      
+#undef ssp_sub_epi16        
+#undef ssp_sub_epi32        
+#undef ssp_sub_epi64        
+#undef ssp_sub_epi8         
+#undef ssp_sub_pd           
+#undef ssp_sub_sd           
+#undef ssp_sub_si64         
+#undef ssp_subs_epi16       
+#undef ssp_subs_epi8        
+#undef ssp_subs_epu16       
+#undef ssp_subs_epu8        
+#undef ssp_ucomieq_sd       
+#undef ssp_ucomige_sd       
+#undef ssp_ucomigt_sd       
+#undef ssp_ucomile_sd       
+#undef ssp_ucomilt_sd       
+#undef ssp_ucomineq_sd      
+#undef ssp_unpackhi_epi16   
+#undef ssp_unpackhi_epi32   
+#undef ssp_unpackhi_epi64   
+#undef ssp_unpackhi_epi8    
+#undef ssp_unpackhi_pd      
+#undef ssp_unpacklo_epi16   
+#undef ssp_unpacklo_epi32   
+#undef ssp_unpacklo_epi64   
+#undef ssp_unpacklo_epi8    
+#undef ssp_unpacklo_pd      
+#undef ssp_xor_pd           
+#undef ssp_xor_si128        
+#undef ssp_cvttss_si32      
+#undef ssp_cvttps_pi32      
+#undef ssp_maskmove_si64    
+#undef ssp_avg_pu8          
+#undef ssp_set1_ps          
+#undef ssp_load1_ps         
+#undef ssp_store1_ps        
+
+// SSE3
+#undef ssp_addsub_pd        
+#undef ssp_addsub_ps        
+#undef ssp_hadd_pd          
+#undef ssp_hadd_ps          
+#undef ssp_hsub_pd2         
+#undef ssp_hsub_ps          
+#undef ssp_lddqu_si128      
+#undef ssp_loaddup_pd       
+#undef ssp_movedup_pd       
+#undef ssp_movehdup_ps      
+#undef ssp_moveldup_ps      
+
+// SSSE3
+#undef ssp_abs_epi16        
+#undef ssp_abs_epi32        
+#undef ssp_abs_epi8         
+#undef ssp_abs_pi16         
+#undef ssp_abs_pi32         
+#undef ssp_abs_pi8          
+#undef ssp_alignr_epi8      
+#undef ssp_alignr_pi8       
+#undef ssp_hadd_epi16       
+#undef ssp_hadd_epi32       
+#undef ssp_hadd_pi16        
+#undef ssp_hadd_pi32        
+#undef ssp_hadds_epi16      
+#undef ssp_hadds_pi16       
+#undef ssp_hsub_epi16       
+#undef ssp_hsub_epi32       
+#undef ssp_hsub_pi16        
+#undef ssp_hsub_pi32        
+#undef ssp_hsubs_epi16      
+#undef ssp_hsubs_pi16       
+#undef ssp_maddubs_epi16    
+#undef ssp_maddubs_pi16     
+#undef ssp_mulhrs_epi16     
+#undef ssp_mulhrs_pi16      
+#undef ssp_shuffle_epi8     
+#undef ssp_shuffle_pi8      
+#undef ssp_sign_epi16       
+#undef ssp_sign_epi32       
+#undef ssp_sign_epi8        
+#undef ssp_sign_pi16        
+#undef ssp_sign_pi32        
+#undef ssp_sign_pi8         
+
+//SSE4A
+#undef       ssp_extract_si64_SSE2          
+#undef       ssp_extracti_si64_SSE2         
+#undef       ssp_insert_si64_SSE2           
+#undef       ssp_inserti_si64_SSE2          
+#undef       ssp_stream_sd_SSE2             
+#undef       ssp_stream_ss_SSE2             
+
+// SSE4.1
+#undef ssp_blend_epi16            
+#undef ssp_blend_pd               
+#undef ssp_blend_ps               
+#undef ssp_blendv_epi8            
+#undef ssp_blendv_pd              
+#undef ssp_blendv_ps              
+#undef ssp_ceil_pd                
+#undef ssp_ceil_ps                
+#undef ssp_ceil_sd                
+#undef ssp_ceil_ss                
+#undef ssp_cmpeq_epi64            
+#undef ssp_cvtepi16_epi32         
+#undef ssp_cvtepi16_epi64         
+#undef ssp_cvtepi32_epi64         
+#undef ssp_cvtepi8_epi16          
+#undef ssp_cvtepi8_epi32          
+#undef ssp_cvtepi8_epi64          
+#undef ssp_cvtepu16_epi32         
+#undef ssp_cvtepu16_epi64         
+#undef ssp_cvtepu32_epi64         
+#undef ssp_cvtepu8_epi16          
+#undef ssp_cvtepu8_epi32          
+#undef ssp_cvtepu8_epi64          
+#undef ssp_dp_pd                  
+#undef ssp_dp_ps                  
+#undef ssp_extract_epi32          
+#undef ssp_extract_epi64          
+#undef ssp_extract_epi8           
+#undef ssp_extract_ps             
+#undef ssp_floor_pd               
+#undef ssp_floor_ps               
+#undef ssp_floor_sd               
+#undef ssp_floor_ss               
+#undef ssp_insert_epi32           
+#undef ssp_insert_epi64           
+#undef ssp_insert_epi8            
+#undef ssp_insert_ps              
+#undef ssp_max_epi32              
+#undef ssp_max_epi8               
+#undef ssp_max_epu16              
+#undef ssp_max_epu32              
+#undef ssp_min_epi32              
+#undef ssp_min_epi8               
+#undef ssp_min_epu16              
+#undef ssp_min_epu32              
+#undef ssp_minpos_epu16           
+#undef ssp_mpsadbw_epu8           
+#undef ssp_mul_epi32              
+#undef ssp_mullo_epi32            
+#undef ssp_packus_epi32           
+#undef ssp_round_pd               
+#undef ssp_round_ps               
+#undef ssp_round_sd               
+#undef ssp_round_ss               
+#undef ssp_stream_load_si128      
+#undef ssp_testc_si128            
+#undef ssp_testnzc_si128          
+#undef ssp_testz_si128            
+                                  
+//SSE4.2                          
+#undef ssp_cmpestra               
+#undef ssp_cmpestrc               
+#undef ssp_cmpestri               
+#undef ssp_cmpestrm               
+#undef ssp_cmpestro               
+#undef ssp_cmpestrs               
+#undef ssp_cmpestrz               
+#undef ssp_cmpgt_epi64            
+#undef ssp_cmpistra               
+#undef ssp_cmpistrc               
+#undef ssp_cmpistri               
+#undef ssp_cmpistrm               
+#undef ssp_cmpistro               
+#undef ssp_cmpistrs               
+#undef ssp_cmpistrz               
+#undef ssp_crc32_u16              
+#undef ssp_crc32_u32              
+#undef ssp_crc32_u64              
+#undef ssp_crc32_u8               
+#undef ssp_popcnt_u32             
+#undef ssp_popcnt_u64 
+
+// Arithmetic
+#undef ssp_arithmetic_hadd4_epi16 
+#undef ssp_arithmetic_hadd4_dup_ps
+
+// Logical
+#undef ssp_logical_bitwise_select 
+   
+
+//no header guard
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,36 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_MEMORY_REF_H__
+#define __SSEPLUS_MEMORY_REF_H__
+
+#include "../SSEPlus_base.h"
+
+/** @addtogroup supplimental_REF   
+ *  @{ 
+ *  @name Memory Operations
+ */
+
+__m128i ssp_memory_load1_epu8_REF( unsigned char a )
+{
+    ssp_m128 A;
+    
+    A.u8[0] = a;
+    A.u8[1] = a;
+    A.u8[2] = a;
+    A.u8[3] = a;
+
+    A.u32[1] = A.u32[0];
+    A.u32[2] = A.u32[0];
+    A.u32[3] = A.u32[0]; 
+    
+    return A.i;
+}
+
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_MEMORY_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/memory/SSEPlus_memory_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,39 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_MEMORY_SSE2_H__
+#define __SSEPLUS_MEMORY_SSE2_H__
+
+#include "../SSEPlus_base.h"
+#include <emmintrin.h>  // SSE2
+
+/** @addtogroup supplimental_SSE2
+ *  @{ 
+ *  @name Memory Operations
+ */
+
+
+
+__m128i ssp_memory_load1_epu8_SSE2( unsigned char a )
+{
+    ssp_m128 A;
+    
+    A.u8[0] = a;
+    A.u8[1] = a;
+    A.u8[2] = a;
+    A.u8[3] = a;
+
+    A.u32[1] = A.u32[0];
+    A.u32[2] = A.u32[0];
+    A.u32[3] = A.u32[0]; 
+    
+    return A.i;
+}
+
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_MEMORY_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,804 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE_H__
+#define __SSEPLUS_NATIVE_SSE_H__
+
+#include "../SSEPlus_base.h"
+#include <xmmintrin.h>  // SSE
+
+/** @addtogroup native_SSE   
+ *  @{ 
+ *  @name Native SSE Operations
+ */
+
+/*
+ * FP, arithmetic
+ */
+
+/** \SSE{Native,_mm_add_ss} */
+SSP_FORCEINLINE __m128 ssp_add_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_add_ss( a, b );
+}
+
+/** \SSE{Native,_mm_add_ps} */
+SSP_FORCEINLINE __m128 ssp_add_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_add_ps( a, b );
+}
+
+/** \SSE{Native,_mm_sub_ss} */
+SSP_FORCEINLINE __m128 ssp_sub_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_sub_ss( a, b );
+}
+
+/** \SSE{Native,_mm_sub_ps} */
+SSP_FORCEINLINE __m128 ssp_sub_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_sub_ps( a, b );
+}
+
+/** \SSE{Native,_mm_mul_ss} */
+SSP_FORCEINLINE __m128 ssp_mul_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_mul_ss( a, b );
+}
+
+/** \SSE{Native,_mm_mul_ps} */
+SSP_FORCEINLINE __m128 ssp_mul_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_mul_ps( a, b );
+}
+
+/** \SSE{Native,_mm_div_ss} */
+SSP_FORCEINLINE __m128 ssp_div_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_div_ss( a, b );
+}
+
+/** \SSE{Native,_mm_div_ps} */
+SSP_FORCEINLINE __m128 ssp_div_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_div_ps( a, b );
+}
+
+/** \SSE{Native,_mm_sqrt_ss} */
+SSP_FORCEINLINE __m128 ssp_sqrt_ss_SSE( __m128 a )
+{
+    return _mm_sqrt_ss( a );
+}
+
+/** \SSE{Native,_mm_sqrt_ps} */
+SSP_FORCEINLINE __m128 ssp_sqrt_ps_SSE( __m128 a )
+{
+    return _mm_sqrt_ps( a );
+}
+
+/** \SSE{Native,_mm_rcp_ss} */
+SSP_FORCEINLINE __m128 ssp_rcp_ss_SSE( __m128 a )
+{
+    return _mm_rcp_ss( a );
+}
+
+/** \SSE{Native,_mm_rcp_ps} */
+SSP_FORCEINLINE __m128 ssp_rcp_ps_SSE( __m128 a )
+{
+    return _mm_rcp_ps( a );
+}
+
+/** \SSE{Native,_mm_rsqrt_ss} */
+SSP_FORCEINLINE __m128 ssp_rsqrt_ss_SSE( __m128 a )
+{
+    return _mm_rsqrt_ss( a );
+}
+
+/** \SSE{Native,_mm_rsqrt_ps} */
+SSP_FORCEINLINE __m128 ssp_rsqrt_ps_SSE( __m128 a )
+{
+    return _mm_rsqrt_ps( a );
+}
+
+/** \SSE{Native,_mm_min_ss} */
+SSP_FORCEINLINE __m128 ssp_min_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_min_ss( a, b );
+}
+
+/** \SSE{Native,_mm_min_ps} */
+SSP_FORCEINLINE __m128 ssp_min_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_min_ps( a, b );
+}
+
+/** \SSE{Native,_mm_max_ss} */
+SSP_FORCEINLINE __m128 ssp_max_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_max_ss( a, b );
+}
+
+/** \SSE{Native,_mm_max_ps} */
+SSP_FORCEINLINE __m128 ssp_max_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_max_ps( a, b );
+}
+
+/** \SSE{Native,_mm_and_ps} */
+SSP_FORCEINLINE __m128 ssp_and_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_and_ps( a, b );
+}
+
+/** \SSE{Native,_mm_andnot_ps} */
+SSP_FORCEINLINE __m128 ssp_andnot_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_andnot_ps( a, b );
+}
+
+/** \SSE{Native,_mm_or_ps} */
+SSP_FORCEINLINE __m128 ssp_or_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_or_ps( a, b );
+}
+
+/** \SSE{Native,_mm_xor_ps} */
+SSP_FORCEINLINE __m128 ssp_xor_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_xor_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpeq_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpeq_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpeq_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpeq_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpeq_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpeq_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmplt_ss} */
+SSP_FORCEINLINE __m128 ssp_cmplt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmplt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmplt_ps} */
+SSP_FORCEINLINE __m128 ssp_cmplt_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmplt_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmple_ss} */
+SSP_FORCEINLINE __m128 ssp_cmple_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmple_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmple_ps} */
+SSP_FORCEINLINE __m128 ssp_cmple_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmple_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpgt_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpgt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpgt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpgt_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpgt_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpgt_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpge_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpge_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpge_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpge_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpge_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpge_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpneq_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpneq_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpneq_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpneq_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpneq_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpneq_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpnlt_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpnlt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpnlt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpnlt_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpnlt_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpnlt_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpnle_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpnle_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpnle_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpnle_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpnle_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpnle_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpngt_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpngt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpngt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpngt_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpngt_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpngt_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpnge_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpnge_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpnge_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpnge_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpnge_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpnge_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpord_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpord_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpord_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpord_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpord_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpord_ps( a, b );
+}
+
+/** \SSE{Native,_mm_cmpunord_ss} */
+SSP_FORCEINLINE __m128 ssp_cmpunord_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpunord_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cmpunord_ps} */
+SSP_FORCEINLINE __m128 ssp_cmpunord_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_cmpunord_ps( a, b );
+}
+
+/** \SSE{Native,_mm_comieq_ss} */
+SSP_FORCEINLINE int ssp_comieq_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_comieq_ss( a, b );
+}
+
+/** \SSE{Native,_mm_comilt_ss} */
+SSP_FORCEINLINE int ssp_comilt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_comilt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_comile_ss} */
+SSP_FORCEINLINE int ssp_comile_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_comile_ss( a, b );
+}
+
+/** \SSE{Native,_mm_comigt_ss} */
+SSP_FORCEINLINE int ssp_comigt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_comigt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_comige_ss} */
+SSP_FORCEINLINE int ssp_comige_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_comige_ss( a, b );
+}
+
+/** \SSE{Native,_mm_comineq_ss} */
+SSP_FORCEINLINE int ssp_comineq_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_comineq_ss( a, b );
+}
+
+/** \SSE{Native,_mm_ucomieq_ss} */
+SSP_FORCEINLINE int ssp_ucomieq_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_ucomieq_ss( a, b );
+}
+
+/** \SSE{Native,_mm_ucomilt_ss} */
+SSP_FORCEINLINE int ssp_ucomilt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_ucomilt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_ucomile_ss} */
+SSP_FORCEINLINE int ssp_ucomile_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_ucomile_ss( a, b );
+}
+
+/** \SSE{Native,_mm_ucomigt_ss} */
+SSP_FORCEINLINE int ssp_ucomigt_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_ucomigt_ss( a, b );
+}
+
+/** \SSE{Native,_mm_ucomige_ss} */
+SSP_FORCEINLINE int ssp_ucomige_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_ucomige_ss( a, b );
+}
+
+/** \SSE{Native,_mm_ucomineq_ss} */
+SSP_FORCEINLINE int ssp_ucomineq_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_ucomineq_ss( a, b );
+}
+
+/** \SSE{Native,_mm_cvt_ss2si} */
+SSP_FORCEINLINE int ssp_cvt_ss2si_SSE( __m128 a )
+{
+    return _mm_cvt_ss2si( a );
+}
+
+/** \SSE{Native,_mm_cvt_ps2pi} */
+SSP_FORCEINLINE __m64 ssp_cvt_ps2pi_SSE( __m128 a )
+{
+    return _mm_cvt_ps2pi( a );
+}
+
+/** \SSE{Native,_mm_cvtt_ss2si} */
+SSP_FORCEINLINE int ssp_cvtt_ss2si_SSE( __m128 a )
+{
+    return _mm_cvtt_ss2si( a );
+}
+
+/** \SSE{Native,_mm_cvtt_ps2pi} */
+SSP_FORCEINLINE __m64 ssp_cvtt_ps2pi_SSE( __m128 a )
+{
+    return _mm_cvtt_ps2pi( a );
+}
+
+/** \SSE{Native,_mm_cvt_si2ss} */
+SSP_FORCEINLINE __m128 ssp_cvt_si2ss_SSE( __m128 a, int imm )
+{
+    return _mm_cvt_si2ss( a, imm );
+}
+
+/** \SSE{Native,_mm_cvt_pi2ps} */
+SSP_FORCEINLINE __m128 ssp_cvt_pi2ps_SSE( __m128 a, __m64 b )
+{
+    return _mm_cvt_pi2ps( a, b );
+}
+//TODO: Write Ref implementation. Will only work on VS9 & GCC
+//SSP_FORCEINLINE float ssp_cvtss_f32_SSE( __m128 a )
+//{
+//    return _mm_cvtss_f32( a );
+//}
+
+/* 
+ * Support for 64-bit extension intrinsics 
+ */
+#if defined(SYS64)
+//TODO: Write Ref implementation. Will only work on VS9 & GCC
+//SSP_FORCEINLINE __int64 ssp_cvtss_si64_SSE( __m128 a )
+//{
+//    return _mm_cvtss_si64( a );
+//}
+//SSP_FORCEINLINE __int64 ssp_cvttss_si64_SSE( __m128 a )
+//{
+//    return _mm_cvttss_si64( a );
+//}
+//SSP_FORCEINLINE __m128  ssp_cvtsi64_ss_SSE( __m128 a, __int64 b )
+//{
+//    return _mm_cvtsi64_ss( a, b );
+//}
+#endif
+
+/*
+ * FP, misc
+ */
+
+/**\SSE{Native,_mm_shuffle_ps} */
+SSP_FORCEINLINE  __m128 ssp_shuffle_ps_SSE( __m128 a, __m128 b, unsigned int imm8 )
+{
+    switch( imm8 & 0xFF )
+    {
+        CASE_256( _mm_shuffle_ps, a, b );
+    }
+}
+
+/**\SSE{Native,_mm_unpackhi_ps} */
+SSP_FORCEINLINE  __m128 ssp_unpackhi_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_unpackhi_ps( a, b );
+}
+
+/**\SSE{Native,_mm_unpacklo_ps} */
+SSP_FORCEINLINE __m128 ssp_unpacklo_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_unpacklo_ps( a, b );
+}
+
+/**\SSE{Native,_mm_loadh_pi} */
+SSP_FORCEINLINE __m128 ssp_loadh_pi_SSE( __m128 a, __m64 const* b )
+{
+    return _mm_loadh_pi( a, b );
+}
+
+/**\SSE{Native,_mm_movehl_ps} */
+SSP_FORCEINLINE __m128 ssp_movehl_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_movehl_ps( a, b );
+}
+
+/**\SSE{Native,_mm_movelh_ps} */
+SSP_FORCEINLINE __m128 ssp_movelh_ps_SSE( __m128 a, __m128 b )
+{
+    return _mm_movelh_ps( a, b );
+}
+
+/**\SSE{Native,_mm_storeh_pi} */
+SSP_FORCEINLINE void ssp_storeh_pi_SSE( __m64 *a, __m128 b )
+{
+    _mm_storeh_pi( a, b );
+}
+
+/**\SSE{Native,_mm_loadl_pi} */
+SSP_FORCEINLINE __m128 ssp_loadl_pi_SSE( __m128 a, __m64 const* b )
+{
+    return _mm_loadl_pi( a, b );
+}
+
+/**\SSE{Native,_mm_storel_pi} */
+SSP_FORCEINLINE void ssp_storel_pi_SSE( __m64 *a, __m128 b )
+{
+    _mm_storel_pi( a, b );
+}
+
+/**\SSE{Native,_mm_movemask_ps} */
+SSP_FORCEINLINE int ssp_movemask_ps_SSE( __m128 a )
+{
+    return _mm_movemask_ps( a );
+}
+
+
+/*
+ * Integer extensions
+ */
+
+/**\SSE{Native,_mm_pextrw} */
+SSP_FORCEINLINE int ssp_pextrw_SSE( __m64 a, int imm )
+{
+    switch( imm & 0x3 )
+    {
+        CASE_4( _m_pextrw, a );
+    }
+}
+
+/**\SSE{Native,_mm_pinsrw} */
+SSP_FORCEINLINE __m64 ssp_pinsrw_SSE( __m64 a, int b, int imm )
+{
+    switch( imm & 0x3 )
+    {
+        CASE_4( _m_pinsrw, a, b );
+    }
+}
+
+/**\SSE{Native,_mm_pmaxsw} */
+SSP_FORCEINLINE __m64 ssp_pmaxsw_SSE( __m64 a, __m64 b )
+{
+    return _m_pmaxsw( a, b );
+}
+
+/**\SSE{Native,_mm_pmaxub} */
+SSP_FORCEINLINE __m64 ssp_pmaxub_SSE( __m64 a, __m64 b )
+{
+    return _m_pmaxub( a, b );
+}
+
+/**\SSE{Native,_mm_pminsw} */
+SSP_FORCEINLINE __m64 ssp_pminsw_SSE( __m64 a, __m64 b )
+{
+    return _m_pminsw( a, b );
+}
+
+/**\SSE{Native,_mm_pminub} */
+SSP_FORCEINLINE __m64 ssp_pminub_SSE( __m64 a, __m64 b )
+{
+    return _m_pminub( a, b );
+}
+
+/**\SSE{Native,_mm_pmovmskb} */
+SSP_FORCEINLINE int ssp_pmovmskb_SSE( __m64 a )
+{
+    return _m_pmovmskb( a );
+}
+
+/**\SSE{Native,_mm_pmulhuw} */
+SSP_FORCEINLINE __m64 ssp_pmulhuw_SSE( __m64 a, __m64 b )
+{
+    return _m_pmulhuw( a, b );
+}
+
+/**\SSE{Native,_mm_pshufw} */
+SSP_FORCEINLINE __m64 ssp_pshufw_SSE( __m64 a, int imm )
+{
+    switch( imm & 0xFF )
+    {
+        CASE_256( _m_pshufw, a );
+    }
+}
+
+/**\SSE{Native,_mm_maskmovq} */
+SSP_FORCEINLINE void ssp_maskmovq_SSE( __m64 a, __m64 b, char *c )
+{
+    _m_maskmovq( a, a, c );
+}
+
+/**\SSE{Native,_mm_pavgb} */
+SSP_FORCEINLINE __m64 ssp_pavgb_SSE( __m64 a, __m64 b )
+{
+    return _m_pavgb( a, b );
+}
+
+/**\SSE{Native,_mm_pavgw} */
+SSP_FORCEINLINE __m64 ssp_pavgw_SSE( __m64 a, __m64 b )
+{
+    return _m_pavgw( a, b );
+}
+
+/**\SSE{Native,_mm_psadbw} */
+SSP_FORCEINLINE __m64 ssp_psadbw_SSE( __m64 a, __m64 b )
+{
+    return _m_psadbw( a, b );
+}
+
+/*
+ * memory & initialization
+ */
+
+/**\SSE{Native,_mm_set_ss} */
+SSP_FORCEINLINE __m128 ssp_set_ss_SSE( float a )
+{
+    return _mm_set_ss( a );
+}
+
+/**\SSE{Native,_mm_set_ps1} */
+SSP_FORCEINLINE __m128 ssp_set_ps1_SSE( float a )
+{
+    return _mm_set_ps1( a );
+}
+
+/**\SSE{Native,_mm_set_ps_SSE} */
+SSP_FORCEINLINE __m128 _mm_set_ps_SSE( float a, float b, float c, float d )
+{
+    return _mm_set_ps( a, b, c, d );
+}
+
+/**\SSE{Native,_mm_setr_ps} */
+SSP_FORCEINLINE __m128 ssp_setr_ps_SSE( float a, float b, float c, float d )
+{
+    return _mm_setr_ps( a, b, c, d );
+}
+
+/**\SSE{Native,_mm_setzero_ps} */
+SSP_FORCEINLINE __m128 ssp_setzero_ps_SSE( void )
+{
+    return _mm_setzero_ps( );
+}
+
+/**\SSE{Native,_mm_load_ss} */
+SSP_FORCEINLINE __m128 ssp_load_ss_SSE( float const*a )
+{
+    return _mm_load_ss( a );
+}
+
+/**\SSE{Native,_mm_load_ps1} */
+SSP_FORCEINLINE __m128 ssp_load_ps1_SSE( float const*a )
+{
+    return _mm_load_ps1( a );
+}
+
+/**\SSE{Native,_mm_load_ps} */
+SSP_FORCEINLINE __m128 ssp_load_ps_SSE( float const*a )
+{
+    return _mm_load_ps( a );
+}
+
+/**\SSE{Native,_mm_loadr_ps} */
+SSP_FORCEINLINE __m128 ssp_loadr_ps_SSE( float const*a )
+{
+    return _mm_loadr_ps( a );
+}
+
+/**\SSE{Native,_mm_loadu_ps} */
+SSP_FORCEINLINE __m128 ssp_loadu_ps_SSE( float const*a )
+{
+    return _mm_loadu_ps( a );
+}
+
+/**\SSE{Native,_mm_store_ss} */
+SSP_FORCEINLINE void ssp_store_ss_SSE( float *v, __m128 a )
+{
+    _mm_store_ss( v, a );
+}
+
+/**\SSE{Native,_mm_store_ps1} */
+SSP_FORCEINLINE void ssp_store_ps1_SSE( float *v, __m128 a )
+{
+    _mm_store_ps1( v, a );
+}
+
+/**\SSE{Native,_mm_store_ps} */
+SSP_FORCEINLINE void ssp_store_ps_SSE( float *v, __m128 a )
+{
+    _mm_store_ps( v, a );
+}
+
+/**\SSE{Native,_mm_storer_ps} */
+SSP_FORCEINLINE void ssp_storer_ps_SSE( float *v, __m128 a )
+{
+    _mm_storer_ps( v, a );
+}
+
+/**\SSE{Native,_mm_storeu_ps} */
+SSP_FORCEINLINE void ssp_storeu_ps_SSE( float *v, __m128 a )
+{
+    _mm_storeu_ps( v, a );
+}
+
+/**\SSE{Native,_mm_prefetch} */
+SSP_FORCEINLINE void ssp_prefetch_SSE( char *a, int sel )
+{
+    switch( sel & 0x3 )
+    {
+    case 1:  _mm_prefetch( a, _MM_HINT_T0 ); break;
+    case 2:  _mm_prefetch( a, _MM_HINT_T1 ); break;
+    case 3:  _mm_prefetch( a, _MM_HINT_T2 ); break;
+    default: _mm_prefetch( a, _MM_HINT_NTA );
+    } 
+}
+
+/**\SSE{Native,_mm_stream_pi} */
+SSP_FORCEINLINE void ssp_stream_pi_SSE( __m64 *a,  __m64 b )
+{
+    _mm_stream_pi( a,  b );
+}
+
+/**\SSE{Native,_mm_stream_ps} */
+SSP_FORCEINLINE void ssp_stream_ps_SSE( float *a, __m128 b )
+{
+    _mm_stream_ps( a, b );
+}
+
+/**\SSE{Native,_mm_move_ss} */
+SSP_FORCEINLINE __m128 ssp_move_ss_SSE( __m128 a, __m128 b )
+{
+    return _mm_move_ss( a, b );
+}
+
+/**\SSE{Native,_mm_sfence} */
+SSP_FORCEINLINE void ssp_sfence_SSE( void )
+{
+    _mm_sfence( );
+}
+
+/**\SSE{Native,_mm_getcsr} */
+ unsigned int ssp_getcsr_SSE( void )
+{
+    return _mm_getcsr( );
+}
+
+/**\SSE{Native,_mm_setcsr} */
+SSP_FORCEINLINE void ssp_setcsr_SSE( unsigned int a )
+{
+    _mm_setcsr( a );
+}
+
+
+/**\SSE{Native,_mm_cvtpi16_ps} */
+SSP_FORCEINLINE __m128 ssp_cvtpi16_ps_SSE( __m64 a )
+{
+    return _mm_cvtpi16_ps( a );
+}
+
+/**\SSE{Native,_mm_cvtpu16_ps} */
+SSP_FORCEINLINE __m128 ssp_cvtpu16_ps_SSE( __m64 a )
+{
+    return _mm_cvtpu16_ps( a );
+}
+
+/**\SSE{Native,_mm_cvtps_pi16} */
+SSP_FORCEINLINE __m64 ssp_cvtps_pi16_SSE( __m128 a )
+{
+    return _mm_cvtps_pi16( a );
+}
+
+/**\SSE{Native,_mm_cvtpi8_ps} */
+SSP_FORCEINLINE __m128 ssp_cvtpi8_ps_SSE( __m64 a )
+{
+    return _mm_cvtpi8_ps( a );
+}
+
+/**\SSE{Native,_mm_cvtpu8_ps} */
+SSP_FORCEINLINE __m128 ssp_cvtpu8_ps_SSE( __m64 a )
+{
+    return _mm_cvtpu8_ps( a );
+}
+
+/**\SSE{Native,_mm_cvtps_pi8} */
+SSP_FORCEINLINE __m64 ssp_cvtps_pi8_SSE( __m128 a )
+{
+    return _mm_cvtps_pi8( a );
+}
+
+/**\SSE{Native,_mm_cvtpi32x2_ps} */
+SSP_FORCEINLINE __m128 ssp_cvtpi32x2_ps_SSE( __m64 a, __m64 b )
+{
+    return _mm_cvtpi32x2_ps( a, b );
+}
+
+//@}
+
+/**@name Alternate Name Definitions */
+
+/** Alternate intrinsic names definition */
+#define ssp_cvtss_si32_SSE      ssp_cvt_ss2si_SSE       
+#define ssp_cvtps_pi32_SSE      ssp_cvt_ps2pi_SSE       
+#define ssp_cvttss_si32_SSE     ssp_cvtt_ss2si_SSE      
+#define ssp_cvttps_pi32_SSE     ssp_cvtt_ps2pi_SSE      
+#define ssp_cvtsi32_ss_SSE      ssp_cvt_si2ss_SSE       
+#define ssp_cvtpi32_ps_SSE      ssp_cvt_pi2ps_SSE       
+#define ssp_extract_pi16_SSE    ssp_pextrw_SSE          
+#define ssp_insert_pi16_SSE     ssp_pinsrw_SSE          
+#define ssp_max_pi16_SSE        ssp_pmaxsw_SSE          
+#define ssp_max_pu8_SSE         ssp_pmaxub_SSE          
+#define ssp_min_pi16_SSE        ssp_pminsw_SSE          
+#define ssp_min_pu8_SSE         ssp_pminub_SSE          
+#define ssp_movemask_pi8_SSE    ssp_pmovmskb_SSE        
+#define ssp_mulhi_pu16_SSE      ssp_pmulhuw_SSE         
+#define ssp_shuffle_pi16_SSE    ssp_pshufw_SSE          
+#define ssp_maskmove_si64_SSE   ssp_maskmovq_SSE        
+#define ssp_avg_pu8_SSE         ssp_pavgb_SSE           
+#define ssp_avg_pu16_SSE        ssp_pavgw_SSE           
+#define ssp_sad_pu8_SSE         ssp_psadbw_SSE          
+#define ssp_set1_ps_SSE         ssp_set_ps1_SSE         
+#define ssp_load1_ps_SSE        ssp_load_ps1_SSE        
+#define ssp_store1_ps_SSE       ssp_store_ps1_SSE 
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_NATIVE_SSE_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1160 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE2_H__
+#define __SSEPLUS_NATIVE_SSE2_H__
+
+#include "../SSEPlus_base.h"
+#include <emmintrin.h>  // SSE2
+
+/** @addtogroup native_SSE2  
+ *  @{ 
+ *  @name Native SSE2 Operations
+ */
+
+
+/** \SSE2{Native,_mm_add_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_add_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_add_epi16( a, b );
+}
+/** \SSE2{Native,_mm_add_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_add_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_add_epi32( a, b );
+}
+/** \SSE2{Native,_mm_add_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_add_epi64_SSE2( __m128i a, __m128i b )
+{
+    return _mm_add_epi64( a, b );
+}
+/** \SSE2{Native,_mm_add_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_add_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_add_epi8( a, b );
+}
+/** \SSE2{Native,_mm_add_pd} */ 
+SSP_FORCEINLINE __m128d ssp_add_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_add_pd( a, b );
+}
+/** \SSE2{Native,_mm_add_sd} */ 
+SSP_FORCEINLINE __m128d ssp_add_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_add_sd( a, b );
+}
+/** \SSE2{Native,_mm_add_si64} */ 
+SSP_FORCEINLINE __m64 ssp_add_si64_SSE2( __m64 a, __m64 b)
+{
+    return _mm_add_si64( a, b );
+}
+/** \SSE2{Native,_mm_adds_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_adds_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_adds_epi16( a, b );
+}
+/** \SSE2{Native,_mm_adds_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_adds_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_adds_epi8( a, b );
+}
+/** \SSE2{Native,_mm_adds_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_adds_epu16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_adds_epu16( a, b );
+}
+/** \SSE2{Native,_mm_adds_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_adds_epu8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_adds_epu8( a, b );
+}
+/** \SSE2{Native,_mm_and_pd} */ 
+SSP_FORCEINLINE __m128d ssp_and_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_and_pd( a, b );
+}
+/** \SSE2{Native,_mm_and_si128} */ 
+SSP_FORCEINLINE __m128i ssp_and_si128_SSE2( __m128i a, __m128i b )
+{
+    return _mm_and_si128( a, b );
+}
+/** \SSE2{Native,_mm_andnot_pd} */ 
+SSP_FORCEINLINE __m128d ssp_andnot_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_andnot_pd( a, b );
+}
+/** \SSE2{Native,_mm_andnot_si128} */ 
+SSP_FORCEINLINE __m128i ssp_andnot_si128_SSE2( __m128i a, __m128i b )
+{
+    return _mm_andnot_si128( a, b );
+}
+/** \SSE2{Native,_mm_avg_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_avg_epu16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_avg_epu16( a, b );
+}
+/** \SSE2{Native,_mm_avg_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_avg_epu8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_avg_epu8( a, b );
+}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_castpd_ps}
+//SSP_FORCEINLINE __m128 ssp_castpd_ps_SSE2( __m128d a )
+//{
+//    return _mm_castpd_ps( a );
+//}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_castpd_si128}
+//SSP_FORCEINLINE __m128i ssp_castpd_si128_SSE2( __m128d a )
+//{
+//    return _mm_castpd_si128( a );
+//}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_castps_pd}  
+//SSP_FORCEINLINE __m128d ssp_castps_pd_SSE2( __m128 a )
+//{
+//    return _mm_castps_pd( a );
+//}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_castps_si128} 
+//SSP_FORCEINLINE __m128i ssp_castps_si128_SSE2( __m128 a )
+//{
+//    return _mm_castps_si128( a );
+//}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_castsi128_pd} 
+//SSP_FORCEINLINE __m128d ssp_castsi128_pd_SSE2( __m128i a )
+//{
+//    return _mm_castsi128_pd( a );
+//}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_castsi128_ps} 
+//SSP_FORCEINLINE __m128 ssp_castsi128_ps_SSE2( __m128i a )
+//{
+//    return _mm_castsi128_ps( a );
+//}
+
+/** \SSE2{Native,_mm_clflush} */ 
+SSP_FORCEINLINE void ssp_clflush_SSE2( void const *p )
+{
+    _mm_clflush( p );
+}
+/** \SSE2{Native,_mm_cmpeq_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_cmpeq_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmpeq_epi16( a, b );
+}
+/** \SSE2{Native,_mm_cmpeq_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cmpeq_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmpeq_epi32( a, b );
+}
+/** \SSE2{Native,_mm_cmpeq_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_cmpeq_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmpeq_epi8( a, b );
+}
+/** \SSE2{Native,_mm_cmpeq_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpeq_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpeq_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpeq_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpeq_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpeq_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpge_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpge_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpge_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpge_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpge_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpge_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpgt_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_cmpgt_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmpgt_epi16( a, b );
+}
+/** \SSE2{Native,_mm_cmpgt_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cmpgt_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmpgt_epi32( a, b );
+}
+/** \SSE2{Native,_mm_cmpgt_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_cmpgt_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmpgt_epi8( a, b );
+}
+/** \SSE2{Native,_mm_cmpgt_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpgt_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpgt_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpgt_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpgt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpgt_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmple_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmple_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmple_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmple_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmple_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmple_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmplt_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_cmplt_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmplt_epi16( a, b );
+}
+/** \SSE2{Native,_mm_cmplt_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cmplt_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmplt_epi32( a, b );
+}
+/** \SSE2{Native,_mm_cmplt_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_cmplt_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_cmplt_epi8( a, b );
+}
+/** \SSE2{Native,_mm_cmplt_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmplt_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmplt_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmplt_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmplt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmplt_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpneq_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpneq_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpneq_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpneq_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpneq_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpneq_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpnge_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpnge_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpnge_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpnge_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpnge_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpnge_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpngt_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpngt_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpngt_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpngt_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpngt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpngt_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpnle_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpnle_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpnle_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpnle_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpnle_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpnle_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpnlt_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpnlt_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpnlt_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpnlt_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpnlt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpnlt_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpord_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpord_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpord_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpord_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpord_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpord_sd( a, b );
+}
+/** \SSE2{Native,_mm_cmpunord_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpunord_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpunord_pd( a, b );
+}
+/** \SSE2{Native,_mm_cmpunord_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cmpunord_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_cmpunord_sd( a, b );
+}
+/** \SSE2{Native,_mm_comieq_sd} */ 
+SSP_FORCEINLINE int ssp_comieq_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_comieq_sd( a, b );
+}
+/** \SSE2{Native,_mm_comige_sd} */ 
+SSP_FORCEINLINE int ssp_comige_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_comige_sd( a, b );
+}
+/** \SSE2{Native,_mm_comigt_sd} */ 
+SSP_FORCEINLINE int ssp_comigt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_comigt_sd( a, b );
+}
+/** \SSE2{Native,_mm_comile_sd} */ 
+SSP_FORCEINLINE int ssp_comile_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_comile_sd( a, b );
+}
+/** \SSE2{Native,_mm_comilt_sd} */ 
+SSP_FORCEINLINE int ssp_comilt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_comilt_sd( a, b );
+}
+/** \SSE2{Native,_mm_comineq_sd} */ 
+SSP_FORCEINLINE int ssp_comineq_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_comineq_sd( a, b );
+}
+/** \SSE2{Native,_mm_cvtepi32_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cvtepi32_pd_SSE2( __m128i a )
+{
+    return _mm_cvtepi32_pd( a );
+}
+/** \SSE2{Native,_mm_cvtepi32_ps} */ 
+SSP_FORCEINLINE __m128 ssp_cvtepi32_ps_SSE2( __m128i a )
+{
+    return _mm_cvtepi32_ps( a );
+}
+/** \SSE2{Native,_mm_cvtpd_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvtpd_epi32_SSE2( __m128d a )
+{
+    return _mm_cvtpd_epi32( a );
+}
+/** \SSE2{Native,_mm_cvtpd_pi32} */ 
+SSP_FORCEINLINE __m64 ssp_cvtpd_pi32_SSE2( __m128d a )
+{
+    return _mm_cvtpd_pi32( a );
+}
+/** \SSE2{Native,_mm_cvtpd_ps} */ 
+SSP_FORCEINLINE __m128 ssp_cvtpd_ps_SSE2( __m128d a )
+{
+    return _mm_cvtpd_ps( a );
+}
+/** \SSE2{Native,_mm_cvtpi32_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cvtpi32_pd_SSE2( __m64 a )
+{
+    return _mm_cvtpi32_pd( a );
+}
+/** \SSE2{Native,_mm_cvtps_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvtps_epi32_SSE2( __m128 a )
+{
+    return _mm_cvtps_epi32( a );
+}
+/** \SSE2{Native,_mm_cvtps_pd} */ 
+SSP_FORCEINLINE __m128d ssp_cvtps_pd_SSE2( __m128 a )
+{
+    return _mm_cvtps_pd( a );
+}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+//SSE2{Native,_mm_cvtsd_f64} */ 
+//SSP_FORCEINLINE double ssp_cvtsd_f64_SSE2( __m128d a )
+//{
+//    return _mm_cvtsd_f64( a );
+//}
+
+/** \SSE2{Native,_mm_cvtsd_si32} */ 
+SSP_FORCEINLINE int ssp_cvtsd_si32_SSE2( __m128d a )
+{
+    return _mm_cvtsd_si32( a );
+}
+#ifdef SYS64
+//// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+///** \SSE2{Native,_mm_cvtsd_si64} */ 
+//SSP_FORCEINLINE __int64 ssp_cvtsd_si64_SSE2( __m128d a )
+//{
+//    return _mm_cvtsd_si64( a );
+//}
+#endif
+/** \SSE2{Native,_mm_cvtsd_ss} */ 
+SSP_FORCEINLINE __m128 ssp_cvtsd_ss_SSE2( __m128 a, __m128d b )
+{
+    return _mm_cvtsd_ss( a, b );
+}
+/** \SSE2{Native,_mm_cvtsi128_si32} */ 
+SSP_FORCEINLINE int ssp_cvtsi128_si32_SSE2( __m128i a )
+{
+    return _mm_cvtsi128_si32( a );
+}
+#ifdef SYS64
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+///** \SSE2{Native,_mm_cvtsi128_si64} */ 
+//SSP_FORCEINLINE __int64 ssp_cvtsi128_si64_SSE2( __m128i a )
+//{
+//    return _mm_cvtsi128_si64( a );
+//}
+#endif
+/** \SSE2{Native,_mm_cvtsi32_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cvtsi32_sd_SSE2( __m128d a, int b )
+{
+    return _mm_cvtsi32_sd( a, b );
+}
+/** \SSE2{Native,_mm_cvtsi32_si128} */ 
+SSP_FORCEINLINE __m128i ssp_cvtsi32_si128_SSE2( int a )
+{
+    return _mm_cvtsi32_si128( a );
+}
+#ifdef SYS64
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+///** \SSE2{Native,_mm_cvtsi64_sd} */ 
+//SSP_FORCEINLINE __m128d ssp_cvtsi64_sd_SSE2( __m128d a, __int64 b )
+//{
+//    return _mm_cvtsi64_sd( a, b );
+//}
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+///** \SSE2{Native,_mm_cvtsi64_si128} */ 
+//SSP_FORCEINLINE __m128i ssp_cvtsi64_si128_SSE2( __int64 a )
+//{
+//    return _mm_cvtsi64_si128( a );
+//}
+#endif
+/** \SSE2{Native,_mm_cvtss_sd} */ 
+SSP_FORCEINLINE __m128d ssp_cvtss_sd_SSE2( __m128d a, __m128 b )
+{
+    return _mm_cvtss_sd( a, b );
+}
+/** \SSE2{Native,_mm_cvttpd_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvttpd_epi32_SSE2( __m128d a )
+{
+    return _mm_cvttpd_epi32( a );
+}
+/** \SSE2{Native,_mm_cvttpd_pi32} */ 
+SSP_FORCEINLINE __m64 ssp_cvttpd_pi32_SSE2( __m128d a )
+{
+    return _mm_cvttpd_pi32( a );
+}
+/** \SSE2{Native,_mm_cvttps_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvttps_epi32_SSE2( __m128 a )
+{
+    return _mm_cvttps_epi32( a );
+}
+/** \SSE2{Native,_mm_cvttsd_si32} */ 
+SSP_FORCEINLINE int ssp_cvttsd_si32_SSE2( __m128d a )
+{
+    return _mm_cvttsd_si32( a );
+}
+#ifdef SYS64
+// *** Microsoft Specific Intrinsic TODO: Write Reference for VS8
+///** \SSE2{Native,_mm_cvttsd_si64} */ 
+//SSP_FORCEINLINE __int64 ssp_cvttsd_si64_SSE2( __m128d a )
+//{
+//    return _mm_cvttsd_si64( a );
+//}
+#endif
+/** \SSE2{Native,_mm_div_pd} */ 
+SSP_FORCEINLINE __m128d ssp_div_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_div_pd( a, b );
+}
+/** \SSE2{Native,_mm_div_sd} */ 
+SSP_FORCEINLINE __m128d ssp_div_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_div_sd( a, b );
+}
+/** \SSE2{Native,_mm_extract_epi16} */ 
+SSP_FORCEINLINE int ssp_extract_epi16_SSE2( __m128i a, int imm )
+{
+    switch( imm & 0x7 )
+    {
+        CASE_8( _mm_extract_epi16, a );
+    }
+}
+/** \SSE2{Native,_mm_insert_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_insert_epi16_SSE2( __m128i a, int b, int imm )
+{
+    switch( imm & 0x7 )
+    {
+        CASE_8( _mm_insert_epi16, a, b );
+    }
+}
+/** \SSE2{Native,_mm_lfence} */ 
+SSP_FORCEINLINE void ssp_lfence_SSE2( void )
+{
+    _mm_lfence();
+}
+/** \SSE2{Native,_mm_load_pd} */ 
+SSP_FORCEINLINE __m128d ssp_load_pd_SSE2( double const*dp )
+{
+    return _mm_load_pd( dp );
+}
+/** \SSE2{Native,_mm_load_sd} */ 
+SSP_FORCEINLINE __m128d ssp_load_sd_SSE2( double const*dp )
+{
+    return _mm_load_sd( dp );
+}
+/** \SSE2{Native,_mm_load_si128} */ 
+SSP_FORCEINLINE __m128i ssp_load_si128_SSE2( __m128i const*p )
+{
+    return _mm_load_si128( p );
+}
+/** \SSE2{Native,_mm_load1_pd} */ 
+SSP_FORCEINLINE __m128d ssp_load1_pd_SSE2( double const*dp )
+{
+    return _mm_load1_pd( dp );
+}
+/** \SSE2{Native,_mm_loadh_pd} */ 
+SSP_FORCEINLINE __m128d ssp_loadh_pd_SSE2( __m128d a, double const*dp )
+{
+    return _mm_loadh_pd( a, dp );
+}
+/** \SSE2{Native,_mm_loadl_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_loadl_epi64_SSE2( __m128i const*p)
+{
+    return _mm_loadl_epi64( p );
+}
+/** \SSE2{Native,_mm_loadl_pd} */ 
+SSP_FORCEINLINE __m128d ssp_loadl_pd_SSE2( __m128d a, double const*dp )
+{
+    return _mm_loadl_pd( a, dp );
+}
+/** \SSE2{Native,_mm_loadr_pd} */ 
+SSP_FORCEINLINE __m128d ssp_loadr_pd_SSE2( double const*dp )
+{
+    return _mm_loadr_pd( dp );
+}
+/** \SSE2{Native,_mm_loadu_pd} */ 
+SSP_FORCEINLINE __m128d ssp_loadu_pd_SSE2( double const*dp )
+{
+    return _mm_loadu_pd( dp );
+}
+/** \SSE2{Native,_mm_loadu_si128} */ 
+SSP_FORCEINLINE __m128i ssp_loadu_si128_SSE2( __m128i const*p )
+{
+    return _mm_loadu_si128( p );
+}
+/** \SSE2{Native,_mm_madd_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_madd_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_madd_epi16( a, b );
+}
+/** \SSE2{Native,_mm_maskmoveu_si128} */ 
+SSP_FORCEINLINE void ssp_maskmoveu_si128_SSE2( __m128i a, __m128i b, char *c )
+{
+    _mm_maskmoveu_si128( a, b, c );
+}
+/** \SSE2{Native,_mm_max_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_max_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_max_epi16( a, b );
+}
+/** \SSE2{Native,_mm_max_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_max_epu8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_max_epu8( a, b );
+}
+/** \SSE2{Native,_mm_max_pd} */ 
+SSP_FORCEINLINE __m128d ssp_max_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_max_pd( a, b );
+}
+/** \SSE2{Native,_mm_max_sd} */ 
+SSP_FORCEINLINE __m128d ssp_max_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_max_sd( a, b );
+}
+/** \SSE2{Native,_mm_mfence} */ 
+SSP_FORCEINLINE void ssp_mfence_SSE2( void )
+{
+    _mm_mfence( );
+}
+/** \SSE2{Native,_mm_min_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_min_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_min_epi16( a, b );
+}
+/** \SSE2{Native,_mm_min_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_min_epu8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_min_epu8( a, b );
+}
+/** \SSE2{Native,_mm_min_pd} */ 
+SSP_FORCEINLINE __m128d ssp_min_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_min_pd( a, b );
+}
+/** \SSE2{Native,_mm_min_sd} */ 
+SSP_FORCEINLINE __m128d ssp_min_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_min_sd( a, b );
+}
+/** \SSE2{Native,_mm_move_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_move_epi64_SSE2( __m128i a )
+{
+    return _mm_move_epi64( a );
+}
+/** \SSE2{Native,_mm_move_sd} */ 
+SSP_FORCEINLINE __m128d ssp_move_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_move_sd( a, b );
+}
+/** \SSE2{Native,_mm_movemask_epi8} */ 
+SSP_FORCEINLINE int ssp_movemask_epi8_SSE2( __m128i a )
+{
+    return _mm_movemask_epi8( a );
+}
+/** \SSE2{Native,_mm_movemask_pd} */ 
+SSP_FORCEINLINE int ssp_movemask_pd_SSE2( __m128d a )
+{
+    return _mm_movemask_pd( a );
+}
+/** \SSE2{Native,_mm_movepi64_pi64} */ 
+SSP_FORCEINLINE __m64 ssp_movepi64_pi64_SSE2( __m128i a )
+{
+    return _mm_movepi64_pi64( a );
+}
+/** \SSE2{Native,_mm_movpi64_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_movpi64_epi64_SSE2( __m64 a )
+{
+    return _mm_movpi64_epi64( a );
+}
+/** \SSE2{Native,_mm_mul_epu32} */ 
+SSP_FORCEINLINE __m128i ssp_mul_epu32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_mul_epu32( a, b );
+}
+/** \SSE2{Native,_mm_mul_pd} */ 
+SSP_FORCEINLINE __m128d ssp_mul_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_mul_pd( a, b );
+}
+/** \SSE2{Native,_mm_mul_sd} */ 
+SSP_FORCEINLINE __m128d ssp_mul_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_mul_sd( a, b );
+}
+/** \SSE2{Native,_mm_mul_su32} */ 
+SSP_FORCEINLINE __m64 ssp_mul_su32_SSE2( __m64 a, __m64 b)
+{
+    return _mm_mul_su32( a, b );
+}
+/** \SSE2{Native,_mm_mulhi_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_mulhi_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_mulhi_epi16( a, b );
+}
+/** \SSE2{Native,_mm_mulhi_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_mulhi_epu16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_mulhi_epu16( a, b );
+}
+/** \SSE2{Native,_mm_mullo_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_mullo_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_mullo_epi16( a, b );
+}
+/** \SSE2{Native,_mm_or_pd} */ 
+SSP_FORCEINLINE __m128d ssp_or_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_or_pd( a, b );
+}
+/** \SSE2{Native,_mm_or_si128} */ 
+SSP_FORCEINLINE __m128i ssp_or_si128_SSE2( __m128i a, __m128i b )
+{
+    return _mm_or_si128( a, b );
+}
+/** \SSE2{Native,_mm_packs_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_packs_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_packs_epi16( a, b );
+}
+/** \SSE2{Native,_mm_packs_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_packs_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_packs_epi32( a, b );
+}
+/** \SSE2{Native,_mm_packus_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_packus_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_packus_epi16( a, b );
+}
+/** \SSE2{Native,_mm_pause} */ 
+SSP_FORCEINLINE void ssp_pause_SSE2( void )
+{
+    _mm_pause();
+}
+/** \SSE2{Native,_mm_sad_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_sad_epu8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_sad_epu8( a, b );
+}
+/** \SSE2{Native,_mm_set_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_set_epi16_SSE2( short w7, short w6, short w5, short w4, short w3, short w2, short w1, short w0 )
+{
+    return _mm_set_epi16( w7, w6, w5, w4, w3, w2, w1, w0 );
+}
+/** \SSE2{Native,_mm_set_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_set_epi32_SSE2( int i3, int i2, int i1, int i0 )
+{
+    return _mm_set_epi32( i3, i2, i1, i0 );
+}
+/** \SSE2{Native,_mm_set_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_set_epi64_SSE2( __m64 a1, __m64 a0 )
+{
+    return _mm_set_epi64( a1, a0 );
+}
+/** \SSE2{Native,_mm_set_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_set_epi8_SSE2( char b15, char b14, char b13, char b12, char b11, char b10, char b9, char b8, char b7, char b6, char b5, char b4, char b3, char b2, char b1, char b0 )
+{
+    return _mm_set_epi8( b15, b14, b13, b12, b11, b10, b9, b8, b7, b6, b5, b4, b3, b2, b1, b0 );
+}
+/** \SSE2{Native,_mm_set_pd} */ 
+SSP_FORCEINLINE __m128d ssp_set_pd_SSE2( double a1, double a0)
+{
+    return _mm_set_pd( a1, a0 );
+}
+/** \SSE2{Native,_mm_set_sd} */ 
+SSP_FORCEINLINE __m128d ssp_set_sd_SSE2( double w)
+{
+    return _mm_set_sd( w );
+}
+/** \SSE2{Native,_mm_set1_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_set1_epi16_SSE2( short w)
+{
+    return _mm_set1_epi16( w );
+}
+/** \SSE2{Native,_mm_set1_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_set1_epi32_SSE2( int i )
+{
+    return _mm_set1_epi32( i );
+}
+/** \SSE2{Native,_mm_set1_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_set1_epi64_SSE2( __m64 a )
+{
+    return _mm_set1_epi64( a );
+}
+/** \SSE2{Native,_mm_set1_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_set1_epi8_SSE2( char b )
+{
+    return _mm_set1_epi8( b );
+}
+/** \SSE2{Native,_mm_set1_pd} */ 
+SSP_FORCEINLINE __m128d ssp_set1_pd_SSE2( double a )
+{
+    return _mm_set1_pd( a );
+}
+// Composite intrinsic not supported in GCC
+#ifdef SSP_MSVC
+/** \SSE2{Native,_mm_setl_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_setl_epi64_SSE2( __m128i a )
+{
+    return _mm_setl_epi64( a );
+}
+#endif
+/** \SSE2{Native,_mm_setr_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_setr_epi16_SSE2( short w0, short w1, short w2, short w3, short w4, short w5, short w6, short w7 )
+{
+    return _mm_setr_epi16( w0, w1, w2, w3, w4, w5, w6, w7 );
+}
+/** \SSE2{Native,_mm_setr_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_setr_epi32_SSE2( int i0, int i1, int i2, int i3)
+{
+    return _mm_setr_epi32( i0, i1, i2, i3);
+}
+/** \SSE2{Native,_mm_setr_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_setr_epi64_SSE2( __m64 a0, __m64 a1)
+{
+    return _mm_setr_epi64( a0, a1);
+}
+/** \SSE2{Native,_mm_setr_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_setr_epi8_SSE2( char b15, char b14, char b13, char b12, char b11, char b10, char b9, char b8, char b7, char b6, char b5, char b4, char b3, char b2, char b1, char b0 )
+{
+    return _mm_setr_epi8( b15, b14, b13, b12, b11, b10, b9, b8, b7, b6, b5, b4, b3, b2, b1, b0 );
+}
+/** \SSE2{Native,_mm_setr_pd} */ 
+SSP_FORCEINLINE __m128d ssp_setr_pd_SSE2( double a0, double a1 )
+{
+    return _mm_setr_pd( a0, a1);
+}
+/** \SSE2{Native,_mm_setzero_pd} */ 
+SSP_FORCEINLINE __m128d ssp_setzero_pd_SSE2( void )
+{
+    return _mm_setzero_pd( );
+}
+/** \SSE2{Native,_mm_setzero_si128} */ 
+SSP_FORCEINLINE __m128i ssp_setzero_si128_SSE2( void )
+{
+    return _mm_setzero_si128( );
+}
+/** \SSE2{Native,_mm_shuffle_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_shuffle_epi32_SSE2( __m128i a, int imm )
+{
+    switch( imm & 0xFF )
+    {
+        CASE_256( _mm_shuffle_epi32, a );
+    }
+}
+/** \SSE2{Native,_mm_shuffle_pd} */ 
+SSP_FORCEINLINE __m128d ssp_shuffle_pd_SSE2( __m128d a, __m128d b, int imm )
+{
+    switch( imm & 0xFF )
+    {
+        CASE_4( _mm_shuffle_pd, a, b );
+    }
+}
+/** \SSE2{Native,_mm_shufflehi_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_shufflehi_epi16_SSE2( __m128i a, int imm )
+{
+    switch( imm & 0xFF )
+    {
+        CASE_256( _mm_shufflehi_epi16, a );
+    }
+}
+/** \SSE2{Native,_mm_shufflelo_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_shufflelo_epi16_SSE2( __m128i a, int imm )
+{
+    switch( imm & 0xFF )
+    {
+        CASE_256( _mm_shufflelo_epi16, a );
+    }
+}
+/** \SSE2{Native,_mm_sll_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_sll_epi16_SSE2( __m128i a, __m128i count )
+{
+    return _mm_sll_epi16( a, count );
+}
+/** \SSE2{Native,_mm_sll_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_sll_epi32_SSE2( __m128i a, __m128i count )
+{
+    return _mm_sll_epi32( a, count );
+}
+/** \SSE2{Native,_mm_sll_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_sll_epi64_SSE2( __m128i a, __m128i count )
+{
+    return _mm_sll_epi64( a, count );
+}
+/** \SSE2{Native,_mm_slli_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_slli_epi16_SSE2( __m128i a, int count )
+{
+    return _mm_slli_epi16( a, count );
+}
+/** \SSE2{Native,_mm_slli_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_slli_epi32_SSE2( __m128i a, int count )
+{
+    return _mm_slli_epi32( a, count );
+}
+/** \SSE2{Native,_mm_slli_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_slli_epi64_SSE2( __m128i a, int count )
+{
+    return _mm_slli_epi64( a, count );
+}
+/** \SSE2{Native,_mm_slli_si128} */ 
+SSP_FORCEINLINE __m128i ssp_slli_si128_SSE2( __m128i a, int imm )
+{
+    switch( imm & 0x7F )
+    {
+        CASE_128( _mm_slli_si128, a );
+    }
+}
+/** \SSE2{Native,_mm_sqrt_pd} */ 
+SSP_FORCEINLINE __m128d ssp_sqrt_pd_SSE2( __m128d a )
+{
+    return _mm_sqrt_pd( a );
+}
+/** \SSE2{Native,_mm_sqrt_sd} */ 
+SSP_FORCEINLINE __m128d ssp_sqrt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_sqrt_sd( a, b );
+}
+/** \SSE2{Native,_mm_sra_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_sra_epi16_SSE2( __m128i a, __m128i count )
+{
+    return _mm_sra_epi16( a, count );
+}
+/** \SSE2{Native,_mm_sra_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_sra_epi32_SSE2( __m128i a, __m128i count )
+{
+    return _mm_sra_epi32( a, count );
+}
+/** \SSE2{Native,_mm_srai_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_srai_epi16_SSE2( __m128i a, int count )
+{
+    return _mm_srai_epi16( a, count );
+}
+/** \SSE2{Native,_mm_srai_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_srai_epi32_SSE2( __m128i a, int count )
+{
+    return _mm_srai_epi32( a, count );
+}
+/** \SSE2{Native,_mm_srl_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_srl_epi16_SSE2( __m128i a, __m128i count )
+{
+    return _mm_srl_epi16( a, count );
+}
+/** \SSE2{Native,_mm_srl_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_srl_epi32_SSE2( __m128i a, __m128i count )
+{
+    return _mm_srl_epi32( a, count );
+}
+/** \SSE2{Native,_mm_srl_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_srl_epi64_SSE2( __m128i a, __m128i count )
+{
+    return _mm_srl_epi64( a, count );
+}
+/** \SSE2{Native,_mm_srli_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_srli_epi16_SSE2( __m128i a, int count )
+{
+    return _mm_srli_epi16( a, count );
+}
+/** \SSE2{Native,_mm_srli_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_srli_epi32_SSE2( __m128i a, int count )
+{
+    return _mm_srli_epi32( a, count );
+}
+/** \SSE2{Native,_mm_srli_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_srli_epi64_SSE2( __m128i a, int count )
+{
+    return _mm_srli_epi64( a, count );
+}
+/** \SSE2{Native,_mm_srli_si128} */ 
+SSP_FORCEINLINE __m128i ssp_srli_si128_SSE2( __m128i a, int imm )
+{
+    switch( imm & 0x7F )
+    {
+        CASE_128( _mm_srli_si128, a );
+    }
+}
+/** \SSE2{Native,_mm_store_pd} */ 
+SSP_FORCEINLINE void ssp_store_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_store_pd( dp, a );
+}
+/** \SSE2{Native,_mm_store_sd} */ 
+SSP_FORCEINLINE void ssp_store_sd_SSE2( double *dp, __m128d a )
+{
+    _mm_store_sd( dp, a );
+}
+/** \SSE2{Native,_mm_store_si128} */ 
+SSP_FORCEINLINE void ssp_store_si128_SSE2( __m128i *p, __m128i b )
+{
+    _mm_store_si128( p, b );
+}
+/** \SSE2{Native,_mm_store1_pd} */ 
+SSP_FORCEINLINE void ssp_store1_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_store1_pd( dp, a );
+}
+/** \SSE2{Native,_mm_storeh_pd} */ 
+SSP_FORCEINLINE void ssp_storeh_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_storeh_pd( dp, a );
+}
+/** \SSE2{Native,_mm_storel_epi64} */ 
+SSP_FORCEINLINE void ssp_storel_epi64_SSE2( __m128i *p, __m128i b )
+{
+    _mm_storel_epi64( p, b );
+}
+/** \SSE2{Native,_mm_storel_pd} */ 
+SSP_FORCEINLINE void ssp_storel_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_storel_pd( dp, a );
+}
+/** \SSE2{Native,_mm_storer_pd} */ 
+SSP_FORCEINLINE void ssp_storer_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_storer_pd( dp, a );
+}
+/** \SSE2{Native,_mm_storeu_pd} */ 
+SSP_FORCEINLINE void ssp_storeu_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_storeu_pd( dp, a );
+}
+/** \SSE2{Native,_mm_storeu_si128} */ 
+SSP_FORCEINLINE void ssp_storeu_si128_SSE2( __m128i *p, __m128i b )
+{
+    _mm_storeu_si128( p, b );
+}
+/** \SSE2{Native,_mm_stream_pd} */ 
+SSP_FORCEINLINE void ssp_stream_pd_SSE2( double *dp, __m128d a )
+{
+    _mm_stream_pd( dp, a );
+}
+/** \SSE2{Native,_mm_stream_si128} */ 
+SSP_FORCEINLINE void ssp_stream_si128_SSE2( __m128i *p, __m128i a )
+{
+    _mm_stream_si128( p, a );
+}
+/** \SSE2{Native,_mm_stream_si32} */ 
+SSP_FORCEINLINE void ssp_stream_si32_SSE2( int *p, int i )
+{
+    _mm_stream_si32( p, i );
+}
+/** \SSE2{Native,_mm_sub_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_sub_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_sub_epi16( a, b );
+}
+/** \SSE2{Native,_mm_sub_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_sub_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_sub_epi32( a, b );
+}
+/** \SSE2{Native,_mm_sub_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_sub_epi64_SSE2( __m128i a, __m128i b )
+{
+    return _mm_sub_epi64( a, b );
+}
+/** \SSE2{Native,_mm_sub_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_sub_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_sub_epi8( a, b );
+}
+/** \SSE2{Native,_mm_sub_pd} */ 
+SSP_FORCEINLINE __m128d ssp_sub_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_sub_pd( a, b );
+}
+/** \SSE2{Native,_mm_sub_sd} */ 
+SSP_FORCEINLINE __m128d ssp_sub_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_sub_sd( a, b );
+}
+/** \SSE2{Native,_mm_sub_si64} */ 
+SSP_FORCEINLINE __m64 ssp_sub_si64_SSE2( __m64 a, __m64 b)
+{
+    return _mm_sub_si64( a, b );
+}
+/** \SSE2{Native,_mm_subs_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_subs_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_subs_epi16( a, b );
+}
+/** \SSE2{Native,_mm_subs_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_subs_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_subs_epi8( a, b );
+}
+/** \SSE2{Native,_mm_subs_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_subs_epu16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_subs_epu16( a, b );
+}
+/** \SSE2{Native,_mm_subs_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_subs_epu8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_subs_epu8( a, b );
+}
+/** \SSE2{Native,_mm_ucomieq_sd} */ 
+SSP_FORCEINLINE int ssp_ucomieq_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_ucomieq_sd( a, b );
+}
+/** \SSE2{Native,_mm_ucomige_sd} */ 
+SSP_FORCEINLINE int ssp_ucomige_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_ucomige_sd( a, b );
+}
+/** \SSE2{Native,_mm_ucomigt_sd} */ 
+SSP_FORCEINLINE int ssp_ucomigt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_ucomigt_sd( a, b );
+}
+/** \SSE2{Native,_mm_ucomile_sd} */ 
+SSP_FORCEINLINE int ssp_ucomile_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_ucomile_sd( a, b );
+}
+/** \SSE2{Native,_mm_ucomilt_sd} */ 
+SSP_FORCEINLINE int ssp_ucomilt_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_ucomilt_sd( a, b );
+}
+/** \SSE2{Native,_mm_ucomineq_sd} */ 
+SSP_FORCEINLINE int ssp_ucomineq_sd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_ucomineq_sd( a, b );
+}
+/** \SSE2{Native,_mm_unpackhi_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_unpackhi_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpackhi_epi16( a, b );
+}
+/** \SSE2{Native,_mm_unpackhi_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_unpackhi_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpackhi_epi32( a, b );
+}
+/** \SSE2{Native,_mm_unpackhi_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_unpackhi_epi64_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpackhi_epi64( a, b );
+}
+/** \SSE2{Native,_mm_unpackhi_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_unpackhi_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpackhi_epi8( a, b );
+}
+/** \SSE2{Native,_mm_unpackhi_pd} */ 
+SSP_FORCEINLINE __m128d ssp_unpackhi_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_unpackhi_pd( a, b );
+}
+/** \SSE2{Native,_mm_unpacklo_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_unpacklo_epi16_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpacklo_epi16( a, b );
+}
+/** \SSE2{Native,_mm_unpacklo_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_unpacklo_epi32_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpacklo_epi32( a, b );
+}
+/** \SSE2{Native,_mm_unpacklo_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_unpacklo_epi64_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpacklo_epi64( a, b );
+}
+/** \SSE2{Native,_mm_unpacklo_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_unpacklo_epi8_SSE2( __m128i a, __m128i b )
+{
+    return _mm_unpacklo_epi8( a, b );
+}
+/** \SSE2{Native,_mm_unpacklo_pd} */ 
+SSP_FORCEINLINE __m128d ssp_unpacklo_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_unpacklo_pd( a, b );
+}
+/** \SSE2{Native,_mm_xor_pd} */ 
+SSP_FORCEINLINE __m128d ssp_xor_pd_SSE2( __m128d a, __m128d b )
+{
+    return _mm_xor_pd( a, b );
+}
+/** \SSE2{Native,_mm_xor_si128} */ 
+SSP_FORCEINLINE __m128i ssp_xor_si128_SSE2( __m128i a, __m128i b )
+{
+    return _mm_xor_si128( a, b );
+}
+
+//@}
+//@}
+
+#endif // __SSP_NATIVE_SSE2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,76 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE3_H__
+#define __SSEPLUS_NATIVE_SSE3_H__
+
+#include "../SSEPlus_base.h"
+#include SSP_INCLUDE_FILE_SSE3
+
+
+/** @addtogroup native_SSE3   
+ *  @{ 
+ *  @name Native SSE3 Operations
+ */
+
+/** \SSE3{Native,_mm_addsub_ps} */ 
+SSP_FORCEINLINE __m128 ssp_addsub_ps_SSE3(__m128 a, __m128 b)                              
+{
+    return _mm_addsub_ps( a, b );
+}
+/** \SSE3{Native,_mm_addsub_pd} */ 
+SSP_FORCEINLINE __m128d ssp_addsub_pd_SSE3( __m128d a, __m128d b)
+{
+    return _mm_addsub_pd( a, b );
+}
+/** \SSE3{Native,_mm_hadd_pd} */ 
+SSP_FORCEINLINE __m128d ssp_hadd_pd_SSE3( __m128d a, __m128d b)
+{
+    return _mm_hadd_pd(a, b );
+}
+/** \SSE3{Native,_mm_hadd_ps} */ 
+SSP_FORCEINLINE __m128 ssp_hadd_ps_SSE3( __m128 a, __m128 b)
+{
+    return _mm_hadd_ps( a, b);    
+}
+/** \SSE3{Native,_mm_hsub_pd} */ 
+SSP_FORCEINLINE __m128d ssp_hsub_pd_SSE3( __m128d a, __m128d b)
+{
+    return _mm_hsub_pd (a, b);
+}
+/** \SSE3{Native,_mm_hsub_ps} */ 
+SSP_FORCEINLINE __m128 ssp_hsub_ps_SSE3( __m128 a, __m128 b)
+{
+    return _mm_hsub_ps (a, b);
+}
+/** \SSE3{Native,_mm_lddqu_si128} */ 
+SSP_FORCEINLINE __m128i ssp_lddqu_si128_SSE3( __m128i const *p)
+{
+    return _mm_lddqu_si128 (p);    
+}
+/** \SSE3{Native,_mm_loaddup_pd} */ 
+SSP_FORCEINLINE __m128d ssp_loaddup_pd_SSE3 (double const * dp)
+{
+    return _mm_loaddup_pd (dp);
+}
+/** \SSE3{Native,_mm_movedup_pd} */ 
+SSP_FORCEINLINE __m128d ssp_movedup_pd_SSE3( __m128d a)
+{
+    return _mm_movedup_pd (a);    
+}
+/** \SSE3{Native,_mm_movehdup_ps} */ 
+SSP_FORCEINLINE __m128 ssp_movehdup_ps_SSE3( __m128 a)
+{
+    return _mm_movehdup_ps (a);
+}
+/** \SSE3{Native,_mm_moveldup_ps} */ 
+SSP_FORCEINLINE __m128 ssp_moveldup_ps_SSE3( __m128 a)
+{
+    return _mm_moveldup_ps (a);
+}
+
+//@}
+//@}
+
+#endif // __SSEPLUS_NATIVE_SSE3_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.1.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.1.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.1.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.1.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,382 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE4_1_H__
+#define __SSEPLUS_NATIVE_SSE4_1_H__
+
+#include "../SSEPlus_base.h"
+#include <smmintrin.h> // SSE4.1
+
+/** @addtogroup native_SSE4_1   
+ *  @{ 
+ *  @name Native SSE4.1 Operations
+ */
+
+
+/** \SSE4_1{Native,_mm_ceil_pd} */ 
+SSP_FORCEINLINE __m128d ssp_ceil_pd_SSE4_1(__m128d a)
+{
+    return _mm_ceil_pd( a );
+}
+/** \SSE4_1{Native,_mm_ceil_sd} */ 
+SSP_FORCEINLINE __m128d ssp_ceil_sd_SSE4_1(__m128d dst, __m128d a)
+{
+    return _mm_ceil_sd( dst, a );
+}
+/** \SSE4_1{Native,_mm_floor_pd} */ 
+SSP_FORCEINLINE __m128d ssp_floor_pd_SSE4_1(__m128d a)
+{
+    return _mm_floor_pd( a );
+}
+/** \SSE4_1{Native,_mm_floor_sd} */ 
+SSP_FORCEINLINE __m128d ssp_floor_sd_SSE4_1(__m128d dst, __m128d a)
+{
+    return _mm_floor_sd( dst, a );
+}
+/** \SSE4_1{Native,_mm_ceil_ps} */ 
+SSP_FORCEINLINE __m128 ssp_ceil_ps_SSE4_1(__m128 a)
+{
+    return _mm_ceil_ps( a );
+}
+/** \SSE4_1{Native,_mm_ceil_ss} */ 
+SSP_FORCEINLINE __m128 ssp_ceil_ss_SSE4_1(__m128 dst, __m128 a)
+{
+    return _mm_ceil_ss( dst, a );
+}
+/** \SSE4_1{Native,_mm_floor_ps} */ 
+SSP_FORCEINLINE __m128 ssp_floor_ps_SSE4_1(__m128 a)
+{
+    return _mm_floor_ps( a );
+}
+/** \SSE4_1{Native,_mm_floor_ss} */ 
+SSP_FORCEINLINE __m128 ssp_floor_ss_SSE4_1(__m128 dst, __m128 a)
+{
+    return _mm_floor_ss( dst, a );
+}
+/** \SSE4_1{Native,_mm_blend_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_blend_epi16_SSE4_1(__m128i a, __m128i b,const int mask)
+{
+    switch( mask & 0xFF )
+    {
+        CASE_256( _mm_blend_epi16, a, b );
+    }
+}
+/** \SSE4_1{Native,_mm_blendv_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_blendv_epi8_SSE4_1(__m128i a, __m128i b, __m128i mask)
+{
+    return _mm_blendv_epi8( a, b, mask );
+}
+/** \SSE4_1{Native,_mm_blend_ps} */ 
+SSP_FORCEINLINE __m128 ssp_blend_ps_SSE4_1(__m128  a, __m128  b, const int mask)
+{
+    switch( mask & 0x0F )
+    {
+        CASE_16( _mm_blend_ps, a, b );
+    }
+}
+/** \SSE4_1{Native,_mm_blendv_ps} */ 
+SSP_FORCEINLINE __m128 ssp_blendv_ps_SSE4_1(__m128  a, __m128  b, __m128 mask)
+{
+    return _mm_blendv_ps( a, b, mask);
+}
+/** \SSE4_1{Native,_mm_blend_pd} */ 
+SSP_FORCEINLINE __m128d ssp_blend_pd_SSE4_1(__m128d a, __m128d b, const int mask)
+{
+    switch(mask&0x3)
+    {
+        CASE_4( _mm_blend_pd, a, b );
+    }
+}
+/** \SSE4_1{Native,_mm_blendv_pd} */ 
+SSP_FORCEINLINE __m128d ssp_blendv_pd_SSE4_1(__m128d a, __m128d b, __m128d mask)
+{
+    return _mm_blendv_pd( a, b, mask);
+}
+/** \SSE4_1{Native,_mm_dp_ps} */ 
+SSP_FORCEINLINE __m128 ssp_dp_ps_SSE4_1(__m128  a, __m128  b, const int mask)
+{
+    switch( mask & 0xFF )
+    {
+        CASE_256( _mm_dp_ps, a, b );
+    }
+}
+/** \SSE4_1{Native,_mm_dp_pd} */ 
+SSP_FORCEINLINE __m128d ssp_dp_pd_SSE4_1(__m128d a, __m128d b, const int mask)
+{
+    switch( mask & 0x3F )
+    {
+        CASE_128( _mm_dp_pd, a, b );
+    }
+}
+/** \SSE4_1{Native,_mm_cmpeq_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cmpeq_epi64_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_cmpeq_epi64( a, b);
+}
+/** \SSE4_1{Native,_mm_min_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_min_epi8_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_min_epi8( a, b);
+}
+/** \SSE4_1{Native,_mm_max_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_max_epi8_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_max_epi8( a, b);
+}
+/** \SSE4_1{Native,_mm_min_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_min_epu16_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_min_epu16( a, b);
+}
+/** \SSE4_1{Native,_mm_max_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_max_epu16_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_max_epu16( a, b);
+}
+/** \SSE4_1{Native,_mm_min_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_min_epi32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_min_epi32( a, b);
+}
+/** \SSE4_1{Native,_mm_max_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_max_epi32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_max_epi32( a, b);
+}
+/** \SSE4_1{Native,_mm_min_epu32} */ 
+SSP_FORCEINLINE __m128i ssp_min_epu32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_min_epu32( a, b);
+}
+/** \SSE4_1{Native,_mm_max_epu32} */ 
+SSP_FORCEINLINE __m128i ssp_max_epu32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_max_epu32( a, b);
+}
+/** \SSE4_1{Native,_mm_mullo_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_mullo_epi32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_mullo_epi32( a, b);
+}
+/** \SSE4_1{Native,_mm_mul_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_mul_epi32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_mul_epi32( a, b);
+}
+
+/** \SSE4_1{Native,_mm_insert_ps} */ 
+SSP_FORCEINLINE __m128 ssp_insert_ps_SSE4_1(__m128 dst, __m128 src, const int ndx)
+{
+    switch( ndx & 0xFF )
+    {
+        CASE_256( _mm_insert_ps, dst, src );
+    }
+}
+/** \SSE4_1{Native,_mm_extract_ps} */ 
+SSP_FORCEINLINE int ssp_extract_ps_SSE4_1(__m128 src, const int ndx)
+{
+    switch(ndx&0x3)
+    {
+        CASE_4( _mm_extract_ps, src )
+    }
+}
+/** \SSE4_1{Native,_mm_insert_epi8} */ 
+SSP_FORCEINLINE __m128i ssp_insert_epi8_SSE4_1(__m128i dst, int s, const int ndx)
+{
+    switch( ndx & 0xF )
+    {
+        CASE_16( _mm_insert_epi8, dst, s );
+    }
+}
+/** \SSE4_1{Native,_mm_insert_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_insert_epi32_SSE4_1(__m128i dst, int s, const int ndx)
+{
+    switch( ndx & 0x3 )
+    {
+        CASE_4( _mm_insert_epi32, dst, s );
+    }
+}
+
+/** \SSE4_1{Native,_mm_extract_epi8} */ 
+SSP_FORCEINLINE int ssp_extract_epi8_SSE4_1(__m128i src, const int ndx)
+{
+    switch( ndx & 0xF )
+    {
+        CASE_16( _mm_extract_epi8, src );
+    }
+}
+/** \SSE4_1{Native,_mm_extract_epi32} */ 
+SSP_FORCEINLINE int ssp_extract_epi32_SSE4_1(__m128i src, const int ndx)
+{
+    switch( ndx & 0x3 )
+    {
+        CASE_4( _mm_extract_epi32, src );
+    }
+}
+
+/** \SSE4_1{Native,_mm_minpos_epu16} */ 
+SSP_FORCEINLINE __m128i ssp_minpos_epu16_SSE4_1(__m128i shortValues)
+{
+    return _mm_minpos_epu16( shortValues );
+}
+
+/** \SSE4_1{Native,_mm_cvtepi8_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepi8_epi32_SSE4_1(__m128i byteValues)
+{
+    return _mm_cvtepi8_epi32( byteValues );
+}
+/** \SSE4_1{Native,_mm_cvtepi16_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepi16_epi32_SSE4_1(__m128i shortValues)
+{
+    return _mm_cvtepi16_epi32( shortValues );
+}
+/** \SSE4_1{Native,_mm_cvtepi8_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepi8_epi64_SSE4_1(__m128i byteValues)
+{
+    return _mm_cvtepi8_epi64( byteValues );
+}
+/** \SSE4_1{Native,_mm_cvtepi32_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepi32_epi64_SSE4_1(__m128i intValues)
+{
+    return _mm_cvtepi32_epi64( intValues );
+}
+/** \SSE4_1{Native,_mm_cvtepi16_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepi16_epi64_SSE4_1(__m128i shortValues)
+{
+    return _mm_cvtepi16_epi64( shortValues );
+}
+/** \SSE4_1{Native,_mm_cvtepi8_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepi8_epi16_SSE4_1(__m128i byteValues)
+{
+    return _mm_cvtepi8_epi16( byteValues );
+}
+/** \SSE4_1{Native,_mm_cvtepu8_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepu8_epi32_SSE4_1(__m128i byteValues)
+{
+    return _mm_cvtepu8_epi32( byteValues );
+}
+/** \SSE4_1{Native,_mm_cvtepu16_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepu16_epi32_SSE4_1(__m128i shortValues)
+{
+    return _mm_cvtepu16_epi32( shortValues );
+}
+/** \SSE4_1{Native,_mm_cvtepu8_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepu8_epi64_SSE4_1(__m128i shortValues)
+{
+    return _mm_cvtepu8_epi64( shortValues );
+}
+/** \SSE4_1{Native,_mm_cvtepu32_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepu32_epi64_SSE4_1(__m128i intValues)
+{
+    return _mm_cvtepu32_epi64( intValues );
+}
+/** \SSE4_1{Native,_mm_cvtepu16_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepu16_epi64_SSE4_1(__m128i shortValues)
+{
+    return _mm_cvtepu16_epi64( shortValues );
+}
+/** \SSE4_1{Native,_mm_cvtepu8_epi16} */ 
+SSP_FORCEINLINE __m128i ssp_cvtepu8_epi16_SSE4_1(__m128i byteValues)
+{
+    return _mm_cvtepu8_epi16( byteValues );
+}
+/** \SSE4_1{Native,_mm_packus_epi32} */ 
+SSP_FORCEINLINE __m128i ssp_packus_epi32_SSE4_1(__m128i a, __m128i b)
+{
+    return _mm_packus_epi32( a, b );
+}
+/** \SSE4_1{Native,_mm_mpsadbw_epu8} */ 
+SSP_FORCEINLINE __m128i ssp_mpsadbw_epu8_SSE4_1(__m128i a, __m128i b, const int msk)
+{
+    switch( msk & 0x7 )
+    {
+        CASE_8( _mm_mpsadbw_epu8, a, b );
+    }
+}
+/** \SSE4_1{Native,_mm_stream_load_si128} */ 
+SSP_FORCEINLINE __m128i ssp_stream_load_si128_SSE4_1(__m128i* a)
+{
+    return _mm_stream_load_si128( a );
+}
+
+
+//
+// Functions common with SSE5
+//
+
+/** \SSE4_1{Native,_mm_testz_si128} */ 
+SSP_FORCEINLINE int ssp_testz_si128_SSE4_1(__m128i mask, __m128i a)
+{
+    return _mm_testz_si128( mask, a);
+}
+/** \SSE4_1{Native,_mm_testc_si128} */ 
+SSP_FORCEINLINE int ssp_testc_si128_SSE4_1(__m128i mask, __m128i a)
+{
+    return _mm_testc_si128( mask, a);
+}
+/** \SSE4_1{Native,_mm_testnzc_si128} */ 
+SSP_FORCEINLINE int ssp_testnzc_si128_SSE4_1(__m128i mask, __m128i b)
+{
+    return _mm_testnzc_si128( mask, b);
+}
+
+/** \SSE45{Native,_mm_round_pd, roundpd} */ 
+SSP_FORCEINLINE __m128d ssp_round_pd_SSE4_1(__m128d a, int iRoundMode)
+{
+    switch( iRoundMode & 0xF )
+    {
+        CASE_16( _mm_round_pd, a );
+    }
+}
+/** \SSE45{Native,_mm_round_sd, roundsd} */ 
+SSP_FORCEINLINE __m128d ssp_round_sd_SSE4_1(__m128d dst, __m128d a, int iRoundMode)
+{
+    switch( iRoundMode & 0xF )
+    {
+        CASE_16( _mm_round_sd, dst, a );
+    }
+}
+/** \SSE45{Native,_mm_round_ps, roundps} */ 
+SSP_FORCEINLINE __m128 ssp_round_ps_SSE4_1(__m128  a, int iRoundMode)
+{
+    switch( iRoundMode & 0xF )
+    {
+        CASE_16( _mm_round_ps, a );
+    }
+}
+/** \SSE45{Native,_mm_round_ss, roundss} */ 
+SSP_FORCEINLINE __m128 ssp_round_ss_SSE4_1(__m128 dst, __m128  a, int iRoundMode)
+{
+    switch( iRoundMode & 0xF )
+    {
+        CASE_16( _mm_round_ss, dst, a );
+    }
+}
+
+
+#ifdef SYS64
+/** \SSE4_1{Native,_mm_insert_epi64} */ 
+SSP_FORCEINLINE __m128i ssp_insert_epi64_SSE4_1(__m128i dst, ssp_s64 s, const int ndx)
+{
+    switch( ndx & 0x1 )
+    {
+        CASE_2( _mm_insert_epi64, dst, s );
+    }
+}
+
+/** \SSE4_1{Native,_mm_extract_epi64} */ 
+SSP_FORCEINLINE ssp_s64 ssp_extract_epi64_SSE4_1(__m128i src, const int ndx)
+{
+    switch( ndx & 0x1 )
+    {
+        CASE_2( _mm_extract_epi64, src );
+    }
+}
+#endif
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_NATIVE_SSE4_1_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.2.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.2.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.2.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4.2.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,173 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE4_2_H__
+#define __SSEPLUS_NATIVE_SSE4_2_H__
+
+#include "../SSEPlus_base.h"
+#include <nmmintrin.h>  // SSE4.2
+
+/** @addtogroup native_SSE4_2   
+ *  @{ 
+ *  @name Native SSE4.2 Operations
+ */
+
+/** \SSE4_2{Native,_mm_cmpestra} */ 
+SSP_FORCEINLINE int ssp_cmpestra_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF )
+    {
+        CASE_256( _mm_cmpestra, a, la, b, lb );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpestrc} */ 
+SSP_FORCEINLINE int ssp_cmpestrc_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF )
+    {
+        CASE_256( _mm_cmpestrc, a, la, b, lb );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpestri} */ 
+SSP_FORCEINLINE int ssp_cmpestri_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF )
+    {
+        CASE_256( _mm_cmpestri, a, la, b, lb );
+    }
+}
+SSP_FORCEINLINE __m128i	ssp_cmpestrm_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF )
+    {
+        CASE_256( _mm_cmpestrm, a, la, b, lb );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpestro} */ 
+SSP_FORCEINLINE int ssp_cmpestro_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF )
+    {
+        CASE_256( _mm_cmpestro, a, la, b, lb );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpestrs} */ 
+SSP_FORCEINLINE int ssp_cmpestrs_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpestrs, a, la, b, lb );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpestrz} */ 
+SSP_FORCEINLINE int ssp_cmpestrz_SSE4_2 ( __m128i a, int la, __m128i b, int lb, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpestrz, a, la, b, lb );
+    }
+}
+SSP_FORCEINLINE __m128i	ssp_cmpgt_epi64_SSE4_2 ( __m128i a, __m128i b )
+{
+    return _mm_cmpgt_epi64 ( a, b );
+}
+/** \SSE4_2{Native,_mm_cmpistra} */ 
+SSP_FORCEINLINE int ssp_cmpistra_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistra, a, b );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpistrc} */ 
+SSP_FORCEINLINE int ssp_cmpistrc_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistrc, a, b );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpistri} */ 
+SSP_FORCEINLINE int ssp_cmpistri_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistri, a, b );
+    }
+}
+SSP_FORCEINLINE __m128i	ssp_cmpistrm_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistrm	, a, b );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpistro} */ 
+SSP_FORCEINLINE int ssp_cmpistro_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistro, a, b );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpistrs} */ 
+SSP_FORCEINLINE int ssp_cmpistrs_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistrs, a, b );
+    }
+}
+/** \SSE4_2{Native,_mm_cmpistrz} */ 
+SSP_FORCEINLINE int ssp_cmpistrz_SSE4_2 ( __m128i a, __m128i b, const int mode )
+{
+    switch( mode & 0xFF)
+    {
+        CASE_256( _mm_cmpistrz, a, b );
+    }
+}
+
+/** \SSE4_2{Native,_mm_crc32_u16} */ 
+SSP_FORCEINLINE unsigned int ssp_crc32_u16_SSE4_2 ( unsigned int crc, unsigned short   v )
+{
+    return _mm_crc32_u16( crc, v );
+}
+
+/** \SSE4_2{Native,_mm_crc32_u32} */ 
+SSP_FORCEINLINE unsigned int ssp_crc32_u32_SSE4_2 ( unsigned int crc, unsigned int     v )
+{
+    return _mm_crc32_u32( crc, v );
+}
+
+#ifdef SYS64
+/** \SSE4_2{Native,_mm_crc32_u64} */ 
+SSP_FORCEINLINE ssp_u64 ssp_crc32_u64_SSE4_2 ( unsigned int crc, ssp_u64 v )
+{
+    return _mm_crc32_u64( crc, v );
+}
+#endif
+
+/** \SSE4_2{Native,_mm_crc32_u8} */ 
+SSP_FORCEINLINE unsigned int ssp_crc32_u8_SSE4_2 ( unsigned int crc, unsigned char    v )
+{
+    return _mm_crc32_u8( crc, v );
+}
+/** \SSE4_2{Native,_mm_popcnt_u32} */ 
+SSP_FORCEINLINE int ssp_popcnt_u32_SSE4_2 ( unsigned int a     )
+{
+    return _mm_popcnt_u32( a );
+}
+#ifdef SYS64
+/** \SSE4_2{Native,_mm_popcnt_u64} */ 
+SSP_FORCEINLINE int ssp_popcnt_u64_SSE4_2 ( ssp_u64 a )
+{
+    return _mm_popcnt_u64( a );
+}
+#endif 
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_NATIVE_SSE4_2_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4a.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4a.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4a.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSE4a.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,94 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE4A_H__
+#define __SSEPLUS_NATIVE_SSE4A_H__
+
+#include "../SSEPlus_base.h"
+#include SSP_INCLUDE_FILE_SSE4a
+
+/** @addtogroup native_SSE4A  
+ *  @{ 
+ *  @name Native SSE4A Operations{
+ */
+
+/** \SSE4a{Native,_mm_extract_si64} */ 
+SSP_FORCEINLINE __m128i ssp_extract_si64_SSE4A( __m128i a ,__m128i b )
+{
+    return _mm_extract_si64( a, b );
+}
+/** \SSE4a{Native,_mm_extracti_si64} */ 
+SSP_FORCEINLINE __m128i ssp_extracti_si64_SSE4A( __m128i a, int len, int ndx )
+{
+    return _mm_extracti_si64( a, len, ndx );
+}
+/** \SSE4a{Native,_mm_insert_si64} */ 
+SSP_FORCEINLINE __m128i ssp_insert_si64_SSE4A( __m128i a, __m128i b )
+{
+    return _mm_insert_si64( a, b );
+}
+/** \SSE4a{Native,_mm_inserti_si64} */ 
+SSP_FORCEINLINE __m128i ssp_inserti_si64_SSE4A( __m128i a, __m128i b, int len, int ndx )
+{
+    return _mm_inserti_si64( a, b, len, ndx );
+}
+/** \SSE4a{Native,_mm_stream_sd} */ 
+SSP_FORCEINLINE void ssp_stream_sd_SSE4A( double *dst ,__m128d src )
+{
+    _mm_stream_sd( dst, src );
+}
+/** \SSE4a{Native,_mm_stream_ss} */ 
+SSP_FORCEINLINE void ssp_stream_ss_SSE4A( float *dst, __m128 src )
+{
+    _mm_stream_ss( dst, src );
+}
+
+/** \SSE4a{Native,__lzcnt16} 
+\n \b NOTE: Support for the LZCNT instruction is indicated by ECX bit 5 (LZCNT) as returned by CPUID function 8000_0001h. If the LZCNT instruction is not available, the encoding is treated as the BSR instruction. Software MUST check the CPUID bit once per program or library initialization before using the LZCNT instruction, or inconsistent behavior may result.
+*/ 
+SSP_FORCEINLINE unsigned short ssp_lzcnt16_SSE4A( unsigned short val )
+{
+    return __lzcnt16( val );
+}
+/** \SSE4a{Native,__lzcnt} 
+\n \b NOTE: Support for the LZCNT instruction is indicated by ECX bit 5 (LZCNT) as returned by CPUID function 8000_0001h. If the LZCNT instruction is not available, the encoding is treated as the BSR instruction. Software MUST check the CPUID bit once per program or library initialization before using the LZCNT instruction, or inconsistent behavior may result.
+*/ 
+SSP_FORCEINLINE unsigned int ssp_lzcnt_SSE4A( unsigned int val )
+{
+    return __lzcnt( val );
+}
+
+#ifdef SYS64
+/** \SSE4a{Native,__lzcnt64} */ 
+SSP_FORCEINLINE ssp_u64 ssp_lzcnt64_SSE4A( ssp_u64 val )
+{
+    return __lzcnt64( val );
+}
+#endif
+
+/** \SSE4a{Native,__popcnt16} */ 
+SSP_FORCEINLINE unsigned short ssp_popcnt16_SSE4A( unsigned short val )
+{
+    return __popcnt16( val );
+}
+/** \SSE4a{Native,__popcnt} */ 
+SSP_FORCEINLINE unsigned int ssp_popcnt_SSE4A( unsigned int val )
+{
+    return __popcnt( val );
+}
+
+#ifdef SYS64
+/** \SSE4a{Native,__popcnt64} */ 
+SSP_FORCEINLINE ssp_u64 ssp_popcnt64_SSE4A( ssp_u64 val )
+{
+    return __popcnt64( val );
+}
+#endif
+
+/** @} 
+ *  @}
+ */
+
+
+#endif // __SSEPLUS_NATIVE_SSE4A_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSSE3.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSSE3.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSSE3.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/SSEPlus_native_SSSE3.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,190 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSSE3_H__
+#define __SSEPLUS_NATIVE_SSSE3_H__
+
+#include "../SSEPlus_base.h"
+#include <tmmintrin.h> // SSSE3
+#include "../emulation/SSEPlus_emulation_REF.h"
+
+/** @addtogroup native_SSSE3   
+ *  @{ 
+ *  @name Native SSSE3 Operations
+ */
+
+/** \SSSE3{Native,_mm_abs_epi16} */
+SSP_FORCEINLINE __m128i ssp_abs_epi16_SSSE3 (__m128i a)
+{
+    return ssp_abs_epi16_REF (a);
+}
+/** \SSSE3{Native,_mm_abs_epi32} */
+SSP_FORCEINLINE __m128i ssp_abs_epi32_SSSE3 (__m128i a)
+{
+    return ssp_abs_epi32_REF(a);
+}
+/** \SSSE3{Native,_mm_abs_epi8} */
+SSP_FORCEINLINE __m128i ssp_abs_epi8_SSSE3 (__m128i a)
+{
+    return ssp_abs_epi8_REF(a);
+}
+/** \SSSE3{Native,_mm_abs_pi16} */
+SSP_FORCEINLINE __m64 ssp_abs_pi16_SSSE3 (__m64 a)
+{
+    return ssp_abs_pi16_REF(a);
+}
+/** \SSSE3{Native,_mm_abs_pi32} */
+SSP_FORCEINLINE __m64 ssp_abs_pi32_SSSE3 (__m64 a)
+{
+    return ssp_abs_pi32_REF(a);
+}
+/** \SSSE3{Native,_mm_abs_pi8} */
+SSP_FORCEINLINE __m64 ssp_abs_pi8_SSSE3 (__m64 a)
+{
+    return ssp_abs_pi8_REF(a);
+}
+/** \SSSE3{Native,_mm_alignr_epi8} */
+SSP_FORCEINLINE __m128i ssp_alignr_epi8_SSSE3 (__m128i a, __m128i b, int n)
+{
+    n = (n>=32) ?  32 : n;
+    switch( n )
+    {
+        CASE_32( ssp_alignr_epi8_REF, a, b);
+    }
+}
+/** \SSSE3{Native,_mm_alignr_pi8} */
+SSP_FORCEINLINE __m64 ssp_alignr_pi8_SSSE3 (__m64 a, __m64 b, int n)
+{
+    n = (n>=16) ?  16 : n;
+    switch( n )
+    {
+        CASE_16( ssp_alignr_pi8_REF, a, b);
+    }
+}
+/** \SSSE3{Native,_mm_hadd_epi16} */
+SSP_FORCEINLINE __m128i ssp_hadd_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_hadd_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hadd_epi32} */
+SSP_FORCEINLINE __m128i ssp_hadd_epi32_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_hadd_epi32_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hadd_pi16} */
+SSP_FORCEINLINE __m64 ssp_hadd_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_hadd_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hadd_pi32} */
+SSP_FORCEINLINE __m64 ssp_hadd_pi32_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_hadd_pi32_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hadds_epi16} */
+SSP_FORCEINLINE __m128i ssp_hadds_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_hadds_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hadds_pi16} */
+SSP_FORCEINLINE __m64 ssp_hadds_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_hadds_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hsub_epi16} */
+SSP_FORCEINLINE __m128i ssp_hsub_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_hsub_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hsub_epi32} */
+SSP_FORCEINLINE __m128i ssp_hsub_epi32_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_hsub_epi32_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hsub_pi16} */
+SSP_FORCEINLINE __m64 ssp_hsub_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_hsub_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hsub_pi32} */
+SSP_FORCEINLINE __m64 ssp_hsub_pi32_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_hsub_pi32_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hsubs_epi16} */
+SSP_FORCEINLINE __m128i ssp_hsubs_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_hsubs_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_hsubs_pi16} */
+SSP_FORCEINLINE __m64 ssp_hsubs_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_hsubs_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_maddubs_epi16} */
+SSP_FORCEINLINE __m128i ssp_maddubs_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_maddubs_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_maddubs_pi16} */
+SSP_FORCEINLINE __m64 ssp_maddubs_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_maddubs_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_mulhrs_epi16} */
+SSP_FORCEINLINE __m128i ssp_mulhrs_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_mulhrs_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_mulhrs_pi16} */
+SSP_FORCEINLINE __m64 ssp_mulhrs_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_mulhrs_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_shuffle_epi8} */
+SSP_FORCEINLINE __m128i ssp_shuffle_epi8_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_shuffle_epi8_REF(a, b);
+}
+/** \SSSE3{Native,_mm_shuffle_pi8} */
+SSP_FORCEINLINE __m64 ssp_shuffle_pi8_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_shuffle_pi8_REF(a, b);
+}
+/** \SSSE3{Native,_mm_sign_epi16} */
+SSP_FORCEINLINE __m128i ssp_sign_epi16_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_sign_epi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_sign_epi32} */
+SSP_FORCEINLINE __m128i ssp_sign_epi32_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_sign_epi32_REF(a, b);
+}
+/** \SSSE3{Native,_mm_sign_epi8} */
+SSP_FORCEINLINE __m128i ssp_sign_epi8_SSSE3 (__m128i a, __m128i b)
+{
+    return ssp_sign_epi8_REF(a, b);
+}
+/** \SSSE3{Native,_mm_sign_pi16} */
+SSP_FORCEINLINE __m64 ssp_sign_pi16_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_sign_pi16_REF(a, b);
+}
+/** \SSSE3{Native,_mm_sign_pi32} */
+SSP_FORCEINLINE __m64 ssp_sign_pi32_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_sign_pi32_REF(a, b);
+}
+/** \SSSE3{Native,_mm_sign_pi8} */
+SSP_FORCEINLINE __m64 ssp_sign_pi8_SSSE3 (__m64 a, __m64 b)
+{
+    return ssp_sign_pi8_REF(a, b);
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_NATIVE_SSSE3_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/sseplus_native_SSE5.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/sseplus_native_SSE5.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/native/sseplus_native_SSE5.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/native/sseplus_native_SSE5.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1066 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NATIVE_SSE5_H__
+#define __SSEPLUS_NATIVE_SSE5_H__
+
+#include "../SSEPlus_base.h"
+
+#include SSP_INCLUDE_FILE_SSE5
+#include SSP_INCLUDE_FILE_SSE4_1_SSE5
+
+/** @addtogroup native_SSE5   
+ *  @{ 
+ *  @name Native SSE5 Operations
+ */
+
+//
+// Functions unique to SSE5
+//
+
+/** \SSE5{Native,_mm_cmov_si128, pcmov } */
+SSP_FORCEINLINE __m128i ssp_cmov_si128_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_cmov_si128 (a, b, c);
+}
+
+/** \SSE5{Native,_mm_comeq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comeq_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comeq_ps (a, b);
+}
+/** \SSE5{Native,_mm_comlt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comlt_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comlt_ps (a, b);
+}
+/** \SSE5{Native,_mm_comle_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comle_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comle_ps (a, b);
+}
+/** \SSE5{Native,_mm_comunord_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comunord_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comunord_ps (a, b);
+}
+/** \SSE5{Native,_mm_comneq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comneq_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comneq_ps (a, b);
+}
+/** \SSE5{Native,_mm_comnlt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comnlt_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comnlt_ps (a, b);
+}
+/** \SSE5{Native,_mm_comnle_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comnle_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comnle_ps (a, b);
+}
+/** \SSE5{Native,_mm_comord_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comord_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comord_ps (a, b);
+}
+/** \SSE5{Native,_mm_comueq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comueq_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comueq_ps (a, b);
+}
+/** \SSE5{Native,_mm_comnge_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comnge_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comnge_ps (a, b);
+}
+/** \SSE5{Native,_mm_comngt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comngt_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comngt_ps (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comfalse_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comfalse_ps (a, b);
+}
+/** \SSE5{Native,_mm_comoneq_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comoneq_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comoneq_ps (a, b);
+}
+/** \SSE5{Native,_mm_comge_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comge_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comge_ps (a, b);
+}
+/** \SSE5{Native,_mm_comgt_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comgt_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comgt_ps (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_ps, comps } */
+SSP_FORCEINLINE __m128 ssp_comtrue_ps_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comtrue_ps (a, b);
+}
+
+/** \SSE5{Native,_mm_comeq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comeq_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comeq_pd (a, b);
+}
+/** \SSE5{Native,_mm_comlt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comlt_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comlt_pd (a, b);
+}
+/** \SSE5{Native,_mm_comle_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comle_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comle_pd (a, b);
+}
+/** \SSE5{Native,_mm_comunord_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comunord_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comunord_pd (a, b);
+}
+/** \SSE5{Native,_mm_comneq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comneq_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comneq_pd (a, b);
+}
+/** \SSE5{Native,_mm_comnlt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comnlt_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comnlt_pd (a, b);
+}
+/** \SSE5{Native,_mm_comnle_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comnle_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comnle_pd (a, b);
+}
+/** \SSE5{Native,_mm_comord_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comord_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comord_pd (a, b);
+}
+/** \SSE5{Native,_mm_comueq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comueq_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comueq_pd (a, b);
+}
+/** \SSE5{Native,_mm_comnge_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comnge_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comnge_pd (a, b);
+}
+/** \SSE5{Native,_mm_comngt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comngt_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comngt_pd (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comfalse_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comfalse_pd (a, b);
+}
+/** \SSE5{Native,_mm_comoneq_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comoneq_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comoneq_pd (a, b);
+}
+/** \SSE5{Native,_mm_comge_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comge_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comge_pd (a, b);
+}
+/** \SSE5{Native,_mm_comgt_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comgt_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comgt_pd (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_pd, compd } */
+SSP_FORCEINLINE __m128d ssp_comtrue_pd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comtrue_pd (a, b);
+}
+/** \SSE5{Native,_mm_comeq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comeq_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comeq_ss (a, b);
+}
+/** \SSE5{Native,_mm_comlt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comlt_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comlt_ss (a, b);
+}
+/** \SSE5{Native,_mm_comle_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comle_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comle_ss (a, b);
+}
+/** \SSE5{Native,_mm_comunord_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comunord_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comunord_ss (a, b);
+}
+/** \SSE5{Native,_mm_comneq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comneq_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comneq_ss (a, b);
+}
+/** \SSE5{Native,_mm_comnlt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comnlt_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comnlt_ss (a, b);
+}
+/** \SSE5{Native,_mm_comnle_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comnle_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comnle_ss (a, b);
+}
+/** \SSE5{Native,_mm_comord_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comord_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comord_ss (a, b);
+}
+/** \SSE5{Native,_mm_comueq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comueq_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comueq_ss (a, b);
+}
+/** \SSE5{Native,_mm_comnge_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comnge_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comnge_ss (a, b);
+}
+/** \SSE5{Native,_mm_comngt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comngt_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comngt_ss (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comfalse_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comfalse_ss (a, b);
+}
+/** \SSE5{Native,_mm_comoneq_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comoneq_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comoneq_ss (a, b);
+}
+/** \SSE5{Native,_mm_comge_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comge_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comge_ss (a, b);
+}
+/** \SSE5{Native,_mm_comgt_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comgt_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comgt_ss (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_ss, comss } */
+SSP_FORCEINLINE __m128 ssp_comtrue_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_comtrue_ss (a, b);
+}
+/** \SSE5{Native,_mm_comeq_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comeq_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comeq_sd (a, b);
+}
+/** \SSE5{Native,_mm_comlt_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comlt_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comlt_sd (a, b);
+}
+/** \SSE5{Native,_mm_comle_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comle_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comle_sd (a, b);
+}
+/** \SSE5{Native,_mm_comunord_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comunord_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comunord_sd (a, b);
+}
+/** \SSE5{Native,_mm_comneq_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comneq_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comneq_sd (a, b);
+}
+/** \SSE5{Native,_mm_comnlt_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comnlt_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comnlt_sd (a, b);
+}
+/** \SSE5{Native,_mm_comnle_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comnle_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comnle_sd (a, b);
+}
+/** \SSE5{Native,_mm_comord_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comord_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comord_sd (a, b);
+}
+/** \SSE5{Native,_mm_comueq_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comueq_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comueq_sd (a, b);
+}
+/** \SSE5{Native,_mm_comnge_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comnge_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comnge_sd (a, b);
+}
+/** \SSE5{Native,_mm_comngt_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comngt_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comngt_sd (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comfalse_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comfalse_sd (a, b);
+}
+/** \SSE5{Native,_mm_comoneq_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comoneq_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comoneq_sd (a, b);
+}
+/** \SSE5{Native,_mm_comge_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comge_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comge_sd (a, b);
+}
+/** \SSE5{Native,_mm_comgt_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comgt_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comgt_sd (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_sd, commsd } */
+SSP_FORCEINLINE __m128d ssp_comtrue_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_comtrue_sd (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comle_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comge_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epu8, pcomub } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epu8 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comle_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comge_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epu16,pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epu16, pcomuw } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epu16 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comle_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comge_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epu32,pcomud } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epu32, pcomud } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epu32 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comlt_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comle_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comgt_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comge_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comeq_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comneq_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epu64,pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epu64, pcomuq } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epu64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epu64 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comle_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comge_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epi8, pcomb } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comle_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comge_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epi16,pcomw } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epi16, pcomw } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comle_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comge_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epi32,pcomd } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epi32, pcomd } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_comlt_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comlt_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comlt_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comle_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comle_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comle_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comgt_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comgt_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comgt_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comge_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comge_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comge_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comeq_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comeq_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comeq_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comneq_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comneq_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comneq_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comfalse_epi64,pcomq } */
+SSP_FORCEINLINE __m128i ssp_comfalse_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comfalse_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_comtrue_epi64, pcomq } */
+SSP_FORCEINLINE __m128i ssp_comtrue_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_comtrue_epi64 (a, b);
+}
+
+/** \SSE5{Native,_mm_frcz_ps, frczps } */
+SSP_FORCEINLINE __m128 ssp_frcz_ps_SSE5(__m128 a)
+{
+    return _mm_frcz_ps (a);
+}
+/** \SSE5{Native,_mm_frcz_pd, frczpd } */
+SSP_FORCEINLINE __m128d ssp_frcz_pd_SSE5(__m128d a)
+{
+    return _mm_frcz_pd (a);
+}
+
+/** \SSE5{Native,_mm_frcz_ss, frczss } */
+SSP_FORCEINLINE __m128 ssp_frcz_ss_SSE5(__m128 a, __m128 b)
+{
+    return _mm_frcz_ss (a, b);
+}
+
+/** \SSE5{Native,_mm_frcz_sd, frczsd } */
+SSP_FORCEINLINE __m128d ssp_frcz_sd_SSE5(__m128d a, __m128d b)
+{
+    return _mm_frcz_sd (a, b);
+}
+/** \SSE5{Native,_mm_haddw_epi8,	 phaddbw } */
+SSP_FORCEINLINE __m128i ssp_haddw_epi8_SSE5(__m128i a)
+{
+    return _mm_haddw_epi8 (a);
+}
+/** \SSE5{Native,_mm_haddd_epi8, phaddbd } */
+SSP_FORCEINLINE __m128i ssp_haddd_epi8_SSE5(__m128i a)
+{
+    return _mm_haddd_epi8 (a);
+}
+/** \SSE5{Native,_mm_haddq_epi8, phaddbq } */
+SSP_FORCEINLINE __m128i ssp_haddq_epi8_SSE5(__m128i a)
+{
+    return _mm_haddq_epi8 (a);
+}
+/** \SSE5{Native,_mm_haddd_epi16, phaddwd } */
+SSP_FORCEINLINE __m128i ssp_haddd_epi16_SSE5(__m128i a)
+{
+    return _mm_haddd_epi16 (a);
+}
+/** \SSE5{Native,_mm_haddq_epi16, phaddwq } */
+SSP_FORCEINLINE __m128i ssp_haddq_epi16_SSE5(__m128i a)
+{
+    return _mm_haddq_epi16 (a);
+}
+/** \SSE5{Native,_mm_haddq_epi32, phadddq } */
+SSP_FORCEINLINE __m128i ssp_haddq_epi32_SSE5(__m128i a)
+{
+    return _mm_haddq_epi32 (a);
+}
+/** \SSE5{Native,_mm_haddw_epu8, phaddubw} */
+SSP_FORCEINLINE __m128i ssp_haddw_epu8_SSE5(__m128i a)
+{
+    return _mm_haddw_epu8 (a);
+}
+/** \SSE5{Native,_mm_haddd_epu8, phaddubd} */
+SSP_FORCEINLINE __m128i ssp_haddd_epu8_SSE5(__m128i a)
+{
+    return _mm_haddd_epu8 (a);
+}
+/** \SSE5{Native,_mm_haddq_epu8, phaddubq} */
+SSP_FORCEINLINE __m128i ssp_haddq_epu8_SSE5(__m128i a)
+{
+    return _mm_haddq_epu8 (a);
+}
+/** \SSE5{Native,_mm_haddd_epu16, phadduwd} */
+SSP_FORCEINLINE __m128i ssp_haddd_epu16_SSE5(__m128i a)
+{
+    return _mm_haddd_epu16 (a);
+}
+/** \SSE5{Native,_mm_haddq_epu16, phadduwq} */
+SSP_FORCEINLINE __m128i ssp_haddq_epu16_SSE5(__m128i a)
+{
+    return _mm_haddq_epu16 (a);
+}
+/** \SSE5{Native,_mm_haddq_epu32, phaddudq} */
+SSP_FORCEINLINE __m128i ssp_haddq_epu32_SSE5(__m128i a)
+{
+    return _mm_haddq_epu32 (a);
+}
+/** \SSE5{Native,_mm_hsubw_epi8, phsubbw } */
+SSP_FORCEINLINE __m128i ssp_hsubw_epi8_SSE5(__m128i a)
+{
+    return _mm_hsubw_epi8 (a);
+}
+/** \SSE5{Native,_mm_hsubd_epi16, phsubwd } */
+SSP_FORCEINLINE __m128i ssp_hsubd_epi16_SSE5(__m128i a)
+{
+    return _mm_hsubd_epi16 (a);
+}
+/** \SSE5{Native,_mm_hsubq_epi32, phsubdq } */
+SSP_FORCEINLINE __m128i ssp_hsubq_epi32_SSE5(__m128i a)
+{
+    return _mm_hsubq_epi32 (a);
+}
+
+/** \SSE5{Native,_mm_macc_epi16, pmacsww } */
+SSP_FORCEINLINE __m128i ssp_macc_epi16_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_macc_epi16 (a, b, c);
+}
+/** \SSE5{Native,_mm_macc_epi32, pmacsdd } */
+SSP_FORCEINLINE __m128i ssp_macc_epi32_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_macc_epi32 (a, b, c);
+}
+/** \SSE5{Native,_mm_macc_pd, fmaddpd } */
+SSP_FORCEINLINE __m128d ssp_macc_pd_SSE5(__m128d a, __m128d b, __m128d c)
+{
+    return _mm_macc_pd(a, b, c);
+}
+/** \SSE5{Native,_mm_macc_ps, fmaddps } */
+SSP_FORCEINLINE __m128 ssp_macc_ps_SSE5(__m128 a, __m128 b, __m128 c)
+{
+    return _mm_macc_ps( a, b, c);
+}
+/** \SSE5{Native,_mm_macc_sd, fmaddsd } */
+SSP_FORCEINLINE __m128d ssp_macc_sd_SSE5(__m128d a, __m128d b, __m128d c)
+{
+    return _mm_macc_sd (a, b, c);
+}
+/** \SSE5{Native,_mm_macc_sd, fmaddss } */
+SSP_FORCEINLINE __m128 ssp_macc_ss_SSE5(__m128 a, __m128 b, __m128 c)
+{
+    return _mm_macc_ss(a, b, c);
+}
+
+/** \SSE5{Native,_mm_maccd_epi16, pmacswd } */
+SSP_FORCEINLINE __m128i ssp_maccd_epi16_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maccd_epi16 (a, b, c);
+}
+
+/** \SSE5{Native,_mm_maccs_epi16, pmacssww} */
+SSP_FORCEINLINE __m128i ssp_maccs_epi16_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maccs_epi16 (a,b, c);
+}
+/** \SSE5{Native,_mm_maccs_epi32, pmacssdd} */
+SSP_FORCEINLINE __m128i ssp_maccs_epi32_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maccs_epi32 (a, b, c);
+}
+/** \SSE5{Native,_mm_maccsd_epi16, pmacsswd} */
+SSP_FORCEINLINE __m128i ssp_maccsd_epi16_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maccsd_epi16 (a, b, c);
+}
+/** \SSE5{Native,_mm_maccslo_epi32, pmacssdql}*/
+SSP_FORCEINLINE __m128i ssp_maccslo_epi32_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maccslo_epi32 (a, b, c);
+}
+/** \SSE5{Native,_mm_macclo_epi32, pmacsdql} */
+SSP_FORCEINLINE __m128i ssp_macclo_epi32_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_macclo_epi32 (a, b, c);
+}
+/** \SSE5{Native,_mm_maccshi_epi32, pmacssdqh}*/
+SSP_FORCEINLINE __m128i ssp_maccshi_epi32_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maccshi_epi32 (a, b, c);
+}
+/** \SSE5{Native,_mm_macchi_epi32, pmacsdqh} */
+SSP_FORCEINLINE __m128i ssp_macchi_epi32_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_macchi_epi32 (a, b, c);
+}
+
+/** \SSE5{Native,_mm_maddsd_epi16, pmadcsswd}*/
+SSP_FORCEINLINE __m128i ssp_maddsd_epi16_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maddsd_epi16 (a,b,c);
+}
+/** \SSE5{Native,_mm_maddd_epi16,	 pmadcswd} */
+SSP_FORCEINLINE __m128i ssp_maddd_epi16_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_maddd_epi16 (a,b,c);
+}
+
+/** \SSE5{Native,_mm_msub_pd, fmsubpd } */
+SSP_FORCEINLINE __m128d ssp_msub_pd_SSE5	     (__m128d a, __m128d b, __m128d c)
+{
+    return _mm_msub_pd (a, b, c);
+}
+/** \SSE5{Native,_mm_msub_ps, fmsubps } */
+SSP_FORCEINLINE __m128 ssp_msub_ps_SSE5	     (__m128 a, __m128 b, __m128 c)
+{
+    return _mm_msub_ps (a, b, c);
+}
+/** \SSE5{Native,_mm_msub_sd, fmsubsd } */
+SSP_FORCEINLINE __m128d ssp_msub_sd_SSE5	     (__m128d a, __m128d b, __m128d c)
+{
+    return _mm_msub_sd (a, b, c);
+}
+/** \SSE5{Native,_mm_msub_ss, fmsubss } */
+SSP_FORCEINLINE __m128 ssp_msub_ss_SSE5	     (__m128 a, __m128 b, __m128 c)
+{
+    return _mm_msub_ss (a, b, c);
+}
+
+/** \SSE5{Native,_mm_nmacc_pd, fnmaddpd} */
+SSP_FORCEINLINE __m128d ssp_nmacc_pd_SSE5(__m128d a, __m128d b, __m128d c)
+{
+    return _mm_nmacc_pd (a, b, c);
+}
+/** \SSE5{Native,_mm_nmacc_ps, fnmaddps} */
+SSP_FORCEINLINE __m128 ssp_nmacc_ps_SSE5(__m128 a, __m128 b, __m128 c)
+{
+    return _mm_nmacc_ps (a, b, c);
+}
+/** \SSE5{Native,_mm_nmacc_sd, fnmaddsd} */
+SSP_FORCEINLINE __m128d ssp_nmacc_sd_SSE5(__m128d a, __m128d b, __m128d c)
+{
+    return _mm_nmacc_sd (a, b, c);
+}
+/** \SSE5{Native,_mm_nmacc_ss, fnmaddss} */
+SSP_FORCEINLINE __m128 ssp_nmacc_ss_SSE5(__m128 a, __m128 b, __m128 c)
+{
+    return _mm_nmacc_ss (a, b, c);
+}
+/** \SSE5{Native,_mm_nmsub_pd, fnmsubpd} */
+SSP_FORCEINLINE __m128d ssp_nmsub_pd_SSE5(__m128d a, __m128d b, __m128d c)
+{
+    return _mm_nmsub_pd (a, b, c);
+}
+/** \SSE5{Native,_mm_nmsub_ps, fnmsubps} */
+SSP_FORCEINLINE __m128 ssp_nmsub_ps_SSE5(__m128 a, __m128 b, __m128 c)
+{
+    return _mm_nmsub_ps (a, b, c);
+}
+/** \SSE5{Native,_mm_nmsub_sd, fnmsubsd} */
+SSP_FORCEINLINE __m128d ssp_nmsub_sd_SSE5(__m128d a, __m128d b, __m128d c)
+{
+    return _mm_nmsub_sd (a, b, c);
+}
+/** \SSE5{Native,_mm_nmsub_ss, fnmsubss} */
+SSP_FORCEINLINE __m128 ssp_nmsub_ss_SSE5(__m128 a, __m128 b, __m128 c)
+{
+    return _mm_nmsub_ss (a, b, c);
+}
+
+/** \SSE5{Native,_mm_perm_epi8, pperm } */
+SSP_FORCEINLINE __m128i ssp_perm_epi8_SSE5(__m128i a, __m128i b, __m128i c)
+{
+    return _mm_perm_epi8 (a, b, c);
+}
+/** \SSE5{Native,_mm_perm_ps,		 permps } */
+SSP_FORCEINLINE __m128 ssp_perm_ps_SSE5(__m128 a, __m128 b, __m128i c)
+{
+    return _mm_perm_ps (a, b, c);
+}
+/** \SSE5{Native,_mm_perm_pd,		 permpd } */
+SSP_FORCEINLINE __m128d ssp_perm_pd_SSE5(__m128d a, __m128d b, __m128i c)
+{
+    return _mm_perm_pd (a, b, c);
+}
+
+/** \SSE5{Native,_mm_rot_epi8,		 protb } */
+SSP_FORCEINLINE __m128i ssp_rot_epi8_SSE5(__m128i a, __m128i b  )
+{
+    return _mm_rot_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_rot_epi16,	 protw } */
+SSP_FORCEINLINE __m128i ssp_rot_epi16_SSE5(__m128i a, __m128i b  )
+{
+    return _mm_rot_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_rot_epi32,	 protd } */
+SSP_FORCEINLINE __m128i ssp_rot_epi32_SSE5(__m128i a, __m128i b  )
+{
+    return _mm_rot_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_rot_epi64,	 protq } */
+SSP_FORCEINLINE __m128i ssp_rot_epi64_SSE5(__m128i a, __m128i b  )
+{
+    return _mm_rot_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_roti_epi8, protb } */
+SSP_FORCEINLINE __m128i ssp_roti_epi8_SSE5(__m128i a, const int b)
+{
+    return _mm_roti_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_roti_epi16, protw } */
+SSP_FORCEINLINE __m128i ssp_roti_epi16_SSE5(__m128i a, const int b)
+{
+    return _mm_roti_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_roti_epi32, protd } */
+SSP_FORCEINLINE __m128i ssp_roti_epi32_SSE5(__m128i a, const int b)
+{
+    return _mm_roti_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_roti_epi64, protq } */
+SSP_FORCEINLINE __m128i ssp_roti_epi64_SSE5(__m128i a, const int b)
+{
+    return _mm_roti_epi64 (a, b);
+}
+
+/** \SSE5{Native,_mm_shl_epi8, pshlb } */
+SSP_FORCEINLINE __m128i ssp_shl_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_shl_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_shl_epi16, pshlw } */
+SSP_FORCEINLINE __m128i ssp_shl_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_shl_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_shl_epi32, pshld } */
+SSP_FORCEINLINE __m128i ssp_shl_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_shl_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_shl_epi64, pshlq } */
+SSP_FORCEINLINE __m128i ssp_shl_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_shl_epi64 (a, b);
+}
+/** \SSE5{Native,_mm_sha_epi8, pshab } */
+SSP_FORCEINLINE __m128i ssp_sha_epi8_SSE5(__m128i a, __m128i b)
+{
+    return _mm_sha_epi8 (a, b);
+}
+/** \SSE5{Native,_mm_sha_epi16, pshaw } */
+SSP_FORCEINLINE __m128i ssp_sha_epi16_SSE5(__m128i a, __m128i b)
+{
+    return _mm_sha_epi16 (a, b);
+}
+/** \SSE5{Native,_mm_sha_epi32, pshad } */
+SSP_FORCEINLINE __m128i ssp_sha_epi32_SSE5(__m128i a, __m128i b)
+{
+    return _mm_sha_epi32 (a, b);
+}
+/** \SSE5{Native,_mm_sha_epi64, pshaq } */
+SSP_FORCEINLINE __m128i ssp_sha_epi64_SSE5(__m128i a, __m128i b)
+{
+    return _mm_sha_epi64 (a, b);
+}
+
+//
+// Functions common with SSE4.1
+//
+
+/** \SSE45{Native,_mm_testz_si128, ptest } */
+SSP_FORCEINLINE int ssp_testz_si128_SSE5(__m128i mask, __m128i a)
+{
+    return _mm_testz_si128( mask, a);
+}
+/** \SSE45{Native,_mm_testc_si128, ptest } */
+SSP_FORCEINLINE int ssp_testc_si128_SSE5(__m128i mask, __m128i a)
+{
+    return _mm_testc_si128( mask, a);
+}
+/** \SSE45{Native,_mm_testnzc_si128, ptest } */
+SSP_FORCEINLINE int ssp_testnzc_si128_SSE5(__m128i mask, __m128i b)
+{
+    return _mm_testnzc_si128( mask, b);
+}
+/** \SSE45{Native,_mm_round_pd, roundpd } */ 
+SSP_FORCEINLINE __m128d ssp_round_pd_SSE5(__m128d a, int iRoundMode)
+{   switch( iRoundMode & 0xF )
+    {        
+       CASE_16( _mm_round_pd, a );    
+    }
+}
+/** \SSE45{Native,_mm_round_sd,roundsd} */ 
+SSP_FORCEINLINE __m128d ssp_round_sd_SSE5(__m128d dst, __m128d a, int iRoundMode)
+{    
+   switch( iRoundMode & 0xF )
+   {        
+      CASE_16( _mm_round_sd, dst, a );    
+   }
+}
+/** \SSE45{Native,_mm_round_ps,roundps} */ 
+SSP_FORCEINLINE __m128 ssp_round_ps_SSE5(__m128 a, int iRoundMode)
+{    
+   switch( iRoundMode & 0xF )
+   {        
+      CASE_16( _mm_round_ps, a );    
+   }
+}
+/** \SSE45{Native,_mm_round_ss,roundss} */ 
+SSP_FORCEINLINE __m128 ssp_round_ss_SSE5(__m128 dst, __m128 a, int iRoundMode)
+{    
+   switch( iRoundMode & 0xF )
+   {        
+      CASE_16( _mm_round_ss, dst, a );    
+   }
+}
+
+//@}
+//@}
+
+#endif // __SSEPLUS_NATIVE_SSE5_H__
\ No newline at end of file
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/number/SSEPlus_number_REF.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/number/SSEPlus_number_REF.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/SSEPlus/number/SSEPlus_number_REF.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/SSEPlus/number/SSEPlus_number_REF.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,74 @@
+//
+// Copyright (c) 2006-2008 Advanced Micro Devices, Inc. All Rights Reserved.
+// This software is subject to the Apache v2.0 License.
+//
+#ifndef __SSEPLUS_NUMBER_REF_H__
+#define __SSEPLUS_NUMBER_REF_H__
+
+#ifndef KERNEL
+#include <math.h>
+#endif
+
+/** @addtogroup supplimental_REF   
+ *  @{ 
+ *  @name Number Operations{
+ */
+
+SSP_FORCEINLINE int ssp_number_isValidNumber_F32_REF( ssp_s32* val )//TODO: move into utility collection
+{
+    // Check for NAN, +infin, or -infin (exponent: 111 1111 1)
+    // Are the exponent bits all 1's?
+    if( (*val & 0x7F800000) == 0x7F800000 )
+    {
+        return 0;
+    }
+    return 1;
+}
+SSP_FORCEINLINE int ssp_number_isValidNumber_F64_REF( ssp_s64* val )   //TODO: move into utility collection
+{
+    // Check for NAN, +infin, or -infin (exponent: 1111 1111)
+    // Are the exponent bits all 1's?
+    if( (*val & 0x7FF0000000000000ll) == 0x7FF0000000000000ll )
+    {
+        return 0;
+    }
+    return 1;
+}
+
+SSP_FORCEINLINE ssp_f32 ssp_number_changeSNanToQNaN_F32_REF( ssp_s32* val )//TODO: move into utility collection
+{
+    ssp_f32* retVal = (ssp_f32*)val;
+    // Check if the value is already a QNaN
+    if( (*val & 0x00400000) != 0x00400000 )
+    {
+        // Check if the value is + or - infinitie
+        if( (*val | 0x7F800000) != 0x7F800000 )
+        {
+            // Convert SNan To QNaN
+            *retVal = (ssp_f32)( *val | 0x00400000 );
+        }
+    }
+    return *retVal;
+}
+
+SSP_FORCEINLINE ssp_f64 ssp_number_changeSNanToQNaN_F64_REF( ssp_s64* val )//TODO: move into utility collection
+{
+    ssp_f64* retVal = (ssp_f64*)val;
+    // Check if the value is already a QNaN
+    if( (*val & 0x0008000000000000ll) != 0x0008000000000000ll )
+    {
+        // Check if the value is + or - infinitie
+        if( (*val | 0x7FF0000000000000ll) != 0x7FF0000000000000ll )
+        {
+            // Convert SNan To QNaN
+            *retVal = (ssp_f64)( *val | 0x0008000000000000ll );
+        }
+    }
+    return *retVal;
+}
+
+/** @} 
+ *  @}
+ */
+
+#endif // __SSEPLUS_NUMBER_REF_H__
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/corecrypto/ccblowfish.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/corecrypto/ccblowfish.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/corecrypto/ccblowfish.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/corecrypto/ccblowfish.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,37 @@
+/*
+ *  ccblowfish.h
+ *  corecrypto
+ *
+ *  Created by Michael Brouwer on 12/10/10.
+ *  Copyright 2010,2011 Apple Inc. All rights reserved.
+ *
+ */
+
+#ifndef _CORECRYPTO_CCBLOWFISH_H_
+#define _CORECRYPTO_CCBLOWFISH_H_
+
+#include <corecrypto/cc_config.h>
+#include <corecrypto/ccmode.h>
+
+#define CCBLOWFISH_BLOCK_SIZE 16
+#define CCBLOWFISH_MIN_KEY_SIZE 4
+#define CCBLOWFISH_MAX_KEY_SIZE 56
+
+extern const struct ccmode_ecb ccblowfish_ltc_ecb_decrypt_mode;
+extern const struct ccmode_ecb ccblowfish_ltc_ecb_encrypt_mode;
+
+/* Implementation Selectors: */
+const struct ccmode_ecb *ccblowfish_ecb_encrypt_mode(void);
+const struct ccmode_cbc *ccblowfish_cbc_encrypt_mode(void);
+const struct ccmode_cfb *ccblowfish_cfb_encrypt_mode(void);
+const struct ccmode_cfb8 *ccblowfish_cfb8_encrypt_mode(void);
+
+const struct ccmode_ecb *ccblowfish_ecb_decrypt_mode(void);
+const struct ccmode_cbc *ccblowfish_cbc_decrypt_mode(void);
+const struct ccmode_cfb *ccblowfish_cfb_decrypt_mode(void);
+const struct ccmode_cfb8 *ccblowfish_cfb8_decrypt_mode(void);
+
+const struct ccmode_ctr *ccblowfish_ctr_crypt_mode(void);
+const struct ccmode_ofb *ccblowfish_ofb_crypt_mode(void);
+
+#endif /* _CORECRYPTO_CCBLOWFISH_H_ */
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/emmintrin.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/emmintrin.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/emmintrin.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/emmintrin.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1451 @@
+/*===---- emmintrin.h - SSE2 intrinsics ------------------------------------===
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#ifndef __EMMINTRIN_H
+#define __EMMINTRIN_H
+
+#ifndef __SSE2__
+#error "SSE2 instruction set not enabled"
+#else
+
+#include <xmmintrin.h>
+
+typedef double __m128d __attribute__((__vector_size__(16)));
+typedef long long __m128i __attribute__((__vector_size__(16)));
+
+/* Type defines.  */
+typedef double __v2df __attribute__ ((__vector_size__ (16)));
+typedef long long __v2di __attribute__ ((__vector_size__ (16)));
+typedef short __v8hi __attribute__((__vector_size__(16)));
+typedef char __v16qi __attribute__((__vector_size__(16)));
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_add_sd(__m128d __a, __m128d __b)
+{
+  __a[0] += __b[0];
+  return __a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_add_pd(__m128d __a, __m128d __b)
+{
+  return __a + __b;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_sub_sd(__m128d __a, __m128d __b)
+{
+  __a[0] -= __b[0];
+  return __a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_sub_pd(__m128d __a, __m128d __b)
+{
+  return __a - __b;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_mul_sd(__m128d __a, __m128d __b)
+{
+  __a[0] *= __b[0];
+  return __a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_mul_pd(__m128d __a, __m128d __b)
+{
+  return __a * __b;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_div_sd(__m128d __a, __m128d __b)
+{
+  __a[0] /= __b[0];
+  return __a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_div_pd(__m128d __a, __m128d __b)
+{
+  return __a / __b;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_sqrt_sd(__m128d __a, __m128d __b)
+{
+  __m128d __c = __builtin_ia32_sqrtsd(__b);
+  return (__m128d) { __c[0], __a[1] };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_sqrt_pd(__m128d __a)
+{
+  return __builtin_ia32_sqrtpd(__a);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_min_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_minsd(__a, __b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_min_pd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_minpd(__a, __b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_max_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_maxsd(__a, __b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_max_pd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_maxpd(__a, __b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_and_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)((__v4si)__a & (__v4si)__b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_andnot_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)(~(__v4si)__a & (__v4si)__b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_or_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)((__v4si)__a | (__v4si)__b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_xor_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)((__v4si)__a ^ (__v4si)__b);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 0);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 1);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmple_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 2);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__b, __a, 1);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpge_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__b, __a, 2);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpord_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 7);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpunord_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 3);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpneq_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 4);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnlt_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 5);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnle_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__a, __b, 6);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpngt_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__b, __a, 5);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnge_pd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmppd(__b, __a, 6);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 0);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 1);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmple_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 2);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_sd(__m128d __a, __m128d __b)
+{
+  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 1);
+  return (__m128d) { __c[0], __a[1] };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpge_sd(__m128d __a, __m128d __b)
+{
+  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 2);
+  return (__m128d) { __c[0], __a[1] };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpord_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 7);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpunord_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 3);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpneq_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 4);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnlt_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 5);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnle_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 6);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpngt_sd(__m128d __a, __m128d __b)
+{
+  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 5);
+  return (__m128d) { __c[0], __a[1] };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnge_sd(__m128d __a, __m128d __b)
+{
+  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 6);
+  return (__m128d) { __c[0], __a[1] };
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comieq_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_comisdeq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comilt_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_comisdlt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comile_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_comisdle(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comigt_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_comisdgt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comige_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_comisdge(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comineq_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_comisdneq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomieq_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_ucomisdeq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomilt_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_ucomisdlt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomile_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_ucomisdle(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomigt_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_ucomisdgt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomige_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_ucomisdge(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomineq_sd(__m128d __a, __m128d __b)
+{
+  return __builtin_ia32_ucomisdneq(__a, __b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpd_ps(__m128d __a)
+{
+  return __builtin_ia32_cvtpd2ps(__a);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cvtps_pd(__m128 __a)
+{
+  return __builtin_ia32_cvtps2pd(__a);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cvtepi32_pd(__m128i __a)
+{
+  return __builtin_ia32_cvtdq2pd((__v4si)__a);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpd_epi32(__m128d __a)
+{
+  return __builtin_ia32_cvtpd2dq(__a);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsd_si32(__m128d __a)
+{
+  return __builtin_ia32_cvtsd2si(__a);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsd_ss(__m128 __a, __m128d __b)
+{
+  __a[0] = __b[0];
+  return __a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi32_sd(__m128d __a, int __b)
+{
+  __a[0] = __b;
+  return __a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cvtss_sd(__m128d __a, __m128 __b)
+{
+  __a[0] = __b[0];
+  return __a;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cvttpd_epi32(__m128d __a)
+{
+  return (__m128i)__builtin_ia32_cvttpd2dq(__a);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvttsd_si32(__m128d __a)
+{
+  return __a[0];
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpd_pi32(__m128d __a)
+{
+  return (__m64)__builtin_ia32_cvtpd2pi(__a);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvttpd_pi32(__m128d __a)
+{
+  return (__m64)__builtin_ia32_cvttpd2pi(__a);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpi32_pd(__m64 __a)
+{
+  return __builtin_ia32_cvtpi2pd((__v2si)__a);
+}
+
+static __inline__ double __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsd_f64(__m128d __a)
+{
+  return __a[0];
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_load_pd(double const *__dp)
+{
+  return *(__m128d*)__dp;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_load1_pd(double const *__dp)
+{
+  struct __mm_load1_pd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  double __u = ((struct __mm_load1_pd_struct*)__dp)->__u;
+  return (__m128d){ __u, __u };
+}
+
+#define        _mm_load_pd1(dp)        _mm_load1_pd(dp)
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_loadr_pd(double const *__dp)
+{
+  __m128d __u = *(__m128d*)__dp;
+  return __builtin_shufflevector(__u, __u, 1, 0);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_loadu_pd(double const *__dp)
+{
+  struct __loadu_pd {
+    __m128d __v;
+  } __attribute__((packed, may_alias));
+  return ((struct __loadu_pd*)__dp)->__v;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_load_sd(double const *__dp)
+{
+  struct __mm_load_sd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  double __u = ((struct __mm_load_sd_struct*)__dp)->__u;
+  return (__m128d){ __u, 0 };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_loadh_pd(__m128d __a, double const *__dp)
+{
+  struct __mm_loadh_pd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  double __u = ((struct __mm_loadh_pd_struct*)__dp)->__u;
+  return (__m128d){ __a[0], __u };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_loadl_pd(__m128d __a, double const *__dp)
+{
+  struct __mm_loadl_pd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  double __u = ((struct __mm_loadl_pd_struct*)__dp)->__u;
+  return (__m128d){ __u, __a[1] };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_set_sd(double __w)
+{
+  return (__m128d){ __w, 0 };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_set1_pd(double __w)
+{
+  return (__m128d){ __w, __w };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_set_pd(double __w, double __x)
+{
+  return (__m128d){ __x, __w };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_setr_pd(double __w, double __x)
+{
+  return (__m128d){ __w, __x };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_setzero_pd(void)
+{
+  return (__m128d){ 0, 0 };
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_move_sd(__m128d __a, __m128d __b)
+{
+  return (__m128d){ __b[0], __a[1] };
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store_sd(double *__dp, __m128d __a)
+{
+  struct __mm_store_sd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __mm_store_sd_struct*)__dp)->__u = __a[0];
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store1_pd(double *__dp, __m128d __a)
+{
+  struct __mm_store1_pd_struct {
+    double __u[2];
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __mm_store1_pd_struct*)__dp)->__u[0] = __a[0];
+  ((struct __mm_store1_pd_struct*)__dp)->__u[1] = __a[0];
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store_pd(double *__dp, __m128d __a)
+{
+  *(__m128d *)__dp = __a;
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storeu_pd(double *__dp, __m128d __a)
+{
+  __builtin_ia32_storeupd(__dp, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storer_pd(double *__dp, __m128d __a)
+{
+  __a = __builtin_shufflevector(__a, __a, 1, 0);
+  *(__m128d *)__dp = __a;
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storeh_pd(double *__dp, __m128d __a)
+{
+  struct __mm_storeh_pd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[1];
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storel_pd(double *__dp, __m128d __a)
+{
+  struct __mm_storeh_pd_struct {
+    double __u;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[0];
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_add_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v16qi)__a + (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_add_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v8hi)__a + (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_add_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v4si)__a + (__v4si)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_add_si64(__m64 __a, __m64 __b)
+{
+  return __a + __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_add_epi64(__m128i __a, __m128i __b)
+{
+  return __a + __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_adds_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_paddsb128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_adds_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_paddsw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_adds_epu8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_paddusb128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_adds_epu16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_paddusw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_avg_epu8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pavgb128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_avg_epu16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pavgw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_madd_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pmaddwd128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_max_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pmaxsw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_max_epu8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pmaxub128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_min_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pminsw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_min_epu8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pminub128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_mulhi_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pmulhw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_mulhi_epu16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_pmulhuw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_mullo_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v8hi)__a * (__v8hi)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_mul_su32(__m64 __a, __m64 __b)
+{
+  return __builtin_ia32_pmuludq((__v2si)__a, (__v2si)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_mul_epu32(__m128i __a, __m128i __b)
+{
+  return __builtin_ia32_pmuludq128((__v4si)__a, (__v4si)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sad_epu8(__m128i __a, __m128i __b)
+{
+  return __builtin_ia32_psadbw128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sub_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v16qi)__a - (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sub_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v8hi)__a - (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sub_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v4si)__a - (__v4si)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sub_si64(__m64 __a, __m64 __b)
+{
+  return __a - __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sub_epi64(__m128i __a, __m128i __b)
+{
+  return __a - __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_subs_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_psubsb128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_subs_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_psubsw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_subs_epu8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_psubusb128((__v16qi)__a, (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_subs_epu16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_psubusw128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_and_si128(__m128i __a, __m128i __b)
+{
+  return __a & __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_andnot_si128(__m128i __a, __m128i __b)
+{
+  return ~__a & __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_or_si128(__m128i __a, __m128i __b)
+{
+  return __a | __b;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_xor_si128(__m128i __a, __m128i __b)
+{
+  return __a ^ __b;
+}
+
+#define _mm_slli_si128(a, count) __extension__ ({ \
+  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
+  __m128i __a = (a); \
+   _Pragma("clang diagnostic pop"); \
+  (__m128i)__builtin_ia32_pslldqi128(__a, (count)*8); })
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_slli_epi16(__m128i __a, int __count)
+{
+  return (__m128i)__builtin_ia32_psllwi128((__v8hi)__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sll_epi16(__m128i __a, __m128i __count)
+{
+  return (__m128i)__builtin_ia32_psllw128((__v8hi)__a, (__v8hi)__count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_slli_epi32(__m128i __a, int __count)
+{
+  return (__m128i)__builtin_ia32_pslldi128((__v4si)__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sll_epi32(__m128i __a, __m128i __count)
+{
+  return (__m128i)__builtin_ia32_pslld128((__v4si)__a, (__v4si)__count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_slli_epi64(__m128i __a, int __count)
+{
+  return __builtin_ia32_psllqi128(__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sll_epi64(__m128i __a, __m128i __count)
+{
+  return __builtin_ia32_psllq128(__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srai_epi16(__m128i __a, int __count)
+{
+  return (__m128i)__builtin_ia32_psrawi128((__v8hi)__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sra_epi16(__m128i __a, __m128i __count)
+{
+  return (__m128i)__builtin_ia32_psraw128((__v8hi)__a, (__v8hi)__count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srai_epi32(__m128i __a, int __count)
+{
+  return (__m128i)__builtin_ia32_psradi128((__v4si)__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_sra_epi32(__m128i __a, __m128i __count)
+{
+  return (__m128i)__builtin_ia32_psrad128((__v4si)__a, (__v4si)__count);
+}
+
+
+#define _mm_srli_si128(a, count) __extension__ ({ \
+  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
+  __m128i __a = (a); \
+  _Pragma("clang diagnostic pop"); \
+  (__m128i)__builtin_ia32_psrldqi128(__a, (count)*8); })
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srli_epi16(__m128i __a, int __count)
+{
+  return (__m128i)__builtin_ia32_psrlwi128((__v8hi)__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srl_epi16(__m128i __a, __m128i __count)
+{
+  return (__m128i)__builtin_ia32_psrlw128((__v8hi)__a, (__v8hi)__count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srli_epi32(__m128i __a, int __count)
+{
+  return (__m128i)__builtin_ia32_psrldi128((__v4si)__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srl_epi32(__m128i __a, __m128i __count)
+{
+  return (__m128i)__builtin_ia32_psrld128((__v4si)__a, (__v4si)__count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srli_epi64(__m128i __a, int __count)
+{
+  return __builtin_ia32_psrlqi128(__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_srl_epi64(__m128i __a, __m128i __count)
+{
+  return __builtin_ia32_psrlq128(__a, __count);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v16qi)__a == (__v16qi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v8hi)__a == (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v4si)__a == (__v4si)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_epi8(__m128i __a, __m128i __b)
+{
+  /* This function always performs a signed comparison, but __v16qi is a char
+     which may be signed or unsigned. */
+  typedef signed char __v16qs __attribute__((__vector_size__(16)));
+  return (__m128i)((__v16qs)__a > (__v16qs)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v8hi)__a > (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)((__v4si)__a > (__v4si)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_epi8(__m128i __a, __m128i __b)
+{
+  return _mm_cmpgt_epi8(__b, __a);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_epi16(__m128i __a, __m128i __b)
+{
+  return _mm_cmpgt_epi16(__b, __a);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_epi32(__m128i __a, __m128i __b)
+{
+  return _mm_cmpgt_epi32(__b, __a);
+}
+
+#ifdef __x86_64__
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi64_sd(__m128d __a, long long __b)
+{
+  __a[0] = __b;
+  return __a;
+}
+
+static __inline__ long long __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsd_si64(__m128d __a)
+{
+  return __builtin_ia32_cvtsd2si64(__a);
+}
+
+static __inline__ long long __attribute__((__always_inline__, __nodebug__))
+_mm_cvttsd_si64(__m128d __a)
+{
+  return __a[0];
+}
+#endif
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtepi32_ps(__m128i __a)
+{
+  return __builtin_ia32_cvtdq2ps((__v4si)__a);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cvtps_epi32(__m128 __a)
+{
+  return (__m128i)__builtin_ia32_cvtps2dq(__a);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cvttps_epi32(__m128 __a)
+{
+  return (__m128i)__builtin_ia32_cvttps2dq(__a);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi32_si128(int __a)
+{
+  return (__m128i)(__v4si){ __a, 0, 0, 0 };
+}
+
+#ifdef __x86_64__
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi64_si128(long long __a)
+{
+  return (__m128i){ __a, 0 };
+}
+#endif
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi128_si32(__m128i __a)
+{
+  __v4si __b = (__v4si)__a;
+  return __b[0];
+}
+
+#ifdef __x86_64__
+static __inline__ long long __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi128_si64(__m128i __a)
+{
+  return __a[0];
+}
+#endif
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_load_si128(__m128i const *__p)
+{
+  return *__p;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_loadu_si128(__m128i const *__p)
+{
+  struct __loadu_si128 {
+    __m128i __v;
+  } __attribute__((packed, may_alias));
+  return ((struct __loadu_si128*)__p)->__v;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_loadl_epi64(__m128i const *__p)
+{
+  struct __mm_loadl_epi64_struct {
+    long long __u;
+  } __attribute__((__packed__, __may_alias__));
+  return (__m128i) { ((struct __mm_loadl_epi64_struct*)__p)->__u, 0};
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set_epi64x(long long q1, long long q0)
+{
+  return (__m128i){ q0, q1 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set_epi64(__m64 q1, __m64 q0)
+{
+  return (__m128i){ (long long)q0, (long long)q1 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set_epi32(int i3, int i2, int i1, int i0)
+{
+  return (__m128i)(__v4si){ i0, i1, i2, i3};
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set_epi16(short w7, short w6, short w5, short w4, short w3, short w2, short w1, short w0)
+{
+  return (__m128i)(__v8hi){ w0, w1, w2, w3, w4, w5, w6, w7 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set_epi8(char b15, char b14, char b13, char b12, char b11, char b10, char b9, char b8, char b7, char b6, char b5, char b4, char b3, char b2, char b1, char b0)
+{
+  return (__m128i)(__v16qi){ b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set1_epi64x(long long __q)
+{
+  return (__m128i){ __q, __q };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set1_epi64(__m64 __q)
+{
+  return (__m128i){ (long long)__q, (long long)__q };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set1_epi32(int __i)
+{
+  return (__m128i)(__v4si){ __i, __i, __i, __i };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set1_epi16(short __w)
+{
+  return (__m128i)(__v8hi){ __w, __w, __w, __w, __w, __w, __w, __w };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_set1_epi8(char __b)
+{
+  return (__m128i)(__v16qi){ __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_setr_epi64(__m64 q0, __m64 q1)
+{
+  return (__m128i){ (long long)q0, (long long)q1 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_setr_epi32(int i0, int i1, int i2, int i3)
+{
+  return (__m128i)(__v4si){ i0, i1, i2, i3};
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_setr_epi16(short w0, short w1, short w2, short w3, short w4, short w5, short w6, short w7)
+{
+  return (__m128i)(__v8hi){ w0, w1, w2, w3, w4, w5, w6, w7 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_setr_epi8(char b0, char b1, char b2, char b3, char b4, char b5, char b6, char b7, char b8, char b9, char b10, char b11, char b12, char b13, char b14, char b15)
+{
+  return (__m128i)(__v16qi){ b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_setzero_si128(void)
+{
+  return (__m128i){ 0LL, 0LL };
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store_si128(__m128i *__p, __m128i __b)
+{
+  *__p = __b;
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storeu_si128(__m128i *__p, __m128i __b)
+{
+  __builtin_ia32_storedqu((char *)__p, (__v16qi)__b);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_maskmoveu_si128(__m128i __d, __m128i __n, char *__p)
+{
+  __builtin_ia32_maskmovdqu((__v16qi)__d, (__v16qi)__n, __p);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storel_epi64(__m128i *__p, __m128i __a)
+{
+  struct __mm_storel_epi64_struct {
+    long long __u;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __mm_storel_epi64_struct*)__p)->__u = __a[0];
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_stream_pd(double *__p, __m128d __a)
+{
+  __builtin_ia32_movntpd(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_stream_si128(__m128i *__p, __m128i __a)
+{
+  __builtin_ia32_movntdq(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_stream_si32(int *__p, int __a)
+{
+  __builtin_ia32_movnti(__p, __a);
+}
+
+#ifdef __x86_64__
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_stream_si64(long long *__p, long long __a)
+{
+  __builtin_ia32_movnti64(__p, __a);
+}
+#endif
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_clflush(void const *__p)
+{
+  __builtin_ia32_clflush(__p);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_lfence(void)
+{
+  __builtin_ia32_lfence();
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_mfence(void)
+{
+  __builtin_ia32_mfence();
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_packs_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_packsswb128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_packs_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_packssdw128((__v4si)__a, (__v4si)__b);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_packus_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_ia32_packuswb128((__v8hi)__a, (__v8hi)__b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_extract_epi16(__m128i __a, int __imm)
+{
+  __v8hi __b = (__v8hi)__a;
+  return (unsigned short)__b[__imm & 7];
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_insert_epi16(__m128i __a, int __b, int __imm)
+{
+  __v8hi __c = (__v8hi)__a;
+  __c[__imm & 7] = __b;
+  return (__m128i)__c;
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_movemask_epi8(__m128i __a)
+{
+  return __builtin_ia32_pmovmskb128((__v16qi)__a);
+}
+
+#define _mm_shuffle_epi32(a, imm) __extension__ ({ \
+  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
+  __m128i __a = (a); \
+  _Pragma("clang diagnostic pop"); \
+  (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si) _mm_set1_epi32(0), \
+                                   (imm) & 0x3, ((imm) & 0xc) >> 2, \
+                                   ((imm) & 0x30) >> 4, ((imm) & 0xc0) >> 6); })
+
+#define _mm_shufflelo_epi16(a, imm) __extension__ ({ \
+  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
+  __m128i __a = (a); \
+  _Pragma("clang diagnostic pop"); \
+  (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi) _mm_set1_epi16(0), \
+                                   (imm) & 0x3, ((imm) & 0xc) >> 2, \
+                                   ((imm) & 0x30) >> 4, ((imm) & 0xc0) >> 6, \
+                                   4, 5, 6, 7); })
+
+#define _mm_shufflehi_epi16(a, imm) __extension__ ({ \
+  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
+  __m128i __a = (a); \
+  _Pragma("clang diagnostic pop"); \
+  (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi) _mm_set1_epi16(0), \
+                                   0, 1, 2, 3, \
+                                   4 + (((imm) & 0x03) >> 0), \
+                                   4 + (((imm) & 0x0c) >> 2), \
+                                   4 + (((imm) & 0x30) >> 4), \
+                                   4 + (((imm) & 0xc0) >> 6)); })
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 4, 8+4, 5, 8+5, 6, 8+6, 7, 8+7);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 2, 4+2, 3, 4+3);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_epi64(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector(__a, __b, 1, 2+1);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_epi8(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_epi16(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 0, 8+0, 1, 8+1, 2, 8+2, 3, 8+3);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_epi32(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 0, 4+0, 1, 4+1);
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_epi64(__m128i __a, __m128i __b)
+{
+  return (__m128i)__builtin_shufflevector(__a, __b, 0, 2+0);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_movepi64_pi64(__m128i __a)
+{
+  return (__m64)__a[0];
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_movpi64_epi64(__m64 __a)
+{
+  return (__m128i){ (long long)__a, 0 };
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_move_epi64(__m128i __a)
+{
+  return __builtin_shufflevector(__a, (__m128i){ 0 }, 0, 2);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_pd(__m128d __a, __m128d __b)
+{
+  return __builtin_shufflevector(__a, __b, 1, 2+1);
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_pd(__m128d __a, __m128d __b)
+{
+  return __builtin_shufflevector(__a, __b, 0, 2+0);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_movemask_pd(__m128d __a)
+{
+  return __builtin_ia32_movmskpd(__a);
+}
+
+#define _mm_shuffle_pd(a, b, i) __extension__ ({ \
+  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
+  __m128d __a = (a); \
+  __m128d __b = (b); \
+  _Pragma("clang diagnostic pop"); \
+  __builtin_shufflevector(__a, __b, (i) & 1, (((i) & 2) >> 1) + 2); })
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_castpd_ps(__m128d __a)
+{
+  return (__m128)__a;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_castpd_si128(__m128d __a)
+{
+  return (__m128i)__a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_castps_pd(__m128 __a)
+{
+  return (__m128d)__a;
+}
+
+static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
+_mm_castps_si128(__m128 __a)
+{
+  return (__m128i)__a;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_castsi128_ps(__m128i __a)
+{
+  return (__m128)__a;
+}
+
+static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
+_mm_castsi128_pd(__m128i __a)
+{
+  return (__m128d)__a;
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_pause(void)
+{
+  __asm__ volatile ("pause");
+}
+
+#define _MM_SHUFFLE2(x, y) (((x) << 1) | (y))
+
+#endif /* __SSE2__ */
+
+#endif /* __EMMINTRIN_H */
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/mm_malloc.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/mm_malloc.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/mm_malloc.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/mm_malloc.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,75 @@
+/*===---- mm_malloc.h - Allocating and Freeing Aligned Memory Blocks -------===
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#ifndef __MM_MALLOC_H
+#define __MM_MALLOC_H
+
+#include <stdlib.h>
+
+#ifdef _WIN32
+#include <malloc.h>
+#else
+#ifndef __cplusplus
+extern int posix_memalign(void **__memptr, size_t __alignment, size_t __size);
+#else
+// Some systems (e.g. those with GNU libc) declare posix_memalign with an
+// exception specifier. Via an "egregious workaround" in
+// Sema::CheckEquivalentExceptionSpec, Clang accepts the following as a valid
+// redeclaration of glibc's declaration.
+extern "C" int posix_memalign(void **__memptr, size_t __alignment, size_t __size);
+#endif
+#endif
+
+#if !(defined(_WIN32) && defined(_mm_malloc))
+static __inline__ void *__attribute__((__always_inline__, __nodebug__,
+                                       __malloc__))
+_mm_malloc(size_t __size, size_t __align)
+{
+  if (__align == 1) {
+    return malloc(__size);
+  }
+
+  if (!(__align & (__align - 1)) && __align < sizeof(void *))
+    __align = sizeof(void *);
+
+  void *__mallocedMemory;
+#if defined(__MINGW32__)
+  __mallocedMemory = __mingw_aligned_malloc(__size, __align);
+#elif defined(_WIN32)
+  __mallocedMemory = _aligned_malloc(__size, __align);
+#else
+  if (posix_memalign(&__mallocedMemory, __align, __size))
+    return 0;
+#endif
+
+  return __mallocedMemory;
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_free(void *__p)
+{
+  free(__p);
+}
+#endif
+
+#endif /* __MM_MALLOC_H */
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/mmintrin.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/mmintrin.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/mmintrin.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/mmintrin.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,503 @@
+/*===---- mmintrin.h - MMX intrinsics --------------------------------------===
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#ifndef __MMINTRIN_H
+#define __MMINTRIN_H
+
+#ifndef __MMX__
+#error "MMX instruction set not enabled"
+#else
+
+typedef long long __m64 __attribute__((__vector_size__(8)));
+
+typedef int __v2si __attribute__((__vector_size__(8)));
+typedef short __v4hi __attribute__((__vector_size__(8)));
+typedef char __v8qi __attribute__((__vector_size__(8)));
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_empty(void)
+{
+    __builtin_ia32_emms();
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi32_si64(int __i)
+{
+    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi64_si32(__m64 __m)
+{
+    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi64_m64(long long __i)
+{
+    return (__m64)__i;
+}
+
+static __inline__ long long __attribute__((__always_inline__, __nodebug__))
+_mm_cvtm64_si64(__m64 __m)
+{
+    return (long long)__m;
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_packs_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_packs_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_packs_pu16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_add_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_add_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_add_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_adds_pi8(__m64 __m1, __m64 __m2) 
+{
+    return (__m64)__builtin_ia32_paddsb((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_adds_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_paddsw((__v4hi)__m1, (__v4hi)__m2);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_adds_pu8(__m64 __m1, __m64 __m2) 
+{
+    return (__m64)__builtin_ia32_paddusb((__v8qi)__m1, (__v8qi)__m2);
+}
+ 
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_adds_pu16(__m64 __m1, __m64 __m2) 
+{
+    return (__m64)__builtin_ia32_paddusw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sub_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubb((__v8qi)__m1, (__v8qi)__m2);
+}
+ 
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sub_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubw((__v4hi)__m1, (__v4hi)__m2);
+}
+ 
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sub_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubd((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_subs_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubsb((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_subs_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubsw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_subs_pu8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubusb((__v8qi)__m1, (__v8qi)__m2);
+}
+ 
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_subs_pu16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_psubusw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_madd_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pmaddwd((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_mulhi_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pmulhw((__v4hi)__m1, (__v4hi)__m2);
+}
+ 
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_mullo_pi16(__m64 __m1, __m64 __m2) 
+{
+    return (__m64)__builtin_ia32_pmullw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sll_pi16(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psllw((__v4hi)__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_slli_pi16(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psllwi((__v4hi)__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sll_pi32(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_pslld((__v2si)__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_slli_pi32(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_pslldi((__v2si)__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sll_si64(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psllq(__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_slli_si64(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psllqi(__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sra_pi16(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psraw((__v4hi)__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srai_pi16(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psrawi((__v4hi)__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sra_pi32(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psrad((__v2si)__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srai_pi32(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psradi((__v2si)__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srl_pi16(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psrlw((__v4hi)__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srli_pi16(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psrlwi((__v4hi)__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srl_pi32(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psrld((__v2si)__m, __count);       
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srli_pi32(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psrldi((__v2si)__m, __count);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srl_si64(__m64 __m, __m64 __count)
+{
+    return (__m64)__builtin_ia32_psrlq(__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_srli_si64(__m64 __m, int __count)
+{
+    return (__m64)__builtin_ia32_psrlqi(__m, __count);    
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_and_si64(__m64 __m1, __m64 __m2)
+{
+    return __builtin_ia32_pand(__m1, __m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_andnot_si64(__m64 __m1, __m64 __m2)
+{
+    return __builtin_ia32_pandn(__m1, __m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_or_si64(__m64 __m1, __m64 __m2)
+{
+    return __builtin_ia32_por(__m1, __m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_xor_si64(__m64 __m1, __m64 __m2)
+{
+    return __builtin_ia32_pxor(__m1, __m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pcmpeqb((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pcmpeqw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pcmpeqd((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_pi8(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pcmpgtb((__v8qi)__m1, (__v8qi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_pi16(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pcmpgtw((__v4hi)__m1, (__v4hi)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_pi32(__m64 __m1, __m64 __m2)
+{
+    return (__m64)__builtin_ia32_pcmpgtd((__v2si)__m1, (__v2si)__m2);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_setzero_si64(void)
+{
+    return (__m64){ 0LL };
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_set_pi32(int __i1, int __i0)
+{
+    return (__m64)__builtin_ia32_vec_init_v2si(__i0, __i1);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_set_pi16(short __s3, short __s2, short __s1, short __s0)
+{
+    return (__m64)__builtin_ia32_vec_init_v4hi(__s0, __s1, __s2, __s3);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_set_pi8(char __b7, char __b6, char __b5, char __b4, char __b3, char __b2,
+            char __b1, char __b0)
+{
+    return (__m64)__builtin_ia32_vec_init_v8qi(__b0, __b1, __b2, __b3,
+                                               __b4, __b5, __b6, __b7);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_set1_pi32(int __i)
+{
+    return _mm_set_pi32(__i, __i);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_set1_pi16(short __w)
+{
+    return _mm_set_pi16(__w, __w, __w, __w);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_set1_pi8(char __b)
+{
+    return _mm_set_pi8(__b, __b, __b, __b, __b, __b, __b, __b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_setr_pi32(int __i0, int __i1)
+{
+    return _mm_set_pi32(__i1, __i0);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_setr_pi16(short __w0, short __w1, short __w2, short __w3)
+{
+    return _mm_set_pi16(__w3, __w2, __w1, __w0);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_setr_pi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
+             char __b6, char __b7)
+{
+    return _mm_set_pi8(__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
+}
+
+
+/* Aliases for compatibility. */
+#define _m_empty _mm_empty
+#define _m_from_int _mm_cvtsi32_si64
+#define _m_to_int _mm_cvtsi64_si32
+#define _m_packsswb _mm_packs_pi16
+#define _m_packssdw _mm_packs_pi32
+#define _m_packuswb _mm_packs_pu16
+#define _m_punpckhbw _mm_unpackhi_pi8
+#define _m_punpckhwd _mm_unpackhi_pi16
+#define _m_punpckhdq _mm_unpackhi_pi32
+#define _m_punpcklbw _mm_unpacklo_pi8
+#define _m_punpcklwd _mm_unpacklo_pi16
+#define _m_punpckldq _mm_unpacklo_pi32
+#define _m_paddb _mm_add_pi8
+#define _m_paddw _mm_add_pi16
+#define _m_paddd _mm_add_pi32
+#define _m_paddsb _mm_adds_pi8
+#define _m_paddsw _mm_adds_pi16
+#define _m_paddusb _mm_adds_pu8
+#define _m_paddusw _mm_adds_pu16
+#define _m_psubb _mm_sub_pi8
+#define _m_psubw _mm_sub_pi16
+#define _m_psubd _mm_sub_pi32
+#define _m_psubsb _mm_subs_pi8
+#define _m_psubsw _mm_subs_pi16
+#define _m_psubusb _mm_subs_pu8
+#define _m_psubusw _mm_subs_pu16
+#define _m_pmaddwd _mm_madd_pi16
+#define _m_pmulhw _mm_mulhi_pi16
+#define _m_pmullw _mm_mullo_pi16
+#define _m_psllw _mm_sll_pi16
+#define _m_psllwi _mm_slli_pi16
+#define _m_pslld _mm_sll_pi32
+#define _m_pslldi _mm_slli_pi32
+#define _m_psllq _mm_sll_si64
+#define _m_psllqi _mm_slli_si64
+#define _m_psraw _mm_sra_pi16
+#define _m_psrawi _mm_srai_pi16
+#define _m_psrad _mm_sra_pi32
+#define _m_psradi _mm_srai_pi32
+#define _m_psrlw _mm_srl_pi16
+#define _m_psrlwi _mm_srli_pi16
+#define _m_psrld _mm_srl_pi32
+#define _m_psrldi _mm_srli_pi32
+#define _m_psrlq _mm_srl_si64
+#define _m_psrlqi _mm_srli_si64
+#define _m_pand _mm_and_si64
+#define _m_pandn _mm_andnot_si64
+#define _m_por _mm_or_si64
+#define _m_pxor _mm_xor_si64
+#define _m_pcmpeqb _mm_cmpeq_pi8
+#define _m_pcmpeqw _mm_cmpeq_pi16
+#define _m_pcmpeqd _mm_cmpeq_pi32
+#define _m_pcmpgtb _mm_cmpgt_pi8
+#define _m_pcmpgtw _mm_cmpgt_pi16
+#define _m_pcmpgtd _mm_cmpgt_pi32
+
+#endif /* __MMX__ */
+
+#endif /* __MMINTRIN_H */
+
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/pmmintrin.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/pmmintrin.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/pmmintrin.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/pmmintrin.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,29 @@
+/*===---- pmmintrin.h - SSE3 intrinsics ------------------------------------===
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+ 
+#ifndef __PMMINTRIN_H
+#define __PMMINTRIN_H
+
+#include <emmintrin.h>
+
+#endif /* __PMMINTRIN_H */
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/tmmintrin.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/tmmintrin.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/tmmintrin.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/tmmintrin.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,304 @@
+/* APPLE LOCAL file ssse3 4424835 */
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING.  If not, write to
+   the Free Software Foundation, 59 Temple Place - Suite 330,
+   Boston, MA 02111-1307, USA.  */
+
+/* As a special exception, if you include this header file into source
+   files compiled by GCC, this header file does not by itself cause
+   the resulting executable to be covered by the GNU General Public
+   License.  This exception does not however invalidate any other
+   reasons why the executable file might be covered by the GNU General
+   Public License.  */
+
+/* Implemented from the specification included in the Intel C++ Compiler
+   User Guide and Reference, version 9.1.  */
+
+#ifndef _TMMINTRIN_H_INCLUDED
+#define _TMMINTRIN_H_INCLUDED
+
+#ifdef __SSSE3__
+#include <pmmintrin.h>
+
+/* APPLE LOCAL begin nodebug inline */
+#define __always_inline__ __always_inline__, __nodebug__
+/* APPLE LOCAL end nodebug inline */
+
+/* APPLE LOCAL begin radar 5618945 */
+#undef __STATIC_INLINE
+#ifdef __GNUC_STDC_INLINE__
+#define __STATIC_INLINE __inline
+#else
+#define __STATIC_INLINE static __inline
+#endif
+/* APPLE LOCAL end radar 5618945 */
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hadd_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_phaddw128 ((__v8hi)__X, (__v8hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hadd_epi32 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_phaddd128 ((__v4si)__X, (__v4si)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hadds_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_phaddsw128 ((__v8hi)__X, (__v8hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hadd_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_phaddw ((__v4hi)__X, (__v4hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hadd_pi32 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_phaddd ((__v2si)__X, (__v2si)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hadds_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_phaddsw ((__v4hi)__X, (__v4hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hsub_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_phsubw128 ((__v8hi)__X, (__v8hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hsub_epi32 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_phsubd128 ((__v4si)__X, (__v4si)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hsubs_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_phsubsw128 ((__v8hi)__X, (__v8hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hsub_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_phsubw ((__v4hi)__X, (__v4hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hsub_pi32 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_phsubd ((__v2si)__X, (__v2si)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_hsubs_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_phsubsw ((__v4hi)__X, (__v4hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_maddubs_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_pmaddubsw128 ((__v16qi)__X, (__v16qi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_maddubs_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_pmaddubsw ((__v8qi)__X, (__v8qi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_mulhrs_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_pmulhrsw128 ((__v8hi)__X, (__v8hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_mulhrs_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_pmulhrsw ((__v4hi)__X, (__v4hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_shuffle_epi8 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_pshufb128 ((__v16qi)__X, (__v16qi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_shuffle_pi8 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_pshufb ((__v8qi)__X, (__v8qi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_sign_epi8 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_psignb128 ((__v16qi)__X, (__v16qi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_sign_epi16 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_psignw128 ((__v8hi)__X, (__v8hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_sign_epi32 (__m128i __X, __m128i __Y)
+{
+  return (__m128i) __builtin_ia32_psignd128 ((__v4si)__X, (__v4si)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_sign_pi8 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_psignb ((__v8qi)__X, (__v8qi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_sign_pi16 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_psignw ((__v4hi)__X, (__v4hi)__Y);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_sign_pi32 (__m64 __X, __m64 __Y)
+{
+  return (__m64) __builtin_ia32_psignd ((__v2si)__X, (__v2si)__Y);
+}
+
+/* APPLE LOCAL begin 5814283 */
+#define _mm_alignr_epi8(__X, __Y, __N) \
+  ((__m128i)__builtin_ia32_palignr128 ((__v2di)(__X), (__v2di)(__Y), (__N) * 8))
+/* APPLE LOCAL end 5814283 */
+
+#define _mm_alignr_pi8(__X, __Y, __N) \
+  ((__m64)__builtin_ia32_palignr ((long long) (__X), (long long) (__Y), (__N) * 8))
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_abs_epi8 (__m128i __X)
+{
+  return (__m128i) __builtin_ia32_pabsb128 ((__v16qi)__X);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_abs_epi16 (__m128i __X)
+{
+  return (__m128i) __builtin_ia32_pabsw128 ((__v8hi)__X);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m128i __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_abs_epi32 (__m128i __X)
+{
+  return (__m128i) __builtin_ia32_pabsd128 ((__v4si)__X);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_abs_pi8 (__m64 __X)
+{
+  return (__m64) __builtin_ia32_pabsb ((__v8qi)__X);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_abs_pi16 (__m64 __X)
+{
+  return (__m64) __builtin_ia32_pabsw ((__v4hi)__X);
+}
+
+/* APPLE LOCAL begin radar 5618945 */
+__STATIC_INLINE __m64 __attribute__((__always_inline__))
+/* APPLE LOCAL end radar 5618945 */
+_mm_abs_pi32 (__m64 __X)
+{
+  return (__m64) __builtin_ia32_pabsd ((__v2si)__X);
+}
+
+/* APPLE LOCAL begin nodebug inline */
+#undef __always_inline__
+/* APPLE LOCAL end nodebug inline */
+
+#endif /* __SSSE3__ */
+
+#endif /* _TMMINTRIN_H_INCLUDED */
diff -Nur xnu-3247.1.106/EXTERNAL_HEADERS/xmmintrin.h xnu-3247.1.106-AnV/EXTERNAL_HEADERS/xmmintrin.h
--- xnu-3247.1.106/EXTERNAL_HEADERS/xmmintrin.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/EXTERNAL_HEADERS/xmmintrin.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1001 @@
+/*===---- xmmintrin.h - SSE intrinsics -------------------------------------===
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+ 
+#ifndef __XMMINTRIN_H
+#define __XMMINTRIN_H
+ 
+#ifndef __SSE__
+#error "SSE instruction set not enabled"
+#else
+
+#include <mmintrin.h>
+
+typedef int __v4si __attribute__((__vector_size__(16)));
+typedef float __v4sf __attribute__((__vector_size__(16)));
+typedef float __m128 __attribute__((__vector_size__(16)));
+
+// This header should only be included in a hosted environment as it depends on
+// a standard library to provide allocation routines.
+#if __STDC_HOSTED__
+#include <mm_malloc.h>
+#endif
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_add_ss(__m128 __a, __m128 __b)
+{
+  __a[0] += __b[0];
+  return __a;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_add_ps(__m128 __a, __m128 __b)
+{
+  return __a + __b;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_sub_ss(__m128 __a, __m128 __b)
+{
+  __a[0] -= __b[0];
+  return __a;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_sub_ps(__m128 __a, __m128 __b)
+{
+  return __a - __b;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_mul_ss(__m128 __a, __m128 __b)
+{
+  __a[0] *= __b[0];
+  return __a;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_mul_ps(__m128 __a, __m128 __b)
+{
+  return __a * __b;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_div_ss(__m128 __a, __m128 __b)
+{
+  __a[0] /= __b[0];
+  return __a;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_div_ps(__m128 __a, __m128 __b)
+{
+  return __a / __b;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_sqrt_ss(__m128 __a)
+{
+  __m128 __c = __builtin_ia32_sqrtss(__a);
+  return (__m128) { __c[0], __a[1], __a[2], __a[3] };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_sqrt_ps(__m128 __a)
+{
+  return __builtin_ia32_sqrtps(__a);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_rcp_ss(__m128 __a)
+{
+  __m128 __c = __builtin_ia32_rcpss(__a);
+  return (__m128) { __c[0], __a[1], __a[2], __a[3] };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_rcp_ps(__m128 __a)
+{
+  return __builtin_ia32_rcpps(__a);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_rsqrt_ss(__m128 __a)
+{
+  __m128 __c = __builtin_ia32_rsqrtss(__a);
+  return (__m128) { __c[0], __a[1], __a[2], __a[3] };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_rsqrt_ps(__m128 __a)
+{
+  return __builtin_ia32_rsqrtps(__a);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_min_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_minss(__a, __b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_min_ps(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_minps(__a, __b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_max_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_maxss(__a, __b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_max_ps(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_maxps(__a, __b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_and_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)((__v4si)__a & (__v4si)__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_andnot_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)(~(__v4si)__a & (__v4si)__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_or_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)((__v4si)__a | (__v4si)__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_xor_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)((__v4si)__a ^ (__v4si)__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 0);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpeq_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 0);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 1);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmplt_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 1);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmple_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 2);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmple_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 2);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_shufflevector(__a,
+                                         __builtin_ia32_cmpss(__b, __a, 1),
+                                         4, 1, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpgt_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__b, __a, 1);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpge_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_shufflevector(__a,
+                                         __builtin_ia32_cmpss(__b, __a, 2),
+                                         4, 1, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpge_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__b, __a, 2);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpneq_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 4);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpneq_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 4);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnlt_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 5);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnlt_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 5);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnle_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 6);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnle_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 6);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpngt_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_shufflevector(__a,
+                                         __builtin_ia32_cmpss(__b, __a, 5),
+                                         4, 1, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpngt_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__b, __a, 5);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnge_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_shufflevector(__a,
+                                         __builtin_ia32_cmpss(__b, __a, 6),
+                                         4, 1, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpnge_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__b, __a, 6);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpord_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 7);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpord_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 7);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpunord_ss(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpss(__a, __b, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cmpunord_ps(__m128 __a, __m128 __b)
+{
+  return (__m128)__builtin_ia32_cmpps(__a, __b, 3);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comieq_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_comieq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comilt_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_comilt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comile_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_comile(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comigt_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_comigt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comige_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_comige(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_comineq_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_comineq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomieq_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_ucomieq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomilt_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_ucomilt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomile_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_ucomile(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomigt_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_ucomigt(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomige_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_ucomige(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_ucomineq_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_ia32_ucomineq(__a, __b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvtss_si32(__m128 __a)
+{
+  return __builtin_ia32_cvtss2si(__a);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvt_ss2si(__m128 __a)
+{
+  return _mm_cvtss_si32(__a);
+}
+
+#ifdef __x86_64__
+
+static __inline__ long long __attribute__((__always_inline__, __nodebug__))
+_mm_cvtss_si64(__m128 __a)
+{
+  return __builtin_ia32_cvtss2si64(__a);
+}
+
+#endif
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtps_pi32(__m128 __a)
+{
+  return (__m64)__builtin_ia32_cvtps2pi(__a);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvt_ps2pi(__m128 __a)
+{
+  return _mm_cvtps_pi32(__a);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvttss_si32(__m128 __a)
+{
+  return __a[0];
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_cvtt_ss2si(__m128 __a)
+{
+  return _mm_cvttss_si32(__a);
+}
+
+static __inline__ long long __attribute__((__always_inline__, __nodebug__))
+_mm_cvttss_si64(__m128 __a)
+{
+  return __a[0];
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvttps_pi32(__m128 __a)
+{
+  return (__m64)__builtin_ia32_cvttps2pi(__a);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtt_ps2pi(__m128 __a)
+{
+  return _mm_cvttps_pi32(__a);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi32_ss(__m128 __a, int __b)
+{
+  __a[0] = __b;
+  return __a;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvt_si2ss(__m128 __a, int __b)
+{
+  return _mm_cvtsi32_ss(__a, __b);
+}
+
+#ifdef __x86_64__
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtsi64_ss(__m128 __a, long long __b)
+{
+  __a[0] = __b;
+  return __a;
+}
+
+#endif
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpi32_ps(__m128 __a, __m64 __b)
+{
+  return __builtin_ia32_cvtpi2ps(__a, (__v2si)__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvt_pi2ps(__m128 __a, __m64 __b)
+{
+  return _mm_cvtpi32_ps(__a, __b);
+}
+
+static __inline__ float __attribute__((__always_inline__, __nodebug__))
+_mm_cvtss_f32(__m128 __a)
+{
+  return __a[0];
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_loadh_pi(__m128 __a, const __m64 *__p)
+{
+  typedef float __mm_loadh_pi_v2f32 __attribute__((__vector_size__(8)));
+  struct __mm_loadh_pi_struct {
+    __mm_loadh_pi_v2f32 __u;
+  } __attribute__((__packed__, __may_alias__));
+  __mm_loadh_pi_v2f32 __b = ((struct __mm_loadh_pi_struct*)__p)->__u;
+  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
+  return __builtin_shufflevector(__a, __bb, 0, 1, 4, 5);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_loadl_pi(__m128 __a, const __m64 *__p)
+{
+  typedef float __mm_loadl_pi_v2f32 __attribute__((__vector_size__(8)));
+  struct __mm_loadl_pi_struct {
+    __mm_loadl_pi_v2f32 __u;
+  } __attribute__((__packed__, __may_alias__));
+  __mm_loadl_pi_v2f32 __b = ((struct __mm_loadl_pi_struct*)__p)->__u;
+  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
+  return __builtin_shufflevector(__a, __bb, 4, 5, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_load_ss(const float *__p)
+{
+  struct __mm_load_ss_struct {
+    float __u;
+  } __attribute__((__packed__, __may_alias__));
+  float __u = ((struct __mm_load_ss_struct*)__p)->__u;
+  return (__m128){ __u, 0, 0, 0 };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_load1_ps(const float *__p)
+{
+  struct __mm_load1_ps_struct {
+    float __u;
+  } __attribute__((__packed__, __may_alias__));
+  float __u = ((struct __mm_load1_ps_struct*)__p)->__u;
+  return (__m128){ __u, __u, __u, __u };
+}
+
+#define        _mm_load_ps1(p) _mm_load1_ps(p)
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_load_ps(const float *__p)
+{
+  return *(__m128*)__p;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_loadu_ps(const float *__p)
+{
+  struct __loadu_ps {
+    __m128 __v;
+  } __attribute__((__packed__, __may_alias__));
+  return ((struct __loadu_ps*)__p)->__v;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_loadr_ps(const float *__p)
+{
+  __m128 __a = _mm_load_ps(__p);
+  return __builtin_shufflevector(__a, __a, 3, 2, 1, 0);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_set_ss(float __w)
+{
+  return (__m128){ __w, 0, 0, 0 };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_set1_ps(float __w)
+{
+  return (__m128){ __w, __w, __w, __w };
+}
+
+// Microsoft specific.
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_set_ps1(float __w)
+{
+    return _mm_set1_ps(__w);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_set_ps(float __z, float __y, float __x, float __w)
+{
+  return (__m128){ __w, __x, __y, __z };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_setr_ps(float __z, float __y, float __x, float __w)
+{
+  return (__m128){ __z, __y, __x, __w };
+}
+
+static __inline__ __m128 __attribute__((__always_inline__))
+_mm_setzero_ps(void)
+{
+  return (__m128){ 0, 0, 0, 0 };
+}
+
+static __inline__ void __attribute__((__always_inline__))
+_mm_storeh_pi(__m64 *__p, __m128 __a)
+{
+  __builtin_ia32_storehps((__v2si *)__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__))
+_mm_storel_pi(__m64 *__p, __m128 __a)
+{
+  __builtin_ia32_storelps((__v2si *)__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__))
+_mm_store_ss(float *__p, __m128 __a)
+{
+  struct __mm_store_ss_struct {
+    float __u;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __mm_store_ss_struct*)__p)->__u = __a[0];
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storeu_ps(float *__p, __m128 __a)
+{
+  __builtin_ia32_storeups(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store1_ps(float *__p, __m128 __a)
+{
+  __a = __builtin_shufflevector(__a, __a, 0, 0, 0, 0);
+  _mm_storeu_ps(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store_ps1(float *__p, __m128 __a)
+{
+    return _mm_store1_ps(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_store_ps(float *__p, __m128 __a)
+{
+  *(__m128 *)__p = __a;
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_storer_ps(float *__p, __m128 __a)
+{
+  __a = __builtin_shufflevector(__a, __a, 3, 2, 1, 0);
+  _mm_store_ps(__p, __a);
+}
+
+#define _MM_HINT_T0 3
+#define _MM_HINT_T1 2
+#define _MM_HINT_T2 1
+#define _MM_HINT_NTA 0
+
+/* FIXME: We have to #define this because "sel" must be a constant integer, and
+   Sema doesn't do any form of constant propagation yet. */
+
+#define _mm_prefetch(a, sel) (__builtin_prefetch((void *)(a), 0, (sel)))
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_stream_pi(__m64 *__p, __m64 __a)
+{
+  __builtin_ia32_movntq(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_stream_ps(float *__p, __m128 __a)
+{
+  __builtin_ia32_movntps(__p, __a);
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_sfence(void)
+{
+  __builtin_ia32_sfence();
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_extract_pi16(__m64 __a, int __n)
+{
+  __v4hi __b = (__v4hi)__a;
+  return (unsigned short)__b[__n & 3];
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_insert_pi16(__m64 __a, int __d, int __n)
+{
+   __v4hi __b = (__v4hi)__a;
+   __b[__n & 3] = __d;
+   return (__m64)__b;
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_max_pi16(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pmaxsw((__v4hi)__a, (__v4hi)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_max_pu8(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pmaxub((__v8qi)__a, (__v8qi)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_min_pi16(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pminsw((__v4hi)__a, (__v4hi)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_min_pu8(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pminub((__v8qi)__a, (__v8qi)__b);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_movemask_pi8(__m64 __a)
+{
+  return __builtin_ia32_pmovmskb((__v8qi)__a);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_mulhi_pu16(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pmulhuw((__v4hi)__a, (__v4hi)__b);
+}
+
+#define _mm_shuffle_pi16(a, n) __extension__ ({ \
+  __m64 __a = (a); \
+  (__m64)__builtin_ia32_pshufw((__v4hi)__a, (n)); })
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_maskmove_si64(__m64 __d, __m64 __n, char *__p)
+{
+  __builtin_ia32_maskmovq((__v8qi)__d, (__v8qi)__n, __p);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_avg_pu8(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pavgb((__v8qi)__a, (__v8qi)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_avg_pu16(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_pavgw((__v4hi)__a, (__v4hi)__b);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_sad_pu8(__m64 __a, __m64 __b)
+{
+  return (__m64)__builtin_ia32_psadbw((__v8qi)__a, (__v8qi)__b);
+}
+
+static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
+_mm_getcsr(void)
+{
+  return __builtin_ia32_stmxcsr();
+}
+
+static __inline__ void __attribute__((__always_inline__, __nodebug__))
+_mm_setcsr(unsigned int __i)
+{
+  __builtin_ia32_ldmxcsr(__i);
+}
+
+#define _mm_shuffle_ps(a, b, mask) __extension__ ({ \
+  __m128 __a = (a); \
+  __m128 __b = (b); \
+  (__m128)__builtin_shufflevector((__v4sf)__a, (__v4sf)__b, \
+                                  (mask) & 0x3, ((mask) & 0xc) >> 2, \
+                                  (((mask) & 0x30) >> 4) + 4, \
+                                  (((mask) & 0xc0) >> 6) + 4); })
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_unpackhi_ps(__m128 __a, __m128 __b)
+{
+  return __builtin_shufflevector(__a, __b, 2, 6, 3, 7);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_unpacklo_ps(__m128 __a, __m128 __b)
+{
+  return __builtin_shufflevector(__a, __b, 0, 4, 1, 5);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_move_ss(__m128 __a, __m128 __b)
+{
+  return __builtin_shufflevector(__a, __b, 4, 1, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_movehl_ps(__m128 __a, __m128 __b)
+{
+  return __builtin_shufflevector(__a, __b, 6, 7, 2, 3);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_movelh_ps(__m128 __a, __m128 __b)
+{
+  return __builtin_shufflevector(__a, __b, 0, 1, 4, 5);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpi16_ps(__m64 __a)
+{
+  __m64 __b, __c;
+  __m128 __r;
+
+  __b = _mm_setzero_si64();
+  __b = _mm_cmpgt_pi16(__b, __a);
+  __c = _mm_unpackhi_pi16(__a, __b);
+  __r = _mm_setzero_ps();
+  __r = _mm_cvtpi32_ps(__r, __c);
+  __r = _mm_movelh_ps(__r, __r);
+  __c = _mm_unpacklo_pi16(__a, __b);
+  __r = _mm_cvtpi32_ps(__r, __c);
+
+  return __r;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpu16_ps(__m64 __a)
+{
+  __m64 __b, __c;
+  __m128 __r;
+
+  __b = _mm_setzero_si64();
+  __c = _mm_unpackhi_pi16(__a, __b);
+  __r = _mm_setzero_ps();
+  __r = _mm_cvtpi32_ps(__r, __c);
+  __r = _mm_movelh_ps(__r, __r);
+  __c = _mm_unpacklo_pi16(__a, __b);
+  __r = _mm_cvtpi32_ps(__r, __c);
+
+  return __r;
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpi8_ps(__m64 __a)
+{
+  __m64 __b;
+  
+  __b = _mm_setzero_si64();
+  __b = _mm_cmpgt_pi8(__b, __a);
+  __b = _mm_unpacklo_pi8(__a, __b);
+
+  return _mm_cvtpi16_ps(__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpu8_ps(__m64 __a)
+{
+  __m64 __b;
+  
+  __b = _mm_setzero_si64();
+  __b = _mm_unpacklo_pi8(__a, __b);
+
+  return _mm_cvtpi16_ps(__b);
+}
+
+static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtpi32x2_ps(__m64 __a, __m64 __b)
+{
+  __m128 __c;
+  
+  __c = _mm_setzero_ps();
+  __c = _mm_cvtpi32_ps(__c, __b);
+  __c = _mm_movelh_ps(__c, __c);
+
+  return _mm_cvtpi32_ps(__c, __a);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtps_pi16(__m128 __a)
+{
+  __m64 __b, __c;
+  
+  __b = _mm_cvtps_pi32(__a);
+  __a = _mm_movehl_ps(__a, __a);
+  __c = _mm_cvtps_pi32(__a);
+  
+  return _mm_packs_pi32(__b, __c);
+}
+
+static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
+_mm_cvtps_pi8(__m128 __a)
+{
+  __m64 __b, __c;
+  
+  __b = _mm_cvtps_pi16(__a);
+  __c = _mm_setzero_si64();
+  
+  return _mm_packs_pi16(__b, __c);
+}
+
+static __inline__ int __attribute__((__always_inline__, __nodebug__))
+_mm_movemask_ps(__m128 __a)
+{
+  return __builtin_ia32_movmskps(__a);
+}
+
+#define _MM_SHUFFLE(z, y, x, w) (((z) << 6) | ((y) << 4) | ((x) << 2) | (w))
+
+#define _MM_EXCEPT_INVALID    (0x0001)
+#define _MM_EXCEPT_DENORM     (0x0002)
+#define _MM_EXCEPT_DIV_ZERO   (0x0004)
+#define _MM_EXCEPT_OVERFLOW   (0x0008)
+#define _MM_EXCEPT_UNDERFLOW  (0x0010)
+#define _MM_EXCEPT_INEXACT    (0x0020)
+#define _MM_EXCEPT_MASK       (0x003f)
+
+#define _MM_MASK_INVALID      (0x0080)
+#define _MM_MASK_DENORM       (0x0100)
+#define _MM_MASK_DIV_ZERO     (0x0200)
+#define _MM_MASK_OVERFLOW     (0x0400)
+#define _MM_MASK_UNDERFLOW    (0x0800)
+#define _MM_MASK_INEXACT      (0x1000)
+#define _MM_MASK_MASK         (0x1f80)
+
+#define _MM_ROUND_NEAREST     (0x0000)
+#define _MM_ROUND_DOWN        (0x2000)
+#define _MM_ROUND_UP          (0x4000)
+#define _MM_ROUND_TOWARD_ZERO (0x6000)
+#define _MM_ROUND_MASK        (0x6000)
+
+#define _MM_FLUSH_ZERO_MASK   (0x8000)
+#define _MM_FLUSH_ZERO_ON     (0x8000)
+#define _MM_FLUSH_ZERO_OFF    (0x0000)
+
+#define _MM_GET_EXCEPTION_MASK() (_mm_getcsr() & _MM_MASK_MASK)
+#define _MM_GET_EXCEPTION_STATE() (_mm_getcsr() & _MM_EXCEPT_MASK)
+#define _MM_GET_FLUSH_ZERO_MODE() (_mm_getcsr() & _MM_FLUSH_ZERO_MASK)
+#define _MM_GET_ROUNDING_MODE() (_mm_getcsr() & _MM_ROUND_MASK)
+
+#define _MM_SET_EXCEPTION_MASK(x) (_mm_setcsr((_mm_getcsr() & ~_MM_MASK_MASK) | (x)))
+#define _MM_SET_EXCEPTION_STATE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_EXCEPT_MASK) | (x)))
+#define _MM_SET_FLUSH_ZERO_MODE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_FLUSH_ZERO_MASK) | (x)))
+#define _MM_SET_ROUNDING_MODE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_ROUND_MASK) | (x)))
+
+#define _MM_TRANSPOSE4_PS(row0, row1, row2, row3) \
+do { \
+  __m128 tmp3, tmp2, tmp1, tmp0; \
+  tmp0 = _mm_unpacklo_ps((row0), (row1)); \
+  tmp2 = _mm_unpacklo_ps((row2), (row3)); \
+  tmp1 = _mm_unpackhi_ps((row0), (row1)); \
+  tmp3 = _mm_unpackhi_ps((row2), (row3)); \
+  (row0) = _mm_movelh_ps(tmp0, tmp2); \
+  (row1) = _mm_movehl_ps(tmp2, tmp0); \
+  (row2) = _mm_movelh_ps(tmp1, tmp3); \
+  (row3) = _mm_movehl_ps(tmp3, tmp1); \
+} while (0)
+
+/* Aliases for compatibility. */
+#define _m_pextrw _mm_extract_pi16
+#define _m_pinsrw _mm_insert_pi16
+#define _m_pmaxsw _mm_max_pi16
+#define _m_pmaxub _mm_max_pu8
+#define _m_pminsw _mm_min_pi16
+#define _m_pminub _mm_min_pu8
+#define _m_pmovmskb _mm_movemask_pi8
+#define _m_pmulhuw _mm_mulhi_pu16
+#define _m_pshufw _mm_shuffle_pi16
+#define _m_maskmovq _mm_maskmove_si64
+#define _m_pavgb _mm_avg_pu8
+#define _m_pavgw _mm_avg_pu16
+#define _m_psadbw _mm_sad_pu8
+#define _m_ _mm_
+#define _m_ _mm_
+
+/* Ugly hack for backwards-compatibility (compatible with gcc) */
+#ifdef __SSE2__
+#include <emmintrin.h>
+#endif
+
+#endif /* __SSE__ */
+
+#endif /* __XMMINTRIN_H */
diff -Nur xnu-3247.1.106/bsd/crypto/blowfish/bf_locl.h xnu-3247.1.106-AnV/bsd/crypto/blowfish/bf_locl.h
--- xnu-3247.1.106/bsd/crypto/blowfish/bf_locl.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/crypto/blowfish/bf_locl.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,218 @@
+/* crypto/bf/bf_locl.h */
+/* Copyright (C) 1995-1997 Eric Young (eay@cryptsoft.com)
+ * All rights reserved.
+ *
+ * This package is an SSL implementation written
+ * by Eric Young (eay@cryptsoft.com).
+ * The implementation was written so as to conform with Netscapes SSL.
+ * 
+ * This library is free for commercial and non-commercial use as long as
+ * the following conditions are aheared to.  The following conditions
+ * apply to all code found in this distribution, be it the RC4, RSA,
+ * lhash, DES, etc., code; not just the SSL code.  The SSL documentation
+ * included with this distribution is covered by the same copyright terms
+ * except that the holder is Tim Hudson (tjh@cryptsoft.com).
+ * 
+ * Copyright remains Eric Young's, and as such any Copyright notices in
+ * the code are not to be removed.
+ * If this package is used in a product, Eric Young should be given attribution
+ * as the author of the parts of the library used.
+ * This can be in the form of a textual message at program startup or
+ * in documentation (online or textual) provided with the package.
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *    "This product includes cryptographic software written by
+ *     Eric Young (eay@cryptsoft.com)"
+ *    The word 'cryptographic' can be left out if the rouines from the library
+ *    being used are not cryptographic related :-).
+ * 4. If you include any Windows specific code (or a derivative thereof) from 
+ *    the apps directory (application code) you must include an acknowledgement:
+ *    "This product includes software written by Tim Hudson (tjh@cryptsoft.com)"
+ * 
+ * THIS SOFTWARE IS PROVIDED BY ERIC YOUNG ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ * 
+ * The licence and distribution terms for any publically available version or
+ * derivative of this code cannot be changed.  i.e. this code cannot simply be
+ * copied and put under another distribution licence
+ * [including the GNU Public Licence.]
+ */
+
+#ifndef HEADER_BF_LOCL_H
+#define HEADER_BF_LOCL_H
+
+#undef c2l
+#define c2l(c,l)	(l =((unsigned long)(*((c)++)))    , \
+			 l|=((unsigned long)(*((c)++)))<< 8L, \
+			 l|=((unsigned long)(*((c)++)))<<16L, \
+			 l|=((unsigned long)(*((c)++)))<<24L)
+
+/* NOTE - c is not incremented as per c2l */
+#undef c2ln
+#define c2ln(c,l1,l2,n)	{ \
+			c+=n; \
+			l1=l2=0; \
+			switch (n) { \
+			case 8: l2 =((unsigned long)(*(--(c))))<<24L; \
+			case 7: l2|=((unsigned long)(*(--(c))))<<16L; \
+			case 6: l2|=((unsigned long)(*(--(c))))<< 8L; \
+			case 5: l2|=((unsigned long)(*(--(c))));     \
+			case 4: l1 =((unsigned long)(*(--(c))))<<24L; \
+			case 3: l1|=((unsigned long)(*(--(c))))<<16L; \
+			case 2: l1|=((unsigned long)(*(--(c))))<< 8L; \
+			case 1: l1|=((unsigned long)(*(--(c))));     \
+				} \
+			}
+
+#undef l2c
+#define l2c(l,c)	(*((c)++)=(unsigned char)(((l)     )&0xff), \
+			 *((c)++)=(unsigned char)(((l)>> 8L)&0xff), \
+			 *((c)++)=(unsigned char)(((l)>>16L)&0xff), \
+			 *((c)++)=(unsigned char)(((l)>>24L)&0xff))
+
+/* NOTE - c is not incremented as per l2c */
+#undef l2cn
+#define l2cn(l1,l2,c,n)	{ \
+			c+=n; \
+			switch (n) { \
+			case 8: *(--(c))=(unsigned char)(((l2)>>24L)&0xff); \
+			case 7: *(--(c))=(unsigned char)(((l2)>>16L)&0xff); \
+			case 6: *(--(c))=(unsigned char)(((l2)>> 8L)&0xff); \
+			case 5: *(--(c))=(unsigned char)(((l2)     )&0xff); \
+			case 4: *(--(c))=(unsigned char)(((l1)>>24L)&0xff); \
+			case 3: *(--(c))=(unsigned char)(((l1)>>16L)&0xff); \
+			case 2: *(--(c))=(unsigned char)(((l1)>> 8L)&0xff); \
+			case 1: *(--(c))=(unsigned char)(((l1)     )&0xff); \
+				} \
+			}
+
+/* NOTE - c is not incremented as per n2l */
+#define n2ln(c,l1,l2,n)	{ \
+			c+=n; \
+			l1=l2=0; \
+			switch (n) { \
+			case 8: l2 =((BF_LONG)(*(--(c))))    ; \
+			case 7: l2|=((BF_LONG)(*(--(c))))<< 8; \
+			case 6: l2|=((BF_LONG)(*(--(c))))<<16; \
+			case 5: l2|=((BF_LONG)(*(--(c))))<<24; \
+			case 4: l1 =((BF_LONG)(*(--(c))))    ; \
+			case 3: l1|=((BF_LONG)(*(--(c))))<< 8; \
+			case 2: l1|=((BF_LONG)(*(--(c))))<<16; \
+			case 1: l1|=((BF_LONG)(*(--(c))))<<24; \
+				} \
+			}
+
+/* NOTE - c is not incremented as per l2n */
+#define l2nn(l1,l2,c,n)	{ \
+			c+=n; \
+			switch (n) { \
+			case 8: *(--(c))=(unsigned char)(((l2)    )&0xff); \
+			case 7: *(--(c))=(unsigned char)(((l2)>> 8)&0xff); \
+			case 6: *(--(c))=(unsigned char)(((l2)>>16)&0xff); \
+			case 5: *(--(c))=(unsigned char)(((l2)>>24)&0xff); \
+			case 4: *(--(c))=(unsigned char)(((l1)    )&0xff); \
+			case 3: *(--(c))=(unsigned char)(((l1)>> 8)&0xff); \
+			case 2: *(--(c))=(unsigned char)(((l1)>>16)&0xff); \
+			case 1: *(--(c))=(unsigned char)(((l1)>>24)&0xff); \
+				} \
+			}
+
+#undef n2l
+#define n2l(c,l)        (l =((BF_LONG)(*((c)++)))<<24L, \
+                         l|=((BF_LONG)(*((c)++)))<<16L, \
+                         l|=((BF_LONG)(*((c)++)))<< 8L, \
+                         l|=((BF_LONG)(*((c)++))))
+
+#undef l2n
+#define l2n(l,c)        (*((c)++)=(unsigned char)(((l)>>24L)&0xff), \
+                         *((c)++)=(unsigned char)(((l)>>16L)&0xff), \
+                         *((c)++)=(unsigned char)(((l)>> 8L)&0xff), \
+                         *((c)++)=(unsigned char)(((l)     )&0xff))
+
+/* This is actually a big endian algorithm, the most significant byte
+ * is used to lookup array 0 */
+
+#if defined(BF_PTR2)
+
+/*
+ * This is basically a special Intel version. Point is that Intel
+ * doesn't have many registers, but offers a reach choice of addressing
+ * modes. So we spare some registers by directly traversing BF_KEY
+ * structure and hiring the most decorated addressing mode. The code
+ * generated by EGCS is *perfectly* competitive with assembler
+ * implementation!
+ */
+#define BF_ENC(LL,R,KEY,Pi) (\
+	LL^=KEY[Pi], \
+	t=  KEY[BF_ROUNDS+2 +   0 + ((R>>24)&0xFF)], \
+	t+= KEY[BF_ROUNDS+2 + 256 + ((R>>16)&0xFF)], \
+	t^= KEY[BF_ROUNDS+2 + 512 + ((R>>8 )&0xFF)], \
+	t+= KEY[BF_ROUNDS+2 + 768 + ((R    )&0xFF)], \
+	LL^=t \
+	)
+
+#elif defined(BF_PTR)
+
+#ifndef BF_LONG_LOG2
+#define BF_LONG_LOG2  2       /* default to BF_LONG being 32 bits */
+#endif
+#define BF_M  (0xFF<<BF_LONG_LOG2)
+#define BF_0  (24-BF_LONG_LOG2)
+#define BF_1  (16-BF_LONG_LOG2)
+#define BF_2  ( 8-BF_LONG_LOG2)
+#define BF_3  BF_LONG_LOG2 /* left shift */
+
+/*
+ * This is normally very good on RISC platforms where normally you
+ * have to explicitly "multiply" array index by sizeof(BF_LONG)
+ * in order to calculate the effective address. This implementation
+ * excuses CPU from this extra work. Power[PC] uses should have most
+ * fun as (R>>BF_i)&BF_M gets folded into a single instruction, namely
+ * rlwinm. So let'em double-check if their compiler does it.
+ */
+
+#define BF_ENC(LL,R,S,P) ( \
+	LL^=P, \
+	LL^= (((*(BF_LONG *)((unsigned char *)&(S[  0])+((R>>BF_0)&BF_M))+ \
+		*(BF_LONG *)((unsigned char *)&(S[256])+((R>>BF_1)&BF_M)))^ \
+		*(BF_LONG *)((unsigned char *)&(S[512])+((R>>BF_2)&BF_M)))+ \
+		*(BF_LONG *)((unsigned char *)&(S[768])+((R<<BF_3)&BF_M))) \
+	)
+#else
+
+/*
+ * This is a *generic* version. Seem to perform best on platforms that
+ * offer explicit support for extraction of 8-bit nibbles preferably
+ * complemented with "multiplying" of array index by sizeof(BF_LONG).
+ * For the moment of this writing the list comprises Alpha CPU featuring
+ * extbl and s[48]addq instructions.
+ */
+
+#define BF_ENC(LL,R,S,P) ( \
+	LL^=P, \
+	LL^=(((	S[       ((int)(R>>24)&0xff)] + \
+		S[0x0100+((int)(R>>16)&0xff)])^ \
+		S[0x0200+((int)(R>> 8)&0xff)])+ \
+		S[0x0300+((int)(R    )&0xff)])&0xffffffffL \
+	)
+#endif
+
+#endif
diff -Nur xnu-3247.1.106/bsd/crypto/blowfish/blowfish.h xnu-3247.1.106-AnV/bsd/crypto/blowfish/blowfish.h
--- xnu-3247.1.106/bsd/crypto/blowfish/blowfish.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/crypto/blowfish/blowfish.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,127 @@
+/* crypto/bf/blowfish.h */
+/* Copyright (C) 1995-1998 Eric Young (eay@cryptsoft.com)
+ * All rights reserved.
+ *
+ * This package is an SSL implementation written
+ * by Eric Young (eay@cryptsoft.com).
+ * The implementation was written so as to conform with Netscapes SSL.
+ * 
+ * This library is free for commercial and non-commercial use as long as
+ * the following conditions are aheared to.  The following conditions
+ * apply to all code found in this distribution, be it the RC4, RSA,
+ * lhash, DES, etc., code; not just the SSL code.  The SSL documentation
+ * included with this distribution is covered by the same copyright terms
+ * except that the holder is Tim Hudson (tjh@cryptsoft.com).
+ * 
+ * Copyright remains Eric Young's, and as such any Copyright notices in
+ * the code are not to be removed.
+ * If this package is used in a product, Eric Young should be given attribution
+ * as the author of the parts of the library used.
+ * This can be in the form of a textual message at program startup or
+ * in documentation (online or textual) provided with the package.
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *    "This product includes cryptographic software written by
+ *     Eric Young (eay@cryptsoft.com)"
+ *    The word 'cryptographic' can be left out if the rouines from the library
+ *    being used are not cryptographic related :-).
+ * 4. If you include any Windows specific code (or a derivative thereof) from 
+ *    the apps directory (application code) you must include an acknowledgement:
+ *    "This product includes software written by Tim Hudson (tjh@cryptsoft.com)"
+ * 
+ * THIS SOFTWARE IS PROVIDED BY ERIC YOUNG ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ * 
+ * The licence and distribution terms for any publically available version or
+ * derivative of this code cannot be changed.  i.e. this code cannot simply be
+ * copied and put under another distribution licence
+ * [including the GNU Public Licence.]
+ */
+
+#ifndef HEADER_BLOWFISH_H
+#define HEADER_BLOWFISH_H
+
+#ifdef  __cplusplus
+extern "C" {
+#endif
+
+#ifdef OPENSSL_NO_BF
+#error BF is disabled.
+#endif
+
+#define BF_ENCRYPT	1
+#define BF_DECRYPT	0
+
+/*
+ * !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+ * ! BF_LONG has to be at least 32 bits wide. If it's wider, then !
+ * ! BF_LONG_LOG2 has to be defined along.                        !
+ * !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+ */
+
+#if defined(__LP32__)
+#define BF_LONG unsigned long
+#elif defined(OPENSSL_SYS_CRAY) || defined(__ILP64__)
+#define BF_LONG unsigned long
+#define BF_LONG_LOG2 3
+/*
+ * _CRAY note. I could declare short, but I have no idea what impact
+ * does it have on performance on none-T3E machines. I could declare
+ * int, but at least on C90 sizeof(int) can be chosen at compile time.
+ * So I've chosen long...
+ *					<appro@fy.chalmers.se>
+ */
+#else
+#define BF_LONG unsigned int
+#endif
+
+#define BF_ROUNDS	16
+#define BF_BLOCK	8
+
+typedef struct bf_key_st
+	{
+	BF_LONG P[BF_ROUNDS+2];
+	BF_LONG S[4*256];
+	} BF_KEY;
+
+#ifdef OPENSSL_FIPS 
+void private_BF_set_key(BF_KEY *key, int len, const unsigned char *data);
+#endif
+void BF_set_key(BF_KEY *key, int len, const unsigned char *data);
+
+void BF_encrypt(BF_LONG *data,const BF_KEY *key);
+void BF_decrypt(BF_LONG *data,const BF_KEY *key);
+
+void BF_ecb_encrypt(const unsigned char *in, unsigned char *out,
+                    const BF_KEY *key, int enc);
+void BF_cbc_decrypt(const unsigned char *in, unsigned char *out, long length,
+                    const BF_KEY *schedule, unsigned char *ivec);
+void BF_cfb64_encrypt(const unsigned char *in, unsigned char *out, long length,
+	const BF_KEY *schedule, unsigned char *ivec, int *num, int enc);
+void BF_ofb64_encrypt(const unsigned char *in, unsigned char *out, long length,
+	const BF_KEY *schedule, unsigned char *ivec, int *num);
+const char *BF_options(void);
+
+#ifdef  __cplusplus
+}
+#endif
+
+#endif
diff -Nur xnu-3247.1.106/bsd/dev/i386/sysctl.c xnu-3247.1.106-AnV/bsd/dev/i386/sysctl.c
--- xnu-3247.1.106/bsd/dev/i386/sysctl.c	2015-12-06 01:31:01.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/dev/i386/sysctl.c	2015-12-13 17:08:10.000000000 +0100
@@ -164,7 +164,10 @@
         return ENOENT;
 
     buf[0] = '\0';
-    cpuid_get_leaf7_feature_names(leaf7_features, buf, sizeof(buf));
+    if (IsIntelCPU())
+    {
+    	cpuid_get_leaf7_feature_names(leaf7_features, buf, sizeof(buf));
+    }
 
     return SYSCTL_OUT(req, buf, strlen(buf) + 1);
 }
diff -Nur xnu-3247.1.106/bsd/kern/kern_cs.c xnu-3247.1.106-AnV/bsd/kern/kern_cs.c
--- xnu-3247.1.106/bsd/kern/kern_cs.c	2015-12-06 01:31:07.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/kern_cs.c	2015-12-13 18:22:15.000000000 +0100
@@ -77,25 +77,9 @@
 int cs_force_kill = 0;
 int cs_force_hard = 0;
 int cs_debug = 0;
-#if SECURE_KERNEL
-const int cs_enforcement_enable = 1;
-const int cs_library_val_enable = 1;
-#else /* !SECURE_KERNEL */
 int cs_enforcement_panic=0;
-
-#if CONFIG_ENFORCE_SIGNED_CODE
-int cs_enforcement_enable = 1;
-#else
 int cs_enforcement_enable = 0;
-#endif
-
-#if CONFIG_ENFORCE_LIBRARY_VALIDATION
-int cs_library_val_enable = 1;
-#else
 int cs_library_val_enable = 0;
-#endif
-
-#endif /* !SECURE_KERNEL */
 int cs_all_vnodes = 0;
 
 static lck_grp_t *cs_lockgrp;
@@ -149,58 +133,28 @@
 }
 
 int
-cs_allow_invalid(struct proc *p)
+cs_allow_invalid(struct proc * __unused p)
 {
-#if MACH_ASSERT
-	lck_mtx_assert(&p->p_mlock, LCK_MTX_ASSERT_NOTOWNED);
-#endif
-#if CONFIG_MACF && CONFIG_ENFORCE_SIGNED_CODE
-	/* There needs to be a MAC policy to implement this hook, or else the
-	 * kill bits will be cleared here every time. If we have 
-	 * CONFIG_ENFORCE_SIGNED_CODE, we can assume there is a policy
-	 * implementing the hook. 
-	 */
-	if( 0 != mac_proc_check_run_cs_invalid(p)) {
-		if(cs_debug) printf("CODE SIGNING: cs_allow_invalid() "
-				    "not allowed: pid %d\n", 
-				    p->p_pid);
-		return 0;
-	}
-	if(cs_debug) printf("CODE SIGNING: cs_allow_invalid() "
-			    "allowed: pid %d\n", 
-			    p->p_pid);
-	proc_lock(p);
-	p->p_csflags &= ~(CS_KILL | CS_HARD);
-	proc_unlock(p);
-	vm_map_switch_protect(get_task_map(p->task), FALSE);
-#endif
-	return (p->p_csflags & (CS_KILL | CS_HARD)) == 0;
+	return 1;
 }
 
 int
 cs_invalid_page(
-	addr64_t vaddr)
+	addr64_t __unused vaddr)
 {
 	struct proc	*p;
-	int		send_kill = 0, retval = 0, verbose = cs_debug;
-	uint32_t	csflags;
+	/*int		send_kill = 0, retval = 0, verbose = cs_debug;
+	uint32_t	csflags;*/
 
 	p = current_proc();
-
-	if (verbose)
-		printf("CODE SIGNING: cs_invalid_page(0x%llx): p=%d[%s]\n",
-		    vaddr, p->p_pid, p->p_comm);
-
 	proc_lock(p);
 
 	/* XXX for testing */
-	if (cs_force_kill)
-		p->p_csflags |= CS_KILL;
-	if (cs_force_hard)
-		p->p_csflags |= CS_HARD;
+	p->p_csflags ~= CS_KILL;
+	p->p_csflags ~= CS_HARD;
 
 	/* CS_KILL triggers a kill signal, and no you can't have the page. Nothing else. */
-	if (p->p_csflags & CS_KILL) {
+	/*if (p->p_csflags & CS_KILL) {
 		if (panic_on_cs_killed &&
 		    vaddr >= SHARED_REGION_BASE &&
 		    vaddr < SHARED_REGION_BASE + SHARED_REGION_SIZE) {
@@ -218,10 +172,10 @@
 	    vaddr < SHARED_REGION_BASE + SHARED_REGION_SIZE) {
 		panic("<rdar://14393620> cs_invalid_page(va=0x%llx): cs error p=%p\n", (uint64_t) vaddr, p);
 	}
-#endif /* __x86_64__ */
+#endif*/ /* __x86_64__ */
 
 	/* CS_HARD means fail the mapping operation so the process stays valid. */
-	if (p->p_csflags & CS_HARD) {
+	/*if (p->p_csflags & CS_HARD) {
 		retval = 1;
 	} else {
 		if (p->p_csflags & CS_VALID) {
@@ -244,7 +198,11 @@
 		threadsignal(current_thread(), SIGKILL, EXC_BAD_ACCESS);
 
 
-	return retval;
+	return retval;*/
+
+    proc_unlock(p);
+
+    return 0;
 }
 
 /*
@@ -252,17 +210,17 @@
  */
 
 int
-cs_enforcement(struct proc *p)
+cs_enforcement(struct proc __unused *p)
 {
 
-	if (cs_enforcement_enable)
-		return 1;
+//	if (cs_enforcement_enable)
+//		return 1;
 	
-	if (p == NULL)
-		p = current_proc();
+//	if (p == NULL)
+//		p = current_proc();
 
-	if (p != NULL && (p->p_csflags & CS_ENFORCEMENT))
-		return 1;
+//	if (p != NULL && (p->p_csflags & CS_ENFORCEMENT))
+//		return 1;
 
 	return 0;
 }
diff -Nur xnu-3247.1.106/bsd/kern/kern_exec.c xnu-3247.1.106-AnV/bsd/kern/kern_exec.c
--- xnu-3247.1.106/bsd/kern/kern_exec.c	2015-12-06 01:31:08.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/kern_exec.c	2015-12-13 17:08:10.000000000 +0100
@@ -894,7 +894,7 @@
 	 * Set code-signing flags if this binary is signed, or if parent has
 	 * requested them on exec.
 	 */
-	if (load_result.csflags & CS_VALID) {
+	/*if (load_result.csflags & CS_VALID) {
 		imgp->ip_csflags |= load_result.csflags & 
 			(CS_VALID|
 			 CS_HARD|CS_KILL|CS_RESTRICT|CS_ENFORCEMENT|CS_REQUIRE_LV|CS_DYLD_PLATFORM|
@@ -910,7 +910,10 @@
 	if (p->p_csflags & CS_EXEC_SET_ENFORCEMENT)
 		imgp->ip_csflags |= CS_ENFORCEMENT;
 	if (p->p_csflags & CS_EXEC_SET_INSTALLER)
-		imgp->ip_csflags |= CS_INSTALLER;
+		imgp->ip_csflags |= CS_INSTALLER;*/
+
+	load_result.csflags |= CS_VALID;
+ 	imgp->ip_csflags |= CS_VALID;
 
 
 	/*
diff -Nur xnu-3247.1.106/bsd/kern/kern_exec.c.orig xnu-3247.1.106-AnV/bsd/kern/kern_exec.c.orig
--- xnu-3247.1.106/bsd/kern/kern_exec.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/kern_exec.c.orig	2015-12-06 01:31:08.000000000 +0100
@@ -0,0 +1,4855 @@
+/*
+ * Copyright (c) 2000-2011 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/* Copyright (c) 1995 NeXT Computer, Inc. All Rights Reserved */
+/*
+ * Mach Operating System
+ * Copyright (c) 1987 Carnegie-Mellon University
+ * All rights reserved.  The CMU software License Agreement specifies
+ * the terms and conditions for use and redistribution.
+ */
+ 
+/*-
+ * Copyright (c) 1982, 1986, 1991, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ * (c) UNIX System Laboratories, Inc.
+ * All or some portions of this file are derived from material licensed
+ * to the University of California by American Telephone and Telegraph
+ * Co. or Unix System Laboratories, Inc. and are reproduced herein with
+ * the permission of UNIX System Laboratories, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *	This product includes software developed by the University of
+ *	California, Berkeley and its contributors.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	from: @(#)kern_exec.c	8.1 (Berkeley) 6/10/93
+ */
+/*
+ * NOTICE: This file was modified by SPARTA, Inc. in 2005 to introduce
+ * support for mandatory and extensible security protections.  This notice
+ * is included in support of clause 2.2 (b) of the Apple Public License,
+ * Version 2.0.
+ */
+#include <machine/reg.h>
+#include <machine/cpu_capabilities.h>
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/filedesc.h>
+#include <sys/kernel.h>
+#include <sys/proc_internal.h>
+#include <sys/kauth.h>
+#include <sys/user.h>
+#include <sys/socketvar.h>
+#include <sys/malloc.h>
+#include <sys/namei.h>
+#include <sys/mount_internal.h>
+#include <sys/vnode_internal.h>		
+#include <sys/file_internal.h>
+#include <sys/stat.h>
+#include <sys/uio_internal.h>
+#include <sys/acct.h>
+#include <sys/exec.h>
+#include <sys/kdebug.h>
+#include <sys/signal.h>
+#include <sys/aio_kern.h>
+#include <sys/sysproto.h>
+#if SYSV_SHM
+#include <sys/shm_internal.h>		/* shmexec() */
+#endif
+#include <sys/ubc_internal.h>		/* ubc_map() */
+#include <sys/spawn.h>
+#include <sys/spawn_internal.h>
+#include <sys/process_policy.h>
+#include <sys/codesign.h>
+#include <crypto/sha1.h>
+
+#include <libkern/libkern.h>
+
+#include <security/audit/audit.h>
+
+#include <ipc/ipc_types.h>
+
+#include <mach/mach_types.h>
+#include <mach/port.h>
+#include <mach/task.h>
+#include <mach/task_access.h>
+#include <mach/thread_act.h>
+#include <mach/vm_map.h>
+#include <mach/mach_vm.h>
+#include <mach/vm_param.h>
+
+#include <kern/sched_prim.h> /* thread_wakeup() */
+#include <kern/affinity.h>
+#include <kern/assert.h>
+#include <kern/task.h>
+#include <kern/coalition.h>
+
+#if CONFIG_MACF
+#include <security/mac.h>
+#include <security/mac_mach_internal.h>
+#endif
+
+#include <vm/vm_map.h>
+#include <vm/vm_kern.h>
+#include <vm/vm_protos.h>
+#include <vm/vm_kern.h>
+#include <vm/vm_fault.h>
+#include <vm/vm_pageout.h>
+
+#include <kdp/kdp_dyld.h>
+
+#include <machine/pal_routines.h>
+
+#include <pexpert/pexpert.h>
+
+#if CONFIG_MEMORYSTATUS
+#include <sys/kern_memorystatus.h>
+#endif
+
+#if CONFIG_DTRACE
+/* Do not include dtrace.h, it redefines kmem_[alloc/free] */
+extern void (*dtrace_fasttrap_exec_ptr)(proc_t);
+extern void (*dtrace_proc_waitfor_exec_ptr)(proc_t);
+extern void (*dtrace_helpers_cleanup)(proc_t);
+extern void dtrace_lazy_dofs_destroy(proc_t);
+
+/*
+ * Since dtrace_proc_waitfor_exec_ptr can be added/removed in dtrace_subr.c,
+ * we will store its value before actually calling it.
+ */
+static void (*dtrace_proc_waitfor_hook)(proc_t) = NULL;
+
+#include <sys/dtrace_ptss.h>
+#endif
+
+/* support for child creation in exec after vfork */
+thread_t fork_create_child(task_t parent_task, coalition_t *parent_coalition, proc_t child_proc, int inherit_memory, int is64bit);
+void vfork_exit(proc_t p, int rv);
+extern void proc_apply_task_networkbg_internal(proc_t, thread_t);
+
+/*
+ * Mach things for which prototypes are unavailable from Mach headers
+ */
+void		ipc_task_reset(
+			task_t		task);
+void		ipc_thread_reset(
+			thread_t	thread);
+kern_return_t ipc_object_copyin(
+	ipc_space_t		space,
+	mach_port_name_t	name,
+	mach_msg_type_name_t	msgt_name,
+	ipc_object_t		*objectp);
+void ipc_port_release_send(ipc_port_t);
+
+#if DEVELOPMENT || DEBUG
+void task_importance_update_owner_info(task_t);
+#endif
+
+extern struct savearea *get_user_regs(thread_t);
+
+__attribute__((noinline)) int __EXEC_WAITING_ON_TASKGATED_CODE_SIGNATURE_UPCALL__(mach_port_t task_access_port, int32_t new_pid);
+
+#include <kern/thread.h>
+#include <kern/task.h>
+#include <kern/ast.h>
+#include <kern/mach_loader.h>
+#include <kern/mach_fat.h>
+#include <mach-o/fat.h>
+#include <mach-o/loader.h>
+#include <machine/vmparam.h>
+#include <sys/imgact.h>
+
+#include <sys/sdt.h>
+
+
+/*
+ * EAI_ITERLIMIT	The maximum number of times to iterate an image
+ *			activator in exec_activate_image() before treating
+ *			it as malformed/corrupt.
+ */
+#define EAI_ITERLIMIT		3
+
+/*
+ * For #! interpreter parsing
+ */
+#define IS_WHITESPACE(ch) ((ch == ' ') || (ch == '\t'))
+#define IS_EOL(ch) ((ch == '#') || (ch == '\n'))
+
+extern vm_map_t bsd_pageable_map;
+extern const struct fileops vnops;
+
+#define	USER_ADDR_ALIGN(addr, val) \
+	( ( (user_addr_t)(addr) + (val) - 1) \
+		& ~((val) - 1) )
+
+struct image_params;	/* Forward */
+static int exec_activate_image(struct image_params *imgp);
+static int exec_copyout_strings(struct image_params *imgp, user_addr_t *stackp);
+static int load_return_to_errno(load_return_t lrtn);
+static int execargs_alloc(struct image_params *imgp);
+static int execargs_free(struct image_params *imgp);
+static int exec_check_permissions(struct image_params *imgp);
+static int exec_extract_strings(struct image_params *imgp);
+static int exec_add_apple_strings(struct image_params *imgp);
+static int exec_handle_sugid(struct image_params *imgp);
+static int sugid_scripts = 0;
+SYSCTL_INT (_kern, OID_AUTO, sugid_scripts, CTLFLAG_RW | CTLFLAG_LOCKED, &sugid_scripts, 0, "");
+static kern_return_t create_unix_stack(vm_map_t map, load_result_t* load_result, proc_t p);
+static int copyoutptr(user_addr_t ua, user_addr_t ptr, int ptr_size);
+static void exec_resettextvp(proc_t, struct image_params *);
+static int check_for_signature(proc_t, struct image_params *);
+static void exec_prefault_data(proc_t, struct image_params *, load_result_t *);
+static errno_t exec_handle_port_actions(struct image_params *imgp, short psa_flags, boolean_t * portwatch_present, ipc_port_t * portwatch_ports);
+static errno_t exec_handle_spawnattr_policy(proc_t p, int psa_apptype, uint64_t psa_qos_clamp, uint64_t psa_darwin_role,
+                             ipc_port_t * portwatch_ports, int portwatch_count);
+
+/*
+ * exec_add_user_string
+ *
+ * Add the requested string to the string space area.
+ *
+ * Parameters;	struct image_params *		image parameter block
+ *		user_addr_t			string to add to strings area
+ *		int				segment from which string comes
+ *		boolean_t			TRUE if string contributes to NCARGS
+ *
+ * Returns:	0			Success
+ *		!0			Failure errno from copyinstr()
+ *
+ * Implicit returns:
+ *		(imgp->ip_strendp)	updated location of next add, if any
+ *		(imgp->ip_strspace)	updated byte count of space remaining
+ *		(imgp->ip_argspace) updated byte count of space in NCARGS
+ */
+static int
+exec_add_user_string(struct image_params *imgp, user_addr_t str, int seg, boolean_t is_ncargs)
+{
+	int error = 0;
+	
+	do {
+		size_t len = 0;
+		int space;
+		
+		if (is_ncargs)
+			space = imgp->ip_argspace; /* by definition smaller than ip_strspace */
+		else
+			space = imgp->ip_strspace;
+		
+		if (space <= 0) {
+			error = E2BIG;
+			break;
+		}
+		
+		if (!UIO_SEG_IS_USER_SPACE(seg)) {
+			char *kstr = CAST_DOWN(char *,str);	/* SAFE */
+			error = copystr(kstr, imgp->ip_strendp, space, &len);
+		} else  {
+			error = copyinstr(str, imgp->ip_strendp, space, &len);
+		}
+
+		imgp->ip_strendp += len;
+		imgp->ip_strspace -= len;
+		if (is_ncargs)
+			imgp->ip_argspace -= len;
+		
+	} while (error == ENAMETOOLONG);
+	
+	return error;
+}
+
+/*
+ * dyld is now passed the executable path as a getenv-like variable
+ * in the same fashion as the stack_guard and malloc_entropy keys.
+ */
+#define	EXECUTABLE_KEY "executable_path="
+
+/*
+ * exec_save_path
+ *
+ * To support new app package launching for Mac OS X, the dyld needs the
+ * first argument to execve() stored on the user stack.
+ *
+ * Save the executable path name at the bottom of the strings area and set
+ * the argument vector pointer to the location following that to indicate
+ * the start of the argument and environment tuples, setting the remaining
+ * string space count to the size of the string area minus the path length.
+ *
+ * Parameters;	struct image_params *		image parameter block
+ *		char *				path used to invoke program
+ *		int				segment from which path comes
+ *
+ * Returns:	int			0	Success
+ *		EFAULT				Bad address
+ *	copy[in]str:EFAULT			Bad address
+ *	copy[in]str:ENAMETOOLONG		Filename too long
+ *
+ * Implicit returns:
+ *		(imgp->ip_strings)		saved path
+ *		(imgp->ip_strspace)		space remaining in ip_strings
+ *		(imgp->ip_strendp)		start of remaining copy area
+ *		(imgp->ip_argspace)		space remaining of NCARGS
+ *		(imgp->ip_applec)		Initial applev[0]
+ *
+ * Note:	We have to do this before the initial namei() since in the
+ *		path contains symbolic links, namei() will overwrite the
+ *		original path buffer contents.  If the last symbolic link
+ *		resolved was a relative pathname, we would lose the original
+ *		"path", which could be an absolute pathname. This might be
+ *		unacceptable for dyld.
+ */
+static int
+exec_save_path(struct image_params *imgp, user_addr_t path, int seg, const char **excpath)
+{
+	int error;
+	size_t len;
+	char *kpath;
+
+	// imgp->ip_strings can come out of a cache, so we need to obliterate the
+	// old path.
+	memset(imgp->ip_strings, '\0', strlen(EXECUTABLE_KEY) + MAXPATHLEN);
+
+	len = MIN(MAXPATHLEN, imgp->ip_strspace);
+
+	switch(seg) {
+	case UIO_USERSPACE32:
+	case UIO_USERSPACE64:	/* Same for copyin()... */
+		error = copyinstr(path, imgp->ip_strings + strlen(EXECUTABLE_KEY), len, &len);
+		break;
+	case UIO_SYSSPACE:
+		kpath = CAST_DOWN(char *,path);	/* SAFE */
+		error = copystr(kpath, imgp->ip_strings + strlen(EXECUTABLE_KEY), len, &len);
+		break;
+	default:
+		error = EFAULT;
+		break;
+	}
+
+	if (!error) {
+		bcopy(EXECUTABLE_KEY, imgp->ip_strings, strlen(EXECUTABLE_KEY));
+		len += strlen(EXECUTABLE_KEY);
+
+		imgp->ip_strendp += len;
+		imgp->ip_strspace -= len;
+
+		if (excpath) {
+			*excpath = imgp->ip_strings + strlen(EXECUTABLE_KEY);
+		}
+	}
+
+	return(error);
+}
+
+/*
+ * exec_reset_save_path
+ *
+ * If we detect a shell script, we need to reset the string area
+ * state so that the interpreter can be saved onto the stack.
+
+ * Parameters;	struct image_params *		image parameter block
+ *
+ * Returns:	int			0	Success
+ *
+ * Implicit returns:
+ *		(imgp->ip_strings)		saved path
+ *		(imgp->ip_strspace)		space remaining in ip_strings
+ *		(imgp->ip_strendp)		start of remaining copy area
+ *		(imgp->ip_argspace)		space remaining of NCARGS
+ *
+ */
+static int
+exec_reset_save_path(struct image_params *imgp)
+{
+	imgp->ip_strendp = imgp->ip_strings;
+	imgp->ip_argspace = NCARGS;
+	imgp->ip_strspace = ( NCARGS + PAGE_SIZE );
+
+	return (0);
+}
+
+/*
+ * exec_shell_imgact
+ *
+ * Image activator for interpreter scripts.  If the image begins with
+ * the characters "#!", then it is an interpreter script.  Verify the
+ * length of the script line indicating the interpreter is not in
+ * excess of the maximum allowed size.  If this is the case, then
+ * break out the arguments, if any, which are separated by white
+ * space, and copy them into the argument save area as if they were
+ * provided on the command line before all other arguments.  The line
+ * ends when we encounter a comment character ('#') or newline.
+ *
+ * Parameters;	struct image_params *	image parameter block
+ *
+ * Returns:	-1			not an interpreter (keep looking)
+ *		-3			Success: interpreter: relookup
+ *		>0			Failure: interpreter: error number
+ *
+ * A return value other than -1 indicates subsequent image activators should
+ * not be given the opportunity to attempt to activate the image.
+ */
+static int
+exec_shell_imgact(struct image_params *imgp)
+{
+	char *vdata = imgp->ip_vdata;
+	char *ihp;
+	char *line_startp, *line_endp;
+	char *interp;
+	proc_t p;
+	struct fileproc *fp;
+	int fd;
+	int error;
+
+	/*
+	 * Make sure it's a shell script.  If we've already redirected
+	 * from an interpreted file once, don't do it again.
+	 */
+	if (vdata[0] != '#' ||
+	    vdata[1] != '!' ||
+	    (imgp->ip_flags & IMGPF_INTERPRET) != 0) {
+		return (-1);
+	}
+
+	if (imgp->ip_origcputype != 0) {
+		/* Fat header previously matched, don't allow shell script inside */
+		return (-1);
+	}
+
+	imgp->ip_flags |= IMGPF_INTERPRET;
+	imgp->ip_interp_sugid_fd = -1;
+	imgp->ip_interp_buffer[0] = '\0';
+
+	/* Check to see if SUGID scripts are permitted.  If they aren't then
+	 * clear the SUGID bits.
+	 * imgp->ip_vattr is known to be valid.
+	 */
+	if (sugid_scripts == 0) {
+		imgp->ip_origvattr->va_mode &= ~(VSUID | VSGID);
+	}
+
+	/* Try to find the first non-whitespace character */
+	for( ihp = &vdata[2]; ihp < &vdata[IMG_SHSIZE]; ihp++ ) {
+		if (IS_EOL(*ihp)) {
+			/* Did not find interpreter, "#!\n" */
+			return (ENOEXEC);
+		} else if (IS_WHITESPACE(*ihp)) {
+			/* Whitespace, like "#!    /bin/sh\n", keep going. */
+		} else {
+			/* Found start of interpreter */
+			break;
+		}
+	}
+
+	if (ihp == &vdata[IMG_SHSIZE]) {
+		/* All whitespace, like "#!           " */
+		return (ENOEXEC);
+	}
+
+	line_startp = ihp;
+
+	/* Try to find the end of the interpreter+args string */
+	for ( ; ihp < &vdata[IMG_SHSIZE]; ihp++ ) {
+		if (IS_EOL(*ihp)) {
+			/* Got it */
+			break;
+		} else {
+			/* Still part of interpreter or args */
+		}
+	}
+
+	if (ihp == &vdata[IMG_SHSIZE]) {
+		/* A long line, like "#! blah blah blah" without end */
+		return (ENOEXEC);
+	}
+
+	/* Backtrack until we find the last non-whitespace */
+	while (IS_EOL(*ihp) || IS_WHITESPACE(*ihp)) {
+		ihp--;
+	}
+
+	/* The character after the last non-whitespace is our logical end of line */
+	line_endp = ihp + 1;
+
+	/*
+	 * Now we have pointers to the usable part of:
+	 *
+	 * "#!  /usr/bin/int first    second   third    \n"
+	 *      ^ line_startp                       ^ line_endp
+	 */
+
+	/* copy the interpreter name */
+	interp = imgp->ip_interp_buffer;
+	for ( ihp = line_startp; (ihp < line_endp) && !IS_WHITESPACE(*ihp); ihp++)
+		*interp++ = *ihp;
+	*interp = '\0';
+
+	exec_reset_save_path(imgp);
+	exec_save_path(imgp, CAST_USER_ADDR_T(imgp->ip_interp_buffer),
+							UIO_SYSSPACE, NULL);
+
+	/* Copy the entire interpreter + args for later processing into argv[] */
+	interp = imgp->ip_interp_buffer;
+	for ( ihp = line_startp; (ihp < line_endp); ihp++)
+		*interp++ = *ihp;
+	*interp = '\0';
+
+	/*
+	 * If we have a SUID oder SGID script, create a file descriptor
+	 * from the vnode and pass /dev/fd/%d instead of the actual
+	 * path name so that the script does not get opened twice
+	 */
+	if (imgp->ip_origvattr->va_mode & (VSUID | VSGID)) {
+		p = vfs_context_proc(imgp->ip_vfs_context);
+		error = falloc(p, &fp, &fd, imgp->ip_vfs_context);
+		if (error)
+			return(error);
+
+		fp->f_fglob->fg_flag = FREAD;
+		fp->f_fglob->fg_ops = &vnops;
+		fp->f_fglob->fg_data = (caddr_t)imgp->ip_vp;
+		
+		proc_fdlock(p);
+		procfdtbl_releasefd(p, fd, NULL);
+		fp_drop(p, fd, fp, 1);
+		proc_fdunlock(p);
+		vnode_ref(imgp->ip_vp);
+
+		imgp->ip_interp_sugid_fd = fd;
+	}
+
+	return (-3);
+}
+
+
+
+/*
+ * exec_fat_imgact
+ *
+ * Image activator for fat 1.0 binaries.  If the binary is fat, then we
+ * need to select an image from it internally, and make that the image
+ * we are going to attempt to execute.  At present, this consists of
+ * reloading the first page for the image with a first page from the
+ * offset location indicated by the fat header.
+ *
+ * Parameters;	struct image_params *	image parameter block
+ *
+ * Returns:	-1			not a fat binary (keep looking)
+ *		-2			Success: encapsulated binary: reread
+ *		>0			Failure: error number
+ *
+ * Important:	This image activator is byte order neutral.
+ *
+ * Note:	A return value other than -1 indicates subsequent image
+ *		activators should not be given the opportunity to attempt
+ *		to activate the image.
+ *
+ * 		If we find an encapsulated binary, we make no assertions
+ *		about its  validity; instead, we leave that up to a rescan
+ *		for an activator to claim it, and, if it is claimed by one,
+ *		that activator is responsible for determining validity.
+ */
+static int
+exec_fat_imgact(struct image_params *imgp)
+{
+	proc_t p = vfs_context_proc(imgp->ip_vfs_context);
+	kauth_cred_t cred = kauth_cred_proc_ref(p);
+	struct fat_header *fat_header = (struct fat_header *)imgp->ip_vdata;
+	struct _posix_spawnattr *psa = NULL;
+	struct fat_arch fat_arch;
+	int resid, error;
+	load_return_t lret;
+
+	if (imgp->ip_origcputype != 0) {
+		/* Fat header previously matched, don't allow another fat file inside */
+		return (-1);
+	}
+
+	/* Make sure it's a fat binary */
+	if (OSSwapBigToHostInt32(fat_header->magic) != FAT_MAGIC) {
+		error = -1; /* not claimed */
+		goto bad;
+	}
+
+	/* imgp->ip_vdata has PAGE_SIZE, zerofilled if the file is smaller */
+	lret = fatfile_validate_fatarches((vm_offset_t)fat_header, PAGE_SIZE);
+	if (lret != LOAD_SUCCESS) {
+		error = load_return_to_errno(lret);
+		goto bad;
+	}
+
+	/* If posix_spawn binprefs exist, respect those prefs. */
+	psa = (struct _posix_spawnattr *) imgp->ip_px_sa;
+	if (psa != NULL && psa->psa_binprefs[0] != 0) {
+		uint32_t pr = 0;
+
+		/* Check each preference listed against all arches in header */
+		for (pr = 0; pr < NBINPREFS; pr++) {
+			cpu_type_t pref = psa->psa_binprefs[pr];
+			if (pref == 0) {
+				/* No suitable arch in the pref list */
+				error = EBADARCH;
+				goto bad;
+			}
+
+			if (pref == CPU_TYPE_ANY) {
+				/* Fall through to regular grading */
+				goto regular_grading;
+			}
+
+			lret = fatfile_getbestarch_for_cputype(pref,
+							(vm_offset_t)fat_header,
+							PAGE_SIZE,
+							&fat_arch);
+			if (lret == LOAD_SUCCESS) {
+				goto use_arch;
+			}
+		}
+
+		/* Requested binary preference was not honored */
+		error = EBADEXEC;
+		goto bad;
+	}
+
+regular_grading:
+	/* Look up our preferred architecture in the fat file. */
+	lret = fatfile_getbestarch((vm_offset_t)fat_header,
+				PAGE_SIZE,
+				&fat_arch);
+	if (lret != LOAD_SUCCESS) {
+		error = load_return_to_errno(lret);
+		goto bad;
+	}
+
+use_arch:
+	/* Read the Mach-O header out of fat_arch */
+	error = vn_rdwr(UIO_READ, imgp->ip_vp, imgp->ip_vdata,
+			PAGE_SIZE, fat_arch.offset,
+			UIO_SYSSPACE, (IO_UNIT|IO_NODELOCKED),
+			cred, &resid, p);
+	if (error) {
+		goto bad;
+	}
+
+	if (resid) {
+		memset(imgp->ip_vdata + (PAGE_SIZE - resid), 0x0, resid);
+	}
+
+	/* Success.  Indicate we have identified an encapsulated binary */
+	error = -2;
+	imgp->ip_arch_offset = (user_size_t)fat_arch.offset;
+	imgp->ip_arch_size = (user_size_t)fat_arch.size;
+	imgp->ip_origcputype = fat_arch.cputype;
+	imgp->ip_origcpusubtype = fat_arch.cpusubtype;
+
+bad:
+	kauth_cred_unref(&cred);
+	return (error);
+}
+
+/*
+ * exec_mach_imgact
+ *
+ * Image activator for mach-o 1.0 binaries.
+ *
+ * Parameters;	struct image_params *	image parameter block
+ *
+ * Returns:	-1			not a fat binary (keep looking)
+ *		-2			Success: encapsulated binary: reread
+ *		>0			Failure: error number
+ *		EBADARCH		Mach-o binary, but with an unrecognized
+ *					architecture
+ *		ENOMEM			No memory for child process after -
+ *					can only happen after vfork()
+ *
+ * Important:	This image activator is NOT byte order neutral.
+ *
+ * Note:	A return value other than -1 indicates subsequent image
+ *		activators should not be given the opportunity to attempt
+ *		to activate the image.
+ *
+ * TODO:	More gracefully handle failures after vfork
+ */
+static int
+exec_mach_imgact(struct image_params *imgp)
+{
+	struct mach_header *mach_header = (struct mach_header *)imgp->ip_vdata;
+	proc_t			p = vfs_context_proc(imgp->ip_vfs_context);
+	int			error = 0;
+	task_t			task;
+	task_t			new_task = NULL; /* protected by vfexec */
+	thread_t		thread;
+	struct uthread		*uthread;
+	vm_map_t old_map = VM_MAP_NULL;
+	vm_map_t map;
+	load_return_t		lret;
+	load_result_t		load_result;
+	struct _posix_spawnattr *psa = NULL;
+	int			spawn = (imgp->ip_flags & IMGPF_SPAWN);
+	int			vfexec = (imgp->ip_flags & IMGPF_VFORK_EXEC);
+	int			p_name_len;
+
+	/*
+	 * make sure it's a Mach-O 1.0 or Mach-O 2.0 binary; the difference
+	 * is a reserved field on the end, so for the most part, we can
+	 * treat them as if they were identical. Reverse-endian Mach-O
+	 * binaries are recognized but not compatible.
+ 	 */
+	if ((mach_header->magic == MH_CIGAM) ||
+	    (mach_header->magic == MH_CIGAM_64)) {
+		error = EBADARCH;
+		goto bad;
+	}
+
+	if ((mach_header->magic != MH_MAGIC) &&
+	    (mach_header->magic != MH_MAGIC_64)) {
+		error = -1;
+		goto bad;
+	}
+
+	if (mach_header->filetype != MH_EXECUTE) {
+		error = -1;
+		goto bad;
+	}
+
+	if (imgp->ip_origcputype != 0) {
+		/* Fat header previously had an idea about this thin file */
+		if (imgp->ip_origcputype != mach_header->cputype ||
+			imgp->ip_origcpusubtype != mach_header->cpusubtype) {
+			error = EBADARCH;
+			goto bad;
+		}
+	} else {
+		imgp->ip_origcputype = mach_header->cputype;
+		imgp->ip_origcpusubtype = mach_header->cpusubtype;
+	}
+
+	task = current_task();
+	thread = current_thread();
+	uthread = get_bsdthread_info(thread);
+
+	if ((mach_header->cputype & CPU_ARCH_ABI64) == CPU_ARCH_ABI64)
+		imgp->ip_flags |= IMGPF_IS_64BIT;
+
+	/* If posix_spawn binprefs exist, respect those prefs. */
+	psa = (struct _posix_spawnattr *) imgp->ip_px_sa;
+	if (psa != NULL && psa->psa_binprefs[0] != 0) {
+		int pr = 0;
+		for (pr = 0; pr < NBINPREFS; pr++) {
+			cpu_type_t pref = psa->psa_binprefs[pr];
+			if (pref == 0) {
+				/* No suitable arch in the pref list */
+				error = EBADARCH;
+				goto bad;
+			}
+
+			if (pref == CPU_TYPE_ANY) {
+				/* Jump to regular grading */
+				goto grade;
+			}
+
+			if (pref == imgp->ip_origcputype) {
+				/* We have a match! */
+				goto grade;
+			}
+		}
+		error = EBADARCH;
+		goto bad;
+	}
+grade:
+	if (!grade_binary(imgp->ip_origcputype, imgp->ip_origcpusubtype & ~CPU_SUBTYPE_MASK)) {
+		error = EBADARCH;
+		goto bad;
+	}
+
+	/* Copy in arguments/environment from the old process */
+	error = exec_extract_strings(imgp);
+	if (error)
+		goto bad;
+
+	error = exec_add_apple_strings(imgp);
+	if (error)
+		goto bad;
+
+	AUDIT_ARG(argv, imgp->ip_startargv, imgp->ip_argc, 
+	    imgp->ip_endargv - imgp->ip_startargv);
+	AUDIT_ARG(envv, imgp->ip_endargv, imgp->ip_envc,
+	    imgp->ip_endenvv - imgp->ip_endargv);
+
+	/*
+	 * We are being called to activate an image subsequent to a vfork()
+	 * operation; in this case, we know that our task, thread, and
+	 * uthread are actually those of our parent, and our proc, which we
+	 * obtained indirectly from the image_params vfs_context_t, is the
+	 * new child process.
+	 */
+	if (vfexec || spawn) {
+		if (vfexec) {
+			imgp->ip_new_thread = fork_create_child(task, NULL, p, FALSE, (imgp->ip_flags & IMGPF_IS_64BIT));
+			if (imgp->ip_new_thread == NULL) {
+				error = ENOMEM;
+				goto bad;
+			}
+		}
+
+		/* reset local idea of thread, uthread, task */
+		thread = imgp->ip_new_thread;
+		uthread = get_bsdthread_info(thread);
+		task = new_task = get_threadtask(thread);
+		map = get_task_map(task);
+	} else {
+		map = VM_MAP_NULL;
+	}
+
+	/*
+	 * We set these flags here; this is OK, since if we fail after
+	 * this point, we have already destroyed the parent process anyway.
+	 */
+	task_set_dyld_info(task, MACH_VM_MIN_ADDRESS, 0);
+	if (imgp->ip_flags & IMGPF_IS_64BIT) {
+		task_set_64bit(task, TRUE);
+		OSBitOrAtomic(P_LP64, &p->p_flag);
+	} else {
+		task_set_64bit(task, FALSE);
+		OSBitAndAtomic(~((uint32_t)P_LP64), &p->p_flag);
+	}
+
+	/*
+	 *	Load the Mach-O file.
+	 *
+	 * NOTE: An error after this point  indicates we have potentially
+	 * destroyed or overwritten some process state while attempting an
+	 * execve() following a vfork(), which is an unrecoverable condition.
+	 * We send the new process an immediate SIGKILL to avoid it executing
+	 * any instructions in the mutated address space. For true spawns,
+	 * this is not the case, and "too late" is still not too late to
+	 * return an error code to the parent process.
+	 */
+
+	/*
+	 * Actually load the image file we previously decided to load.
+	 */
+	lret = load_machfile(imgp, mach_header, thread, map, &load_result);
+
+	if (lret != LOAD_SUCCESS) {
+		error = load_return_to_errno(lret);
+		goto badtoolate;
+	}
+
+	proc_lock(p);
+	p->p_cputype = imgp->ip_origcputype;
+	p->p_cpusubtype = imgp->ip_origcpusubtype;
+	proc_unlock(p);
+
+	vm_map_set_user_wire_limit(get_task_map(task), p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
+
+	/* 
+	 * Set code-signing flags if this binary is signed, or if parent has
+	 * requested them on exec.
+	 */
+	if (load_result.csflags & CS_VALID) {
+		imgp->ip_csflags |= load_result.csflags & 
+			(CS_VALID|
+			 CS_HARD|CS_KILL|CS_RESTRICT|CS_ENFORCEMENT|CS_REQUIRE_LV|CS_DYLD_PLATFORM|
+			 CS_EXEC_SET_HARD|CS_EXEC_SET_KILL|CS_EXEC_SET_ENFORCEMENT);
+	} else {
+		imgp->ip_csflags &= ~CS_VALID;
+	}
+
+	if (p->p_csflags & CS_EXEC_SET_HARD)
+		imgp->ip_csflags |= CS_HARD;
+	if (p->p_csflags & CS_EXEC_SET_KILL)
+		imgp->ip_csflags |= CS_KILL;
+	if (p->p_csflags & CS_EXEC_SET_ENFORCEMENT)
+		imgp->ip_csflags |= CS_ENFORCEMENT;
+	if (p->p_csflags & CS_EXEC_SET_INSTALLER)
+		imgp->ip_csflags |= CS_INSTALLER;
+
+
+	/*
+	 * Set up the system reserved areas in the new address space.
+	 */
+	vm_map_exec(get_task_map(task),
+		    task,
+		    (void *) p->p_fd->fd_rdir,
+		    cpu_type());
+	
+	/*
+	 * Close file descriptors which specify close-on-exec.
+	 */
+	fdexec(p, psa != NULL ? psa->psa_flags : 0);
+
+	/*
+	 * deal with set[ug]id.
+	 */
+	error = exec_handle_sugid(imgp);
+	if (error) {
+		goto badtoolate;
+	}	
+
+	/*
+	 * deal with voucher on exec-calling thread.
+	 */
+	if (imgp->ip_new_thread == NULL)
+		thread_set_mach_voucher(current_thread(), IPC_VOUCHER_NULL);
+
+	/* Make sure we won't interrupt ourself signalling a partial process */
+	if (!vfexec && !spawn && (p->p_lflag & P_LTRACED))
+		psignal(p, SIGTRAP);
+
+	if (load_result.unixproc &&
+		create_unix_stack(get_task_map(task),
+				  &load_result,
+				  p) != KERN_SUCCESS) {
+		error = load_return_to_errno(LOAD_NOSPACE);
+		goto badtoolate;
+	}
+
+	if (vfexec || spawn) {
+		old_map = vm_map_switch(get_task_map(task));
+	}
+
+	if (load_result.unixproc) {
+		user_addr_t	ap;
+
+		/*
+		 * Copy the strings area out into the new process address
+		 * space.
+		 */
+		ap = p->user_stack;
+		error = exec_copyout_strings(imgp, &ap);
+		if (error) {
+			if (vfexec || spawn)
+				vm_map_switch(old_map);
+			goto badtoolate;
+		}
+		/* Set the stack */
+		thread_setuserstack(thread, ap);
+	}
+	
+	if (load_result.dynlinker) {
+		uint64_t	ap;
+		int			new_ptr_size = (imgp->ip_flags & IMGPF_IS_64BIT) ? 8 : 4;
+
+		/* Adjust the stack */
+		ap = thread_adjuserstack(thread, -new_ptr_size);
+		error = copyoutptr(load_result.mach_header, ap, new_ptr_size);
+
+		if (error) {
+			if (vfexec || spawn)
+				vm_map_switch(old_map);
+			goto badtoolate;
+		}
+		task_set_dyld_info(task, load_result.all_image_info_addr,
+		    load_result.all_image_info_size);
+	}
+
+	/* Avoid immediate VM faults back into kernel */
+	exec_prefault_data(p, imgp, &load_result);
+
+	if (vfexec || spawn) {
+		vm_map_switch(old_map);
+	}
+	/* Set the entry point */
+	thread_setentrypoint(thread, load_result.entry_point);
+
+	/* Stop profiling */
+	stopprofclock(p);
+
+	/*
+	 * Reset signal state.
+	 */
+	execsigs(p, thread);
+
+	/*
+	 * need to cancel async IO requests that can be cancelled and wait for those
+	 * already active.  MAY BLOCK!
+	 */
+	_aio_exec( p );
+
+#if SYSV_SHM
+	/* FIXME: Till vmspace inherit is fixed: */
+	if (!vfexec && p->vm_shm)
+		shmexec(p);
+#endif
+#if SYSV_SEM
+	/* Clean up the semaphores */
+	semexit(p);
+#endif
+
+	/*
+	 * Remember file name for accounting.
+	 */
+	p->p_acflag &= ~AFORK;
+
+	/*
+	 * Set p->p_comm and p->p_name to the name passed to exec
+	 */
+	p_name_len = sizeof(p->p_name) - 1;
+	if(imgp->ip_ndp->ni_cnd.cn_namelen > p_name_len)
+		imgp->ip_ndp->ni_cnd.cn_namelen = p_name_len;
+	bcopy((caddr_t)imgp->ip_ndp->ni_cnd.cn_nameptr, (caddr_t)p->p_name,
+		(unsigned)imgp->ip_ndp->ni_cnd.cn_namelen);
+	p->p_name[imgp->ip_ndp->ni_cnd.cn_namelen] = '\0';
+
+	if (imgp->ip_ndp->ni_cnd.cn_namelen > MAXCOMLEN)
+		imgp->ip_ndp->ni_cnd.cn_namelen = MAXCOMLEN;
+	bcopy((caddr_t)imgp->ip_ndp->ni_cnd.cn_nameptr, (caddr_t)p->p_comm,
+		(unsigned)imgp->ip_ndp->ni_cnd.cn_namelen);
+	p->p_comm[imgp->ip_ndp->ni_cnd.cn_namelen] = '\0';
+
+	pal_dbg_set_task_name( p->task );
+
+#if DEVELOPMENT || DEBUG
+	/* 
+	 * Update the pid an proc name for importance base if any
+	 */
+	task_importance_update_owner_info(p->task);
+#endif
+
+	memcpy(&p->p_uuid[0], &load_result.uuid[0], sizeof(p->p_uuid));
+
+// <rdar://6598155> dtrace code cleanup needed
+#if CONFIG_DTRACE
+	/*
+	 * Invalidate any predicate evaluation already cached for this thread by DTrace.
+	 * That's because we've just stored to p_comm and DTrace refers to that when it
+	 * evaluates the "execname" special variable. uid and gid may have changed as well.
+	 */
+	dtrace_set_thread_predcache(current_thread(), 0);
+
+	/*
+	 * Free any outstanding lazy dof entries. It is imperative we
+	 * always call dtrace_lazy_dofs_destroy, rather than null check
+	 * and call if !NULL. If we NULL test, during lazy dof faulting
+	 * we can race with the faulting code and proceed from here to
+	 * beyond the helpers cleanup. The lazy dof faulting will then
+	 * install new helpers which no longer belong to this process!
+	 */
+	dtrace_lazy_dofs_destroy(p);
+
+
+	/*
+    	 * Clean up any DTrace helpers for the process.
+    	 */
+    	if (p->p_dtrace_helpers != NULL && dtrace_helpers_cleanup) {
+    		(*dtrace_helpers_cleanup)(p);
+    	}
+	
+    	/*
+    	 * Cleanup the DTrace provider associated with this process.
+    	 */
+	proc_lock(p);
+	if (p->p_dtrace_probes && dtrace_fasttrap_exec_ptr) {
+		(*dtrace_fasttrap_exec_ptr)(p);
+	}
+	proc_unlock(p);
+#endif
+
+	if (kdebug_enable) {
+		long dbg_arg1, dbg_arg2, dbg_arg3, dbg_arg4;
+
+		/*
+		 * Collect the pathname for tracing
+		 */
+		kdbg_trace_string(p, &dbg_arg1, &dbg_arg2, &dbg_arg3, &dbg_arg4);
+
+		if (vfexec || spawn) {
+			KERNEL_DEBUG_CONSTANT1(TRACE_DATA_EXEC | DBG_FUNC_NONE,
+					p->p_pid ,0,0,0, (uintptr_t)thread_tid(thread));
+			KERNEL_DEBUG_CONSTANT1(TRACE_STRING_EXEC | DBG_FUNC_NONE,
+					dbg_arg1, dbg_arg2, dbg_arg3, dbg_arg4, (uintptr_t)thread_tid(thread));
+		} else {
+			KERNEL_DEBUG_CONSTANT(TRACE_DATA_EXEC | DBG_FUNC_NONE,
+					p->p_pid ,0,0,0,0);
+			KERNEL_DEBUG_CONSTANT(TRACE_STRING_EXEC | DBG_FUNC_NONE,
+					dbg_arg1, dbg_arg2, dbg_arg3, dbg_arg4, 0);
+		}
+	}
+
+	/*
+	 * If posix_spawned with the START_SUSPENDED flag, stop the
+	 * process before it runs.
+	 */
+	if (imgp->ip_px_sa != NULL) {
+		psa = (struct _posix_spawnattr *) imgp->ip_px_sa;
+		if (psa->psa_flags & POSIX_SPAWN_START_SUSPENDED) {
+			proc_lock(p);
+			p->p_stat = SSTOP;
+			proc_unlock(p);
+			(void) task_suspend_internal(p->task);
+		}
+	}
+
+	/*
+	 * mark as execed, wakeup the process that vforked (if any) and tell
+	 * it that it now has its own resources back
+	 */
+	OSBitOrAtomic(P_EXEC, &p->p_flag);
+	proc_resetregister(p);
+	if (p->p_pptr && (p->p_lflag & P_LPPWAIT)) {
+		proc_lock(p);
+		p->p_lflag &= ~P_LPPWAIT;
+		proc_unlock(p);
+		wakeup((caddr_t)p->p_pptr);
+	}
+
+	/*
+	 * Pay for our earlier safety; deliver the delayed signals from
+	 * the incomplete vfexec process now that it's complete.
+	 */
+	if (vfexec && (p->p_lflag & P_LTRACED)) {
+		psignal_vfork(p, new_task, thread, SIGTRAP);
+	}
+
+	goto done;
+
+badtoolate:
+	/* Don't allow child process to execute any instructions */
+	if (!spawn) {
+		if (vfexec) {
+			psignal_vfork(p, new_task, thread, SIGKILL);
+		} else {
+			psignal(p, SIGKILL);
+		}
+
+		/* We can't stop this system call at this point, so just pretend we succeeded */
+		error = 0;
+	}
+	
+done:
+	if (!spawn) {
+		/* notify only if it has not failed due to FP Key error */
+		if ((p->p_lflag & P_LTERM_DECRYPTFAIL) == 0)
+			proc_knote(p, NOTE_EXEC);
+	}
+
+	/* Drop extra references for cases where we don't expect the caller to clean up */
+	if (vfexec || (spawn && error == 0)) {
+		task_deallocate(new_task);
+		thread_deallocate(thread);
+	}
+
+bad:
+	return(error);
+}
+
+
+
+
+/*
+ * Our image activator table; this is the table of the image types we are
+ * capable of loading.  We list them in order of preference to ensure the
+ * fastest image load speed.
+ *
+ * XXX hardcoded, for now; should use linker sets
+ */
+struct execsw {
+	int (*ex_imgact)(struct image_params *);
+	const char *ex_name;
+} execsw[] = {
+	{ exec_mach_imgact,		"Mach-o Binary" },
+	{ exec_fat_imgact,		"Fat Binary" },
+	{ exec_shell_imgact,		"Interpreter Script" },
+	{ NULL, NULL}
+};
+
+
+/*
+ * exec_activate_image
+ *
+ * Description:	Iterate through the available image activators, and activate
+ *		the image associated with the imgp structure.  We start with
+ *		the
+ *
+ * Parameters:	struct image_params *	Image parameter block
+ *
+ * Returns:	0			Success
+ *		EBADEXEC		The executable is corrupt/unknown
+ *	execargs_alloc:EINVAL		Invalid argument
+ *	execargs_alloc:EACCES		Permission denied
+ *	execargs_alloc:EINTR		Interrupted function
+ *	execargs_alloc:ENOMEM		Not enough space
+ *	exec_save_path:EFAULT		Bad address
+ *	exec_save_path:ENAMETOOLONG	Filename too long
+ *	exec_check_permissions:EACCES	Permission denied
+ *	exec_check_permissions:ENOEXEC	Executable file format error
+ *	exec_check_permissions:ETXTBSY	Text file busy [misuse of error code]
+ *	exec_check_permissions:???
+ *	namei:???
+ *	vn_rdwr:???			[anything vn_rdwr can return]
+ *	<ex_imgact>:???			[anything an imgact can return]
+ */
+static int
+exec_activate_image(struct image_params *imgp)
+{
+	struct nameidata *ndp = NULL;
+	const char *excpath;
+	int error;
+	int resid;
+	int once = 1;	/* save SGUID-ness for interpreted files */
+	int i;
+	int itercount = 0;
+	proc_t p = vfs_context_proc(imgp->ip_vfs_context);
+
+	error = execargs_alloc(imgp);
+	if (error)
+		goto bad_notrans;
+	
+	error = exec_save_path(imgp, imgp->ip_user_fname, imgp->ip_seg, &excpath);
+	if (error) {
+		goto bad_notrans;
+	}
+
+	/* Use excpath, which contains the copyin-ed exec path */
+	DTRACE_PROC1(exec, uintptr_t, excpath);
+
+	MALLOC(ndp, struct nameidata *, sizeof(*ndp), M_TEMP, M_WAITOK | M_ZERO);
+	if (ndp == NULL) {
+		error = ENOMEM;
+		goto bad_notrans;
+	}
+
+	NDINIT(ndp, LOOKUP, OP_LOOKUP, FOLLOW | LOCKLEAF | AUDITVNPATH1,
+		   UIO_SYSSPACE, CAST_USER_ADDR_T(excpath), imgp->ip_vfs_context);
+
+again:
+	error = namei(ndp);
+	if (error)
+		goto bad_notrans;
+	imgp->ip_ndp = ndp;	/* successful namei(); call nameidone() later */
+	imgp->ip_vp = ndp->ni_vp;	/* if set, need to vnode_put() at some point */
+
+	/*
+	 * Before we start the transition from binary A to binary B, make
+	 * sure another thread hasn't started exiting the process.  We grab
+	 * the proc lock to check p_lflag initially, and the transition
+	 * mechanism ensures that the value doesn't change after we release
+	 * the lock.
+	 */
+	proc_lock(p);
+	if (p->p_lflag & P_LEXIT) {
+		proc_unlock(p);
+		goto bad_notrans;
+	}
+	error = proc_transstart(p, 1, 0);
+	proc_unlock(p);
+	if (error)
+		goto bad_notrans;
+
+	error = exec_check_permissions(imgp);
+	if (error)
+		goto bad;
+
+	/* Copy; avoid invocation of an interpreter overwriting the original */
+	if (once) {
+		once = 0;
+		*imgp->ip_origvattr = *imgp->ip_vattr;
+	}
+
+	error = vn_rdwr(UIO_READ, imgp->ip_vp, imgp->ip_vdata, PAGE_SIZE, 0,
+			UIO_SYSSPACE, IO_NODELOCKED,
+			vfs_context_ucred(imgp->ip_vfs_context),
+			&resid, vfs_context_proc(imgp->ip_vfs_context));
+	if (error)
+		goto bad;
+
+	if (resid) {
+		memset(imgp->ip_vdata + (PAGE_SIZE - resid), 0x0, resid);
+	}
+
+encapsulated_binary:
+	/* Limit the number of iterations we will attempt on each binary */
+	if (++itercount > EAI_ITERLIMIT) {
+		error = EBADEXEC;
+		goto bad;
+	}
+	error = -1;
+	for(i = 0; error == -1 && execsw[i].ex_imgact != NULL; i++) {
+
+		error = (*execsw[i].ex_imgact)(imgp);
+
+		switch (error) {
+		/* case -1: not claimed: continue */
+		case -2:		/* Encapsulated binary, imgp->ip_XXX set for next iteration */
+			goto encapsulated_binary;
+
+		case -3:		/* Interpreter */
+#if CONFIG_MACF
+			/*
+			 * Copy the script label for later use. Note that
+			 * the label can be different when the script is
+			 * actually read by the interpreter.
+			 */
+			if (imgp->ip_scriptlabelp)
+				mac_vnode_label_free(imgp->ip_scriptlabelp);
+			imgp->ip_scriptlabelp = mac_vnode_label_alloc();
+			if (imgp->ip_scriptlabelp == NULL) {
+				error = ENOMEM;
+				break;
+			}
+			mac_vnode_label_copy(imgp->ip_vp->v_label,
+					     imgp->ip_scriptlabelp);
+
+			/*
+			 * Take a ref of the script vnode for later use.
+			 */
+			if (imgp->ip_scriptvp)
+				vnode_put(imgp->ip_scriptvp);
+			if (vnode_getwithref(imgp->ip_vp) == 0)
+				imgp->ip_scriptvp = imgp->ip_vp;
+#endif
+
+			nameidone(ndp);
+
+			vnode_put(imgp->ip_vp);
+			imgp->ip_vp = NULL;	/* already put */
+			imgp->ip_ndp = NULL; /* already nameidone */
+
+			/* Use excpath, which exec_shell_imgact reset to the interpreter */
+			NDINIT(ndp, LOOKUP, OP_LOOKUP, FOLLOW | LOCKLEAF,
+				   UIO_SYSSPACE, CAST_USER_ADDR_T(excpath), imgp->ip_vfs_context);
+
+			proc_transend(p, 0);
+			goto again;
+
+		default:
+			break;
+		}
+	}
+
+	/*
+	 * Call out to allow 3rd party notification of exec. 
+	 * Ignore result of kauth_authorize_fileop call.
+	 */
+	if (error == 0 && kauth_authorize_fileop_has_listeners()) {
+		kauth_authorize_fileop(vfs_context_ucred(imgp->ip_vfs_context),
+					KAUTH_FILEOP_EXEC,
+					(uintptr_t)ndp->ni_vp, 0);
+	}
+
+bad:
+	proc_transend(p, 0);
+
+bad_notrans:
+	if (imgp->ip_strings)
+		execargs_free(imgp);
+	if (imgp->ip_ndp)
+		nameidone(imgp->ip_ndp);
+	if (ndp)
+		FREE(ndp, M_TEMP);
+
+	return (error);
+}
+
+
+/*
+ * exec_handle_spawnattr_policy
+ *
+ * Description: Decode and apply the posix_spawn apptype, qos clamp, and watchport ports to the task.
+ *
+ * Parameters:  proc_t p                process to apply attributes to
+ *              int psa_apptype         posix spawn attribute apptype
+ *
+ * Returns:     0                       Success
+ */
+static errno_t
+exec_handle_spawnattr_policy(proc_t p, int psa_apptype, uint64_t psa_qos_clamp, uint64_t psa_darwin_role,
+                             ipc_port_t * portwatch_ports, int portwatch_count)
+{
+	int apptype     = TASK_APPTYPE_NONE;
+	int qos_clamp   = THREAD_QOS_UNSPECIFIED;
+	int role        = TASK_UNSPECIFIED;
+
+	if ((psa_apptype & POSIX_SPAWN_PROC_TYPE_MASK) != 0) {
+		int proctype = psa_apptype & POSIX_SPAWN_PROC_TYPE_MASK;
+
+		switch(proctype) {
+			case POSIX_SPAWN_PROC_TYPE_DAEMON_INTERACTIVE:
+				apptype = TASK_APPTYPE_DAEMON_INTERACTIVE;
+				break;
+			case POSIX_SPAWN_PROC_TYPE_DAEMON_STANDARD:
+				apptype = TASK_APPTYPE_DAEMON_STANDARD;
+				break;
+			case POSIX_SPAWN_PROC_TYPE_DAEMON_ADAPTIVE:
+				apptype = TASK_APPTYPE_DAEMON_ADAPTIVE;
+				break;
+			case POSIX_SPAWN_PROC_TYPE_DAEMON_BACKGROUND:
+				apptype = TASK_APPTYPE_DAEMON_BACKGROUND;
+				break;
+			case POSIX_SPAWN_PROC_TYPE_APP_DEFAULT:
+				apptype = TASK_APPTYPE_APP_DEFAULT;
+				break;
+			case POSIX_SPAWN_PROC_TYPE_APP_TAL:
+				apptype = TASK_APPTYPE_APP_TAL;
+				break;
+			default:
+				apptype = TASK_APPTYPE_NONE;
+				/* TODO: Should an invalid value here fail the spawn? */
+				break;
+		}
+	}
+
+	if (psa_qos_clamp != POSIX_SPAWN_PROC_CLAMP_NONE) {
+		switch (psa_qos_clamp) {
+			case POSIX_SPAWN_PROC_CLAMP_UTILITY:
+				qos_clamp = THREAD_QOS_UTILITY;
+				break;
+			case POSIX_SPAWN_PROC_CLAMP_BACKGROUND:
+				qos_clamp = THREAD_QOS_BACKGROUND;
+				break;
+			case POSIX_SPAWN_PROC_CLAMP_MAINTENANCE:
+				qos_clamp = THREAD_QOS_MAINTENANCE;
+				break;
+			default:
+				qos_clamp = THREAD_QOS_UNSPECIFIED;
+				/* TODO: Should an invalid value here fail the spawn? */
+				break;
+		}
+	}
+
+	if (psa_darwin_role != PRIO_DARWIN_ROLE_DEFAULT) {
+		proc_darwin_role_to_task_role(psa_darwin_role, &role);
+	}
+
+	if (apptype   != TASK_APPTYPE_NONE      ||
+	    qos_clamp != THREAD_QOS_UNSPECIFIED ||
+	    role      != TASK_UNSPECIFIED) {
+		proc_set_task_spawnpolicy(p->task, apptype, qos_clamp, role,
+		                          portwatch_ports, portwatch_count);
+	}
+
+	return (0);
+}
+
+
+/*
+ * exec_handle_port_actions
+ *
+ * Description:	Go through the _posix_port_actions_t contents, 
+ * 		calling task_set_special_port, task_set_exception_ports
+ * 		and/or audit_session_spawnjoin for the current task.
+ *
+ * Parameters:	struct image_params *	Image parameter block
+ * 		short psa_flags		posix spawn attribute flags
+ *
+ * Returns:	0			Success
+ * 		EINVAL			Failure
+ * 		ENOTSUP			Illegal posix_spawn attr flag was set
+ */
+static errno_t
+exec_handle_port_actions(struct image_params *imgp, short psa_flags, boolean_t * portwatch_present, ipc_port_t * portwatch_ports)
+{
+	_posix_spawn_port_actions_t pacts = imgp->ip_px_spa;
+	proc_t p = vfs_context_proc(imgp->ip_vfs_context);
+	_ps_port_action_t *act = NULL;
+	task_t task = p->task;
+	ipc_port_t port = NULL;
+	errno_t ret = 0;
+	int i;
+
+	*portwatch_present = FALSE;
+
+	for (i = 0; i < pacts->pspa_count; i++) {
+		act = &pacts->pspa_actions[i];
+
+		if (ipc_object_copyin(get_task_ipcspace(current_task()),
+		    act->new_port, MACH_MSG_TYPE_COPY_SEND,
+		    (ipc_object_t *) &port) != KERN_SUCCESS) {
+			ret = EINVAL;
+			goto done;
+		}
+
+		switch (act->port_type) {
+		case PSPA_SPECIAL:
+			/* Only allowed when not under vfork */
+			if (!(psa_flags & POSIX_SPAWN_SETEXEC))
+				ret = ENOTSUP;
+			else if (task_set_special_port(task,
+			act->which, port) != KERN_SUCCESS)
+				ret = EINVAL;
+			break;
+
+		case PSPA_EXCEPTION:
+			/* Only allowed when not under vfork */
+			if (!(psa_flags & POSIX_SPAWN_SETEXEC))
+				ret = ENOTSUP;
+			else if (task_set_exception_ports(task, 
+			act->mask, port, act->behavior, 
+			act->flavor) != KERN_SUCCESS)
+				ret = EINVAL;
+			break;
+#if CONFIG_AUDIT
+		case PSPA_AU_SESSION:
+			ret = audit_session_spawnjoin(p, port);
+			break;
+#endif
+		case PSPA_IMP_WATCHPORTS:
+			if (portwatch_ports != NULL) {
+				*portwatch_present = TRUE;
+				/* hold on to this till end of spawn */
+				portwatch_ports[i] = port;
+				ret = 0;
+			} else
+				ipc_port_release_send(port);
+			break;
+		default:
+			ret = EINVAL;
+			break;
+		}
+
+		/* action failed, so release port resources */
+
+		if (ret) { 
+			ipc_port_release_send(port);
+			break;
+		}
+	}
+
+done:
+	if (0 != ret)
+		DTRACE_PROC1(spawn__port__failure, mach_port_name_t, act->new_port);
+	return (ret);
+}
+
+/*
+ * exec_handle_file_actions
+ *
+ * Description:	Go through the _posix_file_actions_t contents applying the
+ *		open, close, and dup2 operations to the open file table for
+ *		the current process.
+ *
+ * Parameters:	struct image_params *	Image parameter block
+ *
+ * Returns:	0			Success
+ *		???
+ *
+ * Note:	Actions are applied in the order specified, with the credential
+ *		of the parent process.  This is done to permit the parent
+ *		process to utilize POSIX_SPAWN_RESETIDS to drop privilege in
+ *		the child following operations the child may in fact not be
+ *		normally permitted to perform.
+ */
+static int
+exec_handle_file_actions(struct image_params *imgp, short psa_flags)
+{
+	int error = 0;
+	int action;
+	proc_t p = vfs_context_proc(imgp->ip_vfs_context);
+	_posix_spawn_file_actions_t px_sfap = imgp->ip_px_sfa;
+	int ival[2];		/* dummy retval for system calls) */
+
+	for (action = 0; action < px_sfap->psfa_act_count; action++) {
+		_psfa_action_t *psfa = &px_sfap->psfa_act_acts[ action];
+
+		switch(psfa->psfaa_type) {
+		case PSFA_OPEN: {
+			/*
+			 * Open is different, in that it requires the use of
+			 * a path argument, which is normally copied in from
+			 * user space; because of this, we have to support an
+			 * open from kernel space that passes an address space
+			 * context of UIO_SYSSPACE, and casts the address
+			 * argument to a user_addr_t.
+			 */
+			char *bufp = NULL;
+			struct vnode_attr *vap;
+			struct nameidata *ndp;
+			int mode = psfa->psfaa_openargs.psfao_mode;
+			struct dup2_args dup2a;
+			struct close_nocancel_args ca;
+			int origfd;
+
+			MALLOC(bufp, char *, sizeof(*vap) + sizeof(*ndp), M_TEMP, M_WAITOK | M_ZERO);
+			if (bufp == NULL) {
+				error = ENOMEM;
+				break;
+			}
+
+			vap = (struct vnode_attr *) bufp;
+			ndp = (struct nameidata *) (bufp + sizeof(*vap));
+
+			VATTR_INIT(vap);
+			/* Mask off all but regular access permissions */
+			mode = ((mode &~ p->p_fd->fd_cmask) & ALLPERMS) & ~S_ISTXT;
+			VATTR_SET(vap, va_mode, mode & ACCESSPERMS);
+
+			NDINIT(ndp, LOOKUP, OP_OPEN, FOLLOW | AUDITVNPATH1, UIO_SYSSPACE,
+			       CAST_USER_ADDR_T(psfa->psfaa_openargs.psfao_path),
+			       imgp->ip_vfs_context);
+
+			error = open1(imgp->ip_vfs_context, 
+					ndp,
+					psfa->psfaa_openargs.psfao_oflag,
+					vap,
+					fileproc_alloc_init, NULL,
+					ival);
+
+			FREE(bufp, M_TEMP);
+
+			/*
+			 * If there's an error, or we get the right fd by
+			 * accident, then drop out here.  This is easier than
+			 * reworking all the open code to preallocate fd
+			 * slots, and internally taking one as an argument.
+			 */
+			if (error || ival[0] == psfa->psfaa_filedes)
+				break;
+
+			origfd = ival[0];
+			/*
+			 * If we didn't fall out from an error, we ended up
+			 * with the wrong fd; so now we've got to try to dup2
+			 * it to the right one.
+			 */
+			dup2a.from = origfd;
+			dup2a.to = psfa->psfaa_filedes;
+
+			/*
+			 * The dup2() system call implementation sets
+			 * ival to newfd in the success case, but we
+			 * can ignore that, since if we didn't get the
+			 * fd we wanted, the error will stop us.
+			 */
+			error = dup2(p, &dup2a, ival);
+			if (error)
+				break;
+
+			/*
+			 * Finally, close the original fd.
+			 */
+			ca.fd = origfd;
+
+			error = close_nocancel(p, &ca, ival);
+			}
+			break;
+
+		case PSFA_DUP2: {
+			struct dup2_args dup2a;
+
+			dup2a.from = psfa->psfaa_filedes;
+			dup2a.to = psfa->psfaa_openargs.psfao_oflag;
+
+			/*
+			 * The dup2() system call implementation sets
+			 * ival to newfd in the success case, but we
+			 * can ignore that, since if we didn't get the
+			 * fd we wanted, the error will stop us.
+			 */
+			error = dup2(p, &dup2a, ival);
+			}
+			break;
+
+		case PSFA_CLOSE: {
+			struct close_nocancel_args ca;
+
+			ca.fd = psfa->psfaa_filedes;
+
+			error = close_nocancel(p, &ca, ival);
+			}
+			break;
+
+		case PSFA_INHERIT: {
+			struct fcntl_nocancel_args fcntla;
+
+			/*
+			 * Check to see if the descriptor exists, and
+			 * ensure it's -not- marked as close-on-exec.
+			 *
+			 * Attempting to "inherit" a guarded fd will
+			 * result in a error.
+			 */
+			fcntla.fd = psfa->psfaa_filedes;
+			fcntla.cmd = F_GETFD;
+			if ((error = fcntl_nocancel(p, &fcntla, ival)) != 0)
+				break;
+
+			if ((ival[0] & FD_CLOEXEC) == FD_CLOEXEC) {
+				fcntla.fd = psfa->psfaa_filedes;
+				fcntla.cmd = F_SETFD;
+				fcntla.arg = ival[0] & ~FD_CLOEXEC;
+				error = fcntl_nocancel(p, &fcntla, ival);
+			}
+
+			}
+			break;
+
+		default:
+			error = EINVAL;
+			break;
+		}
+
+		/* All file actions failures are considered fatal, per POSIX */
+
+		if (error) {
+			if (PSFA_OPEN == psfa->psfaa_type) {
+				DTRACE_PROC1(spawn__open__failure, uintptr_t,
+			            psfa->psfaa_openargs.psfao_path);
+			} else {
+				DTRACE_PROC1(spawn__fd__failure, int, psfa->psfaa_filedes);
+			}
+			break;
+		}
+	}
+
+	if (error != 0 || (psa_flags & POSIX_SPAWN_CLOEXEC_DEFAULT) == 0)
+		return (error);
+
+	/*
+	 * If POSIX_SPAWN_CLOEXEC_DEFAULT is set, behave (during
+	 * this spawn only) as if "close on exec" is the default
+	 * disposition of all pre-existing file descriptors.  In this case,
+	 * the list of file descriptors mentioned in the file actions
+	 * are the only ones that can be inherited, so mark them now.
+	 *
+	 * The actual closing part comes later, in fdexec().
+	 */
+	proc_fdlock(p);
+	for (action = 0; action < px_sfap->psfa_act_count; action++) {
+		_psfa_action_t *psfa = &px_sfap->psfa_act_acts[action];
+		int fd = psfa->psfaa_filedes;
+
+		switch (psfa->psfaa_type) {
+		case PSFA_DUP2:
+			fd = psfa->psfaa_openargs.psfao_oflag;
+			/*FALLTHROUGH*/
+		case PSFA_OPEN:
+		case PSFA_INHERIT:
+			*fdflags(p, fd) |= UF_INHERIT;
+			break;
+
+		case PSFA_CLOSE:
+			break;
+		}
+	}
+	proc_fdunlock(p);
+
+	return (0);
+}
+
+#if CONFIG_MACF
+/*
+ * exec_spawnattr_getmacpolicyinfo
+ */
+void *
+exec_spawnattr_getmacpolicyinfo(const void *macextensions, const char *policyname, size_t *lenp)
+{
+	const struct _posix_spawn_mac_policy_extensions *psmx = macextensions;
+	int i;
+
+	if (psmx == NULL)
+		return NULL;
+
+	for (i = 0; i < psmx->psmx_count; i++) {
+		const _ps_mac_policy_extension_t *extension = &psmx->psmx_extensions[i];
+		if (strncmp(extension->policyname, policyname, sizeof(extension->policyname)) == 0) {
+			if (lenp != NULL)
+				*lenp = extension->datalen;
+			return extension->datap;
+		}
+	}
+
+	if (lenp != NULL)
+		*lenp = 0;
+	return NULL;
+}
+
+static int
+spawn_copyin_macpolicyinfo(const struct user__posix_spawn_args_desc *px_args, _posix_spawn_mac_policy_extensions_t *psmxp)
+{
+	_posix_spawn_mac_policy_extensions_t psmx = NULL;
+	int error = 0;
+	int copycnt = 0;
+	int i = 0;
+
+	*psmxp = NULL;
+
+	if (px_args->mac_extensions_size < PS_MAC_EXTENSIONS_SIZE(1) ||
+	    px_args->mac_extensions_size > PAGE_SIZE) {
+		error = EINVAL;
+		goto bad;
+	}
+
+	MALLOC(psmx, _posix_spawn_mac_policy_extensions_t, px_args->mac_extensions_size, M_TEMP, M_WAITOK);
+	if ((error = copyin(px_args->mac_extensions, psmx, px_args->mac_extensions_size)) != 0)
+		goto bad;
+
+	if (PS_MAC_EXTENSIONS_SIZE(psmx->psmx_count) > px_args->mac_extensions_size) {
+		error = EINVAL;
+		goto bad;
+	}
+
+	for (i = 0; i < psmx->psmx_count; i++) {
+		_ps_mac_policy_extension_t *extension = &psmx->psmx_extensions[i];
+		if (extension->datalen == 0 || extension->datalen > PAGE_SIZE) {
+			error = EINVAL;
+			goto bad;
+		}
+	}
+
+	for (copycnt = 0; copycnt < psmx->psmx_count; copycnt++) {
+		_ps_mac_policy_extension_t *extension = &psmx->psmx_extensions[copycnt];
+		void *data = NULL;
+
+		MALLOC(data, void *, extension->datalen, M_TEMP, M_WAITOK);
+		if ((error = copyin(extension->data, data, extension->datalen)) != 0) {
+			FREE(data, M_TEMP);
+			goto bad;
+		}
+		extension->datap = data;
+	}
+
+	*psmxp = psmx;
+	return 0;
+
+bad:
+	if (psmx != NULL) {
+		for (i = 0; i < copycnt; i++)
+			FREE(psmx->psmx_extensions[i].datap, M_TEMP);
+		FREE(psmx, M_TEMP);
+	}
+	return error;
+}
+
+static void
+spawn_free_macpolicyinfo(_posix_spawn_mac_policy_extensions_t psmx)
+{
+	int i;
+
+	if (psmx == NULL)
+		return;
+	for (i = 0; i < psmx->psmx_count; i++)
+		FREE(psmx->psmx_extensions[i].datap, M_TEMP);
+	FREE(psmx, M_TEMP);
+}
+#endif /* CONFIG_MACF */
+
+#if CONFIG_COALITIONS
+static inline void spawn_coalitions_release_all(coalition_t coal[COALITION_NUM_TYPES])
+{
+	for (int c = 0; c < COALITION_NUM_TYPES; c++) {
+		if (coal[c]) {
+			coalition_remove_active(coal[c]);
+			coalition_release(coal[c]);
+		}
+	}
+}
+#endif
+
+void
+proc_set_return_wait(proc_t p)
+{
+	proc_lock(p);
+	p->p_lflag |= P_LRETURNWAIT;
+	proc_unlock(p);
+}
+
+void
+proc_clear_return_wait(proc_t p, thread_t child_thread)
+{
+	proc_lock(p);
+
+	p->p_lflag &= ~P_LRETURNWAIT;
+	if (p->p_lflag & P_LRETURNWAITER) {
+		wakeup(&p->p_lflag);
+	}
+
+	proc_unlock(p);
+
+	(void)thread_resume(child_thread);
+}
+
+void
+proc_wait_to_return()
+{
+	proc_t	p;
+
+	p = current_proc();
+	proc_lock(p);
+
+	if (p->p_lflag & P_LRETURNWAIT) {
+		p->p_lflag |= P_LRETURNWAITER;
+		do {
+			msleep(&p->p_lflag, &p->p_mlock, 0,
+				"thread_check_setup_complete", NULL);
+		} while (p->p_lflag & P_LRETURNWAIT);
+		p->p_lflag &= ~P_LRETURNWAITER;
+	}
+
+	proc_unlock(p);
+	thread_bootstrap_return();
+}
+
+/*
+ * posix_spawn
+ *
+ * Parameters:	uap->pid		Pointer to pid return area
+ *		uap->fname		File name to exec
+ *		uap->argp		Argument list
+ *		uap->envp		Environment list
+ *
+ * Returns:	0			Success
+ *		EINVAL			Invalid argument
+ *		ENOTSUP			Not supported
+ *		ENOEXEC			Executable file format error
+ *	exec_activate_image:EINVAL	Invalid argument
+ *	exec_activate_image:EACCES	Permission denied
+ *	exec_activate_image:EINTR	Interrupted function
+ *	exec_activate_image:ENOMEM	Not enough space
+ *	exec_activate_image:EFAULT	Bad address
+ *	exec_activate_image:ENAMETOOLONG	Filename too long
+ *	exec_activate_image:ENOEXEC	Executable file format error
+ *	exec_activate_image:ETXTBSY	Text file busy [misuse of error code]
+ *	exec_activate_image:EBADEXEC	The executable is corrupt/unknown
+ *	exec_activate_image:???
+ *	mac_execve_enter:???
+ *
+ * TODO:	Expect to need __mac_posix_spawn() at some point...
+ *		Handle posix_spawnattr_t
+ *		Handle posix_spawn_file_actions_t
+ */
+int
+posix_spawn(proc_t ap, struct posix_spawn_args *uap, int32_t *retval)
+{
+	proc_t p = ap;		/* quiet bogus GCC vfork() warning */
+	user_addr_t pid = uap->pid;
+	int ival[2];		/* dummy retval for setpgid() */
+	char *bufp = NULL; 
+	struct image_params *imgp;
+	struct vnode_attr *vap;
+	struct vnode_attr *origvap;
+	struct uthread	*uthread = 0;	/* compiler complains if not set to 0*/
+	int error, sig;
+	int is_64 = IS_64BIT_PROCESS(p);
+	struct vfs_context context;
+	struct user__posix_spawn_args_desc px_args;
+	struct _posix_spawnattr px_sa;
+	_posix_spawn_file_actions_t px_sfap = NULL;
+	_posix_spawn_port_actions_t px_spap = NULL;
+	struct __kern_sigaction vec;
+	boolean_t spawn_no_exec = FALSE;
+	boolean_t proc_transit_set = TRUE;
+	boolean_t exec_done = FALSE;
+	int portwatch_count = 0;
+	ipc_port_t * portwatch_ports = NULL;
+	vm_size_t px_sa_offset = offsetof(struct _posix_spawnattr, psa_ports); 
+
+	/*
+	 * Allocate a big chunk for locals instead of using stack since these  
+	 * structures are pretty big.
+	 */
+	MALLOC(bufp, char *, (sizeof(*imgp) + sizeof(*vap) + sizeof(*origvap)), M_TEMP, M_WAITOK | M_ZERO);
+	imgp = (struct image_params *) bufp;
+	if (bufp == NULL) {
+		error = ENOMEM;
+		goto bad;
+	}
+	vap = (struct vnode_attr *) (bufp + sizeof(*imgp));
+	origvap = (struct vnode_attr *) (bufp + sizeof(*imgp) + sizeof(*vap));
+
+	/* Initialize the common data in the image_params structure */
+	imgp->ip_user_fname = uap->path;
+	imgp->ip_user_argv = uap->argv;
+	imgp->ip_user_envv = uap->envp;
+	imgp->ip_vattr = vap;
+	imgp->ip_origvattr = origvap;
+	imgp->ip_vfs_context = &context;
+	imgp->ip_flags = (is_64 ? IMGPF_WAS_64BIT : IMGPF_NONE);
+	imgp->ip_seg = (is_64 ? UIO_USERSPACE64 : UIO_USERSPACE32);
+	imgp->ip_mac_return = 0;
+	imgp->ip_reserved = NULL;
+
+	if (uap->adesc != USER_ADDR_NULL) {
+		if(is_64) {
+			error = copyin(uap->adesc, &px_args, sizeof(px_args));
+		} else {
+			struct user32__posix_spawn_args_desc px_args32;
+
+			error = copyin(uap->adesc, &px_args32, sizeof(px_args32));
+
+			/*
+			 * Convert arguments descriptor from external 32 bit
+			 * representation to internal 64 bit representation
+			 */
+			px_args.attr_size = px_args32.attr_size;
+			px_args.attrp = CAST_USER_ADDR_T(px_args32.attrp);
+			px_args.file_actions_size = px_args32.file_actions_size;
+			px_args.file_actions = CAST_USER_ADDR_T(px_args32.file_actions);
+			px_args.port_actions_size = px_args32.port_actions_size;
+			px_args.port_actions = CAST_USER_ADDR_T(px_args32.port_actions);
+			px_args.mac_extensions_size = px_args32.mac_extensions_size;
+			px_args.mac_extensions = CAST_USER_ADDR_T(px_args32.mac_extensions);
+			px_args.coal_info_size = px_args32.coal_info_size;
+			px_args.coal_info = CAST_USER_ADDR_T(px_args32.coal_info);
+			px_args.reserved = 0;
+			px_args.reserved_size = 0;
+		}
+		if (error)
+			goto bad;
+
+		if (px_args.attr_size != 0) {
+			/* 
+			 * We are not copying the port_actions pointer, 
+			 * because we already have it from px_args. 
+			 * This is a bit fragile: <rdar://problem/16427422>
+			 */
+
+			if ((error = copyin(px_args.attrp, &px_sa, px_sa_offset) != 0)) 
+			goto bad;
+		
+			bzero( (void *)( (unsigned long) &px_sa + px_sa_offset), sizeof(px_sa) - px_sa_offset );  	
+
+			imgp->ip_px_sa = &px_sa;
+		}
+		if (px_args.file_actions_size != 0) {
+			/* Limit file_actions to allowed number of open files */
+			int maxfa = (p->p_limit ? p->p_rlimit[RLIMIT_NOFILE].rlim_cur : NOFILE);
+			if (px_args.file_actions_size < PSF_ACTIONS_SIZE(1) ||
+				px_args.file_actions_size > PSF_ACTIONS_SIZE(maxfa)) {
+				error = EINVAL;
+				goto bad;
+			}
+			MALLOC(px_sfap, _posix_spawn_file_actions_t, px_args.file_actions_size, M_TEMP, M_WAITOK);
+			if (px_sfap == NULL) {
+				error = ENOMEM;
+				goto bad;
+			}
+			imgp->ip_px_sfa = px_sfap;
+
+			if ((error = copyin(px_args.file_actions, px_sfap, 
+							px_args.file_actions_size)) != 0)
+				goto bad;
+
+			/* Verify that the action count matches the struct size */
+			if (PSF_ACTIONS_SIZE(px_sfap->psfa_act_count) != px_args.file_actions_size) {
+				error = EINVAL;
+				goto bad;
+			}
+		}
+		if (px_args.port_actions_size != 0) {
+			/* Limit port_actions to one page of data */
+			if (px_args.port_actions_size < PS_PORT_ACTIONS_SIZE(1) ||
+				px_args.port_actions_size > PAGE_SIZE) {
+				error = EINVAL;
+				goto bad;
+			}
+
+			MALLOC(px_spap, _posix_spawn_port_actions_t, 
+					px_args.port_actions_size, M_TEMP, M_WAITOK);
+			if (px_spap == NULL) {
+				error = ENOMEM;
+				goto bad;
+			}
+			imgp->ip_px_spa = px_spap;
+
+			if ((error = copyin(px_args.port_actions, px_spap, 
+							px_args.port_actions_size)) != 0)
+				goto bad;
+
+			/* Verify that the action count matches the struct size */
+			if (PS_PORT_ACTIONS_SIZE(px_spap->pspa_count) != px_args.port_actions_size) {
+				error = EINVAL;
+				goto bad;
+			}
+		}
+
+#if CONFIG_MACF
+		if (px_args.mac_extensions_size != 0) {
+			if ((error = spawn_copyin_macpolicyinfo(&px_args, (_posix_spawn_mac_policy_extensions_t *)&imgp->ip_px_smpx)) != 0)
+				goto bad;
+		}
+#endif /* CONFIG_MACF */
+	}
+
+	/* set uthread to parent */
+	uthread = get_bsdthread_info(current_thread());
+
+	/*
+	 * <rdar://6640530>; this does not result in a behaviour change
+	 * relative to Leopard, so there should not be any existing code
+	 * which depends on it.
+	 */
+	if (uthread->uu_flag & UT_VFORK) {
+	    error = EINVAL;
+	    goto bad;
+	}
+
+	/*
+	 * If we don't have the extension flag that turns "posix_spawn()"
+	 * into "execve() with options", then we will be creating a new
+	 * process which does not inherit memory from the parent process,
+	 * which is one of the most expensive things about using fork()
+	 * and execve().
+	 */
+	if (imgp->ip_px_sa == NULL || !(px_sa.psa_flags & POSIX_SPAWN_SETEXEC)){
+
+		/* Set the new task's coalition, if it is requested.  */
+		coalition_t coal[COALITION_NUM_TYPES] = { COALITION_NULL };
+#if CONFIG_COALITIONS
+		int i, ncoals;
+		kern_return_t kr = KERN_SUCCESS;
+		struct _posix_spawn_coalition_info coal_info;
+		int coal_role[COALITION_NUM_TYPES];
+
+		if (imgp->ip_px_sa == NULL || !px_args.coal_info)
+			goto do_fork1;
+
+		memset(&coal_info, 0, sizeof(coal_info));
+
+		if (px_args.coal_info_size > sizeof(coal_info))
+			px_args.coal_info_size = sizeof(coal_info);
+		error = copyin(px_args.coal_info,
+			       &coal_info, px_args.coal_info_size);
+		if (error != 0)
+			goto bad;
+
+		ncoals = 0;
+		for (i = 0; i < COALITION_NUM_TYPES; i++) {
+			uint64_t cid = coal_info.psci_info[i].psci_id;
+			if (cid != 0) {
+				/*
+				 * don't allow tasks which are not in a
+				 * privileged coalition to spawn processes
+				 * into coalitions other than their own
+				 */
+				if (!task_is_in_privileged_coalition(p->task, i)) {
+					coal_dbg("ERROR: %d not in privilegd "
+						 "coalition of type %d",
+						 p->p_pid, i);
+					spawn_coalitions_release_all(coal);
+					error = EPERM;
+					goto bad;
+				}
+
+				coal_dbg("searching for coalition id:%llu", cid);
+				/*
+				 * take a reference and activation on the
+				 * coalition to guard against free-while-spawn
+				 * races
+				 */
+				coal[i] = coalition_find_and_activate_by_id(cid);
+				if (coal[i] == COALITION_NULL) {
+					coal_dbg("could not find coalition id:%llu "
+						 "(perhaps it has been terminated or reaped)", cid);
+					/*
+					 * release any other coalition's we
+					 * may have a reference to
+					 */
+					spawn_coalitions_release_all(coal);
+					error = ESRCH;
+					goto bad;
+				}
+				if (coalition_type(coal[i]) != i) {
+					coal_dbg("coalition with id:%lld is not of type:%d"
+						 " (it's type:%d)", cid, i, coalition_type(coal[i]));
+					error = ESRCH;
+					goto bad;
+				}
+				coal_role[i] = coal_info.psci_info[i].psci_role;
+				ncoals++;
+			}
+		}
+		if (ncoals < COALITION_NUM_TYPES) {
+			/*
+			 * If the user is attempting to spawn into a subset of
+			 * the known coalition types, then make sure they have
+			 * _at_least_ specified a resource coalition. If not,
+			 * the following fork1() call will implicitly force an
+			 * inheritance from 'p' and won't actually spawn the
+			 * new task into the coalitions the user specified.
+			 * (also the call to coalitions_set_roles will panic)
+			 */
+			if (coal[COALITION_TYPE_RESOURCE] == COALITION_NULL) {
+				spawn_coalitions_release_all(coal);
+				error = EINVAL;
+				goto bad;
+			}
+		}
+do_fork1:
+#endif /* CONFIG_COALITIONS */
+
+		error = fork1(p, &imgp->ip_new_thread, PROC_CREATE_SPAWN, coal);
+
+#if CONFIG_COALITIONS
+		/* set the roles of this task within each given coalition */
+		if (error == 0) {
+			kr = coalitions_set_roles(coal, get_threadtask(imgp->ip_new_thread), coal_role);
+			if (kr != KERN_SUCCESS)
+				error = EINVAL;
+		}
+
+		/* drop our references and activations - fork1() now holds them */
+		spawn_coalitions_release_all(coal);
+#endif /* CONFIG_COALITIONS */
+		if (error != 0) {
+			goto bad;
+		}
+		imgp->ip_flags |= IMGPF_SPAWN;	/* spawn w/o exec */
+		spawn_no_exec = TRUE;		/* used in later tests */
+
+	}
+
+	if (spawn_no_exec) {
+		p = (proc_t)get_bsdthreadtask_info(imgp->ip_new_thread);
+		
+		/*
+		 * We had to wait until this point before firing the
+		 * proc:::create probe, otherwise p would not point to the
+		 * child process.
+		 */
+		DTRACE_PROC1(create, proc_t, p);
+	}
+	assert(p != NULL);
+
+	/* By default, the thread everyone plays with is the parent */
+	context.vc_thread = current_thread();
+	context.vc_ucred = p->p_ucred;	/* XXX must NOT be kauth_cred_get() */
+
+	/*
+	 * However, if we're not in the setexec case, redirect the context
+	 * to the newly created process instead
+	 */
+	if (spawn_no_exec)
+		context.vc_thread = imgp->ip_new_thread;
+
+	/*
+	 * Post fdcopy(), pre exec_handle_sugid() - this is where we want
+	 * to handle the file_actions.  Since vfork() also ends up setting
+	 * us into the parent process group, and saved off the signal flags,
+	 * this is also where we want to handle the spawn flags.
+	 */
+
+	/* Has spawn file actions? */
+	if (imgp->ip_px_sfa != NULL) {
+		/*
+		 * The POSIX_SPAWN_CLOEXEC_DEFAULT flag
+		 * is handled in exec_handle_file_actions().
+		 */
+		if ((error = exec_handle_file_actions(imgp,
+		    imgp->ip_px_sa != NULL ? px_sa.psa_flags : 0)) != 0)
+			goto bad;
+	}
+
+	/* Has spawn port actions? */
+	if (imgp->ip_px_spa != NULL) {
+		boolean_t is_adaptive = FALSE;
+		boolean_t portwatch_present = FALSE;
+
+		/* Will this process become adaptive? The apptype isn't ready yet, so we can't look there. */
+		if (imgp->ip_px_sa != NULL && px_sa.psa_apptype == POSIX_SPAWN_PROC_TYPE_DAEMON_ADAPTIVE)
+			is_adaptive = TRUE;
+
+		/*
+		 * portwatch only:
+		 * Allocate a place to store the ports we want to bind to the new task
+		 * We can't bind them until after the apptype is set.
+		 */
+		if (px_spap->pspa_count != 0 && is_adaptive) {
+			portwatch_count = px_spap->pspa_count;
+			MALLOC(portwatch_ports, ipc_port_t *, (sizeof(ipc_port_t) * portwatch_count), M_TEMP, M_WAITOK | M_ZERO);
+		} else {
+			portwatch_ports = NULL;
+		}
+
+		if ((error = exec_handle_port_actions(imgp,
+		    imgp->ip_px_sa != NULL ? px_sa.psa_flags : 0, &portwatch_present, portwatch_ports)) != 0) 
+			goto bad;
+
+		if (portwatch_present == FALSE && portwatch_ports != NULL) {
+			FREE(portwatch_ports, M_TEMP);
+			portwatch_ports = NULL;
+			portwatch_count = 0;
+		}
+	}
+
+	/* Has spawn attr? */
+	if (imgp->ip_px_sa != NULL) {
+		/*
+		 * Set the process group ID of the child process; this has
+		 * to happen before the image activation.
+		 */
+		if (px_sa.psa_flags & POSIX_SPAWN_SETPGROUP) {
+			struct setpgid_args spga;
+			spga.pid = p->p_pid;
+			spga.pgid = px_sa.psa_pgroup;
+			/*
+			 * Effectively, call setpgid() system call; works
+			 * because there are no pointer arguments.
+			 */
+			if((error = setpgid(p, &spga, ival)) != 0)
+				goto bad;
+		}
+
+		/*
+		 * Reset UID/GID to parent's RUID/RGID; This works only
+		 * because the operation occurs *after* the vfork() and
+		 * before the call to exec_handle_sugid() by the image
+		 * activator called from exec_activate_image().  POSIX
+		 * requires that any setuid/setgid bits on the process
+		 * image will take precedence over the spawn attributes
+		 * (re)setting them.
+		 *
+		 * The use of p_ucred is safe, since we are acting on the
+		 * new process, and it has no threads other than the one
+		 * we are creating for it.
+		 */
+		if (px_sa.psa_flags & POSIX_SPAWN_RESETIDS) {
+			kauth_cred_t my_cred = p->p_ucred;
+			kauth_cred_t my_new_cred = kauth_cred_setuidgid(my_cred, kauth_cred_getruid(my_cred), kauth_cred_getrgid(my_cred));
+			if (my_new_cred != my_cred) {
+				p->p_ucred = my_new_cred;
+				/* update cred on proc */
+				PROC_UPDATE_CREDS_ONPROC(p);
+			}
+		}
+
+#if !SECURE_KERNEL
+		/*
+		 * Disable ASLR for the spawned process.
+		 *
+		 * But only do so if we are not embedded + RELEASE.
+		 * While embedded allows for a boot-arg (-disable_aslr)
+		 * to deal with this (which itself is only honored on
+		 * DEVELOPMENT or DEBUG builds of xnu), it is often
+		 * useful or necessary to disable ASLR on a per-process
+		 * basis for unit testing and debugging.
+		 */
+		if (px_sa.psa_flags & _POSIX_SPAWN_DISABLE_ASLR)
+			OSBitOrAtomic(P_DISABLE_ASLR, &p->p_flag);
+#endif /* !SECURE_KERNEL */
+
+		/*
+		 * Forcibly disallow execution from data pages for the spawned process
+		 * even if it would otherwise be permitted by the architecture default.
+		 */
+		if (px_sa.psa_flags & _POSIX_SPAWN_ALLOW_DATA_EXEC)
+			imgp->ip_flags |= IMGPF_ALLOW_DATA_EXEC;
+	}
+
+	/*
+	 * Disable ASLR during image activation.  This occurs either if the
+	 * _POSIX_SPAWN_DISABLE_ASLR attribute was found above or if
+	 * P_DISABLE_ASLR was inherited from the parent process.
+	 */
+	if (p->p_flag & P_DISABLE_ASLR)
+		imgp->ip_flags |= IMGPF_DISABLE_ASLR;
+
+	/* 
+	 * Clear transition flag so we won't hang if exec_activate_image() causes
+	 * an automount (and launchd does a proc sysctl to service it).
+	 *
+	 * <rdar://problem/6848672>, <rdar://problem/5959568>.
+	 */
+	if (spawn_no_exec) {
+		proc_transend(p, 0);
+		proc_transit_set = 0;
+	}
+
+#if MAC_SPAWN	/* XXX */
+	if (uap->mac_p != USER_ADDR_NULL) {
+		error = mac_execve_enter(uap->mac_p, imgp);
+		if (error)
+			goto bad;
+	}
+#endif
+
+	/*
+	 * Activate the image
+	 */
+	error = exec_activate_image(imgp);
+	
+	if (error == 0) {
+		/* process completed the exec */
+		exec_done = TRUE;
+	} else if (error == -1) {
+		/* Image not claimed by any activator? */
+		error = ENOEXEC;
+	}
+
+	/*
+	 * If we have a spawn attr, and it contains signal related flags,
+	 * the we need to process them in the "context" of the new child
+	 * process, so we have to process it following image activation,
+	 * prior to making the thread runnable in user space.  This is
+	 * necessitated by some signal information being per-thread rather
+	 * than per-process, and we don't have the new allocation in hand
+	 * until after the image is activated.
+	 */
+	if (!error && imgp->ip_px_sa != NULL) {
+		thread_t child_thread = current_thread();
+		uthread_t child_uthread = uthread;
+
+		/*
+		 * If we created a new child thread, then the thread and
+		 * uthread are different than the current ones; otherwise,
+		 * we leave them, since we are in the exec case instead.
+		 */
+		if (spawn_no_exec) {
+			child_thread = imgp->ip_new_thread;
+			child_uthread = get_bsdthread_info(child_thread);
+		}
+
+		/*
+		 * Mask a list of signals, instead of them being unmasked, if
+		 * they were unmasked in the parent; note that some signals
+		 * are not maskable.
+		 */
+		if (px_sa.psa_flags & POSIX_SPAWN_SETSIGMASK)
+			child_uthread->uu_sigmask = (px_sa.psa_sigmask & ~sigcantmask);
+		/*
+		 * Default a list of signals instead of ignoring them, if
+		 * they were ignored in the parent.  Note that we pass
+		 * spawn_no_exec to setsigvec() to indicate that we called
+		 * fork1() and therefore do not need to call proc_signalstart()
+		 * internally.
+		 */
+		if (px_sa.psa_flags & POSIX_SPAWN_SETSIGDEF) {
+			vec.sa_handler = SIG_DFL;
+			vec.sa_tramp = 0;
+			vec.sa_mask = 0;
+			vec.sa_flags = 0;
+			for (sig = 0; sig < NSIG; sig++)
+				if (px_sa.psa_sigdefault & (1 << sig)) {
+					error = setsigvec(p, child_thread, sig + 1, &vec, spawn_no_exec);
+			}
+		}
+
+		/*
+		 * Activate the CPU usage monitor, if requested. This is done via a task-wide, per-thread CPU
+		 * usage limit, which will generate a resource exceeded exception if any one thread exceeds the
+		 * limit.
+		 *
+		 * Userland gives us interval in seconds, and the kernel SPI expects nanoseconds.
+		 */
+		if (px_sa.psa_cpumonitor_percent != 0) {
+			/*
+			 * Always treat a CPU monitor activation coming from spawn as entitled. Requiring
+			 * an entitlement to configure the monitor a certain way seems silly, since
+			 * whomever is turning it on could just as easily choose not to do so.
+			 */
+			error = proc_set_task_ruse_cpu(p->task,
+					TASK_POLICY_RESOURCE_ATTRIBUTE_NOTIFY_EXC,
+					px_sa.psa_cpumonitor_percent,
+					px_sa.psa_cpumonitor_interval * NSEC_PER_SEC,
+					0, TRUE);
+		}
+	}
+
+bad:
+
+	if (error == 0) {
+		/* reset delay idle sleep status if set */
+		if ((p->p_flag & P_DELAYIDLESLEEP) == P_DELAYIDLESLEEP)
+			OSBitAndAtomic(~((uint32_t)P_DELAYIDLESLEEP), &p->p_flag);
+		/* upon  successful spawn, re/set the proc control state */
+		if (imgp->ip_px_sa != NULL) {
+			switch (px_sa.psa_pcontrol) {
+				case POSIX_SPAWN_PCONTROL_THROTTLE:
+					p->p_pcaction = P_PCTHROTTLE;
+					break;
+				case POSIX_SPAWN_PCONTROL_SUSPEND:
+					p->p_pcaction = P_PCSUSP;
+					break;
+				case POSIX_SPAWN_PCONTROL_KILL:
+					p->p_pcaction = P_PCKILL;
+					break;
+				case POSIX_SPAWN_PCONTROL_NONE:
+				default:
+					p->p_pcaction = 0;
+					break;
+			};
+		}
+		exec_resettextvp(p, imgp);
+		
+#if CONFIG_MEMORYSTATUS && CONFIG_JETSAM
+		/* Has jetsam attributes? */
+		if (imgp->ip_px_sa != NULL && (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_SET)) {
+			/*
+			 * With 2-level high-water-mark support, POSIX_SPAWN_JETSAM_HIWATER_BACKGROUND is no
+			 * longer relevant, as background limits are described via the inactive limit slots.
+			 * At the kernel layer, the flag is ignored.
+			 *
+			 * That said, however, if the POSIX_SPAWN_JETSAM_HIWATER_BACKGROUND is passed in,
+			 * we attempt to mimic previous behavior by forcing the BG limit data into the
+			 * inactive/non-fatal mode and force the active slots to hold system_wide/fatal mode.
+			 * The kernel layer will flag this mapping.
+			 */
+			if (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_HIWATER_BACKGROUND) {
+				memorystatus_update(p, px_sa.psa_priority, 0,
+					    (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_USE_EFFECTIVE_PRIORITY),
+					    TRUE,
+					    -1, TRUE,
+					    px_sa.psa_memlimit_inactive, FALSE,
+					    (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_HIWATER_BACKGROUND));
+			} else {
+				memorystatus_update(p, px_sa.psa_priority, 0,
+					    (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_USE_EFFECTIVE_PRIORITY),
+					    TRUE,
+					    px_sa.psa_memlimit_active,
+					    (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_MEMLIMIT_ACTIVE_FATAL),
+					    px_sa.psa_memlimit_inactive,
+					    (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_MEMLIMIT_INACTIVE_FATAL),
+					    (px_sa.psa_jetsam_flags & POSIX_SPAWN_JETSAM_HIWATER_BACKGROUND));
+			}
+
+		}
+#endif /* CONFIG_MEMORYSTATUS && CONFIG_JETSAM*/
+	}
+
+	/*
+	 * If we successfully called fork1(), we always need to do this;
+	 * we identify this case by noting the IMGPF_SPAWN flag.  This is
+	 * because we come back from that call with signals blocked in the
+	 * child, and we have to unblock them, but we want to wait until
+	 * after we've performed any spawn actions.  This has to happen
+	 * before check_for_signature(), which uses psignal.
+	 */
+	if (spawn_no_exec) {
+		if (proc_transit_set)
+			proc_transend(p, 0);
+
+		/*
+		 * Drop the signal lock on the child which was taken on our
+		 * behalf by forkproc()/cloneproc() to prevent signals being
+		 * received by the child in a partially constructed state.
+		 */
+		proc_signalend(p, 0);
+
+		/* flag the 'fork' has occurred */
+		proc_knote(p->p_pptr, NOTE_FORK | p->p_pid);
+		/* then flag exec has occurred */
+		/* notify only if it has not failed due to FP Key error */
+		if ((p->p_lflag & P_LTERM_DECRYPTFAIL) == 0)
+			proc_knote(p, NOTE_EXEC);
+	} else if (error == 0) {
+		/* reset the importance attribute from our previous life */
+		task_importance_reset(p->task);
+
+		/* reset atm context from task */
+		task_atm_reset(p->task);
+	}
+
+	/*
+	 * Apply the spawnattr policy, apptype (which primes the task for importance donation),
+	 * and bind any portwatch ports to the new task.
+	 * This must be done after the exec so that the child's thread is ready,
+	 * and after the in transit state has been released, because priority is
+	 * dropped here so we need to be prepared for a potentially long preemption interval
+	 *
+	 * TODO: Consider splitting this up into separate phases
+	 */
+	if (error == 0 && imgp->ip_px_sa != NULL) {
+		struct _posix_spawnattr *psa = (struct _posix_spawnattr *) imgp->ip_px_sa;
+
+		exec_handle_spawnattr_policy(p, psa->psa_apptype, psa->psa_qos_clamp, psa->psa_darwin_role,
+		                              portwatch_ports, portwatch_count);
+	}
+
+	/* Apply the main thread qos */
+	if (error == 0) {
+		thread_t main_thread = (imgp->ip_new_thread != NULL) ? imgp->ip_new_thread : current_thread();
+
+		task_set_main_thread_qos(p->task, main_thread);
+	}
+
+	/*
+	 * Release any ports we kept around for binding to the new task
+	 * We need to release the rights even if the posix_spawn has failed.
+	 */
+	if (portwatch_ports != NULL) {
+		for (int i = 0; i < portwatch_count; i++) {
+			ipc_port_t port = NULL;
+			if ((port = portwatch_ports[i]) != NULL) {
+				ipc_port_release_send(port);
+			}
+		}
+		FREE(portwatch_ports, M_TEMP);
+		portwatch_ports = NULL;
+		portwatch_count = 0;
+	}
+
+	/*
+	 * We have to delay operations which might throw a signal until after
+	 * the signals have been unblocked; however, we want that to happen
+	 * after exec_resettextvp() so that the textvp is correct when they
+	 * fire.
+	 */
+	if (error == 0) {
+		error = check_for_signature(p, imgp);
+
+		/*
+		 * Pay for our earlier safety; deliver the delayed signals from
+		 * the incomplete spawn process now that it's complete.
+		 */
+		if (imgp != NULL && spawn_no_exec && (p->p_lflag & P_LTRACED)) {
+			psignal_vfork(p, p->task, imgp->ip_new_thread, SIGTRAP);
+		}
+	}
+
+
+	if (imgp != NULL) {
+		if (imgp->ip_vp)
+			vnode_put(imgp->ip_vp);
+		if (imgp->ip_scriptvp)
+			vnode_put(imgp->ip_scriptvp);
+		if (imgp->ip_strings)
+			execargs_free(imgp);
+		if (imgp->ip_px_sfa != NULL)
+			FREE(imgp->ip_px_sfa, M_TEMP);
+		if (imgp->ip_px_spa != NULL)
+			FREE(imgp->ip_px_spa, M_TEMP);
+#if CONFIG_MACF
+		if (imgp->ip_px_smpx != NULL)
+			spawn_free_macpolicyinfo(imgp->ip_px_smpx);
+		if (imgp->ip_execlabelp)
+			mac_cred_label_free(imgp->ip_execlabelp);
+		if (imgp->ip_scriptlabelp)
+			mac_vnode_label_free(imgp->ip_scriptlabelp);
+#endif
+	}
+
+#if CONFIG_DTRACE
+	if (spawn_no_exec) {
+		/*
+		 * In the original DTrace reference implementation,
+		 * posix_spawn() was a libc routine that just
+		 * did vfork(2) then exec(2).  Thus the proc::: probes
+		 * are very fork/exec oriented.  The details of this
+		 * in-kernel implementation of posix_spawn() is different
+		 * (while producing the same process-observable effects)
+		 * particularly w.r.t. errors, and which thread/process
+		 * is constructing what on behalf of whom.
+		 */
+		if (error) {
+			DTRACE_PROC1(spawn__failure, int, error);
+		} else {
+			DTRACE_PROC(spawn__success);
+			/*
+			 * Some DTrace scripts, e.g. newproc.d in
+			 * /usr/bin, rely on the the 'exec-success'
+			 * probe being fired in the child after the
+			 * new process image has been constructed
+			 * in order to determine the associated pid.
+			 *
+			 * So, even though the parent built the image
+			 * here, for compatibility, mark the new thread
+			 * so 'exec-success' fires on it as it leaves
+			 * the kernel.
+			 */
+			dtrace_thread_didexec(imgp->ip_new_thread);
+		}
+	} else {
+		if (error) {
+			DTRACE_PROC1(exec__failure, int, error);
+		} else {
+			DTRACE_PROC(exec__success);
+		}
+	}
+
+	if ((dtrace_proc_waitfor_hook = dtrace_proc_waitfor_exec_ptr) != NULL)
+		(*dtrace_proc_waitfor_hook)(p);
+#endif
+
+	/* Return to both the parent and the child? */
+	if (imgp != NULL && spawn_no_exec) {
+		/*
+		 * If the parent wants the pid, copy it out
+		 */
+		if (pid != USER_ADDR_NULL)
+			(void)suword(pid, p->p_pid);
+		retval[0] = error;
+
+		/*
+		 * If we had an error, perform an internal reap ; this is
+		 * entirely safe, as we have a real process backing us.
+		 */
+		if (error) {
+			proc_list_lock();
+			p->p_listflag |= P_LIST_DEADPARENT;
+			proc_list_unlock();
+			proc_lock(p);
+			/* make sure no one else has killed it off... */
+			if (p->p_stat != SZOMB && p->exit_thread == NULL) {
+				p->exit_thread = current_thread();
+				proc_unlock(p);
+				exit1(p, 1, (int *)NULL);
+				proc_clear_return_wait(p, imgp->ip_new_thread);
+				if (exec_done == FALSE) {
+					task_deallocate(get_threadtask(imgp->ip_new_thread));
+					thread_deallocate(imgp->ip_new_thread);
+				}
+			} else {
+				/* someone is doing it for us; just skip it */
+				proc_unlock(p);
+				proc_clear_return_wait(p, imgp->ip_new_thread);
+			}
+		} else {
+
+			/*
+			 * Return to the child
+			 *
+			 * Note: the image activator earlier dropped the
+			 * task/thread references to the newly spawned
+			 * process; this is OK, since we still have suspended
+			 * queue references on them, so we should be fine
+			 * with the delayed resume of the thread here.
+			 */
+			proc_clear_return_wait(p, imgp->ip_new_thread);
+		}
+	}
+	if (bufp != NULL) {
+		FREE(bufp, M_TEMP);
+	}
+	
+	return(error);
+}
+
+
+/*
+ * execve
+ *
+ * Parameters:	uap->fname		File name to exec
+ *		uap->argp		Argument list
+ *		uap->envp		Environment list
+ *
+ * Returns:	0			Success
+ *	__mac_execve:EINVAL		Invalid argument
+ *	__mac_execve:ENOTSUP		Invalid argument
+ *	__mac_execve:EACCES		Permission denied
+ *	__mac_execve:EINTR		Interrupted function
+ *	__mac_execve:ENOMEM		Not enough space
+ *	__mac_execve:EFAULT		Bad address
+ *	__mac_execve:ENAMETOOLONG	Filename too long
+ *	__mac_execve:ENOEXEC		Executable file format error
+ *	__mac_execve:ETXTBSY		Text file busy [misuse of error code]
+ *	__mac_execve:???
+ *
+ * TODO:	Dynamic linker header address on stack is copied via suword()
+ */
+/* ARGSUSED */
+int
+execve(proc_t p, struct execve_args *uap, int32_t *retval)
+{
+	struct __mac_execve_args muap;
+	int err;
+
+	memoryshot(VM_EXECVE, DBG_FUNC_NONE);
+
+	muap.fname = uap->fname;
+	muap.argp = uap->argp;
+	muap.envp = uap->envp;
+	muap.mac_p = USER_ADDR_NULL;
+	err = __mac_execve(p, &muap, retval);
+
+	return(err);
+}
+
+/*
+ * __mac_execve
+ *
+ * Parameters:	uap->fname		File name to exec
+ *		uap->argp		Argument list
+ *		uap->envp		Environment list
+ *		uap->mac_p		MAC label supplied by caller
+ *
+ * Returns:	0			Success
+ *		EINVAL			Invalid argument
+ *		ENOTSUP			Not supported
+ *		ENOEXEC			Executable file format error
+ *	exec_activate_image:EINVAL	Invalid argument
+ *	exec_activate_image:EACCES	Permission denied
+ *	exec_activate_image:EINTR	Interrupted function
+ *	exec_activate_image:ENOMEM	Not enough space
+ *	exec_activate_image:EFAULT	Bad address
+ *	exec_activate_image:ENAMETOOLONG	Filename too long
+ *	exec_activate_image:ENOEXEC	Executable file format error
+ *	exec_activate_image:ETXTBSY	Text file busy [misuse of error code]
+ *	exec_activate_image:EBADEXEC	The executable is corrupt/unknown
+ *	exec_activate_image:???
+ *	mac_execve_enter:???
+ *
+ * TODO:	Dynamic linker header address on stack is copied via suword()
+ */
+int
+__mac_execve(proc_t p, struct __mac_execve_args *uap, int32_t *retval)
+{
+	char *bufp = NULL; 
+	struct image_params *imgp;
+	struct vnode_attr *vap;
+	struct vnode_attr *origvap;
+	int error;
+	int is_64 = IS_64BIT_PROCESS(p);
+	struct vfs_context context;
+	struct uthread	*uthread;
+
+	context.vc_thread = current_thread();
+	context.vc_ucred = kauth_cred_proc_ref(p);	/* XXX must NOT be kauth_cred_get() */
+
+	/* Allocate a big chunk for locals instead of using stack since these  
+	 * structures a pretty big.
+	 */
+	MALLOC(bufp, char *, (sizeof(*imgp) + sizeof(*vap) + sizeof(*origvap)), M_TEMP, M_WAITOK | M_ZERO);
+	imgp = (struct image_params *) bufp;
+	if (bufp == NULL) {
+		error = ENOMEM;
+		goto exit_with_error;
+	}
+	vap = (struct vnode_attr *) (bufp + sizeof(*imgp));
+	origvap = (struct vnode_attr *) (bufp + sizeof(*imgp) + sizeof(*vap));
+	
+	/* Initialize the common data in the image_params structure */
+	imgp->ip_user_fname = uap->fname;
+	imgp->ip_user_argv = uap->argp;
+	imgp->ip_user_envv = uap->envp;
+	imgp->ip_vattr = vap;
+	imgp->ip_origvattr = origvap;
+	imgp->ip_vfs_context = &context;
+	imgp->ip_flags = (is_64 ? IMGPF_WAS_64BIT : IMGPF_NONE) | ((p->p_flag & P_DISABLE_ASLR) ? IMGPF_DISABLE_ASLR : IMGPF_NONE);
+	imgp->ip_seg = (is_64 ? UIO_USERSPACE64 : UIO_USERSPACE32);
+	imgp->ip_mac_return = 0;
+
+	uthread = get_bsdthread_info(current_thread());
+	if (uthread->uu_flag & UT_VFORK) {
+		imgp->ip_flags |= IMGPF_VFORK_EXEC;
+	}
+
+#if CONFIG_MACF
+	if (uap->mac_p != USER_ADDR_NULL) {
+		error = mac_execve_enter(uap->mac_p, imgp);
+		if (error) {
+			kauth_cred_unref(&context.vc_ucred);
+			goto exit_with_error;
+		}
+	}
+#endif
+
+	error = exec_activate_image(imgp);
+
+	kauth_cred_unref(&context.vc_ucred);
+	
+	/* Image not claimed by any activator? */
+	if (error == -1)
+		error = ENOEXEC;
+
+	if (error == 0) {
+		exec_resettextvp(p, imgp);
+		error = check_for_signature(p, imgp);
+	}	
+	if (imgp->ip_vp != NULLVP)
+		vnode_put(imgp->ip_vp);
+	if (imgp->ip_scriptvp != NULLVP)
+		vnode_put(imgp->ip_scriptvp);
+	if (imgp->ip_strings)
+		execargs_free(imgp);
+#if CONFIG_MACF
+	if (imgp->ip_execlabelp)
+		mac_cred_label_free(imgp->ip_execlabelp);
+	if (imgp->ip_scriptlabelp)
+		mac_vnode_label_free(imgp->ip_scriptlabelp);
+#endif
+	if (!error) {
+		/* Sever any extant thread affinity */
+		thread_affinity_exec(current_thread());
+
+		thread_t main_thread = (imgp->ip_new_thread != NULL) ? imgp->ip_new_thread : current_thread();		
+
+		task_set_main_thread_qos(p->task, main_thread);
+
+		/* reset task importance */
+		task_importance_reset(p->task);
+
+		/* reset atm context from task */
+		task_atm_reset(p->task);
+
+		DTRACE_PROC(exec__success);
+
+#if CONFIG_DTRACE
+		if ((dtrace_proc_waitfor_hook = dtrace_proc_waitfor_exec_ptr) != NULL)
+			(*dtrace_proc_waitfor_hook)(p);
+#endif
+
+		if (imgp->ip_flags & IMGPF_VFORK_EXEC) {
+			vfork_return(p, retval, p->p_pid);
+			proc_clear_return_wait(p, imgp->ip_new_thread);
+		}
+	} else {
+		DTRACE_PROC1(exec__failure, int, error);
+	}
+
+exit_with_error:
+	if (bufp != NULL) {
+		FREE(bufp, M_TEMP);
+	}
+	
+	return(error);
+}
+
+
+/*
+ * copyinptr
+ *
+ * Description:	Copy a pointer in from user space to a user_addr_t in kernel
+ *		space, based on 32/64 bitness of the user space
+ *
+ * Parameters:	froma			User space address
+ *		toptr			Address of kernel space user_addr_t
+ *		ptr_size		4/8, based on 'froma' address space
+ *
+ * Returns:	0			Success
+ *		EFAULT			Bad 'froma'
+ *
+ * Implicit returns:
+ *		*ptr_size		Modified
+ */
+static int
+copyinptr(user_addr_t froma, user_addr_t *toptr, int ptr_size)
+{
+	int error;
+
+	if (ptr_size == 4) {
+		/* 64 bit value containing 32 bit address */
+		unsigned int i;
+
+		error = copyin(froma, &i, 4);
+		*toptr = CAST_USER_ADDR_T(i);	/* SAFE */
+	} else {
+		error = copyin(froma, toptr, 8);
+	}
+	return (error);
+}
+
+
+/*
+ * copyoutptr
+ *
+ * Description:	Copy a pointer out from a user_addr_t in kernel space to
+ *		user space, based on 32/64 bitness of the user space
+ *
+ * Parameters:	ua			User space address to copy to
+ *		ptr			Address of kernel space user_addr_t
+ *		ptr_size		4/8, based on 'ua' address space
+ *
+ * Returns:	0			Success
+ *		EFAULT			Bad 'ua'
+ *
+ */
+static int
+copyoutptr(user_addr_t ua, user_addr_t ptr, int ptr_size)
+{
+	int error;
+
+	if (ptr_size == 4) {
+		/* 64 bit value containing 32 bit address */
+		unsigned int i = CAST_DOWN_EXPLICIT(unsigned int,ua);	/* SAFE */
+
+		error = copyout(&i, ptr, 4);
+	} else {
+		error = copyout(&ua, ptr, 8);
+	}
+	return (error);
+}
+
+
+/*
+ * exec_copyout_strings
+ *
+ * Copy out the strings segment to user space.  The strings segment is put
+ * on a preinitialized stack frame.
+ *
+ * Parameters:	struct image_params *	the image parameter block
+ *		int *			a pointer to the stack offset variable
+ *
+ * Returns:	0			Success
+ *		!0			Faiure: errno
+ *
+ * Implicit returns:
+ *		(*stackp)		The stack offset, modified
+ *
+ * Note:	The strings segment layout is backward, from the beginning
+ *		of the top of the stack to consume the minimal amount of
+ *		space possible; the returned stack pointer points to the
+ *		end of the area consumed (stacks grow downward).
+ *
+ *		argc is an int; arg[i] are pointers; env[i] are pointers;
+ *		the 0's are (void *)NULL's
+ *
+ * The stack frame layout is:
+ *
+ *      +-------------+ <- p->user_stack
+ *      |     16b     |
+ *      +-------------+
+ *      | STRING AREA |
+ *      |      :      |
+ *      |      :      |
+ *      |      :      |
+ *      +- -- -- -- --+
+ *      |  PATH AREA  |
+ *      +-------------+
+ *      |      0      |
+ *      +-------------+
+ *      |  applev[n]  |
+ *      +-------------+
+ *             :
+ *             :
+ *      +-------------+
+ *      |  applev[1]  |
+ *      +-------------+
+ *      | exec_path / |
+ *      |  applev[0]  |
+ *      +-------------+
+ *      |      0      |
+ *      +-------------+
+ *      |    env[n]   |
+ *      +-------------+
+ *             :
+ *             :
+ *      +-------------+
+ *      |    env[0]   |
+ *      +-------------+
+ *      |      0      |
+ *      +-------------+
+ *      | arg[argc-1] |
+ *      +-------------+
+ *             :
+ *             :
+ *      +-------------+
+ *      |    arg[0]   |
+ *      +-------------+
+ *      |     argc    |
+ * sp-> +-------------+
+ *
+ * Although technically a part of the STRING AREA, we treat the PATH AREA as
+ * a separate entity.  This allows us to align the beginning of the PATH AREA
+ * to a pointer boundary so that the exec_path, env[i], and argv[i] pointers
+ * which preceed it on the stack are properly aligned.
+ */
+
+static int
+exec_copyout_strings(struct image_params *imgp, user_addr_t *stackp)
+{
+	proc_t p = vfs_context_proc(imgp->ip_vfs_context);
+	int	ptr_size = (imgp->ip_flags & IMGPF_IS_64BIT) ? 8 : 4;
+	int	ptr_area_size;
+	void *ptr_buffer_start, *ptr_buffer;
+	int string_size;
+
+	user_addr_t	string_area;	/* *argv[], *env[] */
+	user_addr_t	ptr_area;	/* argv[], env[], applev[] */
+	user_addr_t argc_area;	/* argc */
+	user_addr_t	stack;
+	int error;
+
+	unsigned i;
+	struct copyout_desc {
+		char	*start_string;
+		int		count;
+#if CONFIG_DTRACE
+		user_addr_t	*dtrace_cookie;
+#endif
+		boolean_t	null_term;
+	} descriptors[] = {
+		{
+			.start_string = imgp->ip_startargv,
+			.count = imgp->ip_argc,
+#if CONFIG_DTRACE
+			.dtrace_cookie = &p->p_dtrace_argv,
+#endif
+			.null_term = TRUE
+		},
+		{
+			.start_string = imgp->ip_endargv,
+			.count = imgp->ip_envc,
+#if CONFIG_DTRACE
+			.dtrace_cookie = &p->p_dtrace_envp,
+#endif
+			.null_term = TRUE
+		},
+		{
+			.start_string = imgp->ip_strings,
+			.count = 1,
+#if CONFIG_DTRACE
+			.dtrace_cookie = NULL,
+#endif
+			.null_term = FALSE
+		},
+		{
+			.start_string = imgp->ip_endenvv,
+			.count = imgp->ip_applec - 1, /* exec_path handled above */
+#if CONFIG_DTRACE
+			.dtrace_cookie = NULL,
+#endif
+			.null_term = TRUE
+		}
+	};
+
+	stack = *stackp;
+
+	/*
+	 * All previous contributors to the string area
+	 * should have aligned their sub-area
+	 */
+	if (imgp->ip_strspace % ptr_size != 0) {
+		error = EINVAL;
+		goto bad;
+	}
+
+	/* Grow the stack down for the strings we've been building up */
+	string_size = imgp->ip_strendp - imgp->ip_strings;
+	stack -= string_size;
+	string_area = stack;
+
+	/*
+	 * Need room for one pointer for each string, plus
+	 * one for the NULLs terminating the argv, envv, and apple areas.
+	 */
+	ptr_area_size = (imgp->ip_argc + imgp->ip_envc + imgp->ip_applec + 3) *
+	    ptr_size;
+	stack -= ptr_area_size;
+	ptr_area = stack;
+
+	/* We'll construct all the pointer arrays in our string buffer,
+	 * which we already know is aligned properly, and ip_argspace
+	 * was used to verify we have enough space.
+	 */
+	ptr_buffer_start = ptr_buffer = (void *)imgp->ip_strendp;
+
+	/*
+	 * Need room for pointer-aligned argc slot.
+	 */
+	stack -= ptr_size;
+	argc_area = stack;
+
+	/*
+	 * Record the size of the arguments area so that sysctl_procargs()
+	 * can return the argument area without having to parse the arguments.
+	 */
+	proc_lock(p);
+	p->p_argc = imgp->ip_argc;
+	p->p_argslen = (int)(*stackp - string_area);
+	proc_unlock(p);
+
+	/* Return the initial stack address: the location of argc */
+	*stackp = stack;
+
+	/*
+	 * Copy out the entire strings area.
+	 */
+	error = copyout(imgp->ip_strings, string_area,
+						   string_size);
+	if (error)
+		goto bad;
+
+	for (i = 0; i < sizeof(descriptors)/sizeof(descriptors[0]); i++) {
+		char *cur_string = descriptors[i].start_string;
+		int j;
+
+#if CONFIG_DTRACE
+		if (descriptors[i].dtrace_cookie) {
+			proc_lock(p);
+			*descriptors[i].dtrace_cookie = ptr_area + ((uintptr_t)ptr_buffer - (uintptr_t)ptr_buffer_start); /* dtrace convenience */
+			proc_unlock(p);
+		}
+#endif /* CONFIG_DTRACE */
+
+		/*
+		 * For each segment (argv, envv, applev), copy as many pointers as requested
+		 * to our pointer buffer.
+		 */
+		for (j = 0; j < descriptors[i].count; j++) {
+			user_addr_t cur_address = string_area + (cur_string - imgp->ip_strings);
+			
+			/* Copy out the pointer to the current string. Alignment has been verified  */
+			if (ptr_size == 8) {
+				*(uint64_t *)ptr_buffer = (uint64_t)cur_address;
+			} else {
+				*(uint32_t *)ptr_buffer = (uint32_t)cur_address;
+			}
+			
+			ptr_buffer = (void *)((uintptr_t)ptr_buffer + ptr_size);
+			cur_string += strlen(cur_string) + 1; /* Only a NUL between strings in the same area */
+		}
+
+		if (descriptors[i].null_term) {
+			if (ptr_size == 8) {
+				*(uint64_t *)ptr_buffer = 0ULL;
+			} else {
+				*(uint32_t *)ptr_buffer = 0;
+			}
+			
+			ptr_buffer = (void *)((uintptr_t)ptr_buffer + ptr_size);
+		}
+	}
+
+	/*
+	 * Copy out all our pointer arrays in bulk.
+	 */
+	error = copyout(ptr_buffer_start, ptr_area,
+					ptr_area_size);
+	if (error)
+		goto bad;
+
+	/* argc (int32, stored in a ptr_size area) */
+	error = copyoutptr((user_addr_t)imgp->ip_argc, argc_area, ptr_size);
+	if (error)
+		goto bad;
+
+bad:
+	return(error);
+}
+
+
+/*
+ * exec_extract_strings
+ *
+ * Copy arguments and environment from user space into work area; we may
+ * have already copied some early arguments into the work area, and if
+ * so, any arguments opied in are appended to those already there.
+ * This function is the primary manipulator of ip_argspace, since
+ * these are the arguments the client of execve(2) knows about. After
+ * each argv[]/envv[] string is copied, we charge the string length
+ * and argv[]/envv[] pointer slot to ip_argspace, so that we can
+ * full preflight the arg list size.
+ *
+ * Parameters:	struct image_params *	the image parameter block
+ *
+ * Returns:	0			Success
+ *		!0			Failure: errno
+ *
+ * Implicit returns;
+ *		(imgp->ip_argc)		Count of arguments, updated
+ *		(imgp->ip_envc)		Count of environment strings, updated
+ *		(imgp->ip_argspace)	Count of remaining of NCARGS
+ *		(imgp->ip_interp_buffer)	Interpreter and args (mutated in place)
+ *
+ *
+ * Note:	The argument and environment vectors are user space pointers
+ *		to arrays of user space pointers.
+ */
+static int
+exec_extract_strings(struct image_params *imgp)
+{
+	int error = 0;
+	int	ptr_size = (imgp->ip_flags & IMGPF_WAS_64BIT) ? 8 : 4;
+	int new_ptr_size = (imgp->ip_flags & IMGPF_IS_64BIT) ? 8 : 4;
+	user_addr_t	argv = imgp->ip_user_argv;
+	user_addr_t	envv = imgp->ip_user_envv;
+
+	/*
+	 * Adjust space reserved for the path name by however much padding it
+	 * needs. Doing this here since we didn't know if this would be a 32- 
+	 * or 64-bit process back in exec_save_path.
+	 */
+	while (imgp->ip_strspace % new_ptr_size != 0) {
+		*imgp->ip_strendp++ = '\0';
+		imgp->ip_strspace--;
+		/* imgp->ip_argspace--; not counted towards exec args total */
+	}
+
+	/*
+	 * From now on, we start attributing string space to ip_argspace
+	 */
+	imgp->ip_startargv = imgp->ip_strendp;
+	imgp->ip_argc = 0;
+
+	if((imgp->ip_flags & IMGPF_INTERPRET) != 0) {
+		user_addr_t	arg;
+		char *argstart, *ch;
+
+		/* First, the arguments in the "#!" string are tokenized and extracted. */
+		argstart = imgp->ip_interp_buffer;
+		while (argstart) {
+			ch = argstart;
+			while (*ch && !IS_WHITESPACE(*ch)) {
+				ch++;
+			}
+
+			if (*ch == '\0') {
+				/* last argument, no need to NUL-terminate */
+				error = exec_add_user_string(imgp, CAST_USER_ADDR_T(argstart), UIO_SYSSPACE, TRUE);
+				argstart = NULL;
+			} else {
+				/* NUL-terminate */
+				*ch = '\0';
+				error = exec_add_user_string(imgp, CAST_USER_ADDR_T(argstart), UIO_SYSSPACE, TRUE);
+
+				/*
+				 * Find the next string. We know spaces at the end of the string have already
+				 * been stripped.
+				 */
+				argstart = ch + 1;
+				while (IS_WHITESPACE(*argstart)) {
+					argstart++;
+				}
+			}
+
+			/* Error-check, regardless of whether this is the last interpreter arg or not */
+			if (error)
+				goto bad;
+			if (imgp->ip_argspace < new_ptr_size) {
+				error = E2BIG;
+				goto bad;
+			}
+			imgp->ip_argspace -= new_ptr_size; /* to hold argv[] entry */
+			imgp->ip_argc++;
+		}
+
+		if (argv != 0LL) {
+			/*
+			 * If we are running an interpreter, replace the av[0] that was
+			 * passed to execve() with the path name that was
+			 * passed to execve() for interpreters which do not use the PATH
+			 * to locate their script arguments.
+			 */
+			error = copyinptr(argv, &arg, ptr_size);
+			if (error)
+				goto bad;
+			if (arg != 0LL) {
+				argv += ptr_size; /* consume without using */
+			}
+		}
+
+		if (imgp->ip_interp_sugid_fd != -1) {
+			char temp[19]; /* "/dev/fd/" + 10 digits + NUL */
+			snprintf(temp, sizeof(temp), "/dev/fd/%d", imgp->ip_interp_sugid_fd);
+			error = exec_add_user_string(imgp, CAST_USER_ADDR_T(temp), UIO_SYSSPACE, TRUE);
+		} else {
+			error = exec_add_user_string(imgp, imgp->ip_user_fname, imgp->ip_seg, TRUE);
+		}
+		
+		if (error)
+			goto bad;
+		if (imgp->ip_argspace < new_ptr_size) {
+			error = E2BIG;
+			goto bad;
+		}
+		imgp->ip_argspace -= new_ptr_size; /* to hold argv[] entry */
+		imgp->ip_argc++;
+	}
+
+	while (argv != 0LL) {
+		user_addr_t	arg;
+
+		error = copyinptr(argv, &arg, ptr_size);
+		if (error)
+			goto bad;
+
+		if (arg == 0LL) {
+			break;
+		}
+
+		argv += ptr_size;
+
+		/*
+		* av[n...] = arg[n]
+		*/
+		error = exec_add_user_string(imgp, arg, imgp->ip_seg, TRUE);
+		if (error)
+			goto bad;
+		if (imgp->ip_argspace < new_ptr_size) {
+			error = E2BIG;
+			goto bad;
+		}
+		imgp->ip_argspace -= new_ptr_size; /* to hold argv[] entry */
+		imgp->ip_argc++;
+	}	 
+
+	/* Save space for argv[] NULL terminator */
+	if (imgp->ip_argspace < new_ptr_size) {
+		error = E2BIG;
+		goto bad;
+	}
+	imgp->ip_argspace -= new_ptr_size;
+	
+	/* Note where the args ends and env begins. */
+	imgp->ip_endargv = imgp->ip_strendp;
+	imgp->ip_envc = 0;
+
+	/* Now, get the environment */
+	while (envv != 0LL) {
+		user_addr_t	env;
+
+		error = copyinptr(envv, &env, ptr_size);
+		if (error)
+			goto bad;
+
+		envv += ptr_size;
+		if (env == 0LL) {
+			break;
+		}
+		/*
+		* av[n...] = env[n]
+		*/
+		error = exec_add_user_string(imgp, env, imgp->ip_seg, TRUE);
+		if (error)
+			goto bad;
+		if (imgp->ip_argspace < new_ptr_size) {
+			error = E2BIG;
+			goto bad;
+		}
+		imgp->ip_argspace -= new_ptr_size; /* to hold envv[] entry */
+		imgp->ip_envc++;
+	}
+
+	/* Save space for envv[] NULL terminator */
+	if (imgp->ip_argspace < new_ptr_size) {
+		error = E2BIG;
+		goto bad;
+	}
+	imgp->ip_argspace -= new_ptr_size;
+
+	/* Align the tail of the combined argv+envv area */
+	while (imgp->ip_strspace % new_ptr_size != 0) {
+		if (imgp->ip_argspace < 1) {
+			error = E2BIG;
+			goto bad;
+		}
+		*imgp->ip_strendp++ = '\0';
+		imgp->ip_strspace--;
+		imgp->ip_argspace--;
+	}
+	
+	/* Note where the envv ends and applev begins. */
+	imgp->ip_endenvv = imgp->ip_strendp;
+
+	/*
+	 * From now on, we are no longer charging argument
+	 * space to ip_argspace.
+	 */
+
+bad:
+	return error;
+}
+
+static char *
+random_hex_str(char *str, int len, boolean_t embedNUL)
+{
+	uint64_t low, high, value;
+	int idx;
+	char digit;
+
+	/* A 64-bit value will only take 16 characters, plus '0x' and NULL. */
+	if (len > 19)
+		len = 19;
+
+	/* We need enough room for at least 1 digit */
+	if (len < 4)
+		return (NULL);
+
+	low = random();
+	high = random();
+	value = high << 32 | low;
+
+	if (embedNUL) {
+		/*
+		 * Zero a byte to protect against C string vulnerabilities
+		 * e.g. for userland __stack_chk_guard.
+		 */ 
+		value &= ~(0xffull << 8);
+	}
+
+	str[0] = '0';
+	str[1] = 'x';
+	for (idx = 2; idx < len - 1; idx++) {
+		digit = value & 0xf;
+		value = value >> 4;
+		if (digit < 10)
+			str[idx] = '0' + digit;
+		else
+			str[idx] = 'a' + (digit - 10);
+	}
+	str[idx] = '\0';
+	return (str);
+}
+
+/*
+ * Libc has an 8-element array set up for stack guard values.  It only fills
+ * in one of those entries, and both gcc and llvm seem to use only a single
+ * 8-byte guard.  Until somebody needs more than an 8-byte guard value, don't
+ * do the work to construct them.
+ */
+#define	GUARD_VALUES 1
+#define	GUARD_KEY "stack_guard="
+
+/*
+ * System malloc needs some entropy when it is initialized.
+ */
+#define	ENTROPY_VALUES 2
+#define ENTROPY_KEY "malloc_entropy="
+
+/*
+ * System malloc engages nanozone for UIAPP.
+ */
+#define NANO_ENGAGE_KEY "MallocNanoZone=1"
+
+#define PFZ_KEY "pfz="
+extern user32_addr_t commpage_text32_location;
+extern user64_addr_t commpage_text64_location;
+/*
+ * Build up the contents of the apple[] string vector
+ */
+static int
+exec_add_apple_strings(struct image_params *imgp)
+{
+	int i, error;
+	int new_ptr_size=4;
+	char guard[19];
+	char guard_vec[strlen(GUARD_KEY) + 19 * GUARD_VALUES + 1];
+
+	char entropy[19];
+	char entropy_vec[strlen(ENTROPY_KEY) + 19 * ENTROPY_VALUES + 1];
+
+	char pfz_string[strlen(PFZ_KEY) + 16 + 4 +1];
+	
+	if( imgp->ip_flags & IMGPF_IS_64BIT) {
+		new_ptr_size = 8;
+		snprintf(pfz_string, sizeof(pfz_string),PFZ_KEY "0x%llx",commpage_text64_location);
+	} else {
+		snprintf(pfz_string, sizeof(pfz_string),PFZ_KEY "0x%x",commpage_text32_location);
+	}
+
+	/* exec_save_path stored the first string */
+	imgp->ip_applec = 1;
+
+	/* adding the pfz string */
+	error = exec_add_user_string(imgp, CAST_USER_ADDR_T(pfz_string),UIO_SYSSPACE,FALSE);
+	if(error)
+		goto bad;
+	imgp->ip_applec++;
+
+	/* adding the NANO_ENGAGE_KEY key */
+	if (imgp->ip_px_sa) {
+		int proc_flags = (((struct _posix_spawnattr *) imgp->ip_px_sa)->psa_flags);
+
+		if ((proc_flags & _POSIX_SPAWN_NANO_ALLOCATOR) == _POSIX_SPAWN_NANO_ALLOCATOR) {
+			char uiapp_string[strlen(NANO_ENGAGE_KEY) + 1];
+
+			snprintf(uiapp_string, sizeof(uiapp_string), NANO_ENGAGE_KEY);
+			error = exec_add_user_string(imgp, CAST_USER_ADDR_T(uiapp_string),UIO_SYSSPACE,FALSE);
+			if (error)
+				goto bad;
+			imgp->ip_applec++;
+		}
+	}
+
+	/*
+	 * Supply libc with a collection of random values to use when
+	 * implementing -fstack-protector.
+	 *
+	 * (The first random string always contains an embedded NUL so that
+	 * __stack_chk_guard also protects against C string vulnerabilities)
+	 */
+	(void)strlcpy(guard_vec, GUARD_KEY, sizeof (guard_vec));
+	for (i = 0; i < GUARD_VALUES; i++) {
+		random_hex_str(guard, sizeof (guard), i == 0);
+		if (i)
+			(void)strlcat(guard_vec, ",", sizeof (guard_vec));
+		(void)strlcat(guard_vec, guard, sizeof (guard_vec));
+	}
+
+	error = exec_add_user_string(imgp, CAST_USER_ADDR_T(guard_vec), UIO_SYSSPACE, FALSE);
+	if (error)
+		goto bad;
+	imgp->ip_applec++;
+
+	/*
+	 * Supply libc with entropy for system malloc.
+	 */
+	(void)strlcpy(entropy_vec, ENTROPY_KEY, sizeof(entropy_vec));
+	for (i = 0; i < ENTROPY_VALUES; i++) {
+		random_hex_str(entropy, sizeof (entropy), FALSE);
+		if (i)
+			(void)strlcat(entropy_vec, ",", sizeof (entropy_vec));
+		(void)strlcat(entropy_vec, entropy, sizeof (entropy_vec));
+	}
+	
+	error = exec_add_user_string(imgp, CAST_USER_ADDR_T(entropy_vec), UIO_SYSSPACE, FALSE);
+	if (error)
+		goto bad;
+	imgp->ip_applec++;
+
+	/* Align the tail of the combined applev area */
+	while (imgp->ip_strspace % new_ptr_size != 0) {
+		*imgp->ip_strendp++ = '\0';
+		imgp->ip_strspace--;
+	}
+
+bad:
+	return error;
+}
+
+#define	unix_stack_size(p)	(p->p_rlimit[RLIMIT_STACK].rlim_cur)
+
+/*
+ * exec_check_permissions
+ *
+ * Description:	Verify that the file that is being attempted to be executed
+ *		is in fact allowed to be executed based on it POSIX file
+ *		permissions and other access control criteria
+ *
+ * Parameters:	struct image_params *	the image parameter block
+ *
+ * Returns:	0			Success
+ *		EACCES			Permission denied
+ *		ENOEXEC			Executable file format error
+ *		ETXTBSY			Text file busy [misuse of error code]
+ *	vnode_getattr:???
+ *	vnode_authorize:???
+ */
+static int
+exec_check_permissions(struct image_params *imgp)
+{
+	struct vnode *vp = imgp->ip_vp;
+	struct vnode_attr *vap = imgp->ip_vattr;
+	proc_t p = vfs_context_proc(imgp->ip_vfs_context);
+	int error;
+	kauth_action_t action;
+
+	/* Only allow execution of regular files */
+	if (!vnode_isreg(vp))
+		return (EACCES);
+	
+	/* Get the file attributes that we will be using here and elsewhere */
+	VATTR_INIT(vap);
+	VATTR_WANTED(vap, va_uid);
+	VATTR_WANTED(vap, va_gid);
+	VATTR_WANTED(vap, va_mode);
+	VATTR_WANTED(vap, va_fsid);
+	VATTR_WANTED(vap, va_fileid);
+	VATTR_WANTED(vap, va_data_size);
+	if ((error = vnode_getattr(vp, vap, imgp->ip_vfs_context)) != 0)
+		return (error);
+
+	/*
+	 * Ensure that at least one execute bit is on - otherwise root
+	 * will always succeed, and we don't want to happen unless the
+	 * file really is executable.
+	 */
+	if (!vfs_authopaque(vnode_mount(vp)) && ((vap->va_mode & (S_IXUSR | S_IXGRP | S_IXOTH)) == 0))
+		return (EACCES);
+
+	/* Disallow zero length files */
+	if (vap->va_data_size == 0)
+		return (ENOEXEC);
+
+	imgp->ip_arch_offset = (user_size_t)0;
+	imgp->ip_arch_size = vap->va_data_size;
+
+	/* Disable setuid-ness for traced programs or if MNT_NOSUID */
+	if ((vp->v_mount->mnt_flag & MNT_NOSUID) || (p->p_lflag & P_LTRACED))
+		vap->va_mode &= ~(VSUID | VSGID);
+
+	/*
+	 * Disable _POSIX_SPAWN_ALLOW_DATA_EXEC and _POSIX_SPAWN_DISABLE_ASLR
+	 * flags for setuid/setgid binaries.
+	 */
+	if (vap->va_mode & (VSUID | VSGID))
+		imgp->ip_flags &= ~(IMGPF_ALLOW_DATA_EXEC | IMGPF_DISABLE_ASLR);
+
+#if CONFIG_MACF
+	error = mac_vnode_check_exec(imgp->ip_vfs_context, vp, imgp);
+	if (error)
+		return (error);
+#endif
+
+  	/* Check for execute permission */
+ 	action = KAUTH_VNODE_EXECUTE;
+  	/* Traced images must also be readable */
+ 	if (p->p_lflag & P_LTRACED)
+ 		action |= KAUTH_VNODE_READ_DATA;
+ 	if ((error = vnode_authorize(vp, NULL, action, imgp->ip_vfs_context)) != 0)
+		return (error);
+
+#if 0
+	/* Don't let it run if anyone had it open for writing */
+	vnode_lock(vp);
+	if (vp->v_writecount) {
+		panic("going to return ETXTBSY %x", vp);
+		vnode_unlock(vp);
+		return (ETXTBSY);
+	}
+	vnode_unlock(vp);
+#endif
+
+
+	/* XXX May want to indicate to underlying FS that vnode is open */
+
+	return (error);
+}
+
+
+/*
+ * exec_handle_sugid
+ *
+ * Initially clear the P_SUGID in the process flags; if an SUGID process is
+ * exec'ing a non-SUGID image, then  this is the point of no return.
+ *
+ * If the image being activated is SUGID, then replace the credential with a
+ * copy, disable tracing (unless the tracing process is root), reset the
+ * mach task port to revoke it, set the P_SUGID bit,
+ *
+ * If the saved user and group ID will be changing, then make sure it happens
+ * to a new credential, rather than a shared one.
+ *
+ * Set the security token (this is probably obsolete, given that the token
+ * should not technically be separate from the credential itself).
+ *
+ * Parameters:	struct image_params *	the image parameter block
+ *
+ * Returns:	void			No failure indication
+ *
+ * Implicit returns:
+ *		<process credential>	Potentially modified/replaced
+ *		<task port>		Potentially revoked
+ *		<process flags>		P_SUGID bit potentially modified
+ *		<security token>	Potentially modified
+ */
+static int
+exec_handle_sugid(struct image_params *imgp)
+{
+	kauth_cred_t		cred = vfs_context_ucred(imgp->ip_vfs_context);
+	proc_t			p = vfs_context_proc(imgp->ip_vfs_context);
+	int			i;
+	int			leave_sugid_clear = 0;
+	int			mac_reset_ipc = 0;
+	int			error = 0;
+#if CONFIG_MACF
+	int			mac_transition, disjoint_cred = 0;
+	int 		label_update_return = 0;
+
+	/*
+	 * Determine whether a call to update the MAC label will result in the
+	 * credential changing.
+	 *
+	 * Note:	MAC policies which do not actually end up modifying
+	 *		the label subsequently are strongly encouraged to
+	 *		return 0 for this check, since a non-zero answer will
+	 *		slow down the exec fast path for normal binaries.
+	 */
+	mac_transition = mac_cred_check_label_update_execve(
+							imgp->ip_vfs_context,
+							imgp->ip_vp,
+							imgp->ip_arch_offset,
+							imgp->ip_scriptvp,
+							imgp->ip_scriptlabelp,
+							imgp->ip_execlabelp,
+							p,
+							imgp->ip_px_smpx);
+#endif
+
+	OSBitAndAtomic(~((uint32_t)P_SUGID), &p->p_flag);
+
+	/*
+	 * Order of the following is important; group checks must go last,
+	 * as we use the success of the 'ismember' check combined with the
+	 * failure of the explicit match to indicate that we will be setting
+	 * the egid of the process even though the new process did not
+	 * require VSUID/VSGID bits in order for it to set the new group as
+	 * its egid.
+	 *
+	 * Note:	Technically, by this we are implying a call to
+	 *		setegid() in the new process, rather than implying
+	 *		it used its VSGID bit to set the effective group,
+	 *		even though there is no code in that process to make
+	 *		such a call.
+	 */
+	if (((imgp->ip_origvattr->va_mode & VSUID) != 0 &&
+	     kauth_cred_getuid(cred) != imgp->ip_origvattr->va_uid) ||
+	    ((imgp->ip_origvattr->va_mode & VSGID) != 0 &&
+		 ((kauth_cred_ismember_gid(cred, imgp->ip_origvattr->va_gid, &leave_sugid_clear) || !leave_sugid_clear) ||
+		 (kauth_cred_getgid(cred) != imgp->ip_origvattr->va_gid)))) {
+
+#if CONFIG_MACF
+/* label for MAC transition and neither VSUID nor VSGID */
+handle_mac_transition:
+#endif
+
+		/*
+		 * Replace the credential with a copy of itself if euid or
+		 * egid change.
+		 *
+		 * Note:	setuid binaries will automatically opt out of
+		 *		group resolver participation as a side effect
+		 *		of this operation.  This is an intentional
+		 *		part of the security model, which requires a
+		 *		participating credential be established by
+		 *		escalating privilege, setting up all other
+		 *		aspects of the credential including whether
+		 *		or not to participate in external group
+		 *		membership resolution, then dropping their
+		 *		effective privilege to that of the desired
+		 *		final credential state.
+		 */
+		if (imgp->ip_origvattr->va_mode & VSUID) {
+			p->p_ucred  = kauth_cred_setresuid(p->p_ucred, KAUTH_UID_NONE, imgp->ip_origvattr->va_uid, imgp->ip_origvattr->va_uid, KAUTH_UID_NONE);
+			/* update cred on proc */
+			PROC_UPDATE_CREDS_ONPROC(p);
+		}
+		if (imgp->ip_origvattr->va_mode & VSGID) {
+			p->p_ucred = kauth_cred_setresgid(p->p_ucred, KAUTH_GID_NONE, imgp->ip_origvattr->va_gid, imgp->ip_origvattr->va_gid);
+			/* update cred on proc */
+			PROC_UPDATE_CREDS_ONPROC(p);
+		}
+
+#if CONFIG_MACF
+		/* 
+		 * If a policy has indicated that it will transition the label,
+		 * before making the call into the MAC policies, get a new
+		 * duplicate credential, so they can modify it without
+		 * modifying any others sharing it.
+		 */
+		if (mac_transition) { 
+			kauth_proc_label_update_execve(p,
+						imgp->ip_vfs_context,
+						imgp->ip_vp, 
+						imgp->ip_arch_offset,
+						imgp->ip_scriptvp,
+						imgp->ip_scriptlabelp,
+						imgp->ip_execlabelp,
+						&imgp->ip_csflags,
+						imgp->ip_px_smpx,
+						&disjoint_cred, /* will be non zero if disjoint */
+						&label_update_return);
+
+			if (disjoint_cred) {
+				/*
+				 * If updating the MAC label resulted in a
+				 * disjoint credential, flag that we need to
+				 * set the P_SUGID bit.  This protects
+				 * against debuggers being attached by an
+				 * insufficiently privileged process onto the
+				 * result of a transition to a more privileged
+				 * credential.
+				 */
+				leave_sugid_clear = 0;
+			}
+			
+			imgp->ip_mac_return = label_update_return;
+		}
+		
+		mac_reset_ipc = mac_proc_check_inherit_ipc_ports(p, p->p_textvp, p->p_textoff, imgp->ip_vp, imgp->ip_arch_offset, imgp->ip_scriptvp);
+
+#endif	/* CONFIG_MACF */
+
+		/*
+		 * If 'leave_sugid_clear' is non-zero, then we passed the
+		 * VSUID and MACF checks, and successfully determined that
+		 * the previous cred was a member of the VSGID group, but
+		 * that it was not the default at the time of the execve,
+		 * and that the post-labelling credential was not disjoint.
+		 * So we don't set the P_SUGID or reset mach ports and fds 
+		 * on the basis of simply running this code.
+		 */
+		if (mac_reset_ipc || !leave_sugid_clear) {
+			/*
+			 * Have mach reset the task and thread ports.
+			 * We don't want anyone who had the ports before
+			 * a setuid exec to be able to access/control the
+			 * task/thread after.
+			 */
+			ipc_task_reset(p->task);
+			ipc_thread_reset((imgp->ip_new_thread != NULL) ?
+				 	 imgp->ip_new_thread : current_thread());
+		}
+
+		if (!leave_sugid_clear) {
+			/*
+			 * Flag the process as setuid.
+			 */
+			OSBitOrAtomic(P_SUGID, &p->p_flag);
+
+			/*
+			 * Radar 2261856; setuid security hole fix
+			 * XXX For setuid processes, attempt to ensure that
+			 * stdin, stdout, and stderr are already allocated.
+			 * We do not want userland to accidentally allocate
+			 * descriptors in this range which has implied meaning
+			 * to libc.
+			 */
+			for (i = 0; i < 3; i++) {
+
+				if (p->p_fd->fd_ofiles[i] != NULL)
+					continue;
+
+				/*
+				 * Do the kernel equivalent of
+				 *
+				 * 	if i == 0
+				 * 		(void) open("/dev/null", O_RDONLY);
+				 * 	else 
+				 * 		(void) open("/dev/null", O_WRONLY);
+				 */
+
+				struct fileproc *fp;
+				int indx;
+				int flag;
+				struct nameidata *ndp = NULL;
+
+				if (i == 0)
+					flag = FREAD;
+				else 
+					flag = FWRITE;
+
+				if ((error = falloc(p,
+				    &fp, &indx, imgp->ip_vfs_context)) != 0)
+					continue;
+
+				MALLOC(ndp, struct nameidata *, sizeof(*ndp), M_TEMP, M_WAITOK | M_ZERO);
+				if (ndp == NULL) {
+					error = ENOMEM;
+					break;
+				}
+
+				NDINIT(ndp, LOOKUP, OP_OPEN, FOLLOW, UIO_SYSSPACE,
+				    CAST_USER_ADDR_T("/dev/null"),
+				    imgp->ip_vfs_context);
+
+				if ((error = vn_open(ndp, flag, 0)) != 0) {
+					fp_free(p, indx, fp);
+					break;
+				}
+
+				struct fileglob *fg = fp->f_fglob;
+
+				fg->fg_flag = flag;
+				fg->fg_ops = &vnops;
+				fg->fg_data = ndp->ni_vp;
+
+				vnode_put(ndp->ni_vp);
+
+				proc_fdlock(p);
+				procfdtbl_releasefd(p, indx, NULL);
+				fp_drop(p, indx, fp, 1);
+				proc_fdunlock(p);
+
+				FREE(ndp, M_TEMP);
+			}
+		}
+	}
+#if CONFIG_MACF
+	else {
+		/*
+		 * We are here because we were told that the MAC label will
+		 * be transitioned, and the binary is not VSUID or VSGID; to
+		 * deal with this case, we could either duplicate a lot of
+		 * code, or we can indicate we want to default the P_SUGID
+		 * bit clear and jump back up.
+		 */
+		if (mac_transition) {
+			leave_sugid_clear = 1;
+			goto handle_mac_transition;
+		}
+	}
+
+#endif	/* CONFIG_MACF */
+
+	/*
+	 * Implement the semantic where the effective user and group become
+	 * the saved user and group in exec'ed programs.
+	 */
+	p->p_ucred = kauth_cred_setsvuidgid(p->p_ucred, kauth_cred_getuid(p->p_ucred),  kauth_cred_getgid(p->p_ucred));
+	/* update cred on proc */
+	PROC_UPDATE_CREDS_ONPROC(p);
+	
+	/* Update the process' identity version and set the security token */
+	p->p_idversion++;
+	set_security_token(p);
+
+	return(error);
+}
+
+
+/*
+ * create_unix_stack
+ *
+ * Description:	Set the user stack address for the process to the provided
+ *		address.  If a custom stack was not set as a result of the
+ *		load process (i.e. as specified by the image file for the
+ *		executable), then allocate the stack in the provided map and
+ *		set up appropriate guard pages for enforcing administrative
+ *		limits on stack growth, if they end up being needed.
+ *
+ * Parameters:	p			Process to set stack on
+ *		load_result		Information from mach-o load commands
+ *		map			Address map in which to allocate the new stack
+ *
+ * Returns:	KERN_SUCCESS		Stack successfully created
+ *		!KERN_SUCCESS		Mach failure code
+ */
+static kern_return_t
+create_unix_stack(vm_map_t map, load_result_t* load_result, 
+			proc_t p)
+{
+	mach_vm_size_t		size, prot_size;
+	mach_vm_offset_t	addr, prot_addr;
+	kern_return_t		kr;
+
+	mach_vm_address_t	user_stack = load_result->user_stack;
+	
+	proc_lock(p);
+	p->user_stack = user_stack;
+	proc_unlock(p);
+
+	if (!load_result->prog_allocated_stack) {
+		/*
+		 * Allocate enough space for the maximum stack size we
+		 * will ever authorize and an extra page to act as
+		 * a guard page for stack overflows. For default stacks,
+		 * vm_initial_limit_stack takes care of the extra guard page.
+		 * Otherwise we must allocate it ourselves.
+		 */
+
+		size = mach_vm_round_page(load_result->user_stack_size);
+		if (load_result->prog_stack_size)
+			size += PAGE_SIZE;
+		addr = mach_vm_trunc_page(load_result->user_stack - size);
+		kr = mach_vm_allocate(map, &addr, size,
+					VM_MAKE_TAG(VM_MEMORY_STACK) |
+					VM_FLAGS_FIXED);
+		if (kr != KERN_SUCCESS) {
+			/* If can't allocate at default location, try anywhere */
+			addr = 0;
+			kr = mach_vm_allocate(map, &addr, size,
+								  VM_MAKE_TAG(VM_MEMORY_STACK) |
+								  VM_FLAGS_ANYWHERE);
+			if (kr != KERN_SUCCESS)
+				return kr;
+
+			user_stack = addr + size;
+			load_result->user_stack = user_stack;
+
+			proc_lock(p);
+			p->user_stack = user_stack;
+			proc_unlock(p);
+		}
+
+		/*
+		 * And prevent access to what's above the current stack
+		 * size limit for this process.
+		 */
+		prot_addr = addr;
+		if (load_result->prog_stack_size)
+			prot_size = PAGE_SIZE;
+		else
+			prot_size = mach_vm_trunc_page(size - unix_stack_size(p));
+		kr = mach_vm_protect(map,
+							 prot_addr,
+							 prot_size,
+							 FALSE,
+							 VM_PROT_NONE);
+		if (kr != KERN_SUCCESS) {
+			(void) mach_vm_deallocate(map, addr, size);
+			return kr;
+		}
+	}
+
+	return KERN_SUCCESS;
+}
+
+#include <sys/reboot.h>
+
+/*
+ * load_init_program_at_path
+ *
+ * Description:	Load the "init" program; in most cases, this will be "launchd"
+ *
+ * Parameters:	p			Process to call execve() to create
+ *					the "init" program
+ *		scratch_addr		Page in p, scratch space
+ *		path			NULL terminated path
+ *
+ * Returns:	KERN_SUCCESS		Success
+ *		!KERN_SUCCESS 		See execve/mac_execve for error codes
+ *
+ * Notes:	The process that is passed in is the first manufactured
+ *		process on the system, and gets here via bsd_ast() firing
+ *		for the first time.  This is done to ensure that bsd_init()
+ *		has run to completion.
+ *
+ *		The address map of the first manufactured process is 32 bit.
+ *		WHEN this becomes 64b, this code will fail; it needs to be
+ *		made 64b capable.
+ */
+static int
+load_init_program_at_path(proc_t p, user_addr_t scratch_addr, const char* path)
+{
+	uint32_t argv[3];
+ 	uint32_t argc = 0;
+	int retval[2];
+	struct execve_args init_exec_args;
+
+	/*
+	 * Validate inputs and pre-conditions
+	 */
+	assert(p);
+	assert(scratch_addr);
+	assert(path);
+
+	if (IS_64BIT_PROCESS(p)) {
+		panic("Init against 64b primordial proc not implemented");
+	}
+
+	/*
+	 * Copy out program name.
+	 */
+	size_t path_length = strlen(path) + 1;
+	(void) copyout(path, scratch_addr, path_length);
+
+	argv[argc++] = (uint32_t)scratch_addr;
+	scratch_addr = USER_ADDR_ALIGN(scratch_addr + path_length, 16);
+
+	/*
+	 * Put out first (and only) argument, similarly.
+	 * Assumes everything fits in a page as allocated above.
+	 */
+	if (boothowto & RB_SINGLE) {
+		const char *init_args = "-s";
+		size_t init_args_length = strlen(init_args)+1;
+
+		copyout(init_args, scratch_addr, init_args_length);
+
+		argv[argc++] = (uint32_t)scratch_addr;
+		scratch_addr = USER_ADDR_ALIGN(scratch_addr + init_args_length, 16);
+	}
+
+	/*
+	 * Null-end the argument list
+	 */
+	argv[argc] = 0;
+	
+	/*
+	 * Copy out the argument list.
+	 */
+	(void) copyout(argv, scratch_addr, sizeof(argv));
+
+	/*
+	 * Set up argument block for fake call to execve.
+	 */
+	init_exec_args.fname = CAST_USER_ADDR_T(argv[0]);
+	init_exec_args.argp = scratch_addr;
+	init_exec_args.envp = USER_ADDR_NULL;
+
+	/*
+	 * So that init task is set with uid,gid 0 token
+	 */
+	set_security_token(p);
+
+	return execve(p, &init_exec_args, retval);
+}
+
+static const char * init_programs[] = {
+#if DEBUG
+	"/usr/local/sbin/launchd.debug",
+#endif
+#if DEVELOPMENT || DEBUG
+	/* Remove DEBUG conditional when <rdar://problem/17931977> is fixed */
+	"/usr/local/sbin/launchd.development",
+#endif
+	"/sbin/launchd",
+};
+
+/*
+ * load_init_program
+ *
+ * Description:	Load the "init" program; in most cases, this will be "launchd"
+ *
+ * Parameters:	p			Process to call execve() to create
+ *					the "init" program
+ *
+ * Returns:	(void)
+ *
+ * Notes:	The process that is passed in is the first manufactured
+ *		process on the system, and gets here via bsd_ast() firing
+ *		for the first time.  This is done to ensure that bsd_init()
+ *		has run to completion.
+ *
+ *		In DEBUG & DEVELOPMENT builds, the launchdsuffix boot-arg
+ *		may be used to select a specific launchd executable. As with
+ *		the kcsuffix boot-arg, setting launchdsuffix to "" or "release"
+ *		will force /sbin/launchd to be selected.
+ *
+ *		The DEBUG kernel will continue to check for a .development
+ *		version until <rdar://problem/17931977> is fixed.
+ *
+ *              Search order by build:
+ *
+ * DEBUG	DEVELOPMENT	RELEASE		PATH
+ * ----------------------------------------------------------------------------------
+ * 1		1		NA		/usr/local/sbin/launchd.$LAUNCHDSUFFIX
+ * 2		NA		NA		/usr/local/sbin/launchd.debug
+ * 3		2		NA		/usr/local/sbin/launchd.development
+ * 4		3		1		/sbin/launchd
+ */
+void
+load_init_program(proc_t p)
+{
+	uint32_t i;
+	int error;
+	vm_offset_t scratch_addr = VM_MIN_ADDRESS;
+
+	(void) vm_allocate(current_map(), &scratch_addr, PAGE_SIZE, VM_FLAGS_ANYWHERE);
+#if CONFIG_MEMORYSTATUS && CONFIG_JETSAM
+	(void) memorystatus_init_at_boot_snapshot();
+#endif /* CONFIG_MEMORYSTATUS && CONFIG_JETSAM */
+
+#if DEBUG || DEVELOPMENT
+	/* Check for boot-arg suffix first */
+	char launchd_suffix[64];
+	if (PE_parse_boot_argn("launchdsuffix", launchd_suffix, sizeof(launchd_suffix))) {
+		char launchd_path[128];
+		boolean_t is_release_suffix = ((launchd_suffix[0] == 0) ||
+					       (strcmp(launchd_suffix, "release") == 0));
+
+		if (is_release_suffix) {
+			error = load_init_program_at_path(p, CAST_USER_ADDR_T(scratch_addr), "/sbin/launchd");
+			if (!error)
+				return;
+
+			panic("Process 1 exec of launchd.release failed, errno %d", error);
+		} else {
+			strlcpy(launchd_path, "/usr/local/sbin/launchd.", sizeof(launchd_path));
+			strlcat(launchd_path, launchd_suffix, sizeof(launchd_path));
+
+			/* All the error data is lost in the loop below, don't
+			 * attempt to save it. */
+			if (!load_init_program_at_path(p, CAST_USER_ADDR_T(scratch_addr), launchd_path)) {
+				return;
+			}
+		}
+	}
+#endif
+
+	error = ENOENT;
+	for (i = 0; i < sizeof(init_programs)/sizeof(init_programs[0]); i++) {
+		error = load_init_program_at_path(p, CAST_USER_ADDR_T(scratch_addr), init_programs[i]);
+		if (!error)
+			return;
+	}
+
+	panic("Process 1 exec of %s failed, errno %d", ((i == 0) ? "<null>" : init_programs[i-1]), error);
+}
+
+/*
+ * load_return_to_errno
+ *
+ * Description:	Convert a load_return_t (Mach error) to an errno (BSD error)
+ *
+ * Parameters:	lrtn			Mach error number
+ *
+ * Returns:	(int)			BSD error number
+ *		0			Success
+ *		EBADARCH		Bad architecture
+ *		EBADMACHO		Bad Mach object file
+ *		ESHLIBVERS		Bad shared library version
+ *		ENOMEM			Out of memory/resource shortage
+ *		EACCES			Access denied
+ *		ENOENT			Entry not found (usually "file does
+ *					does not exist")
+ *		EIO			An I/O error occurred
+ *		EBADEXEC		The executable is corrupt/unknown
+ */
+static int 
+load_return_to_errno(load_return_t lrtn)
+{
+	switch (lrtn) {
+	case LOAD_SUCCESS:
+		return 0;
+	case LOAD_BADARCH:
+		return EBADARCH;
+	case LOAD_BADMACHO:
+		return EBADMACHO;
+	case LOAD_SHLIB:
+		return ESHLIBVERS;
+	case LOAD_NOSPACE:
+	case LOAD_RESOURCE:
+		return ENOMEM;
+	case LOAD_PROTECT:
+		return EACCES;
+	case LOAD_ENOENT:
+		return ENOENT;
+	case LOAD_IOERROR:
+		return EIO;
+	case LOAD_FAILURE:
+	case LOAD_DECRYPTFAIL:
+	default:
+		return EBADEXEC;
+	}
+}
+
+#include <mach/mach_types.h>
+#include <mach/vm_prot.h>
+#include <mach/semaphore.h>
+#include <mach/sync_policy.h>
+#include <kern/clock.h>
+#include <mach/kern_return.h>
+
+/*
+ * execargs_alloc
+ *
+ * Description:	Allocate the block of memory used by the execve arguments.
+ *		At the same time, we allocate a page so that we can read in
+ *		the first page of the image.
+ *
+ * Parameters:	struct image_params *	the image parameter block
+ *
+ * Returns:	0			Success
+ *		EINVAL			Invalid argument
+ *		EACCES			Permission denied
+ *		EINTR			Interrupted function
+ *		ENOMEM			Not enough space
+ *
+ * Notes:	This is a temporary allocation into the kernel address space
+ *		to enable us to copy arguments in from user space.  This is
+ *		necessitated by not mapping the process calling execve() into
+ *		the kernel address space during the execve() system call.
+ *
+ *		We assemble the argument and environment, etc., into this
+ *		region before copying it as a single block into the child
+ *		process address space (at the top or bottom of the stack,
+ *		depending on which way the stack grows; see the function
+ *		exec_copyout_strings() for details).
+ *
+ *		This ends up with a second (possibly unnecessary) copy compared
+ *		with assembing the data directly into the child address space,
+ *		instead, but since we cannot be guaranteed that the parent has
+ *		not modified its environment, we can't really know that it's
+ *		really a block there as well.
+ */
+
+
+static int execargs_waiters = 0;
+lck_mtx_t *execargs_cache_lock;
+
+static void
+execargs_lock_lock(void) {
+	lck_mtx_lock_spin(execargs_cache_lock);
+}
+
+static void
+execargs_lock_unlock(void) {
+	lck_mtx_unlock(execargs_cache_lock);
+}
+
+static wait_result_t
+execargs_lock_sleep(void) {
+	return(lck_mtx_sleep(execargs_cache_lock, LCK_SLEEP_DEFAULT, &execargs_free_count, THREAD_INTERRUPTIBLE));
+}
+
+static kern_return_t
+execargs_purgeable_allocate(char **execarg_address) {
+	kern_return_t kr = vm_allocate(bsd_pageable_map, (vm_offset_t *)execarg_address, BSD_PAGEABLE_SIZE_PER_EXEC, VM_FLAGS_ANYWHERE | VM_FLAGS_PURGABLE);
+	assert(kr == KERN_SUCCESS);
+	return kr;
+}
+
+static kern_return_t
+execargs_purgeable_reference(void *execarg_address) {
+	int state = VM_PURGABLE_NONVOLATILE;
+	kern_return_t kr = vm_purgable_control(bsd_pageable_map, (vm_offset_t) execarg_address, VM_PURGABLE_SET_STATE, &state);
+
+	assert(kr == KERN_SUCCESS);
+	return kr;
+}
+
+static kern_return_t
+execargs_purgeable_volatilize(void *execarg_address) {
+	int state = VM_PURGABLE_VOLATILE | VM_PURGABLE_ORDERING_OBSOLETE;
+	kern_return_t kr;
+	kr = vm_purgable_control(bsd_pageable_map, (vm_offset_t) execarg_address, VM_PURGABLE_SET_STATE, &state);
+
+	assert(kr == KERN_SUCCESS);
+
+	return kr;
+}
+
+static void
+execargs_wakeup_waiters(void) {
+	thread_wakeup(&execargs_free_count);
+}
+
+static int
+execargs_alloc(struct image_params *imgp)
+{
+	kern_return_t kret;
+	wait_result_t res;
+	int i, cache_index = -1;
+
+	execargs_lock_lock();
+
+	while (execargs_free_count == 0) {
+		execargs_waiters++;
+		res = execargs_lock_sleep();
+		execargs_waiters--;
+		if (res != THREAD_AWAKENED) {
+			execargs_lock_unlock();
+			return (EINTR);
+		}
+	}
+
+	execargs_free_count--;
+
+	for (i = 0; i < execargs_cache_size; i++) {
+		vm_offset_t element = execargs_cache[i];
+		if (element) {
+			cache_index = i;
+			imgp->ip_strings = (char *)(execargs_cache[i]);
+			execargs_cache[i] = 0;
+			break;
+		}
+	}
+
+	assert(execargs_free_count >= 0);
+
+	execargs_lock_unlock();
+	
+	if (cache_index == -1) {
+		kret = execargs_purgeable_allocate(&imgp->ip_strings);
+	}
+	else
+		kret = execargs_purgeable_reference(imgp->ip_strings);
+
+	assert(kret == KERN_SUCCESS);
+	if (kret != KERN_SUCCESS) {
+		return (ENOMEM);
+	}
+
+	/* last page used to read in file headers */
+	imgp->ip_vdata = imgp->ip_strings + ( NCARGS + PAGE_SIZE );
+	imgp->ip_strendp = imgp->ip_strings;
+	imgp->ip_argspace = NCARGS;
+	imgp->ip_strspace = ( NCARGS + PAGE_SIZE );
+
+	return (0);
+}
+
+/*
+ * execargs_free
+ *
+ * Description:	Free the block of memory used by the execve arguments and the
+ *		first page of the executable by a previous call to the function
+ *		execargs_alloc().
+ *
+ * Parameters:	struct image_params *	the image parameter block
+ *
+ * Returns:	0			Success
+ *		EINVAL			Invalid argument
+ *		EINTR			Oeration interrupted
+ */
+static int
+execargs_free(struct image_params *imgp)
+{
+	kern_return_t kret;
+	int i;
+	boolean_t needs_wakeup = FALSE;
+	
+	kret = execargs_purgeable_volatilize(imgp->ip_strings);
+
+	execargs_lock_lock();
+	execargs_free_count++;
+
+	for (i = 0; i < execargs_cache_size; i++) {
+		vm_offset_t element = execargs_cache[i];
+		if (element == 0) {
+			execargs_cache[i] = (vm_offset_t) imgp->ip_strings;
+			imgp->ip_strings = NULL;
+			break;
+		}
+	}
+
+	assert(imgp->ip_strings == NULL);
+
+	if (execargs_waiters > 0)
+		needs_wakeup = TRUE;
+	
+	execargs_lock_unlock();
+
+	if (needs_wakeup == TRUE)
+		execargs_wakeup_waiters();
+
+	return ((kret == KERN_SUCCESS ? 0 : EINVAL));
+}
+
+static void
+exec_resettextvp(proc_t p, struct image_params *imgp)
+{
+	vnode_t vp;
+	off_t offset;
+	vnode_t tvp  = p->p_textvp;
+	int ret;
+
+	vp = imgp->ip_vp;
+	offset = imgp->ip_arch_offset;
+
+	if (vp == NULLVP)
+		panic("exec_resettextvp: expected valid vp");
+
+	ret = vnode_ref(vp);
+	proc_lock(p);
+	if (ret == 0) {
+		p->p_textvp = vp;
+		p->p_textoff = offset;
+	} else {
+		p->p_textvp = NULLVP;	/* this is paranoia */
+		p->p_textoff = 0;
+	}
+	proc_unlock(p);
+
+	if ( tvp != NULLVP) {
+		if (vnode_getwithref(tvp) == 0) {
+			vnode_rele(tvp);
+			vnode_put(tvp);
+		}
+	}	
+
+}
+
+/*
+ * If the process is not signed or if it contains entitlements, we
+ * need to communicate through the task_access_port to taskgated.
+ *
+ * taskgated will provide a detached code signature if present, and
+ * will enforce any restrictions on entitlements.
+ */
+
+static boolean_t
+taskgated_required(proc_t p, boolean_t *require_success)
+{
+	size_t length;
+	void *blob;
+	int error;
+
+	if (cs_debug > 2)
+		csvnode_print_debug(p->p_textvp);
+
+	const int can_skip_taskgated = csproc_get_platform_binary(p) && !csproc_get_platform_path(p);
+	if (can_skip_taskgated) {
+		if (cs_debug) printf("taskgated not required for: %s\n", p->p_name);
+		*require_success = FALSE;
+		return FALSE;
+	}
+
+	if ((p->p_csflags & CS_VALID) == 0) {
+		*require_success = FALSE;
+		return TRUE;
+	}
+
+	error = cs_entitlements_blob_get(p, &blob, &length);
+	if (error == 0 && blob != NULL) {
+		/*
+		 * fatal on the desktop when entitlements are present,
+		 * unless we started in single-user mode 
+		 */
+		if ((boothowto & RB_SINGLE) == 0)
+			*require_success = TRUE;
+		/*
+		 * Allow initproc to run without causing taskgated to launch
+		 */
+		if (p == initproc) {
+			*require_success = FALSE;
+			return FALSE;
+		}
+
+		if (cs_debug) printf("taskgated required for: %s\n", p->p_name);
+
+		return TRUE;
+	}
+
+	*require_success = FALSE;
+	return FALSE;
+}
+
+/*
+ * __EXEC_WAITING_ON_TASKGATED_CODE_SIGNATURE_UPCALL__
+ * 
+ * Description: Waits for the userspace daemon to respond to the request
+ * 		we made. Function declared non inline to be visible in
+ *		stackshots and spindumps as well as debugging.
+ */
+__attribute__((noinline)) int 
+__EXEC_WAITING_ON_TASKGATED_CODE_SIGNATURE_UPCALL__(mach_port_t task_access_port, int32_t new_pid)
+{
+	return find_code_signature(task_access_port, new_pid);
+}
+
+static int
+check_for_signature(proc_t p, struct image_params *imgp)
+{
+	mach_port_t port = NULL;
+	kern_return_t kr = KERN_FAILURE;
+	int error = EACCES;
+	boolean_t unexpected_failure = FALSE;
+	unsigned char hash[SHA1_RESULTLEN];
+	boolean_t require_success = FALSE;
+	int spawn = (imgp->ip_flags & IMGPF_SPAWN);
+	int vfexec = (imgp->ip_flags & IMGPF_VFORK_EXEC);
+
+	/*
+	 * Override inherited code signing flags with the
+	 * ones for the process that is being successfully
+	 * loaded
+	 */
+	proc_lock(p);
+	p->p_csflags = imgp->ip_csflags;
+	proc_unlock(p);
+
+	/* Set the switch_protect flag on the map */
+	if(p->p_csflags & (CS_HARD|CS_KILL)) {
+		vm_map_switch_protect(get_task_map(p->task), TRUE);
+	}
+	
+	/*
+	 * image activation may be failed due to policy
+	 * which is unexpected but security framework does not
+	 * approve of exec, kill and return immediately.
+	 */
+	if (imgp->ip_mac_return != 0) {
+		error = imgp->ip_mac_return;
+		unexpected_failure = TRUE;
+		goto done;
+	}
+
+	/* check if callout to taskgated is needed */
+	if (!taskgated_required(p, &require_success)) {
+		error = 0;
+		goto done;
+	}
+
+	kr = task_get_task_access_port(p->task, &port);
+	if (KERN_SUCCESS != kr || !IPC_PORT_VALID(port)) {
+		error = 0;
+		if (require_success)
+			error = EACCES;
+		goto done;
+	}
+
+	/*
+	 * taskgated returns KERN_SUCCESS if it has completed its work
+	 * and the exec should continue, KERN_FAILURE if the exec should 
+	 * fail, or it may error out with different error code in an 
+	 * event of mig failure (e.g. process was signalled during the 
+	 * rpc call, taskgated died, mig server died etc.).
+	 */
+
+	kr = __EXEC_WAITING_ON_TASKGATED_CODE_SIGNATURE_UPCALL__(port, p->p_pid);
+	switch (kr) {
+	case KERN_SUCCESS:
+		error = 0;
+		break;
+	case KERN_FAILURE:
+		error = EACCES;
+		goto done;
+	default:
+		error = EACCES;
+		unexpected_failure = TRUE;
+		goto done;
+	}
+
+	/* Only do this if exec_resettextvp() did not fail */
+	if (p->p_textvp != NULLVP) {
+		/*
+		 * If there's a new code directory, mark this process
+		 * as signed.
+		 */
+		if (0 == ubc_cs_getcdhash(p->p_textvp, p->p_textoff, hash)) {
+			proc_lock(p);
+			p->p_csflags |= CS_VALID;
+			proc_unlock(p);
+		}
+	}
+
+done:
+	if (0 != error) {
+		if (!unexpected_failure)
+			p->p_csflags |= CS_KILLED;
+		/* make very sure execution fails */
+		if (vfexec || spawn) {
+			psignal_vfork(p, p->task, imgp->ip_new_thread, SIGKILL);
+			error = 0;
+		} else {
+			psignal(p, SIGKILL);
+		}
+	}
+	return error;
+}
+
+/*
+ * Typically as soon as we start executing this process, the
+ * first instruction will trigger a VM fault to bring the text
+ * pages (as executable) into the address space, followed soon
+ * thereafter by dyld data structures (for dynamic executable).
+ * To optimize this, as well as improve support for hardware
+ * debuggers that can only access resident pages present
+ * in the process' page tables, we prefault some pages if
+ * possible. Errors are non-fatal.
+ */
+static void exec_prefault_data(proc_t p __unused, struct image_params *imgp, load_result_t *load_result)
+{
+	int ret;
+	size_t expected_all_image_infos_size;
+
+	/*
+	 * Prefault executable or dyld entry point.
+	 */
+	vm_fault(current_map(),
+		 vm_map_trunc_page(load_result->entry_point,
+				   vm_map_page_mask(current_map())),
+		 VM_PROT_READ | VM_PROT_EXECUTE,
+		 FALSE,
+		 THREAD_UNINT, NULL, 0);
+	
+	if (imgp->ip_flags & IMGPF_IS_64BIT) {
+		expected_all_image_infos_size = sizeof(struct user64_dyld_all_image_infos);
+	} else {
+		expected_all_image_infos_size = sizeof(struct user32_dyld_all_image_infos);
+	}
+
+	/* Decode dyld anchor structure from <mach-o/dyld_images.h> */
+	if (load_result->dynlinker &&
+		load_result->all_image_info_addr &&
+		load_result->all_image_info_size >= expected_all_image_infos_size) {
+		union {
+			struct user64_dyld_all_image_infos	infos64;
+			struct user32_dyld_all_image_infos	infos32;
+		} all_image_infos;
+
+		/*
+		 * Pre-fault to avoid copyin() going through the trap handler
+		 * and recovery path.
+		 */
+		vm_fault(current_map(),
+			 vm_map_trunc_page(load_result->all_image_info_addr,
+					   vm_map_page_mask(current_map())),
+			 VM_PROT_READ | VM_PROT_WRITE,
+			 FALSE,
+			 THREAD_UNINT, NULL, 0);
+		if ((load_result->all_image_info_addr & PAGE_MASK) + expected_all_image_infos_size > PAGE_SIZE) {
+			/* all_image_infos straddles a page */
+			vm_fault(current_map(),
+				 vm_map_trunc_page(load_result->all_image_info_addr + expected_all_image_infos_size - 1,
+						   vm_map_page_mask(current_map())),
+				 VM_PROT_READ | VM_PROT_WRITE,
+				 FALSE,
+				 THREAD_UNINT, NULL, 0);
+		}
+
+		ret = copyin(load_result->all_image_info_addr,
+					 &all_image_infos,
+					 expected_all_image_infos_size);
+		if (ret == 0 && all_image_infos.infos32.version >= 9) {
+
+			user_addr_t notification_address;
+			user_addr_t dyld_image_address;
+			user_addr_t dyld_version_address;
+			user_addr_t dyld_all_image_infos_address;
+			user_addr_t dyld_slide_amount;
+
+			if (imgp->ip_flags & IMGPF_IS_64BIT) {
+				notification_address = all_image_infos.infos64.notification;
+				dyld_image_address = all_image_infos.infos64.dyldImageLoadAddress;
+				dyld_version_address = all_image_infos.infos64.dyldVersion;
+				dyld_all_image_infos_address = all_image_infos.infos64.dyldAllImageInfosAddress;
+			} else {
+				notification_address = all_image_infos.infos32.notification;
+				dyld_image_address = all_image_infos.infos32.dyldImageLoadAddress;
+				dyld_version_address = all_image_infos.infos32.dyldVersion;
+				dyld_all_image_infos_address = all_image_infos.infos32.dyldAllImageInfosAddress;
+			}
+
+			/*
+			 * dyld statically sets up the all_image_infos in its Mach-O
+			 * binary at static link time, with pointers relative to its default
+			 * load address. Since ASLR might slide dyld before its first
+			 * instruction is executed, "dyld_slide_amount" tells us how far
+			 * dyld was loaded compared to its default expected load address.
+			 * All other pointers into dyld's image should be adjusted by this
+			 * amount. At some point later, dyld will fix up pointers to take
+			 * into account the slide, at which point the all_image_infos_address
+			 * field in the structure will match the runtime load address, and
+			 * "dyld_slide_amount" will be 0, if we were to consult it again.
+			 */
+
+			dyld_slide_amount = load_result->all_image_info_addr - dyld_all_image_infos_address;
+
+#if 0
+			kprintf("exec_prefault: 0x%016llx 0x%08x 0x%016llx 0x%016llx 0x%016llx 0x%016llx\n",
+					(uint64_t)load_result->all_image_info_addr,
+					all_image_infos.infos32.version,
+					(uint64_t)notification_address,
+					(uint64_t)dyld_image_address,
+					(uint64_t)dyld_version_address,
+					(uint64_t)dyld_all_image_infos_address);
+#endif
+
+			vm_fault(current_map(),
+				 vm_map_trunc_page(notification_address + dyld_slide_amount,
+						   vm_map_page_mask(current_map())),
+				 VM_PROT_READ | VM_PROT_EXECUTE,
+				 FALSE,
+				 THREAD_UNINT, NULL, 0);
+			vm_fault(current_map(),
+				 vm_map_trunc_page(dyld_image_address + dyld_slide_amount,
+						   vm_map_page_mask(current_map())),
+				 VM_PROT_READ | VM_PROT_EXECUTE,
+				 FALSE,
+				 THREAD_UNINT, NULL, 0);
+			vm_fault(current_map(),
+				 vm_map_trunc_page(dyld_version_address + dyld_slide_amount,
+						   vm_map_page_mask(current_map())),
+				 VM_PROT_READ,
+				 FALSE,
+				 THREAD_UNINT, NULL, 0);
+			vm_fault(current_map(),
+				 vm_map_trunc_page(dyld_all_image_infos_address + dyld_slide_amount,
+						   vm_map_page_mask(current_map())),
+				 VM_PROT_READ | VM_PROT_WRITE,
+				 FALSE,
+				 THREAD_UNINT, NULL, 0);
+		}
+	}
+}
diff -Nur xnu-3247.1.106/bsd/kern/kern_mib.c xnu-3247.1.106-AnV/bsd/kern/kern_mib.c
--- xnu-3247.1.106/bsd/kern/kern_mib.c	2015-12-06 01:31:08.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/kern_mib.c	2015-12-13 17:08:10.000000000 +0100
@@ -119,7 +119,13 @@
 #include <i386/cpuid.h>	/* for cpuid_info() */
 #endif
 
-
+#ifndef MACMODEL
+#define MACMODEL "MacPro3,1"
+#endif /* MACMODEL */
+
+#ifndef MACMODELLEN
+#define MACMODELLEN 10
+#endif /* MACMODELLEN */
 
 #ifndef MAX
 #define MAX(a,b) (a >= b ? a : b)
@@ -176,6 +182,7 @@
 	int arg2, struct sysctl_req *req)
 {
 	char dummy[65];
+    char dummylen;
 	int  epochTemp;
 	ml_cpu_info_t cpu_info;
 	int val, doquad;
@@ -282,9 +289,29 @@
 		dummy[64] = 0;
 		return(SYSCTL_OUT(req, dummy, strlen(dummy) + 1));
 	case HW_MODEL:
-		bzero(dummy, sizeof(dummy));
-		if(!PEGetModelName(dummy,64))
-			return(EINVAL);
+		if (PE_parse_boot_argn("macmodel", &dummy, 64))
+		{
+			dummylen = strlen(dummy);
+
+			if (dummylen >= 64)
+			{
+				dummylen = 63;
+			}
+
+			if (dummylen > 1)
+			{
+				dummy[dummylen + 1] = 0;
+				printf("\"macmodel\" boot argument found, Mac model set as \"%s\"\n", dummy);
+			} else {
+				//strncpy(dummy, MACMODEL, (MACMODELLEN + 1));
+				strlcpy(dummy, MACMODEL, MACMODELLEN);
+				printf("\"macmodel\" boot argument not found, Mac model set as \"%s\"\n", MACMODEL);
+			}
+		} else {
+			bzero(dummy, sizeof(dummy));
+			if(!PEGetModelName(dummy,64))
+				return(EINVAL);
+		}
 		dummy[64] = 0;
 		return(SYSCTL_OUT(req, dummy, strlen(dummy) + 1));
 	case HW_USERMEM:
@@ -482,6 +509,16 @@
 #error Unsupported arch
 #endif
 
+    static i386_cpu_info_t	cpuid_cpu_info;
+    i386_cpu_info_t		*info_p = &cpuid_cpu_info;
+
+    uint32_t reg[4];
+    do_cpuid(0, reg);
+	bcopy((char *)&reg[ebx], &info_p->cpuid_vendor[0], 4); /* ug */
+	bcopy((char *)&reg[ecx], &info_p->cpuid_vendor[8], 4);
+	bcopy((char *)&reg[edx], &info_p->cpuid_vendor[4], 4);
+	info_p->cpuid_vendor[12] = 0;
+
 	/*
 	 * Populate the optional portion of the hw.* MIB.
 	 *
@@ -498,19 +535,41 @@
 	/* hw.cpufamily */
 	cpufamily = cpuid_cpufamily();
 
-	/* hw.cacheconfig */
-	cacheconfig[0] = ml_cpu_cache_sharing(0);
-	cacheconfig[1] = ml_cpu_cache_sharing(1);
-	cacheconfig[2] = ml_cpu_cache_sharing(2);
-	cacheconfig[3] = ml_cpu_cache_sharing(3);
-	cacheconfig[4] = 0;
-
-	/* hw.cachesize */
-	cachesize[0] = ml_cpu_cache_size(0);
-	cachesize[1] = ml_cpu_cache_size(1);
-	cachesize[2] = ml_cpu_cache_size(2);
-	cachesize[3] = ml_cpu_cache_size(3);
-	cachesize[4] = 0;
+	if (IsIntelCPU()
+        &&
+        /* Pentium M or higher */
+        (( (cpuid_info()->cpuid_family == CPU_FAMILY_PENTIUM_M) && (cpuid_info()->cpuid_model >= 14) ) ||
+         /* Pentium 4 HT model 3 or higher */
+         ( (cpuid_info()->cpuid_family == CPU_FAMILY_PENTIUM_4) && (cpuid_info()->cpuid_model >=  3) ) ))
+    {
+        /* Use stock code */
+        cacheconfig[0] = ml_cpu_cache_sharing(0);
+        cacheconfig[1] = ml_cpu_cache_sharing(1);
+        cacheconfig[2] = ml_cpu_cache_sharing(2);
+        cacheconfig[3] = ml_cpu_cache_sharing(3);
+        cacheconfig[4] = 0;
+
+        /* hw.cachesize */
+        cachesize[0] = ml_cpu_cache_size(0);
+        cachesize[1] = ml_cpu_cache_size(1);
+        cachesize[2] = ml_cpu_cache_size(2);
+        cachesize[3] = ml_cpu_cache_size(3);
+        cachesize[4] = 0;
+    } else {
+		/* Other CPUs, we just use what we calculated in cpuid.c */
+		cacheconfig[0] = ml_cpu_cache_sharing(0);
+		cacheconfig[1] = cpuid_info()->cache_sharing[L1D];
+		cacheconfig[2] = cpuid_info()->cache_sharing[L2U];
+		cacheconfig[3] = cpuid_info()->cache_sharing[L3U];
+		cacheconfig[4] = 0;
+
+		/* hw.cachesize */
+		cachesize[0] = ml_cpu_cache_size(0);
+		cachesize[1] = cpuid_info()->cache_size[L1D];
+		cachesize[2] = cpuid_info()->cache_size[L2U];
+		cachesize[3] = cpuid_info()->cache_size[L3U];
+		cachesize[4] = 0;
+    };
 
 	/* hw.packages */
 	packages = roundup(ml_cpu_cache_sharing(0), cpuid_info()->thread_count)
diff -Nur xnu-3247.1.106/bsd/kern/kern_mib.c.orig xnu-3247.1.106-AnV/bsd/kern/kern_mib.c.orig
--- xnu-3247.1.106/bsd/kern/kern_mib.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/kern_mib.c.orig	2015-12-06 01:31:08.000000000 +0100
@@ -0,0 +1,523 @@
+/*
+ * Copyright (c) 2000-2007 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*-
+ * Copyright (c) 1982, 1986, 1989, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * Mike Karels at Berkeley Software Design, Inc.
+ *
+ * Quite extensively rewritten by Poul-Henning Kamp of the FreeBSD
+ * project, to make these variables more userfriendly.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *	This product includes software developed by the University of
+ *	California, Berkeley and its contributors.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	@(#)kern_sysctl.c	8.4 (Berkeley) 4/14/94
+ */
+
+#include <sys/param.h>
+#include <sys/kernel.h>
+#include <sys/systm.h>
+#include <sys/sysctl.h>
+#include <sys/proc_internal.h>
+#include <sys/unistd.h>
+
+#if defined(SMP)
+#include <machine/smp.h>
+#endif
+
+#include <sys/param.h>  /* XXX prune includes */
+#include <sys/systm.h>
+#include <sys/kernel.h>
+#include <sys/malloc.h>
+#include <sys/proc.h>
+#include <sys/file_internal.h>
+#include <sys/vnode.h>
+#include <sys/unistd.h>
+#include <sys/ioctl.h>
+#include <sys/namei.h>
+#include <sys/tty.h>
+#include <sys/disklabel.h>
+#include <sys/vm.h>
+#include <sys/sysctl.h>
+#include <sys/user.h>
+#include <mach/machine.h>
+#include <mach/mach_types.h>
+#include <mach/vm_param.h>
+#include <kern/task.h>
+#include <vm/vm_kern.h>
+#include <vm/vm_map.h>
+#include <vm/vm_protos.h>
+#include <mach/host_info.h>
+#include <kern/pms.h>
+
+extern vm_map_t bsd_pageable_map;
+
+#include <sys/mount_internal.h>
+#include <sys/kdebug.h>
+
+#include <IOKit/IOPlatformExpert.h>
+#include <pexpert/pexpert.h>
+
+#include <machine/machine_routines.h>
+#include <machine/cpu_capabilities.h>
+
+#include <mach/mach_host.h>		/* for host_info() */
+
+#if defined(__i386__) || defined(__x86_64__)
+#include <i386/cpuid.h>	/* for cpuid_info() */
+#endif
+
+
+
+#ifndef MAX
+#define MAX(a,b) (a >= b ? a : b)
+#endif
+
+/* XXX This should be in a BSD accessible Mach header, but isn't. */
+extern unsigned int vm_page_wire_count;
+
+static int	cputype, cpusubtype, cputhreadtype, cpufamily, cpu64bit;
+static uint64_t	cacheconfig[10], cachesize[10];
+static int	packages;
+
+SYSCTL_NODE(, 0,	  sysctl, CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"Sysctl internal magic");
+SYSCTL_NODE(, CTL_KERN,	  kern,   CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"High kernel, proc, limits &c");
+SYSCTL_NODE(, CTL_VM,	  vm,     CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"Virtual memory");
+SYSCTL_NODE(, CTL_VFS,	  vfs,     CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"File system");
+SYSCTL_NODE(, CTL_NET,	  net,    CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"Network, (see socket.h)");
+SYSCTL_NODE(, CTL_DEBUG,  debug,  CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"Debugging");
+SYSCTL_NODE(, CTL_HW,	  hw,     CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"hardware");
+SYSCTL_NODE(, CTL_MACHDEP, machdep, CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"machine dependent");
+SYSCTL_NODE(, CTL_USER,	  user,   CTLFLAG_RW|CTLFLAG_LOCKED, 0,
+	"user-level");
+
+#define SYSCTL_RETURN(r, x)	SYSCTL_OUT(r, &x, sizeof(x))
+
+/******************************************************************************
+ * hw.* MIB
+ */
+
+#define CTLHW_RETQUAD	(1 << 31)
+#define CTLHW_LOCAL	(1 << 30)
+
+#define HW_LOCAL_CPUTHREADTYPE	(1 | CTLHW_LOCAL)
+#define HW_LOCAL_PHYSICALCPU	(2 | CTLHW_LOCAL)
+#define HW_LOCAL_PHYSICALCPUMAX	(3 | CTLHW_LOCAL)
+#define HW_LOCAL_LOGICALCPU	(4 | CTLHW_LOCAL)
+#define HW_LOCAL_LOGICALCPUMAX	(5 | CTLHW_LOCAL)
+
+
+/*
+ * Supporting some variables requires us to do "real" work.  We 
+ * gather some of that here.
+ */
+static int
+sysctl_hw_generic(__unused struct sysctl_oid *oidp, __unused void *arg1,
+	int arg2, struct sysctl_req *req)
+{
+	char dummy[65];
+	int  epochTemp;
+	ml_cpu_info_t cpu_info;
+	int val, doquad;
+	long long qval;
+	host_basic_info_data_t hinfo;
+	kern_return_t kret;
+	mach_msg_type_number_t count = HOST_BASIC_INFO_COUNT;
+
+	/*
+	 * Test and mask off the 'return quad' flag.
+	 * Note that only some things here support it.
+	 */
+	doquad = arg2 & CTLHW_RETQUAD;
+	arg2 &= ~CTLHW_RETQUAD;
+
+	ml_cpu_get_info(&cpu_info);
+
+#define BSD_HOST 1
+	kret = host_info((host_t)BSD_HOST, HOST_BASIC_INFO, (host_info_t)&hinfo, &count);
+
+	/*
+	 * Handle various OIDs.
+	 *
+	 * OIDs that can return int or quad set val and qval and then break.
+	 * Errors and int-only values return inline.
+	 */
+	switch (arg2) {
+	case HW_NCPU:
+		if (kret == KERN_SUCCESS) {
+			return(SYSCTL_RETURN(req, hinfo.max_cpus));
+		} else {
+			return(EINVAL);
+		}
+	case HW_AVAILCPU:
+		if (kret == KERN_SUCCESS) {
+			return(SYSCTL_RETURN(req, hinfo.avail_cpus));
+		} else {
+			return(EINVAL);
+		}
+	case HW_LOCAL_PHYSICALCPU:
+		if (kret == KERN_SUCCESS) {
+			return(SYSCTL_RETURN(req, hinfo.physical_cpu));
+		} else {
+			return(EINVAL);
+		}
+	case HW_LOCAL_PHYSICALCPUMAX:
+		if (kret == KERN_SUCCESS) {
+			return(SYSCTL_RETURN(req, hinfo.physical_cpu_max));
+		} else {
+			return(EINVAL);
+		}
+	case HW_LOCAL_LOGICALCPU:
+		if (kret == KERN_SUCCESS) {
+			return(SYSCTL_RETURN(req, hinfo.logical_cpu));
+		} else {
+			return(EINVAL);
+		}
+	case HW_LOCAL_LOGICALCPUMAX:
+		if (kret == KERN_SUCCESS) {
+			return(SYSCTL_RETURN(req, hinfo.logical_cpu_max));
+		} else {
+			return(EINVAL);
+		}
+	case HW_PAGESIZE:
+	{
+		vm_map_t map = get_task_map(current_task());
+		val = vm_map_page_size(map);
+		qval = (long long)val;
+		break;
+	}
+	case HW_CACHELINE:
+		val = cpu_info.cache_line_size;
+		qval = (long long)val;
+		break;
+	case HW_L1ICACHESIZE:
+		val = cpu_info.l1_icache_size;
+		qval = (long long)val;
+		break;
+	case HW_L1DCACHESIZE:
+		val = cpu_info.l1_dcache_size;
+		qval = (long long)val;
+		break;
+	case HW_L2CACHESIZE:
+		if (cpu_info.l2_cache_size == 0xFFFFFFFF)
+			return(EINVAL);
+		val = cpu_info.l2_cache_size;
+		qval = (long long)val;
+		break;
+	case HW_L3CACHESIZE:
+		if (cpu_info.l3_cache_size == 0xFFFFFFFF)
+			return(EINVAL);
+		val = cpu_info.l3_cache_size;
+		qval = (long long)val;
+		break;
+
+		/*
+		 * Deprecated variables.  We still support these for
+		 * backwards compatibility purposes only.
+		 */
+	case HW_MACHINE:
+		bzero(dummy, sizeof(dummy));
+		if(!PEGetMachineName(dummy,64))
+			return(EINVAL);
+		dummy[64] = 0;
+		return(SYSCTL_OUT(req, dummy, strlen(dummy) + 1));
+	case HW_MODEL:
+		bzero(dummy, sizeof(dummy));
+		if(!PEGetModelName(dummy,64))
+			return(EINVAL);
+		dummy[64] = 0;
+		return(SYSCTL_OUT(req, dummy, strlen(dummy) + 1));
+	case HW_USERMEM:
+		{
+		int usermem = mem_size - vm_page_wire_count * page_size;
+
+			return(SYSCTL_RETURN(req, usermem));
+		}
+	case HW_EPOCH:
+	        epochTemp = PEGetPlatformEpoch();
+		if (epochTemp == -1)
+			return(EINVAL);
+		return(SYSCTL_RETURN(req, epochTemp));
+	case HW_VECTORUNIT: {
+		int vector = cpu_info.vector_unit == 0? 0 : 1;
+		return(SYSCTL_RETURN(req, vector));
+	}
+	case HW_L2SETTINGS:
+		if (cpu_info.l2_cache_size == 0xFFFFFFFF)
+			return(EINVAL);
+		return(SYSCTL_RETURN(req, cpu_info.l2_settings));
+	case HW_L3SETTINGS:
+		if (cpu_info.l3_cache_size == 0xFFFFFFFF)
+			return(EINVAL);
+		return(SYSCTL_RETURN(req, cpu_info.l3_settings));
+	default:
+		return(ENOTSUP);
+	}
+	/*
+	 * Callers may come to us with either int or quad buffers.
+	 */
+	if (doquad) {
+		return(SYSCTL_RETURN(req, qval));
+	}
+	return(SYSCTL_RETURN(req, val));
+}
+
+/* hw.pagesize and hw.tbfrequency are expected as 64 bit values */
+static int
+sysctl_pagesize
+(__unused struct sysctl_oid *oidp, __unused void *arg1, __unused int arg2, struct sysctl_req *req)
+{
+	vm_map_t map = get_task_map(current_task());
+	long long l = vm_map_page_size(map);
+	return sysctl_io_number(req, l, sizeof(l), NULL, NULL);
+}
+
+static int
+sysctl_pagesize32
+(__unused struct sysctl_oid *oidp, __unused void *arg1, __unused int arg2, struct sysctl_req *req)
+{
+	long long l;
+	l = (long long) PAGE_SIZE;
+	return sysctl_io_number(req, l, sizeof(l), NULL, NULL);
+}
+
+static int
+sysctl_tbfrequency
+(__unused struct sysctl_oid *oidp, __unused void *arg1, __unused int arg2, struct sysctl_req *req)
+{
+	long long l = gPEClockFrequencyInfo.timebase_frequency_hz;
+	return sysctl_io_number(req, l, sizeof(l), NULL, NULL);
+}
+
+/*
+ * hw.* MIB variables.
+ */
+SYSCTL_PROC    (_hw, HW_NCPU, ncpu, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_NCPU, sysctl_hw_generic, "I", "");
+SYSCTL_PROC    (_hw, HW_AVAILCPU, activecpu, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_AVAILCPU, sysctl_hw_generic, "I", "");
+SYSCTL_PROC    (_hw, OID_AUTO, physicalcpu, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_LOCAL_PHYSICALCPU, sysctl_hw_generic, "I", "");
+SYSCTL_PROC    (_hw, OID_AUTO, physicalcpu_max, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_LOCAL_PHYSICALCPUMAX, sysctl_hw_generic, "I", "");
+SYSCTL_PROC    (_hw, OID_AUTO, logicalcpu, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_LOCAL_LOGICALCPU, sysctl_hw_generic, "I", "");
+SYSCTL_PROC    (_hw, OID_AUTO, logicalcpu_max, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_LOCAL_LOGICALCPUMAX, sysctl_hw_generic, "I", "");
+SYSCTL_INT     (_hw, HW_BYTEORDER, byteorder, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (int *)NULL, BYTE_ORDER, "");
+SYSCTL_INT     (_hw, OID_AUTO, cputype, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &cputype, 0, "");
+SYSCTL_INT     (_hw, OID_AUTO, cpusubtype, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &cpusubtype, 0, "");
+SYSCTL_INT     (_hw, OID_AUTO, cpu64bit_capable, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &cpu64bit, 0, "");
+SYSCTL_INT     (_hw, OID_AUTO, cpufamily, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &cpufamily, 0, "");
+SYSCTL_OPAQUE  (_hw, OID_AUTO, cacheconfig, CTLFLAG_RD | CTLFLAG_LOCKED, &cacheconfig, sizeof(cacheconfig), "Q", "");
+SYSCTL_OPAQUE  (_hw, OID_AUTO, cachesize, CTLFLAG_RD | CTLFLAG_LOCKED, &cachesize, sizeof(cachesize), "Q", "");
+SYSCTL_PROC	   (_hw, OID_AUTO, pagesize, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, 0, sysctl_pagesize, "Q", "");
+SYSCTL_PROC	   (_hw, OID_AUTO, pagesize32, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, 0, sysctl_pagesize32, "Q", "");
+SYSCTL_QUAD    (_hw, OID_AUTO, busfrequency, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.bus_frequency_hz, "");
+SYSCTL_QUAD    (_hw, OID_AUTO, busfrequency_min, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.bus_frequency_min_hz, "");
+SYSCTL_QUAD    (_hw, OID_AUTO, busfrequency_max, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.bus_frequency_max_hz, "");
+SYSCTL_QUAD    (_hw, OID_AUTO, cpufrequency, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.cpu_frequency_hz, "");
+SYSCTL_QUAD    (_hw, OID_AUTO, cpufrequency_min, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.cpu_frequency_min_hz, "");
+SYSCTL_QUAD    (_hw, OID_AUTO, cpufrequency_max, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.cpu_frequency_max_hz, "");
+SYSCTL_PROC    (_hw, OID_AUTO, cachelinesize, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_CACHELINE | CTLHW_RETQUAD, sysctl_hw_generic, "Q", "");
+SYSCTL_PROC    (_hw, OID_AUTO, l1icachesize, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_L1ICACHESIZE | CTLHW_RETQUAD, sysctl_hw_generic, "Q", "");
+SYSCTL_PROC    (_hw, OID_AUTO, l1dcachesize, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_L1DCACHESIZE | CTLHW_RETQUAD, sysctl_hw_generic, "Q", "");
+SYSCTL_PROC    (_hw, OID_AUTO, l2cachesize, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_L2CACHESIZE | CTLHW_RETQUAD, sysctl_hw_generic, "Q", "");
+SYSCTL_PROC    (_hw, OID_AUTO, l3cachesize, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, HW_L3CACHESIZE | CTLHW_RETQUAD, sysctl_hw_generic, "Q", "");
+SYSCTL_PROC(_hw, OID_AUTO, tbfrequency, CTLTYPE_QUAD | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, 0, 0, sysctl_tbfrequency, "Q", "");
+SYSCTL_QUAD    (_hw, HW_MEMSIZE, memsize, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &max_mem, "");
+SYSCTL_INT     (_hw, OID_AUTO, packages, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, &packages, 0, "");
+
+/*
+ * Optional features can register nodes below hw.optional.
+ *
+ * If the feature is not present, the node should either not be registered,
+ * or it should return -1.  If the feature is present, the node should return
+ * 0.  If the feature is present and its use is advised, the node should 
+ * return 1.
+ */
+SYSCTL_NODE(_hw, OID_AUTO, optional, CTLFLAG_RW|CTLFLAG_LOCKED, NULL, "optional features");
+
+SYSCTL_INT(_hw_optional, OID_AUTO, floatingpoint, CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (int *)NULL, 1, "");	/* always set */
+
+/*
+ * Deprecated variables.  These are supported for backwards compatibility
+ * purposes only.  The MASKED flag requests that the variables not be
+ * printed by sysctl(8) and similar utilities.
+ *
+ * The variables named *_compat here are int-sized versions of variables
+ * that are now exported as quads.  The int-sized versions are normally
+ * looked up only by number, wheras the quad-sized versions should be
+ * looked up by name.
+ *
+ * The *_compat nodes are *NOT* visible within the kernel.
+ */
+SYSCTL_PROC(_hw, HW_PAGESIZE,     pagesize_compat, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_PAGESIZE, sysctl_hw_generic, "I", "");
+SYSCTL_COMPAT_INT (_hw, HW_BUS_FREQ,     busfrequency_compat, CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.bus_clock_rate_hz, 0, "");
+SYSCTL_COMPAT_INT (_hw, HW_CPU_FREQ,     cpufrequency_compat, CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.cpu_clock_rate_hz, 0, "");
+SYSCTL_PROC(_hw, HW_CACHELINE,    cachelinesize_compat, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_CACHELINE, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_L1ICACHESIZE, l1icachesize_compat, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_L1ICACHESIZE, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_L1DCACHESIZE, l1dcachesize_compat, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_L1DCACHESIZE, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_L2CACHESIZE,  l2cachesize_compat, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_L2CACHESIZE, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_L3CACHESIZE,  l3cachesize_compat, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_L3CACHESIZE, sysctl_hw_generic, "I", "");
+SYSCTL_COMPAT_INT (_hw, HW_TB_FREQ,      tbfrequency_compat, CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, &gPEClockFrequencyInfo.timebase_frequency_hz, 0, "");
+SYSCTL_PROC(_hw, HW_MACHINE,      machine, CTLTYPE_STRING | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_MACHINE, sysctl_hw_generic, "A", "");
+SYSCTL_PROC(_hw, HW_MODEL,        model, CTLTYPE_STRING | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_MODEL, sysctl_hw_generic, "A", "");
+SYSCTL_COMPAT_UINT(_hw, HW_PHYSMEM,      physmem, CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, &mem_size, 0, "");
+SYSCTL_PROC(_hw, HW_USERMEM,      usermem, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_USERMEM,	sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_EPOCH,        epoch, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_EPOCH, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_VECTORUNIT,   vectorunit, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_VECTORUNIT, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_L2SETTINGS,   l2settings, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_L2SETTINGS, sysctl_hw_generic, "I", "");
+SYSCTL_PROC(_hw, HW_L3SETTINGS,   l3settings, CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_MASKED | CTLFLAG_LOCKED, 0, HW_L3SETTINGS, sysctl_hw_generic, "I", "");
+SYSCTL_INT (_hw, OID_AUTO, cputhreadtype, CTLFLAG_RD | CTLFLAG_NOAUTO | CTLFLAG_KERN | CTLFLAG_LOCKED, &cputhreadtype, 0, "");
+
+#if defined(__i386__) || defined(__x86_64__)
+static int
+sysctl_cpu_capability
+(__unused struct sysctl_oid *oidp, void *arg1, __unused int arg2, struct sysctl_req *req)
+{
+	uint64_t	mask = (uint64_t) (uintptr_t) arg1;
+	boolean_t	is_capable = (_get_cpu_capabilities() & mask) != 0;
+ 
+	return SYSCTL_OUT(req, &is_capable, sizeof(is_capable));
+
+}
+
+SYSCTL_PROC(_hw_optional, OID_AUTO, mmx,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasMMX, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, sse,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasSSE, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, sse2,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasSSE2, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, sse3,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasSSE3, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, supplementalsse3,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasSupplementalSSE3, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, sse4_1,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasSSE4_1, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, sse4_2,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasSSE4_2, 0, sysctl_cpu_capability, "I", "");
+/* "x86_64" is actually a preprocessor symbol on the x86_64 kernel, so we have to hack this */
+#undef x86_64
+SYSCTL_PROC(_hw_optional, OID_AUTO, x86_64,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) k64Bit, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, aes,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasAES, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, avx1_0,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasAVX1_0, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, rdrand,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasRDRAND, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, f16c,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasF16C, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, enfstrg,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasENFSTRG, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, fma,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasFMA, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, avx2_0,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasAVX2_0, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, bmi1,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasBMI1, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, bmi2,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasBMI2, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, rtm,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasRTM, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, hle,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasHLE, 0, sysctl_cpu_capability, "I", "");
+SYSCTL_PROC(_hw_optional, OID_AUTO, adx,	CTLTYPE_INT | CTLFLAG_RD | CTLFLAG_KERN | CTLFLAG_LOCKED, (void *) kHasADX, 0, sysctl_cpu_capability, "I", "");
+#else
+#error Unsupported arch
+#endif /* !__i386__ && !__x86_64 && !__arm__ && ! __arm64__ */
+
+
+/******************************************************************************
+ * Generic MIB initialisation.
+ *
+ * This is a hack, and should be replaced with SYSINITs
+ * at some point.
+ */
+void
+sysctl_mib_init(void)
+{
+	cputype = cpu_type();
+	cpusubtype = cpu_subtype();
+	cputhreadtype = cpu_threadtype();
+#if defined(__i386__) || defined (__x86_64__)
+	cpu64bit = (_get_cpu_capabilities() & k64Bit) == k64Bit;
+#else
+#error Unsupported arch
+#endif
+
+	/*
+	 * Populate the optional portion of the hw.* MIB.
+	 *
+	 * XXX This could be broken out into parts of the code
+	 *     that actually directly relate to the functions in
+	 *     question.
+	 */
+
+	if (cputhreadtype != CPU_THREADTYPE_NONE) {
+		sysctl_register_oid(&sysctl__hw_cputhreadtype);
+	}
+
+#if defined (__i386__) || defined (__x86_64__)
+	/* hw.cpufamily */
+	cpufamily = cpuid_cpufamily();
+
+	/* hw.cacheconfig */
+	cacheconfig[0] = ml_cpu_cache_sharing(0);
+	cacheconfig[1] = ml_cpu_cache_sharing(1);
+	cacheconfig[2] = ml_cpu_cache_sharing(2);
+	cacheconfig[3] = ml_cpu_cache_sharing(3);
+	cacheconfig[4] = 0;
+
+	/* hw.cachesize */
+	cachesize[0] = ml_cpu_cache_size(0);
+	cachesize[1] = ml_cpu_cache_size(1);
+	cachesize[2] = ml_cpu_cache_size(2);
+	cachesize[3] = ml_cpu_cache_size(3);
+	cachesize[4] = 0;
+
+	/* hw.packages */
+	packages = roundup(ml_cpu_cache_sharing(0), cpuid_info()->thread_count)
+			/ cpuid_info()->thread_count;
+
+#else
+#error unknown architecture
+#endif /* !__i386__ && !__x86_64 && !__arm__ && !__arm64__ */
+
+}
diff -Nur xnu-3247.1.106/bsd/kern/mach_process.c xnu-3247.1.106-AnV/bsd/kern/mach_process.c
--- xnu-3247.1.106/bsd/kern/mach_process.c	2015-12-06 01:31:09.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/mach_process.c	2015-12-13 17:08:10.000000000 +0100
@@ -106,6 +106,10 @@
 extern thread_t	port_name_to_thread(mach_port_name_t port_name);
 extern thread_t get_firstthread(task_t);
 
+/* mercurysquad: declare sysctl variable which controls whether PT_DENY_ATTACH is enabled */
+int ptda_enabled = 1;
+SYSCTL_INT(_debug, OID_AUTO, ptracedeny_enabled, CTLFLAG_RW | CTLFLAG_ANYBODY,
+                       &ptda_enabled, 1, "Allow applications to request PT_DENY_ATTACH");
 
 /*
  * sys-trace system call.
@@ -127,7 +131,8 @@
 	AUDIT_ARG(addr, uap->addr);
 	AUDIT_ARG(value32, uap->data);
 
-	if (uap->req == PT_DENY_ATTACH) {
+	/* mercurysquad: only do this when PT_DENY_ATTACH is enabled */
+   	if ((uap->req == PT_DENY_ATTACH) && ptda_enabled) {
 		proc_lock(p);
 		if (ISSET(p->p_lflag, P_LTRACED)) {
 			proc_unlock(p);
diff -Nur xnu-3247.1.106/bsd/kern/ubc_subr.c xnu-3247.1.106-AnV/bsd/kern/ubc_subr.c
--- xnu-3247.1.106/bsd/kern/ubc_subr.c	2015-12-06 01:31:11.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/ubc_subr.c	2015-12-13 17:14:49.000000000 +0100
@@ -74,6 +74,7 @@
 
 #include <security/mac_framework.h>
 #include <stdbool.h>
+#include <kern/voodoo_assert.h>
 
 /* XXX These should be in a BSD accessible Mach header, but aren't. */
 extern kern_return_t memory_object_pages_resident(memory_object_control_t,
@@ -127,21 +128,11 @@
 
 static boolean_t
 cs_valid_range(
-	const void *start,
-	const void *end,
-	const void *lower_bound,
-	const void *upper_bound)
+	const void __unused *start,
+	const void __unused *end,
+	const void __unused *lower_bound,
+	const void __unused *upper_bound)
 {
-	if (upper_bound < lower_bound ||
-	    end < start) {
-		return FALSE;
-	}
-
-	if (start < lower_bound ||
-	    end > upper_bound) {
-		return FALSE;
-	}
-
 	return TRUE;
 }
 
@@ -273,7 +264,7 @@
  * Locating a page hash
  */
 static const unsigned char *
-hashes(
+__unused hashes(
 	const CS_CodeDirectory *cd,
 	uint32_t page,
 	size_t hash_len,
@@ -3370,171 +3361,14 @@
 unsigned long cs_validate_page_bad_hash = 0;
 boolean_t
 cs_validate_page(
-	void			*_blobs,
-	memory_object_t		pager,
-	memory_object_offset_t	page_offset,
-	const void		*data,
-	unsigned		*tainted)
+	void __unused		*_blobs,
+	memory_object_t __unused	pager,
+	memory_object_offset_t __unused	page_offset,
+	const void __unused	*data,
+	boolean_t	*tainted)
 {
-	union cs_hash_union	mdctx;
-	struct cs_hash		*hashtype = NULL;
-	unsigned char		actual_hash[CS_HASH_MAX_SIZE];
-	unsigned char		expected_hash[SHA1_RESULTLEN];
-	boolean_t		found_hash;
-	struct cs_blob		*blobs, *blob;
-	const CS_CodeDirectory	*cd;
-	const CS_SuperBlob	*embedded;
-	const unsigned char	*hash;
-	boolean_t		validated;
-	off_t			offset;	/* page offset in the file */
-	size_t			size;
-	off_t			codeLimit = 0;
-	const char		*lower_bound, *upper_bound;
-	vm_offset_t		kaddr, blob_addr;
-	vm_size_t		ksize;
-	kern_return_t		kr;
-
-	offset = page_offset;
-
-	/* retrieve the expected hash */
-	found_hash = FALSE;
-	blobs = (struct cs_blob *) _blobs;
-
-	for (blob = blobs;
-	     blob != NULL;
-	     blob = blob->csb_next) {
-		offset = page_offset - blob->csb_base_offset;
-		if (offset < blob->csb_start_offset ||
-		    offset >= blob->csb_end_offset) {
-			/* our page is not covered by this blob */
-			continue;
-		}
-
-		/* map the blob in the kernel address space */
-		kaddr = blob->csb_mem_kaddr;
-		if (kaddr == 0) {
-			ksize = (vm_size_t) (blob->csb_mem_size +
-					     blob->csb_mem_offset);
-			kr = vm_map(kernel_map,
-				    &kaddr,
-				    ksize,
-				    0,
-				    VM_FLAGS_ANYWHERE,
-				    blob->csb_mem_handle,
-				    0,
-				    TRUE,
-				    VM_PROT_READ,
-				    VM_PROT_READ,
-				    VM_INHERIT_NONE);
-			if (kr != KERN_SUCCESS) {
-				/* XXX FBDP what to do !? */
-				printf("cs_validate_page: failed to map blob, "
-				       "size=0x%lx kr=0x%x\n",
-				       (size_t)blob->csb_mem_size, kr);
-				break;
-			}
-		}
-
-		blob_addr = kaddr + blob->csb_mem_offset;
-		
-		lower_bound = CAST_DOWN(char *, blob_addr);
-		upper_bound = lower_bound + blob->csb_mem_size;
-
-		embedded = (const CS_SuperBlob *) blob_addr;
-		cd = findCodeDirectory(embedded, lower_bound, upper_bound);
-		if (cd != NULL) {
-			/* all CD's that have been injected is already validated */
-
-			offset = page_offset - blob->csb_base_offset;
-			if (offset < blob->csb_start_offset ||
-			    offset >= blob->csb_end_offset) {
-				/* our page is not covered by this blob */
-				continue;
-			}
-
-			hashtype = blob->csb_hashtype;
-			if (hashtype == NULL)
-				panic("unknown hash type ?");
-			if (hashtype->cs_digest_size > sizeof(actual_hash))
-				panic("hash size too large");
-
-			codeLimit = ntohl(cd->codeLimit);
-
-			hash = hashes(cd, (uint32_t)(offset>>PAGE_SHIFT_4K),
-				      hashtype->cs_size,
-				      lower_bound, upper_bound);
-			if (hash != NULL) {
-				bcopy(hash, expected_hash, sizeof(expected_hash));
-				found_hash = TRUE;
-			}
-
-			break;
-		}
-	}
-
-	if (found_hash == FALSE) {
-		/*
-		 * We can't verify this page because there is no signature
-		 * for it (yet).  It's possible that this part of the object
-		 * is not signed, or that signatures for that part have not
-		 * been loaded yet.
-		 * Report that the page has not been validated and let the
-		 * caller decide if it wants to accept it or not.
-		 */
-		cs_validate_page_no_hash++;
-		if (cs_debug > 1) {
-			printf("CODE SIGNING: cs_validate_page: "
-			       "mobj %p off 0x%llx: no hash to validate !?\n",
-			       pager, page_offset);
-		}
-		validated = FALSE;
-		*tainted = 0;
-	} else {
-
-		*tainted = 0;
-
-		size = PAGE_SIZE_4K;
-		const uint32_t *asha1, *esha1;
-		if ((off_t)(offset + size) > codeLimit) {
-			/* partial page at end of segment */
-			assert(offset < codeLimit);
-			size = (size_t) (codeLimit & PAGE_MASK_4K);
-			*tainted |= CS_VALIDATE_NX;
-		}
-
-		hashtype->cs_init(&mdctx);
-		hashtype->cs_update(&mdctx, data, size);
-		hashtype->cs_final(actual_hash, &mdctx);
-
-		asha1 = (const uint32_t *) actual_hash;
-		esha1 = (const uint32_t *) expected_hash;
-
-		if (bcmp(expected_hash, actual_hash, hashtype->cs_cd_size) != 0) {
-			if (cs_debug) {
-				printf("CODE SIGNING: cs_validate_page: "
-				       "mobj %p off 0x%llx size 0x%lx: "
-				       "actual [0x%x 0x%x 0x%x 0x%x 0x%x] != "
-				       "expected [0x%x 0x%x 0x%x 0x%x 0x%x]\n",
-				       pager, page_offset, size,
-				       asha1[0], asha1[1], asha1[2],
-				       asha1[3], asha1[4],
-				       esha1[0], esha1[1], esha1[2],
-				       esha1[3], esha1[4]);
-			}
-			cs_validate_page_bad_hash++;
-			*tainted |= CS_VALIDATE_TAINTED;
-		} else {
-			if (cs_debug > 10) {
-				printf("CODE SIGNING: cs_validate_page: "
-				       "mobj %p off 0x%llx size 0x%lx: "
-				       "SHA1 OK\n",
-				       pager, page_offset, size);
-			}
-		}
-		validated = TRUE;
-	}
-	
-	return validated;
+    *tainted = FALSE;
+    return TRUE;
 }
 
 int
@@ -3564,7 +3398,8 @@
 
 	if (blob == NULL) {
 		/* we didn't find a blob covering "offset" */
-		ret = EBADEXEC; /* XXX any better error ? */
+		//ret = EBADEXEC; /* XXX any better error ? */
+        ret = 0;
 	} else {
 		/* get the SHA1 hash of that blob */
 		bcopy(blob->csb_cdhash, cdhash, sizeof (blob->csb_cdhash));
@@ -3587,100 +3422,32 @@
  * b) Has someone tried to mount the root filesystem read-write?
  * If answers are (a) yes AND (b) no, then we can use the bitmap.
  */
-#define USE_CODE_SIGN_BITMAP(vp)	( (vp != NULL) && (vp->v_mount != NULL) && (vp->v_mount->mnt_flag & MNT_ROOTFS) && !root_fs_upgrade_try) 
+/* AnV - Don't check CS validation bitmap */
+
+#define USE_CODE_SIGN_BITMAP(vp)	( (vp != NULL) && (vp->v_mount != NULL) && (vp->v_mount->mnt_flag & MNT_ROOTFS) && !root_fs_upgrade_try)
+
 kern_return_t
 ubc_cs_validation_bitmap_allocate(
-	vnode_t		vp)
+	__unused vnode_t		vp)
 {
 	kern_return_t	kr = KERN_SUCCESS;
-	struct ubc_info *uip;
-	char		*target_bitmap;
-	vm_object_size_t	bitmap_size;
-
-	if ( ! USE_CODE_SIGN_BITMAP(vp) || (! UBCINFOEXISTS(vp))) {
-		kr = KERN_INVALID_ARGUMENT;
-	} else {
-		uip = vp->v_ubcinfo;
-
-		if ( uip->cs_valid_bitmap == NULL ) {
-			bitmap_size = stob(uip->ui_size);
-			target_bitmap = (char*) kalloc( (vm_size_t)bitmap_size );
-			if (target_bitmap == 0) {
-				kr = KERN_NO_SPACE;
-			} else {
-				kr = KERN_SUCCESS;
-			}
-			if( kr == KERN_SUCCESS ) {
-				memset( target_bitmap, 0, (size_t)bitmap_size);
-				uip->cs_valid_bitmap = (void*)target_bitmap;
-				uip->cs_valid_bitmap_size = bitmap_size;
-			}
-		}
-	}
 	return kr;
 }
 
 kern_return_t
 ubc_cs_check_validation_bitmap (
-	vnode_t			vp,
-	memory_object_offset_t		offset,
-	int			optype)
+__unused	vnode_t			vp,
+__unused	memory_object_offset_t		offset,
+__unused	int			optype)
 {
 	kern_return_t	kr = KERN_SUCCESS;
-
-	if ( ! USE_CODE_SIGN_BITMAP(vp) || ! UBCINFOEXISTS(vp)) {
-		kr = KERN_INVALID_ARGUMENT;
-	} else {
-		struct ubc_info *uip = vp->v_ubcinfo;
-		char		*target_bitmap = uip->cs_valid_bitmap;
-
-		if ( target_bitmap == NULL ) {
-		       kr = KERN_INVALID_ARGUMENT;
-		} else {
-			uint64_t	bit, byte;
-			bit = atop_64( offset );
-			byte = bit >> 3;
-
-			if ( byte > uip->cs_valid_bitmap_size ) {
-			       kr = KERN_INVALID_ARGUMENT;
-			} else {
-
-				if (optype == CS_BITMAP_SET) {
-					target_bitmap[byte] |= (1 << (bit & 07));
-					kr = KERN_SUCCESS;
-				} else if (optype == CS_BITMAP_CLEAR) {
-					target_bitmap[byte] &= ~(1 << (bit & 07));
-					kr = KERN_SUCCESS;
-				} else if (optype == CS_BITMAP_CHECK) {
-					if ( target_bitmap[byte] & (1 << (bit & 07))) {
-						kr = KERN_SUCCESS;
-					} else {
-						kr = KERN_FAILURE;
-					}
-				}
-			}
-		}
-	}
 	return kr;
 }
 
 void
 ubc_cs_validation_bitmap_deallocate(
-	vnode_t		vp)
+	__unused vnode_t		vp)
 {
-	struct ubc_info *uip;
-	void		*target_bitmap;
-	vm_object_size_t	bitmap_size;
-
-	if ( UBCINFOEXISTS(vp)) {
-		uip = vp->v_ubcinfo;
-
-		if ( (target_bitmap = uip->cs_valid_bitmap) != NULL ) {
-			bitmap_size = uip->cs_valid_bitmap_size;
-			kfree( target_bitmap, (vm_size_t) bitmap_size );
-			uip->cs_valid_bitmap = NULL;
-		}
-	}
 }
 #else
 kern_return_t	ubc_cs_validation_bitmap_allocate(__unused vnode_t vp){
diff -Nur xnu-3247.1.106/bsd/kern/uipc_socket.c xnu-3247.1.106-AnV/bsd/kern/uipc_socket.c
--- xnu-3247.1.106/bsd/kern/uipc_socket.c	2015-12-06 01:31:12.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/uipc_socket.c	2015-12-13 17:08:10.000000000 +0100
@@ -684,8 +684,12 @@
 	}
 
 	so->so_cred = kauth_cred_proc_ref(p);
-	if (!suser(kauth_cred_get(), NULL))
-		so->so_state |= SS_PRIV;
+	/* Peter Bartoli:
+   	 * This will allow raw packet support (ie. MAC spoofing)
+  	 * http://slagheap.net/etherspoof/
+   	 */
+   	if (!suser(kauth_cred_get(), NULL) || prp-> pr_type == SOCK_RAW)
+ 		so->so_state = SS_PRIV;
 
 	so->so_proto = prp;
 	so->so_rcv.sb_flags |= SB_RECV;
diff -Nur xnu-3247.1.106/bsd/kern/uipc_socket.c.orig xnu-3247.1.106-AnV/bsd/kern/uipc_socket.c.orig
--- xnu-3247.1.106/bsd/kern/uipc_socket.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/uipc_socket.c.orig	2015-12-06 01:31:12.000000000 +0100
@@ -0,0 +1,7161 @@
+/*
+ * Copyright (c) 1998-2015 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ *
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ *
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ *
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/* Copyright (c) 1995 NeXT Computer, Inc. All Rights Reserved */
+/*
+ * Copyright (c) 1982, 1986, 1988, 1990, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *	This product includes software developed by the University of
+ *	California, Berkeley and its contributors.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	@(#)uipc_socket.c	8.3 (Berkeley) 4/15/94
+ */
+/*
+ * NOTICE: This file was modified by SPARTA, Inc. in 2005 to introduce
+ * support for mandatory and extensible security protections.  This notice
+ * is included in support of clause 2.2 (b) of the Apple Public License,
+ * Version 2.0.
+ */
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/filedesc.h>
+#include <sys/proc.h>
+#include <sys/proc_internal.h>
+#include <sys/kauth.h>
+#include <sys/file_internal.h>
+#include <sys/fcntl.h>
+#include <sys/malloc.h>
+#include <sys/mbuf.h>
+#include <sys/domain.h>
+#include <sys/kernel.h>
+#include <sys/event.h>
+#include <sys/poll.h>
+#include <sys/protosw.h>
+#include <sys/socket.h>
+#include <sys/socketvar.h>
+#include <sys/resourcevar.h>
+#include <sys/signalvar.h>
+#include <sys/sysctl.h>
+#include <sys/syslog.h>
+#include <sys/uio.h>
+#include <sys/uio_internal.h>
+#include <sys/ev.h>
+#include <sys/kdebug.h>
+#include <sys/un.h>
+#include <sys/user.h>
+#include <sys/priv.h>
+#include <sys/kern_event.h>
+#include <net/route.h>
+#include <net/init.h>
+#include <net/ntstat.h>
+#include <net/content_filter.h>
+#include <netinet/in.h>
+#include <netinet/in_pcb.h>
+#include <netinet/ip6.h>
+#include <netinet6/ip6_var.h>
+#include <netinet/flow_divert.h>
+#include <kern/zalloc.h>
+#include <kern/locks.h>
+#include <machine/limits.h>
+#include <libkern/OSAtomic.h>
+#include <pexpert/pexpert.h>
+#include <kern/assert.h>
+#include <kern/task.h>
+#include <sys/kpi_mbuf.h>
+#include <sys/mcache.h>
+#include <sys/unpcb.h>
+
+#if CONFIG_MACF
+#include <security/mac.h>
+#include <security/mac_framework.h>
+#endif /* MAC */
+
+#if MULTIPATH
+#include <netinet/mp_pcb.h>
+#include <netinet/mptcp_var.h>
+#endif /* MULTIPATH */
+
+#define ROUNDUP(a, b) (((a) + ((b) - 1)) & (~((b) - 1)))
+
+#if DEBUG || DEVELOPMENT
+#define	DEBUG_KERNEL_ADDRPERM(_v) (_v)
+#else
+#define	DEBUG_KERNEL_ADDRPERM(_v) VM_KERNEL_ADDRPERM(_v)
+#endif
+
+/* TODO: this should be in a header file somewhere */
+extern char *proc_name_address(void *p);
+
+static u_int32_t	so_cache_hw;	/* High water mark for socache */
+static u_int32_t	so_cache_timeouts;	/* number of timeouts */
+static u_int32_t	so_cache_max_freed;	/* max freed per timeout */
+static u_int32_t	cached_sock_count = 0;
+STAILQ_HEAD(, socket)	so_cache_head;
+int	max_cached_sock_count = MAX_CACHED_SOCKETS;
+static u_int32_t	so_cache_time;
+static int		socketinit_done;
+static struct zone	*so_cache_zone;
+
+static lck_grp_t	*so_cache_mtx_grp;
+static lck_attr_t	*so_cache_mtx_attr;
+static lck_grp_attr_t	*so_cache_mtx_grp_attr;
+static lck_mtx_t	*so_cache_mtx;
+
+#include <machine/limits.h>
+
+static void	filt_sordetach(struct knote *kn);
+static int	filt_soread(struct knote *kn, long hint);
+static void	filt_sowdetach(struct knote *kn);
+static int	filt_sowrite(struct knote *kn, long hint);
+static void	filt_sockdetach(struct knote *kn);
+static int	filt_sockev(struct knote *kn, long hint);
+static void	filt_socktouch(struct knote *kn, struct kevent_internal_s *kev,
+    long type);
+
+static int sooptcopyin_timeval(struct sockopt *, struct timeval *);
+static int sooptcopyout_timeval(struct sockopt *, const struct timeval *);
+
+static struct filterops soread_filtops = {
+	.f_isfd = 1,
+	.f_detach = filt_sordetach,
+	.f_event = filt_soread,
+};
+
+static struct filterops sowrite_filtops = {
+	.f_isfd = 1,
+	.f_detach = filt_sowdetach,
+	.f_event = filt_sowrite,
+};
+
+static struct filterops sock_filtops = {
+	.f_isfd = 1,
+	.f_detach = filt_sockdetach,
+	.f_event = filt_sockev,
+	.f_touch = filt_socktouch,
+};
+
+SYSCTL_DECL(_kern_ipc);
+
+#define	EVEN_MORE_LOCKING_DEBUG 0
+
+int socket_debug = 0;
+SYSCTL_INT(_kern_ipc, OID_AUTO, socket_debug,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &socket_debug, 0, "");
+
+static int socket_zone = M_SOCKET;
+so_gen_t	so_gencnt;	/* generation count for sockets */
+
+MALLOC_DEFINE(M_SONAME, "soname", "socket name");
+MALLOC_DEFINE(M_PCB, "pcb", "protocol control block");
+
+#define	DBG_LAYER_IN_BEG	NETDBG_CODE(DBG_NETSOCK, 0)
+#define	DBG_LAYER_IN_END	NETDBG_CODE(DBG_NETSOCK, 2)
+#define	DBG_LAYER_OUT_BEG	NETDBG_CODE(DBG_NETSOCK, 1)
+#define	DBG_LAYER_OUT_END	NETDBG_CODE(DBG_NETSOCK, 3)
+#define	DBG_FNC_SOSEND		NETDBG_CODE(DBG_NETSOCK, (4 << 8) | 1)
+#define	DBG_FNC_SOSEND_LIST	NETDBG_CODE(DBG_NETSOCK, (4 << 8) | 3)
+#define	DBG_FNC_SORECEIVE	NETDBG_CODE(DBG_NETSOCK, (8 << 8))
+#define	DBG_FNC_SORECEIVE_LIST	NETDBG_CODE(DBG_NETSOCK, (8 << 8) | 3)
+#define	DBG_FNC_SOSHUTDOWN	NETDBG_CODE(DBG_NETSOCK, (9 << 8))
+
+#define	MAX_SOOPTGETM_SIZE	(128 * MCLBYTES)
+
+int somaxconn = SOMAXCONN;
+SYSCTL_INT(_kern_ipc, KIPC_SOMAXCONN, somaxconn,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &somaxconn, 0, "");
+
+/* Should we get a maximum also ??? */
+static int sosendmaxchain = 65536;
+static int sosendminchain = 16384;
+static int sorecvmincopy  = 16384;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sosendminchain,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &sosendminchain, 0, "");
+SYSCTL_INT(_kern_ipc, OID_AUTO, sorecvmincopy,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &sorecvmincopy, 0, "");
+
+/*
+ * Set to enable jumbo clusters (if available) for large writes when
+ * the socket is marked with SOF_MULTIPAGES; see below.
+ */
+int sosendjcl = 1;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sosendjcl,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &sosendjcl, 0, "");
+
+/*
+ * Set this to ignore SOF_MULTIPAGES and use jumbo clusters for large
+ * writes on the socket for all protocols on any network interfaces,
+ * depending upon sosendjcl above.  Be extra careful when setting this
+ * to 1, because sending down packets that cross physical pages down to
+ * broken drivers (those that falsely assume that the physical pages
+ * are contiguous) might lead to system panics or silent data corruption.
+ * When set to 0, the system will respect SOF_MULTIPAGES, which is set
+ * only for TCP sockets whose outgoing interface is IFNET_MULTIPAGES
+ * capable.  Set this to 1 only for testing/debugging purposes.
+ */
+int sosendjcl_ignore_capab = 0;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sosendjcl_ignore_capab,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &sosendjcl_ignore_capab, 0, "");
+
+/*
+ * Set this to ignore SOF1_IF_2KCL and use big clusters for large
+ * writes on the socket for all protocols on any network interfaces.
+ * Be extra careful when setting this to 1, because sending down packets with
+ * clusters larger that 2 KB might lead to system panics or data corruption.
+ * When set to 0, the system will respect SOF1_IF_2KCL, which is set
+ * on the outgoing interface
+ * Set this to 1  for testing/debugging purposes only.
+ */
+int sosendbigcl_ignore_capab = 0;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sosendbigcl_ignore_capab,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &sosendbigcl_ignore_capab, 0, "");
+
+int sodefunctlog = 0;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sodefunctlog, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&sodefunctlog, 0, "");
+
+int sothrottlelog = 0;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sothrottlelog, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&sothrottlelog, 0, "");
+
+int sorestrictrecv = 1;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sorestrictrecv, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&sorestrictrecv, 0, "Enable inbound interface restrictions");
+
+int sorestrictsend = 1;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sorestrictsend, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&sorestrictsend, 0, "Enable outbound interface restrictions");
+
+int soreserveheadroom = 1;
+SYSCTL_INT(_kern_ipc, OID_AUTO, soreserveheadroom, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&soreserveheadroom, 0, "To allocate contiguous datagram buffers");
+
+extern struct inpcbinfo tcbinfo;
+
+/* TODO: these should be in header file */
+extern int get_inpcb_str_size(void);
+extern int get_tcp_str_size(void);
+
+static unsigned int sl_zone_size;		/* size of sockaddr_list */
+static struct zone *sl_zone;			/* zone for sockaddr_list */
+
+static unsigned int se_zone_size;		/* size of sockaddr_entry */
+static struct zone *se_zone;			/* zone for sockaddr_entry */
+
+vm_size_t	so_cache_zone_element_size;
+
+static int sodelayed_copy(struct socket *, struct uio *, struct mbuf **,
+    user_ssize_t *);
+static void cached_sock_alloc(struct socket **, int);
+static void cached_sock_free(struct socket *);
+
+/*
+ * Maximum of extended background idle sockets per process
+ * Set to zero to disable further setting of the option
+ */
+
+#define	SO_IDLE_BK_IDLE_MAX_PER_PROC	1
+#define	SO_IDLE_BK_IDLE_TIME		600
+#define	SO_IDLE_BK_IDLE_RCV_HIWAT	131072
+
+struct soextbkidlestat soextbkidlestat;
+
+SYSCTL_UINT(_kern_ipc, OID_AUTO, maxextbkidleperproc,
+	CTLFLAG_RW | CTLFLAG_LOCKED, &soextbkidlestat.so_xbkidle_maxperproc, 0,
+	"Maximum of extended background idle sockets per process");
+
+SYSCTL_UINT(_kern_ipc, OID_AUTO, extbkidletime, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&soextbkidlestat.so_xbkidle_time, 0,
+	"Time in seconds to keep extended background idle sockets");
+
+SYSCTL_UINT(_kern_ipc, OID_AUTO, extbkidlercvhiwat, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&soextbkidlestat.so_xbkidle_rcvhiwat, 0,
+	"High water mark for extended background idle sockets");
+
+SYSCTL_STRUCT(_kern_ipc, OID_AUTO, extbkidlestat, CTLFLAG_RD | CTLFLAG_LOCKED,
+	&soextbkidlestat, soextbkidlestat, "");
+
+int so_set_extended_bk_idle(struct socket *, int);
+
+/*
+ * SOTCDB_NO_DSCP is set by default, to prevent the networking stack from
+ * setting the DSCP code on the packet based on the service class; see
+ * <rdar://problem/11277343> for details.
+ */
+__private_extern__ u_int32_t sotcdb = SOTCDB_NO_DSCP;
+SYSCTL_INT(_kern_ipc, OID_AUTO, sotcdb, CTLFLAG_RW | CTLFLAG_LOCKED,
+	&sotcdb, 0, "");
+
+void
+socketinit(void)
+{
+	_CASSERT(sizeof(so_gencnt) == sizeof(uint64_t));
+	VERIFY(IS_P2ALIGNED(&so_gencnt, sizeof(uint32_t)));
+
+#ifdef __LP64__
+	_CASSERT(sizeof(struct sa_endpoints) == sizeof(struct user64_sa_endpoints));
+	_CASSERT(offsetof(struct sa_endpoints, sae_srcif) == offsetof(struct user64_sa_endpoints, sae_srcif));
+	_CASSERT(offsetof(struct sa_endpoints, sae_srcaddr) == offsetof(struct user64_sa_endpoints, sae_srcaddr));
+	_CASSERT(offsetof(struct sa_endpoints, sae_srcaddrlen) == offsetof(struct user64_sa_endpoints, sae_srcaddrlen));
+	_CASSERT(offsetof(struct sa_endpoints, sae_dstaddr) == offsetof(struct user64_sa_endpoints, sae_dstaddr));
+	_CASSERT(offsetof(struct sa_endpoints, sae_dstaddrlen) == offsetof(struct user64_sa_endpoints, sae_dstaddrlen));
+#else
+	_CASSERT(sizeof(struct sa_endpoints) == sizeof(struct user32_sa_endpoints));
+	_CASSERT(offsetof(struct sa_endpoints, sae_srcif) == offsetof(struct user32_sa_endpoints, sae_srcif));
+	_CASSERT(offsetof(struct sa_endpoints, sae_srcaddr) == offsetof(struct user32_sa_endpoints, sae_srcaddr));
+	_CASSERT(offsetof(struct sa_endpoints, sae_srcaddrlen) == offsetof(struct user32_sa_endpoints, sae_srcaddrlen));
+	_CASSERT(offsetof(struct sa_endpoints, sae_dstaddr) == offsetof(struct user32_sa_endpoints, sae_dstaddr));
+	_CASSERT(offsetof(struct sa_endpoints, sae_dstaddrlen) == offsetof(struct user32_sa_endpoints, sae_dstaddrlen));
+#endif
+
+	if (socketinit_done) {
+		printf("socketinit: already called...\n");
+		return;
+	}
+	socketinit_done = 1;
+
+	PE_parse_boot_argn("socket_debug", &socket_debug,
+	    sizeof (socket_debug));
+
+	/*
+	 * allocate lock group attribute and group for socket cache mutex
+	 */
+	so_cache_mtx_grp_attr = lck_grp_attr_alloc_init();
+	so_cache_mtx_grp = lck_grp_alloc_init("so_cache",
+	    so_cache_mtx_grp_attr);
+
+	/*
+	 * allocate the lock attribute for socket cache mutex
+	 */
+	so_cache_mtx_attr = lck_attr_alloc_init();
+
+	/* cached sockets mutex */
+	so_cache_mtx = lck_mtx_alloc_init(so_cache_mtx_grp, so_cache_mtx_attr);
+	if (so_cache_mtx == NULL) {
+		panic("%s: unable to allocate so_cache_mtx\n", __func__);
+		/* NOTREACHED */
+	}
+	STAILQ_INIT(&so_cache_head);
+
+	so_cache_zone_element_size = (vm_size_t)(sizeof (struct socket) + 4
+	    + get_inpcb_str_size() + 4 + get_tcp_str_size());
+
+	so_cache_zone = zinit(so_cache_zone_element_size,
+	    (120000 * so_cache_zone_element_size), 8192, "socache zone");
+	zone_change(so_cache_zone, Z_CALLERACCT, FALSE);
+	zone_change(so_cache_zone, Z_NOENCRYPT, TRUE);
+
+	sl_zone_size = sizeof (struct sockaddr_list);
+	if ((sl_zone = zinit(sl_zone_size, 1024 * sl_zone_size, 1024,
+	    "sockaddr_list")) == NULL) {
+		panic("%s: unable to allocate sockaddr_list zone\n", __func__);
+		/* NOTREACHED */
+	}
+	zone_change(sl_zone, Z_CALLERACCT, FALSE);
+	zone_change(sl_zone, Z_EXPAND, TRUE);
+
+	se_zone_size = sizeof (struct sockaddr_entry);
+	if ((se_zone = zinit(se_zone_size, 1024 * se_zone_size, 1024,
+	    "sockaddr_entry")) == NULL) {
+		panic("%s: unable to allocate sockaddr_entry zone\n", __func__);
+		/* NOTREACHED */
+	}
+	zone_change(se_zone, Z_CALLERACCT, FALSE);
+	zone_change(se_zone, Z_EXPAND, TRUE);
+
+	bzero(&soextbkidlestat, sizeof(struct soextbkidlestat));
+	soextbkidlestat.so_xbkidle_maxperproc = SO_IDLE_BK_IDLE_MAX_PER_PROC;
+	soextbkidlestat.so_xbkidle_time = SO_IDLE_BK_IDLE_TIME;
+	soextbkidlestat.so_xbkidle_rcvhiwat = SO_IDLE_BK_IDLE_RCV_HIWAT;
+
+	in_pcbinit();
+	sflt_init();
+	socket_tclass_init();
+#if MULTIPATH
+	mp_pcbinit();
+#endif /* MULTIPATH */
+}
+
+static void
+cached_sock_alloc(struct socket **so, int waitok)
+{
+	caddr_t	temp;
+	uintptr_t offset;
+
+	lck_mtx_lock(so_cache_mtx);
+
+	if (!STAILQ_EMPTY(&so_cache_head)) {
+		VERIFY(cached_sock_count > 0);
+
+		*so = STAILQ_FIRST(&so_cache_head);
+		STAILQ_REMOVE_HEAD(&so_cache_head, so_cache_ent);
+		STAILQ_NEXT((*so), so_cache_ent) = NULL;
+
+		cached_sock_count--;
+		lck_mtx_unlock(so_cache_mtx);
+
+		temp = (*so)->so_saved_pcb;
+		bzero((caddr_t)*so, sizeof (struct socket));
+
+		(*so)->so_saved_pcb = temp;
+	} else {
+
+		lck_mtx_unlock(so_cache_mtx);
+
+		if (waitok)
+			*so = (struct socket *)zalloc(so_cache_zone);
+		else
+			*so = (struct socket *)zalloc_noblock(so_cache_zone);
+
+		if (*so == NULL)
+			return;
+
+		bzero((caddr_t)*so, sizeof (struct socket));
+
+		/*
+		 * Define offsets for extra structures into our
+		 * single block of memory. Align extra structures
+		 * on longword boundaries.
+		 */
+
+		offset = (uintptr_t)*so;
+		offset += sizeof (struct socket);
+
+		offset = ALIGN(offset);
+
+		(*so)->so_saved_pcb = (caddr_t)offset;
+		offset += get_inpcb_str_size();
+
+		offset = ALIGN(offset);
+
+		((struct inpcb *)(void *)(*so)->so_saved_pcb)->inp_saved_ppcb =
+		    (caddr_t)offset;
+	}
+
+	OSBitOrAtomic(SOF1_CACHED_IN_SOCK_LAYER, &(*so)->so_flags1);
+}
+
+static void
+cached_sock_free(struct socket *so)
+{
+
+	lck_mtx_lock(so_cache_mtx);
+
+	so_cache_time = net_uptime();
+	if (++cached_sock_count > max_cached_sock_count) {
+		--cached_sock_count;
+		lck_mtx_unlock(so_cache_mtx);
+		zfree(so_cache_zone, so);
+	} else {
+		if (so_cache_hw < cached_sock_count)
+			so_cache_hw = cached_sock_count;
+
+		STAILQ_INSERT_TAIL(&so_cache_head, so, so_cache_ent);
+
+		so->cache_timestamp = so_cache_time;
+		lck_mtx_unlock(so_cache_mtx);
+	}
+}
+
+void
+so_update_last_owner_locked(struct socket *so, proc_t self)
+{
+	if (so->last_pid != 0) {
+		/*
+		 * last_pid and last_upid should remain zero for sockets
+		 * created using sock_socket. The check above achieves that
+		 */
+		if (self == PROC_NULL)
+			self = current_proc();
+
+		if (so->last_upid != proc_uniqueid(self) ||
+		    so->last_pid != proc_pid(self)) {
+			so->last_upid = proc_uniqueid(self);
+			so->last_pid = proc_pid(self);
+			proc_getexecutableuuid(self, so->last_uuid,
+			    sizeof (so->last_uuid));
+		}
+		proc_pidoriginatoruuid(so->so_vuuid, sizeof(so->so_vuuid));
+	}
+}
+
+void
+so_update_policy(struct socket *so)
+{
+	if (SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6)
+		(void) inp_update_policy(sotoinpcb(so));
+}
+
+#if NECP
+static void
+so_update_necp_policy(struct socket *so, struct sockaddr *override_local_addr,
+    struct sockaddr *override_remote_addr)
+{
+	if (SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6)
+		inp_update_necp_policy(sotoinpcb(so), override_local_addr,
+		    override_remote_addr, 0);
+}
+#endif /* NECP */
+
+boolean_t
+so_cache_timer(void)
+{
+	struct socket	*p;
+	int		n_freed = 0;
+	boolean_t rc = FALSE;
+
+	lck_mtx_lock(so_cache_mtx);
+	so_cache_timeouts++;
+	so_cache_time = net_uptime();
+
+	while (!STAILQ_EMPTY(&so_cache_head)) {
+		VERIFY(cached_sock_count > 0);
+		p = STAILQ_FIRST(&so_cache_head);
+		if ((so_cache_time - p->cache_timestamp) <
+			SO_CACHE_TIME_LIMIT)
+			break;
+
+		STAILQ_REMOVE_HEAD(&so_cache_head, so_cache_ent);
+		--cached_sock_count;
+
+		zfree(so_cache_zone, p);
+
+		if (++n_freed >= SO_CACHE_MAX_FREE_BATCH) {
+			so_cache_max_freed++;
+			break;
+		}
+	}
+
+	/* Schedule again if there is more to cleanup */
+	if (!STAILQ_EMPTY(&so_cache_head))
+		rc = TRUE;
+
+	lck_mtx_unlock(so_cache_mtx);
+	return (rc);
+}
+
+/*
+ * Get a socket structure from our zone, and initialize it.
+ * We don't implement `waitok' yet (see comments in uipc_domain.c).
+ * Note that it would probably be better to allocate socket
+ * and PCB at the same time, but I'm not convinced that all
+ * the protocols can be easily modified to do this.
+ */
+struct socket *
+soalloc(int waitok, int dom, int type)
+{
+	struct socket *so;
+
+	if ((dom == PF_INET) && (type == SOCK_STREAM)) {
+		cached_sock_alloc(&so, waitok);
+	} else {
+		MALLOC_ZONE(so, struct socket *, sizeof (*so), socket_zone,
+		    M_WAITOK);
+		if (so != NULL)
+			bzero(so, sizeof (*so));
+	}
+	if (so != NULL) {
+		so->so_gencnt = OSIncrementAtomic64((SInt64 *)&so_gencnt);
+		so->so_zone = socket_zone;
+#if CONFIG_MACF_SOCKET
+		/* Convert waitok to  M_WAITOK/M_NOWAIT for MAC Framework. */
+		if (mac_socket_label_init(so, !waitok) != 0) {
+			sodealloc(so);
+			return (NULL);
+		}
+#endif /* MAC_SOCKET */
+	}
+
+	return (so);
+}
+
+int
+socreate_internal(int dom, struct socket **aso, int type, int proto,
+    struct proc *p, uint32_t flags, struct proc *ep)
+{
+	struct protosw *prp;
+	struct socket *so;
+	int error = 0;
+
+#if TCPDEBUG
+	extern int tcpconsdebug;
+#endif
+
+	VERIFY(aso != NULL);
+	*aso = NULL;
+
+	if (proto != 0)
+		prp = pffindproto(dom, proto, type);
+	else
+		prp = pffindtype(dom, type);
+
+	if (prp == NULL || prp->pr_usrreqs->pru_attach == NULL) {
+		if (pffinddomain(dom) == NULL)
+			return (EAFNOSUPPORT);
+		if (proto != 0) {
+			if (pffindprotonotype(dom, proto) != NULL)
+				return (EPROTOTYPE);
+		}
+		return (EPROTONOSUPPORT);
+	}
+	if (prp->pr_type != type)
+		return (EPROTOTYPE);
+	so = soalloc(1, dom, type);
+	if (so == NULL)
+		return (ENOBUFS);
+
+	if (flags & SOCF_ASYNC)
+		so->so_state |= SS_NBIO;
+#if MULTIPATH
+	if (flags & SOCF_MP_SUBFLOW) {
+		/*
+		 * A multipath subflow socket is used internally in the kernel,
+		 * therefore it does not have a file desciptor associated by
+		 * default.
+		 */
+		so->so_state |= SS_NOFDREF;
+		so->so_flags |= SOF_MP_SUBFLOW;
+	}
+#endif /* MULTIPATH */
+
+	TAILQ_INIT(&so->so_incomp);
+	TAILQ_INIT(&so->so_comp);
+	so->so_type = type;
+	so->last_upid = proc_uniqueid(p);
+	so->last_pid = proc_pid(p);
+	proc_getexecutableuuid(p, so->last_uuid, sizeof (so->last_uuid));
+	proc_pidoriginatoruuid(so->so_vuuid, sizeof(so->so_vuuid));
+
+	if (ep != PROC_NULL && ep != p) {
+		so->e_upid = proc_uniqueid(ep);
+		so->e_pid = proc_pid(ep);
+		proc_getexecutableuuid(ep, so->e_uuid, sizeof (so->e_uuid));
+		so->so_flags |= SOF_DELEGATED;
+	}
+
+	so->so_cred = kauth_cred_proc_ref(p);
+	if (!suser(kauth_cred_get(), NULL))
+		so->so_state |= SS_PRIV;
+
+	so->so_proto = prp;
+	so->so_rcv.sb_flags |= SB_RECV;
+	so->so_rcv.sb_so = so->so_snd.sb_so = so;
+	so->next_lock_lr = 0;
+	so->next_unlock_lr = 0;
+
+#if CONFIG_MACF_SOCKET
+	mac_socket_label_associate(kauth_cred_get(), so);
+#endif /* MAC_SOCKET */
+
+	/*
+	 * Attachment will create the per pcb lock if necessary and
+	 * increase refcount for creation, make sure it's done before
+	 * socket is inserted in lists.
+	 */
+	so->so_usecount++;
+
+	error = (*prp->pr_usrreqs->pru_attach)(so, proto, p);
+	if (error != 0) {
+		/*
+		 * Warning:
+		 * If so_pcb is not zero, the socket will be leaked,
+		 * so protocol attachment handler must be coded carefuly
+		 */
+		so->so_state |= SS_NOFDREF;
+		so->so_usecount--;
+		sofreelastref(so, 1);	/* will deallocate the socket */
+		return (error);
+	}
+
+	atomic_add_32(&prp->pr_domain->dom_refs, 1);
+	TAILQ_INIT(&so->so_evlist);
+
+	/* Attach socket filters for this protocol */
+	sflt_initsock(so);
+#if TCPDEBUG
+	if (tcpconsdebug == 2)
+		so->so_options |= SO_DEBUG;
+#endif
+	so_set_default_traffic_class(so);
+
+	/*
+	 * If this thread or task is marked to create backgrounded sockets,
+	 * mark the socket as background.
+	 */
+	if (proc_get_effective_thread_policy(current_thread(),
+	    TASK_POLICY_NEW_SOCKETS_BG)) {
+		socket_set_traffic_mgt_flags(so, TRAFFIC_MGT_SO_BACKGROUND);
+		so->so_background_thread = current_thread();
+	}
+
+	switch (dom) {
+	/*
+	 * Don't mark Unix domain, system or multipath sockets as
+	 * eligible for defunct by default.
+	 */
+	case PF_LOCAL:
+	case PF_SYSTEM:
+	case PF_MULTIPATH:
+		so->so_flags |= SOF_NODEFUNCT;
+		break;
+	default:
+		break;
+	}
+
+	/*
+	 * Entitlements can't be checked at socket creation time except if the
+	 * application requested a feature guarded by a privilege (c.f., socket
+	 * delegation).
+	 * The priv(9) and the Sandboxing APIs are designed with the idea that
+	 * a privilege check should only be triggered by a userland request.
+	 * A privilege check at socket creation time is time consuming and
+	 * could trigger many authorisation error messages from the security
+	 * APIs.
+	 */
+
+	*aso = so;
+
+	return (0);
+}
+
+/*
+ * Returns:	0			Success
+ *		EAFNOSUPPORT
+ *		EPROTOTYPE
+ *		EPROTONOSUPPORT
+ *		ENOBUFS
+ *	<pru_attach>:ENOBUFS[AF_UNIX]
+ *	<pru_attach>:ENOBUFS[TCP]
+ *	<pru_attach>:ENOMEM[TCP]
+ *	<pru_attach>:???		[other protocol families, IPSEC]
+ */
+int
+socreate(int dom, struct socket **aso, int type, int proto)
+{
+	return (socreate_internal(dom, aso, type, proto, current_proc(), 0,
+	    PROC_NULL));
+}
+
+int
+socreate_delegate(int dom, struct socket **aso, int type, int proto, pid_t epid)
+{
+	int error = 0;
+	struct proc *ep = PROC_NULL;
+
+	if ((proc_selfpid() != epid) && ((ep = proc_find(epid)) == PROC_NULL)) {
+		error = ESRCH;
+		goto done;
+	}
+
+	error = socreate_internal(dom, aso, type, proto, current_proc(), 0, ep);
+
+	/*
+	 * It might not be wise to hold the proc reference when calling
+	 * socreate_internal since it calls soalloc with M_WAITOK
+	 */
+done:
+	if (ep != PROC_NULL)
+		proc_rele(ep);
+
+	return (error);
+}
+
+/*
+ * Returns:	0			Success
+ *	<pru_bind>:EINVAL		Invalid argument [COMMON_START]
+ *	<pru_bind>:EAFNOSUPPORT		Address family not supported
+ *	<pru_bind>:EADDRNOTAVAIL	Address not available.
+ *	<pru_bind>:EINVAL		Invalid argument
+ *	<pru_bind>:EAFNOSUPPORT		Address family not supported [notdef]
+ *	<pru_bind>:EACCES		Permission denied
+ *	<pru_bind>:EADDRINUSE		Address in use
+ *	<pru_bind>:EAGAIN		Resource unavailable, try again
+ *	<pru_bind>:EPERM		Operation not permitted
+ *	<pru_bind>:???
+ *	<sf_bind>:???
+ *
+ * Notes:	It's not possible to fully enumerate the return codes above,
+ *		since socket filter authors and protocol family authors may
+ *		not choose to limit their error returns to those listed, even
+ *		though this may result in some software operating incorrectly.
+ *
+ *		The error codes which are enumerated above are those known to
+ *		be returned by the tcp_usr_bind function supplied.
+ */
+int
+sobindlock(struct socket *so, struct sockaddr *nam, int dolock)
+{
+	struct proc *p = current_proc();
+	int error = 0;
+
+	if (dolock)
+		socket_lock(so, 1);
+	VERIFY(so->so_usecount > 1);
+
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+#if NECP
+	so_update_necp_policy(so, nam, NULL);
+#endif /* NECP */
+
+	/*
+	 * If this is a bind request on a socket that has been marked
+	 * as inactive, reject it now before we go any further.
+	 */
+	if (so->so_flags & SOF_DEFUNCT) {
+		error = EINVAL;
+		SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] (%d)\n",
+		    __func__, proc_pid(p), (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so), error));
+		goto out;
+	}
+
+	/* Socket filter */
+	error = sflt_bind(so, nam);
+
+	if (error == 0)
+		error = (*so->so_proto->pr_usrreqs->pru_bind)(so, nam, p);
+out:
+	if (dolock)
+		socket_unlock(so, 1);
+
+	if (error == EJUSTRETURN)
+		error = 0;
+
+	return (error);
+}
+
+void
+sodealloc(struct socket *so)
+{
+	kauth_cred_unref(&so->so_cred);
+
+	/* Remove any filters */
+	sflt_termsock(so);
+
+#if CONTENT_FILTER
+	cfil_sock_detach(so);
+#endif /* CONTENT_FILTER */
+
+	/* Delete the state allocated for msg queues on a socket */
+	if (so->so_flags & SOF_ENABLE_MSGS) {
+		FREE(so->so_msg_state, M_TEMP);
+		so->so_msg_state = NULL;
+	}
+	VERIFY(so->so_msg_state == NULL);
+
+	so->so_gencnt = OSIncrementAtomic64((SInt64 *)&so_gencnt);
+
+#if CONFIG_MACF_SOCKET
+	mac_socket_label_destroy(so);
+#endif /* MAC_SOCKET */
+
+	if (so->so_flags1 & SOF1_CACHED_IN_SOCK_LAYER) {
+		cached_sock_free(so);
+	} else {
+		FREE_ZONE(so, sizeof (*so), so->so_zone);
+	}
+}
+
+/*
+ * Returns:	0			Success
+ *		EINVAL
+ *		EOPNOTSUPP
+ *	<pru_listen>:EINVAL[AF_UNIX]
+ *	<pru_listen>:EINVAL[TCP]
+ *	<pru_listen>:EADDRNOTAVAIL[TCP]	Address not available.
+ *	<pru_listen>:EINVAL[TCP]	Invalid argument
+ *	<pru_listen>:EAFNOSUPPORT[TCP]	Address family not supported [notdef]
+ *	<pru_listen>:EACCES[TCP]	Permission denied
+ *	<pru_listen>:EADDRINUSE[TCP]	Address in use
+ *	<pru_listen>:EAGAIN[TCP]	Resource unavailable, try again
+ *	<pru_listen>:EPERM[TCP]		Operation not permitted
+ *	<sf_listen>:???
+ *
+ * Notes:	Other <pru_listen> returns depend on the protocol family; all
+ *		<sf_listen> returns depend on what the filter author causes
+ *		their filter to return.
+ */
+int
+solisten(struct socket *so, int backlog)
+{
+	struct proc *p = current_proc();
+	int error = 0;
+
+	socket_lock(so, 1);
+
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+#if NECP
+	so_update_necp_policy(so, NULL, NULL);
+#endif /* NECP */
+
+	if (so->so_proto == NULL) {
+		error = EINVAL;
+		goto out;
+	}
+	if ((so->so_proto->pr_flags & PR_CONNREQUIRED) == 0) {
+		error = EOPNOTSUPP;
+		goto out;
+	}
+
+	/*
+	 * If the listen request is made on a socket that is not fully
+	 * disconnected, or on a socket that has been marked as inactive,
+	 * reject the request now.
+	 */
+	if ((so->so_state &
+	    (SS_ISCONNECTED|SS_ISCONNECTING|SS_ISDISCONNECTING)) ||
+	    (so->so_flags & SOF_DEFUNCT)) {
+		error = EINVAL;
+		if (so->so_flags & SOF_DEFUNCT) {
+			SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] "
+			    "(%d)\n", __func__, proc_pid(p),
+			    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+			    SOCK_DOM(so), SOCK_TYPE(so), error));
+		}
+		goto out;
+	}
+
+	if ((so->so_restrictions & SO_RESTRICT_DENY_IN) != 0) {
+		error = EPERM;
+		goto out;
+	}
+
+	error = sflt_listen(so);
+	if (error == 0)
+		error = (*so->so_proto->pr_usrreqs->pru_listen)(so, p);
+
+	if (error) {
+		if (error == EJUSTRETURN)
+			error = 0;
+		goto out;
+	}
+
+	if (TAILQ_EMPTY(&so->so_comp))
+		so->so_options |= SO_ACCEPTCONN;
+	/*
+	 * POSIX: The implementation may have an upper limit on the length of
+	 * the listen queue-either global or per accepting socket. If backlog
+	 * exceeds this limit, the length of the listen queue is set to the
+	 * limit.
+	 *
+	 * If listen() is called with a backlog argument value that is less
+	 * than 0, the function behaves as if it had been called with a backlog
+	 * argument value of 0.
+	 *
+	 * A backlog argument of 0 may allow the socket to accept connections,
+	 * in which case the length of the listen queue may be set to an
+	 * implementation-defined minimum value.
+	 */
+	if (backlog <= 0 || backlog > somaxconn)
+		backlog = somaxconn;
+
+	so->so_qlimit = backlog;
+out:
+	socket_unlock(so, 1);
+	return (error);
+}
+
+void
+sofreelastref(struct socket *so, int dealloc)
+{
+	struct socket *head = so->so_head;
+
+	/* Assume socket is locked */
+
+	if (!(so->so_flags & SOF_PCBCLEARING) || !(so->so_state & SS_NOFDREF)) {
+		selthreadclear(&so->so_snd.sb_sel);
+		selthreadclear(&so->so_rcv.sb_sel);
+		so->so_rcv.sb_flags &= ~(SB_SEL|SB_UPCALL);
+		so->so_snd.sb_flags &= ~(SB_SEL|SB_UPCALL);
+		so->so_event = sonullevent;
+		return;
+	}
+	if (head != NULL) {
+		socket_lock(head, 1);
+		if (so->so_state & SS_INCOMP) {
+			TAILQ_REMOVE(&head->so_incomp, so, so_list);
+			head->so_incqlen--;
+		} else if (so->so_state & SS_COMP) {
+			/*
+			 * We must not decommission a socket that's
+			 * on the accept(2) queue.  If we do, then
+			 * accept(2) may hang after select(2) indicated
+			 * that the listening socket was ready.
+			 */
+			selthreadclear(&so->so_snd.sb_sel);
+			selthreadclear(&so->so_rcv.sb_sel);
+			so->so_rcv.sb_flags &= ~(SB_SEL|SB_UPCALL);
+			so->so_snd.sb_flags &= ~(SB_SEL|SB_UPCALL);
+			so->so_event = sonullevent;
+			socket_unlock(head, 1);
+			return;
+		} else {
+			panic("sofree: not queued");
+		}
+		head->so_qlen--;
+		so->so_state &= ~SS_INCOMP;
+		so->so_head = NULL;
+		socket_unlock(head, 1);
+	}
+	sowflush(so);
+	sorflush(so);
+
+#if FLOW_DIVERT
+	if (so->so_flags & SOF_FLOW_DIVERT) {
+		flow_divert_detach(so);
+	}
+#endif	/* FLOW_DIVERT */
+
+	/* 3932268: disable upcall */
+	so->so_rcv.sb_flags &= ~SB_UPCALL;
+	so->so_snd.sb_flags &= ~SB_UPCALL;
+	so->so_event = sonullevent;
+
+	if (dealloc)
+		sodealloc(so);
+}
+
+void
+soclose_wait_locked(struct socket *so)
+{
+	lck_mtx_t *mutex_held;
+
+	if (so->so_proto->pr_getlock != NULL)
+		mutex_held = (*so->so_proto->pr_getlock)(so, 0);
+	else
+		mutex_held = so->so_proto->pr_domain->dom_mtx;
+	lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+
+	/*
+	 * Double check here and return if there's no outstanding upcall;
+	 * otherwise proceed further only if SOF_UPCALLCLOSEWAIT is set.
+	 */
+	if (!so->so_upcallusecount || !(so->so_flags & SOF_UPCALLCLOSEWAIT))
+		return;
+	so->so_rcv.sb_flags &= ~SB_UPCALL;
+	so->so_snd.sb_flags &= ~SB_UPCALL;
+	so->so_flags |= SOF_CLOSEWAIT;
+	(void) msleep((caddr_t)&so->so_upcallusecount, mutex_held, (PZERO - 1),
+	    "soclose_wait_locked", NULL);
+	lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+	so->so_flags &= ~SOF_CLOSEWAIT;
+}
+
+/*
+ * Close a socket on last file table reference removal.
+ * Initiate disconnect if connected.
+ * Free socket when disconnect complete.
+ */
+int
+soclose_locked(struct socket *so)
+{
+	int error = 0;
+	lck_mtx_t *mutex_held;
+	struct timespec ts;
+
+	if (so->so_usecount == 0) {
+		panic("soclose: so=%p refcount=0\n", so);
+		/* NOTREACHED */
+	}
+
+	sflt_notify(so, sock_evt_closing, NULL);
+
+	if (so->so_upcallusecount)
+		soclose_wait_locked(so);
+
+#if CONTENT_FILTER
+	/*
+	 * We have to wait until the content filters are done
+	 */
+	if ((so->so_flags & SOF_CONTENT_FILTER) != 0) {
+		cfil_sock_close_wait(so);
+		cfil_sock_is_closed(so);
+		cfil_sock_detach(so);
+	}
+#endif /* CONTENT_FILTER */
+
+	if (so->so_flags1 & SOF1_EXTEND_BK_IDLE_INPROG) {
+		soresume(current_proc(), so, 1);
+		so->so_flags1 &= ~SOF1_EXTEND_BK_IDLE_WANTED;
+	}
+
+	if ((so->so_options & SO_ACCEPTCONN)) {
+		struct socket *sp, *sonext;
+		int socklock = 0;
+
+		/*
+		 * We do not want new connection to be added
+		 * to the connection queues
+		 */
+		so->so_options &= ~SO_ACCEPTCONN;
+
+		for (sp = TAILQ_FIRST(&so->so_incomp);
+		    sp != NULL; sp = sonext) {
+			sonext = TAILQ_NEXT(sp, so_list);
+
+			/*
+			 * Radar 5350314
+			 * skip sockets thrown away by tcpdropdropblreq
+			 * they will get cleanup by the garbage collection.
+			 * otherwise, remove the incomp socket from the queue
+			 * and let soabort trigger the appropriate cleanup.
+			 */
+			if (sp->so_flags & SOF_OVERFLOW)
+				continue;
+
+			if (so->so_proto->pr_getlock != NULL) {
+				/*
+				 * Lock ordering for consistency with the
+				 * rest of the stack, we lock the socket
+				 * first and then grabb the head.
+				 */
+				socket_unlock(so, 0);
+				socket_lock(sp, 1);
+				socket_lock(so, 0);
+				socklock = 1;
+			}
+
+			TAILQ_REMOVE(&so->so_incomp, sp, so_list);
+			so->so_incqlen--;
+
+			if (sp->so_state & SS_INCOMP) {
+				sp->so_state &= ~SS_INCOMP;
+				sp->so_head = NULL;
+
+				(void) soabort(sp);
+			}
+
+			if (socklock)
+				socket_unlock(sp, 1);
+		}
+
+		while ((sp = TAILQ_FIRST(&so->so_comp)) != NULL) {
+			/* Dequeue from so_comp since sofree() won't do it */
+			TAILQ_REMOVE(&so->so_comp, sp, so_list);
+			so->so_qlen--;
+
+			if (so->so_proto->pr_getlock != NULL) {
+				socket_unlock(so, 0);
+				socket_lock(sp, 1);
+			}
+
+			if (sp->so_state & SS_COMP) {
+				sp->so_state &= ~SS_COMP;
+				sp->so_head = NULL;
+
+				(void) soabort(sp);
+			}
+
+			if (so->so_proto->pr_getlock != NULL) {
+				socket_unlock(sp, 1);
+				socket_lock(so, 0);
+			}
+		}
+	}
+	if (so->so_pcb == NULL) {
+		/* 3915887: mark the socket as ready for dealloc */
+		so->so_flags |= SOF_PCBCLEARING;
+		goto discard;
+	}
+	if (so->so_state & SS_ISCONNECTED) {
+		if ((so->so_state & SS_ISDISCONNECTING) == 0) {
+			error = sodisconnectlocked(so);
+			if (error)
+				goto drop;
+		}
+		if (so->so_options & SO_LINGER) {
+			if ((so->so_state & SS_ISDISCONNECTING) &&
+			    (so->so_state & SS_NBIO))
+				goto drop;
+			if (so->so_proto->pr_getlock != NULL)
+				mutex_held = (*so->so_proto->pr_getlock)(so, 0);
+			else
+				mutex_held = so->so_proto->pr_domain->dom_mtx;
+			while (so->so_state & SS_ISCONNECTED) {
+				ts.tv_sec = (so->so_linger/100);
+				ts.tv_nsec = (so->so_linger % 100) *
+				    NSEC_PER_USEC * 1000 * 10;
+				error = msleep((caddr_t)&so->so_timeo,
+				    mutex_held, PSOCK | PCATCH, "soclose", &ts);
+				if (error) {
+					/*
+					 * It's OK when the time fires,
+					 * don't report an error
+					 */
+					if (error == EWOULDBLOCK)
+						error = 0;
+					break;
+				}
+			}
+		}
+	}
+drop:
+	if (so->so_usecount == 0) {
+		panic("soclose: usecount is zero so=%p\n", so);
+		/* NOTREACHED */
+	}
+	if (so->so_pcb != NULL && !(so->so_flags & SOF_PCBCLEARING)) {
+		int error2 = (*so->so_proto->pr_usrreqs->pru_detach)(so);
+		if (error == 0)
+			error = error2;
+	}
+	if (so->so_usecount <= 0) {
+		panic("soclose: usecount is zero so=%p\n", so);
+		/* NOTREACHED */
+	}
+discard:
+	if (so->so_pcb != NULL && !(so->so_flags & SOF_MP_SUBFLOW) &&
+	    (so->so_state & SS_NOFDREF)) {
+		panic("soclose: NOFDREF");
+		/* NOTREACHED */
+	}
+	so->so_state |= SS_NOFDREF;
+
+	if (so->so_flags & SOF_MP_SUBFLOW)
+		so->so_flags &= ~SOF_MP_SUBFLOW;
+
+	if ((so->so_flags & SOF_KNOTE) != 0)
+		KNOTE(&so->so_klist, SO_FILT_HINT_LOCKED);
+
+	atomic_add_32(&so->so_proto->pr_domain->dom_refs, -1);
+	evsofree(so);
+
+	so->so_usecount--;
+	sofree(so);
+	return (error);
+}
+
+int
+soclose(struct socket *so)
+{
+	int error = 0;
+	socket_lock(so, 1);
+
+	if (so->so_retaincnt == 0) {
+		error = soclose_locked(so);
+	} else {
+		/*
+		 * if the FD is going away, but socket is
+		 * retained in kernel remove its reference
+		 */
+		so->so_usecount--;
+		if (so->so_usecount < 2)
+			panic("soclose: retaincnt non null and so=%p "
+			    "usecount=%d\n", so, so->so_usecount);
+	}
+	socket_unlock(so, 1);
+	return (error);
+}
+
+/*
+ * Must be called at splnet...
+ */
+/* Should already be locked */
+int
+soabort(struct socket *so)
+{
+	int error;
+
+#ifdef MORE_LOCKING_DEBUG
+	lck_mtx_t *mutex_held;
+
+	if (so->so_proto->pr_getlock != NULL)
+		mutex_held = (*so->so_proto->pr_getlock)(so, 0);
+	else
+		mutex_held = so->so_proto->pr_domain->dom_mtx;
+	lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+#endif
+
+	if ((so->so_flags & SOF_ABORTED) == 0) {
+		so->so_flags |= SOF_ABORTED;
+		error = (*so->so_proto->pr_usrreqs->pru_abort)(so);
+		if (error) {
+			sofree(so);
+			return (error);
+		}
+	}
+	return (0);
+}
+
+int
+soacceptlock(struct socket *so, struct sockaddr **nam, int dolock)
+{
+	int error;
+
+	if (dolock)
+		socket_lock(so, 1);
+
+	so_update_last_owner_locked(so, PROC_NULL);
+	so_update_policy(so);
+#if NECP
+	so_update_necp_policy(so, NULL, NULL);
+#endif /* NECP */
+
+	if ((so->so_state & SS_NOFDREF) == 0)
+		panic("soaccept: !NOFDREF");
+	so->so_state &= ~SS_NOFDREF;
+	error = (*so->so_proto->pr_usrreqs->pru_accept)(so, nam);
+
+	if (dolock)
+		socket_unlock(so, 1);
+	return (error);
+}
+
+int
+soaccept(struct socket *so, struct sockaddr **nam)
+{
+	return (soacceptlock(so, nam, 1));
+}
+
+int
+soacceptfilter(struct socket *so)
+{
+	struct sockaddr *local = NULL, *remote = NULL;
+	int error = 0;
+	struct socket *head = so->so_head;
+
+	/*
+	 * Hold the lock even if this socket has not been made visible
+	 * to the filter(s).  For sockets with global locks, this protects
+	 * against the head or peer going away
+	 */
+	socket_lock(so, 1);
+	if (sogetaddr_locked(so, &remote, 1) != 0 ||
+	    sogetaddr_locked(so, &local, 0) != 0) {
+		so->so_state &= ~(SS_NOFDREF | SS_COMP);
+		so->so_head = NULL;
+		socket_unlock(so, 1);
+		soclose(so);
+		/* Out of resources; try it again next time */
+		error = ECONNABORTED;
+		goto done;
+	}
+
+	error = sflt_accept(head, so, local, remote);
+
+	/*
+	 * If we get EJUSTRETURN from one of the filters, mark this socket
+	 * as inactive and return it anyway.  This newly accepted socket
+	 * will be disconnected later before we hand it off to the caller.
+	 */
+	if (error == EJUSTRETURN) {
+		error = 0;
+		(void) sosetdefunct(current_proc(), so,
+		    SHUTDOWN_SOCKET_LEVEL_DISCONNECT_INTERNAL, FALSE);
+	}
+
+	if (error != 0) {
+		/*
+		 * This may seem like a duplication to the above error
+		 * handling part when we return ECONNABORTED, except
+		 * the following is done while holding the lock since
+		 * the socket has been exposed to the filter(s) earlier.
+		 */
+		so->so_state &= ~(SS_NOFDREF | SS_COMP);
+		so->so_head = NULL;
+		socket_unlock(so, 1);
+		soclose(so);
+		/* Propagate socket filter's error code to the caller */
+	} else {
+		socket_unlock(so, 1);
+	}
+done:
+	/* Callee checks for NULL pointer */
+	sock_freeaddr(remote);
+	sock_freeaddr(local);
+	return (error);
+}
+
+/*
+ * Returns:	0			Success
+ *		EOPNOTSUPP		Operation not supported on socket
+ *		EISCONN			Socket is connected
+ *	<pru_connect>:EADDRNOTAVAIL	Address not available.
+ *	<pru_connect>:EINVAL		Invalid argument
+ *	<pru_connect>:EAFNOSUPPORT	Address family not supported [notdef]
+ *	<pru_connect>:EACCES		Permission denied
+ *	<pru_connect>:EADDRINUSE	Address in use
+ *	<pru_connect>:EAGAIN		Resource unavailable, try again
+ *	<pru_connect>:EPERM		Operation not permitted
+ *	<sf_connect_out>:???		[anything a filter writer might set]
+ */
+int
+soconnectlock(struct socket *so, struct sockaddr *nam, int dolock)
+{
+	int error;
+	struct proc *p = current_proc();
+
+	if (dolock)
+		socket_lock(so, 1);
+
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+#if NECP
+	so_update_necp_policy(so, NULL, nam);
+#endif /* NECP */
+
+	/*
+	 * If this is a listening socket or if this is a previously-accepted
+	 * socket that has been marked as inactive, reject the connect request.
+	 */
+	if ((so->so_options & SO_ACCEPTCONN) || (so->so_flags & SOF_DEFUNCT)) {
+		error = EOPNOTSUPP;
+		if (so->so_flags & SOF_DEFUNCT) {
+			SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] "
+			    "(%d)\n", __func__, proc_pid(p),
+			    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+			    SOCK_DOM(so), SOCK_TYPE(so), error));
+		}
+		if (dolock)
+			socket_unlock(so, 1);
+		return (error);
+	}
+
+	if ((so->so_restrictions & SO_RESTRICT_DENY_OUT) != 0) {
+		if (dolock)
+			socket_unlock(so, 1);
+		return (EPERM);
+	}
+
+	/*
+	 * If protocol is connection-based, can only connect once.
+	 * Otherwise, if connected, try to disconnect first.
+	 * This allows user to disconnect by connecting to, e.g.,
+	 * a null address.
+	 */
+	if (so->so_state & (SS_ISCONNECTED|SS_ISCONNECTING) &&
+	    ((so->so_proto->pr_flags & PR_CONNREQUIRED) ||
+	    (error = sodisconnectlocked(so)))) {
+		error = EISCONN;
+	} else {
+		/*
+		 * Run connect filter before calling protocol:
+		 *  - non-blocking connect returns before completion;
+		 */
+		error = sflt_connectout(so, nam);
+		if (error != 0) {
+			if (error == EJUSTRETURN)
+				error = 0;
+		} else {
+			error = (*so->so_proto->pr_usrreqs->pru_connect)
+			    (so, nam, p);
+		}
+	}
+	if (dolock)
+		socket_unlock(so, 1);
+	return (error);
+}
+
+int
+soconnect(struct socket *so, struct sockaddr *nam)
+{
+	return (soconnectlock(so, nam, 1));
+}
+
+/*
+ * Returns:	0			Success
+ *	<pru_connect2>:EINVAL[AF_UNIX]
+ *	<pru_connect2>:EPROTOTYPE[AF_UNIX]
+ *	<pru_connect2>:???		[other protocol families]
+ *
+ * Notes:	<pru_connect2> is not supported by [TCP].
+ */
+int
+soconnect2(struct socket *so1, struct socket *so2)
+{
+	int error;
+
+	socket_lock(so1, 1);
+	if (so2->so_proto->pr_lock)
+		socket_lock(so2, 1);
+
+	error = (*so1->so_proto->pr_usrreqs->pru_connect2)(so1, so2);
+
+	socket_unlock(so1, 1);
+	if (so2->so_proto->pr_lock)
+		socket_unlock(so2, 1);
+	return (error);
+}
+
+int
+soconnectxlocked(struct socket *so, struct sockaddr_list **src_sl,
+    struct sockaddr_list **dst_sl, struct proc *p, uint32_t ifscope,
+    sae_associd_t aid, sae_connid_t *pcid, uint32_t flags, void *arg,
+    uint32_t arglen, uio_t auio, user_ssize_t *bytes_written)
+{
+	int error;
+
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+	/*
+	 * If this is a listening socket or if this is a previously-accepted
+	 * socket that has been marked as inactive, reject the connect request.
+	 */
+	if ((so->so_options & SO_ACCEPTCONN) || (so->so_flags & SOF_DEFUNCT)) {
+		error = EOPNOTSUPP;
+		if (so->so_flags & SOF_DEFUNCT) {
+			SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] "
+			    "(%d)\n", __func__, proc_pid(p),
+			    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+			    SOCK_DOM(so), SOCK_TYPE(so), error));
+		}
+		return (error);
+	}
+
+	if ((so->so_restrictions & SO_RESTRICT_DENY_OUT) != 0)
+		return (EPERM);
+
+	/*
+	 * If protocol is connection-based, can only connect once
+	 * unless PR_MULTICONN is set.  Otherwise, if connected,
+	 * try to disconnect first.  This allows user to disconnect
+	 * by connecting to, e.g., a null address.
+	 */
+	if ((so->so_state & (SS_ISCONNECTED|SS_ISCONNECTING)) &&
+	    !(so->so_proto->pr_flags & PR_MULTICONN) &&
+	    ((so->so_proto->pr_flags & PR_CONNREQUIRED) ||
+	    (error = sodisconnectlocked(so)) != 0)) {
+		error = EISCONN;
+	} else {
+		/*
+		 * Run connect filter before calling protocol:
+		 *  - non-blocking connect returns before completion;
+		 */
+		error = sflt_connectxout(so, dst_sl);
+		if (error != 0) {
+			if (error == EJUSTRETURN)
+				error = 0;
+		} else {
+			error = (*so->so_proto->pr_usrreqs->pru_connectx)
+			    (so, src_sl, dst_sl, p, ifscope, aid, pcid,
+			    flags, arg, arglen, auio, bytes_written);
+		}
+	}
+
+	return (error);
+}
+
+int
+sodisconnectlocked(struct socket *so)
+{
+	int error;
+
+	if ((so->so_state & SS_ISCONNECTED) == 0) {
+		error = ENOTCONN;
+		goto bad;
+	}
+	if (so->so_state & SS_ISDISCONNECTING) {
+		error = EALREADY;
+		goto bad;
+	}
+
+	error = (*so->so_proto->pr_usrreqs->pru_disconnect)(so);
+	if (error == 0)
+		sflt_notify(so, sock_evt_disconnected, NULL);
+
+bad:
+	return (error);
+}
+
+/* Locking version */
+int
+sodisconnect(struct socket *so)
+{
+	int error;
+
+	socket_lock(so, 1);
+	error = sodisconnectlocked(so);
+	socket_unlock(so, 1);
+	return (error);
+}
+
+int
+sodisconnectxlocked(struct socket *so, sae_associd_t aid, sae_connid_t cid)
+{
+	int error;
+
+	/*
+	 * Call the protocol disconnectx handler; let it handle all
+	 * matters related to the connection state of this session.
+	 */
+	error = (*so->so_proto->pr_usrreqs->pru_disconnectx)(so, aid, cid);
+	if (error == 0) {
+		/*
+		 * The event applies only for the session, not for
+		 * the disconnection of individual subflows.
+		 */
+		if (so->so_state & (SS_ISDISCONNECTING|SS_ISDISCONNECTED))
+			sflt_notify(so, sock_evt_disconnected, NULL);
+	}
+	return (error);
+}
+
+int
+sodisconnectx(struct socket *so, sae_associd_t aid, sae_connid_t cid)
+{
+	int error;
+
+	socket_lock(so, 1);
+	error = sodisconnectxlocked(so, aid, cid);
+	socket_unlock(so, 1);
+	return (error);
+}
+
+int
+sopeelofflocked(struct socket *so, sae_associd_t aid, struct socket **psop)
+{
+	return ((*so->so_proto->pr_usrreqs->pru_peeloff)(so, aid, psop));
+}
+
+#define	SBLOCKWAIT(f)	(((f) & MSG_DONTWAIT) ? 0 : SBL_WAIT)
+
+/*
+ * sosendcheck will lock the socket buffer if it isn't locked and
+ * verify that there is space for the data being inserted.
+ *
+ * Returns:	0			Success
+ *		EPIPE
+ *	sblock:EWOULDBLOCK
+ *	sblock:EINTR
+ *	sbwait:EBADF
+ *	sbwait:EINTR
+ *	[so_error]:???
+ */
+int
+sosendcheck(struct socket *so, struct sockaddr *addr, user_ssize_t resid,
+    int32_t clen, int32_t atomic, int flags, int *sblocked,
+    struct mbuf *control)
+{
+	int	error = 0;
+	int32_t space;
+	int	assumelock = 0;
+
+restart:
+	if (*sblocked == 0) {
+		if ((so->so_snd.sb_flags & SB_LOCK) != 0 &&
+		    so->so_send_filt_thread != 0 &&
+		    so->so_send_filt_thread == current_thread()) {
+			/*
+			 * We're being called recursively from a filter,
+			 * allow this to continue. Radar 4150520.
+			 * Don't set sblocked because we don't want
+			 * to perform an unlock later.
+			 */
+			assumelock = 1;
+		} else {
+			error = sblock(&so->so_snd, SBLOCKWAIT(flags));
+			if (error) {
+				if (so->so_flags & SOF_DEFUNCT)
+					goto defunct;
+				return (error);
+			}
+			*sblocked = 1;
+		}
+	}
+
+	/*
+	 * If a send attempt is made on a socket that has been marked
+	 * as inactive (disconnected), reject the request.
+	 */
+	if (so->so_flags & SOF_DEFUNCT) {
+defunct:
+		error = EPIPE;
+		SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] (%d)\n",
+		    __func__, proc_selfpid(),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so), error));
+		return (error);
+	}
+
+	if (so->so_state & SS_CANTSENDMORE) {
+#if CONTENT_FILTER
+		/*
+		 * Can re-inject data of half closed connections
+		 */
+		if ((so->so_state & SS_ISDISCONNECTED) == 0 &&
+			so->so_snd.sb_cfil_thread == current_thread() &&
+			cfil_sock_data_pending(&so->so_snd) != 0)
+			CFIL_LOG(LOG_INFO,
+				"so %llx ignore SS_CANTSENDMORE",
+				(uint64_t)DEBUG_KERNEL_ADDRPERM(so));
+		else
+#endif /* CONTENT_FILTER */
+			return (EPIPE);
+	}
+	if (so->so_error) {
+		error = so->so_error;
+		so->so_error = 0;
+		return (error);
+	}
+
+	if ((so->so_state & SS_ISCONNECTED) == 0) {
+		if ((so->so_proto->pr_flags & PR_CONNREQUIRED) != 0) {
+			if (((so->so_state & SS_ISCONFIRMING) == 0) &&
+			    (resid != 0 || clen == 0) &&
+			    !(so->so_flags1 & SOF1_PRECONNECT_DATA)) {
+#if MPTCP
+				/*
+				 * MPTCP Fast Join sends data before the
+				 * socket is truly connected.
+				 */
+				if ((so->so_flags & (SOF_MP_SUBFLOW |
+					SOF_MPTCP_FASTJOIN)) !=
+				    (SOF_MP_SUBFLOW | SOF_MPTCP_FASTJOIN))
+#endif /* MPTCP */
+				return (ENOTCONN);
+			}
+		} else if (addr == 0 && !(flags&MSG_HOLD)) {
+			return ((so->so_proto->pr_flags & PR_CONNREQUIRED) ?
+			    ENOTCONN : EDESTADDRREQ);
+		}
+	}
+
+	if (so->so_flags & SOF_ENABLE_MSGS)
+		space = msgq_sbspace(so, control);
+	else
+		space = sbspace(&so->so_snd);
+
+	if (flags & MSG_OOB)
+		space += 1024;
+	if ((atomic && resid > so->so_snd.sb_hiwat) ||
+	    clen > so->so_snd.sb_hiwat)
+		return (EMSGSIZE);
+
+	if ((space < resid + clen &&
+	    (atomic || (space < (int32_t)so->so_snd.sb_lowat) ||
+	    space < clen)) ||
+	    (so->so_type == SOCK_STREAM && so_wait_for_if_feedback(so))) {
+		/*
+		 * don't block the connectx call when there's more data
+		 * than can be copied.
+		 */
+		if (so->so_flags1 & SOF1_PRECONNECT_DATA) {
+			if (space == 0) {
+				return (EWOULDBLOCK);
+			}
+			if (space < (int32_t)so->so_snd.sb_lowat) {
+				return (0);
+			}
+		}
+		if ((so->so_state & SS_NBIO) || (flags & MSG_NBIO) ||
+		    assumelock) {
+			return (EWOULDBLOCK);
+		}
+		sbunlock(&so->so_snd, TRUE);	/* keep socket locked */
+		*sblocked = 0;
+		error = sbwait(&so->so_snd);
+		if (error) {
+			if (so->so_flags & SOF_DEFUNCT)
+				goto defunct;
+			return (error);
+		}
+		goto restart;
+	}
+	return (0);
+}
+
+/*
+ * Send on a socket.
+ * If send must go all at once and message is larger than
+ * send buffering, then hard error.
+ * Lock against other senders.
+ * If must go all at once and not enough room now, then
+ * inform user that this would block and do nothing.
+ * Otherwise, if nonblocking, send as much as possible.
+ * The data to be sent is described by "uio" if nonzero,
+ * otherwise by the mbuf chain "top" (which must be null
+ * if uio is not).  Data provided in mbuf chain must be small
+ * enough to send all at once.
+ *
+ * Returns nonzero on error, timeout or signal; callers
+ * must check for short counts if EINTR/ERESTART are returned.
+ * Data and control buffers are freed on return.
+ * Experiment:
+ * MSG_HOLD: go thru most of sosend(), but just enqueue the mbuf
+ * MSG_SEND: go thru as for MSG_HOLD on current fragment, then
+ *  point at the mbuf chain being constructed and go from there.
+ *
+ * Returns:	0			Success
+ *		EOPNOTSUPP
+ *		EINVAL
+ *		ENOBUFS
+ *	uiomove:EFAULT
+ *	sosendcheck:EPIPE
+ *	sosendcheck:EWOULDBLOCK
+ *	sosendcheck:EINTR
+ *	sosendcheck:EBADF
+ *	sosendcheck:EINTR
+ *	sosendcheck:???			[value from so_error]
+ *	<pru_send>:ECONNRESET[TCP]
+ *	<pru_send>:EINVAL[TCP]
+ *	<pru_send>:ENOBUFS[TCP]
+ *	<pru_send>:EADDRINUSE[TCP]
+ *	<pru_send>:EADDRNOTAVAIL[TCP]
+ *	<pru_send>:EAFNOSUPPORT[TCP]
+ *	<pru_send>:EACCES[TCP]
+ *	<pru_send>:EAGAIN[TCP]
+ *	<pru_send>:EPERM[TCP]
+ *	<pru_send>:EMSGSIZE[TCP]
+ *	<pru_send>:EHOSTUNREACH[TCP]
+ *	<pru_send>:ENETUNREACH[TCP]
+ *	<pru_send>:ENETDOWN[TCP]
+ *	<pru_send>:ENOMEM[TCP]
+ *	<pru_send>:ENOBUFS[TCP]
+ *	<pru_send>:???[TCP]		[ignorable: mostly IPSEC/firewall/DLIL]
+ *	<pru_send>:EINVAL[AF_UNIX]
+ *	<pru_send>:EOPNOTSUPP[AF_UNIX]
+ *	<pru_send>:EPIPE[AF_UNIX]
+ *	<pru_send>:ENOTCONN[AF_UNIX]
+ *	<pru_send>:EISCONN[AF_UNIX]
+ *	<pru_send>:???[AF_UNIX]		[whatever a filter author chooses]
+ *	<sf_data_out>:???		[whatever a filter author chooses]
+ *
+ * Notes:	Other <pru_send> returns depend on the protocol family; all
+ *		<sf_data_out> returns depend on what the filter author causes
+ *		their filter to return.
+ */
+int
+sosend(struct socket *so, struct sockaddr *addr, struct uio *uio,
+    struct mbuf *top, struct mbuf *control, int flags)
+{
+	struct mbuf **mp;
+	struct mbuf *m, *freelist = NULL;
+	user_ssize_t space, len, resid, orig_resid;
+	int clen = 0, error, dontroute, mlen, sendflags;
+	int atomic = sosendallatonce(so) || top;
+	int sblocked = 0;
+	struct proc *p = current_proc();
+	struct mbuf *control_copy = NULL;
+	uint16_t headroom = 0;
+	boolean_t en_tracing = FALSE;
+
+	if (uio != NULL)
+		resid = uio_resid(uio);
+	else
+		resid = top->m_pkthdr.len;
+
+	KERNEL_DEBUG((DBG_FNC_SOSEND | DBG_FUNC_START), so, resid,
+	    so->so_snd.sb_cc, so->so_snd.sb_lowat, so->so_snd.sb_hiwat);
+
+	socket_lock(so, 1);
+
+	/*
+	 * trace if tracing & network (vs. unix) sockets & and
+	 * non-loopback
+	 */
+	if (ENTR_SHOULDTRACE &&
+	    (SOCK_CHECK_DOM(so, AF_INET) || SOCK_CHECK_DOM(so, AF_INET6))) {
+		struct inpcb *inp = sotoinpcb(so);
+		if (inp->inp_last_outifp != NULL &&
+		    !(inp->inp_last_outifp->if_flags & IFF_LOOPBACK)) {
+			en_tracing = TRUE;
+			KERNEL_ENERGYTRACE(kEnTrActKernSockWrite, DBG_FUNC_START,
+			    VM_KERNEL_ADDRPERM(so),
+			    ((so->so_state & SS_NBIO) ? kEnTrFlagNonBlocking : 0),
+			    (int64_t)resid);
+			orig_resid = resid;
+		}
+	}
+
+	/*
+	 * Re-injection should not affect process accounting
+	 */
+	if ((flags & MSG_SKIPCFIL) == 0) {
+		so_update_last_owner_locked(so, p);
+		so_update_policy(so);
+
+#if NECP
+		so_update_necp_policy(so, NULL, addr);
+#endif /* NECP */
+	}
+
+	if (so->so_type != SOCK_STREAM && (flags & MSG_OOB) != 0) {
+		error = EOPNOTSUPP;
+		socket_unlock(so, 1);
+		goto out;
+	}
+
+	/*
+	 * In theory resid should be unsigned.
+	 * However, space must be signed, as it might be less than 0
+	 * if we over-committed, and we must use a signed comparison
+	 * of space and resid.  On the other hand, a negative resid
+	 * causes us to loop sending 0-length segments to the protocol.
+	 *
+	 * Usually, MSG_EOR isn't used on SOCK_STREAM type sockets.
+	 * But it will be used by sockets doing message delivery.
+	 *
+	 * Note: We limit resid to be a positive int value as we use
+	 * imin() to set bytes_to_copy -- radr://14558484
+	 */
+	if (resid < 0 || resid > INT_MAX || (so->so_type == SOCK_STREAM &&
+	    !(so->so_flags & SOF_ENABLE_MSGS) && (flags & MSG_EOR))) {
+		error = EINVAL;
+		socket_unlock(so, 1);
+		goto out;
+	}
+
+	dontroute = (flags & MSG_DONTROUTE) &&
+	    (so->so_options & SO_DONTROUTE) == 0 &&
+	    (so->so_proto->pr_flags & PR_ATOMIC);
+	OSIncrementAtomicLong(&p->p_stats->p_ru.ru_msgsnd);
+
+	if (control != NULL)
+		clen = control->m_len;
+
+	if (soreserveheadroom != 0)
+		headroom = so->so_pktheadroom;
+
+	do {
+		error = sosendcheck(so, addr, resid, clen, atomic, flags,
+		    &sblocked, control);
+		if (error)
+			goto release;
+
+		mp = &top;
+		if (so->so_flags & SOF_ENABLE_MSGS)
+			space = msgq_sbspace(so, control);
+		else
+			space = sbspace(&so->so_snd) - clen;
+		space += ((flags & MSG_OOB) ? 1024 : 0);
+
+		do {
+			if (uio == NULL) {
+				/*
+				 * Data is prepackaged in "top".
+				 */
+				resid = 0;
+				if (flags & MSG_EOR)
+					top->m_flags |= M_EOR;
+			} else {
+				int chainlength;
+				int bytes_to_copy;
+				boolean_t jumbocl;
+				boolean_t bigcl;
+				int bytes_to_alloc;
+
+				bytes_to_copy = imin(resid, space);
+
+				bytes_to_alloc = bytes_to_copy;
+				if (top == NULL)
+					bytes_to_alloc += headroom;
+
+				if (sosendminchain > 0)
+					chainlength = 0;
+				else
+					chainlength = sosendmaxchain;
+
+				/*
+				 * Use big 4 KB cluster when the outgoing interface
+				 * does not prefer 2 KB clusters
+				 */
+				bigcl = !(so->so_flags1 & SOF1_IF_2KCL) ||
+				    sosendbigcl_ignore_capab;
+
+				/*
+				 * Attempt to use larger than system page-size
+				 * clusters for large writes only if there is
+				 * a jumbo cluster pool and if the socket is
+				 * marked accordingly.
+				 */
+				jumbocl = sosendjcl && njcl > 0 &&
+				    ((so->so_flags & SOF_MULTIPAGES) ||
+				    sosendjcl_ignore_capab) &&
+				    bigcl;
+
+				socket_unlock(so, 0);
+
+				do {
+					int num_needed;
+					int hdrs_needed = (top == NULL) ? 1 : 0;
+
+					/*
+					 * try to maintain a local cache of mbuf
+					 * clusters needed to complete this
+					 * write the list is further limited to
+					 * the number that are currently needed
+					 * to fill the socket this mechanism
+					 * allows a large number of mbufs/
+					 * clusters to be grabbed under a single
+					 * mbuf lock... if we can't get any
+					 * clusters, than fall back to trying
+					 * for mbufs if we fail early (or
+					 * miscalcluate the number needed) make
+					 * sure to release any clusters we
+					 * haven't yet consumed.
+					 */
+					if (freelist == NULL &&
+					    bytes_to_alloc > MBIGCLBYTES &&
+					    jumbocl) {
+						num_needed =
+						    bytes_to_alloc / M16KCLBYTES;
+
+						if ((bytes_to_alloc -
+						    (num_needed * M16KCLBYTES))
+						    >= MINCLSIZE)
+							num_needed++;
+
+						freelist =
+						    m_getpackets_internal(
+						    (unsigned int *)&num_needed,
+						    hdrs_needed, M_WAIT, 0,
+						    M16KCLBYTES);
+						/*
+						 * Fall back to 4K cluster size
+						 * if allocation failed
+						 */
+					}
+
+					if (freelist == NULL &&
+					    bytes_to_alloc > MCLBYTES &&
+					    bigcl) {
+						num_needed =
+						    bytes_to_alloc / MBIGCLBYTES;
+
+						if ((bytes_to_alloc -
+						    (num_needed * MBIGCLBYTES)) >=
+						    MINCLSIZE)
+							num_needed++;
+
+						freelist =
+						    m_getpackets_internal(
+						    (unsigned int *)&num_needed,
+						    hdrs_needed, M_WAIT, 0,
+						    MBIGCLBYTES);
+						/*
+						 * Fall back to cluster size
+						 * if allocation failed
+						 */
+					}
+
+					/*
+					 * Allocate a cluster as we want to
+					 * avoid to split the data in more
+					 * that one segment and using MINCLSIZE
+					 * would lead us to allocate two mbufs
+					 */
+					if (soreserveheadroom != 0 &&
+					    freelist == NULL &&
+					    ((top == NULL &&
+					    bytes_to_alloc > _MHLEN) ||
+					    bytes_to_alloc > _MLEN)) {
+						num_needed = ROUNDUP(bytes_to_alloc, MCLBYTES) /
+						    MCLBYTES;
+						freelist =
+						    m_getpackets_internal(
+						    (unsigned int *)&num_needed,
+						    hdrs_needed, M_WAIT, 0,
+						    MCLBYTES);
+						/*
+						 * Fall back to a single mbuf
+						 * if allocation failed
+						 */
+					} else if (freelist == NULL &&
+					    bytes_to_alloc > MINCLSIZE) {
+						num_needed =
+						    bytes_to_alloc / MCLBYTES;
+
+						if ((bytes_to_alloc -
+						    (num_needed * MCLBYTES)) >=
+						    MINCLSIZE)
+							num_needed++;
+
+						freelist =
+						    m_getpackets_internal(
+						    (unsigned int *)&num_needed,
+						    hdrs_needed, M_WAIT, 0,
+						    MCLBYTES);
+						/*
+						 * Fall back to a single mbuf
+						 * if allocation failed
+						 */
+					}
+					/*
+					 * For datagram protocols, leave
+					 * headroom for protocol headers
+					 * in the first cluster of the chain
+					 */
+					if (freelist != NULL && atomic &&
+					    top == NULL && headroom > 0) {
+						freelist->m_data += headroom;
+					}
+					
+					/*
+					 * Fall back to regular mbufs without
+					 * reserving the socket headroom
+					 */
+					if (freelist == NULL) {
+						if (top == NULL)
+							MGETHDR(freelist,
+							    M_WAIT, MT_DATA);
+						else
+							MGET(freelist,
+							    M_WAIT, MT_DATA);
+
+						if (freelist == NULL) {
+							error = ENOBUFS;
+							socket_lock(so, 0);
+							goto release;
+						}
+						/*
+						 * For datagram protocols,
+						 * leave room for protocol
+						 * headers in first mbuf.
+						 */
+						if (atomic && top == NULL &&
+						    bytes_to_copy < MHLEN) {
+							MH_ALIGN(freelist,
+							    bytes_to_copy);
+						}
+					}
+					m = freelist;
+					freelist = m->m_next;
+					m->m_next = NULL;
+
+					if ((m->m_flags & M_EXT))
+						mlen = m->m_ext.ext_size -
+						    m_leadingspace(m);
+					else if ((m->m_flags & M_PKTHDR))
+						mlen =
+						    MHLEN - m_leadingspace(m);
+					else
+						mlen = MLEN - m_leadingspace(m);
+					len = imin(mlen, bytes_to_copy);
+
+					chainlength += len;
+
+					space -= len;
+
+					error = uiomove(mtod(m, caddr_t),
+					    len, uio);
+
+					resid = uio_resid(uio);
+
+					m->m_len = len;
+					*mp = m;
+					top->m_pkthdr.len += len;
+					if (error)
+						break;
+					mp = &m->m_next;
+					if (resid <= 0) {
+						if (flags & MSG_EOR)
+							top->m_flags |= M_EOR;
+						break;
+					}
+					bytes_to_copy = min(resid, space);
+
+				} while (space > 0 &&
+				    (chainlength < sosendmaxchain || atomic ||
+				    resid < MINCLSIZE));
+
+				socket_lock(so, 0);
+
+				if (error)
+					goto release;
+			}
+
+			if (flags & (MSG_HOLD|MSG_SEND)) {
+				/* Enqueue for later, go away if HOLD */
+				struct mbuf *mb1;
+				if (so->so_temp && (flags & MSG_FLUSH)) {
+					m_freem(so->so_temp);
+					so->so_temp = NULL;
+				}
+				if (so->so_temp)
+					so->so_tail->m_next = top;
+				else
+					so->so_temp = top;
+				mb1 = top;
+				while (mb1->m_next)
+					mb1 = mb1->m_next;
+				so->so_tail = mb1;
+				if (flags & MSG_HOLD) {
+					top = NULL;
+					goto release;
+				}
+				top = so->so_temp;
+			}
+			if (dontroute)
+				so->so_options |= SO_DONTROUTE;
+
+			/*
+			 * Compute flags here, for pru_send and NKEs
+			 *
+			 * If the user set MSG_EOF, the protocol
+			 * understands this flag and nothing left to
+			 * send then use PRU_SEND_EOF instead of PRU_SEND.
+			 */
+			sendflags = (flags & MSG_OOB) ? PRUS_OOB :
+			    ((flags & MSG_EOF) &&
+			    (so->so_proto->pr_flags & PR_IMPLOPCL) &&
+			    (resid <= 0)) ? PRUS_EOF :
+			    /* If there is more to send set PRUS_MORETOCOME */
+			    (resid > 0 && space > 0) ? PRUS_MORETOCOME : 0;
+
+			if ((flags & MSG_SKIPCFIL) == 0) {
+				/*
+				 * Socket filter processing
+				 */
+				error = sflt_data_out(so, addr, &top,
+				    &control, (sendflags & MSG_OOB) ?
+				    sock_data_filt_flag_oob : 0);
+				if (error) {
+					if (error == EJUSTRETURN) {
+						error = 0;
+						clen = 0;
+						control = NULL;
+						top = NULL;
+					}
+					goto release;
+				}
+#if CONTENT_FILTER
+				/*
+				 * Content filter processing
+				 */
+				error = cfil_sock_data_out(so, addr, top,
+				    control, (sendflags & MSG_OOB) ?
+				    sock_data_filt_flag_oob : 0);
+				if (error) {
+					if (error == EJUSTRETURN) {
+						error = 0;
+						clen = 0;
+						control = NULL;
+						top = NULL;
+						}
+					goto release;
+				}
+#endif /* CONTENT_FILTER */
+			}
+			if (so->so_flags & SOF_ENABLE_MSGS) {
+				/*
+				 * Make a copy of control mbuf,
+				 * so that msg priority can be
+				 * passed to subsequent mbufs.
+				 */
+				control_copy = m_dup(control, M_NOWAIT);
+			}
+			error = (*so->so_proto->pr_usrreqs->pru_send)
+			    (so, sendflags, top, addr, control, p);
+
+			if (flags & MSG_SEND)
+				so->so_temp = NULL;
+
+			if (dontroute)
+				so->so_options &= ~SO_DONTROUTE;
+
+			clen = 0;
+			control = control_copy;
+			control_copy = NULL;
+			top = NULL;
+			mp = &top;
+			if (error)
+				goto release;
+		} while (resid && space > 0);
+	} while (resid);
+
+release:
+	if (sblocked)
+		sbunlock(&so->so_snd, FALSE);	/* will unlock socket */
+	else
+		socket_unlock(so, 1);
+out:
+	if (top != NULL)
+		m_freem(top);
+	if (control != NULL)
+		m_freem(control);
+	if (freelist != NULL)
+		m_freem_list(freelist);
+	if (control_copy != NULL)
+		m_freem(control_copy);
+
+	/*
+	 * One write has been done. This was enough. Get back to "normal"
+	 * behavior.
+	 */
+	if (so->so_flags1 & SOF1_PRECONNECT_DATA)
+		so->so_flags1 &= ~SOF1_PRECONNECT_DATA;
+
+	if (en_tracing) {
+		/* resid passed here is the bytes left in uio */
+		KERNEL_ENERGYTRACE(kEnTrActKernSockWrite, DBG_FUNC_END,
+		    VM_KERNEL_ADDRPERM(so),
+		    ((error == EWOULDBLOCK) ? kEnTrFlagNoWork : 0),
+		    (int64_t)(orig_resid - resid));
+	}
+	KERNEL_DEBUG(DBG_FNC_SOSEND | DBG_FUNC_END, so, resid,
+	    so->so_snd.sb_cc, space, error);
+
+	return (error);
+}
+
+/*
+ * Supported only connected sockets (no address) without ancillary data
+ * (control mbuf) for atomic protocols
+ */
+int
+sosend_list(struct socket *so, struct uio **uioarray, u_int uiocnt, int flags)
+{
+	struct mbuf *m, *freelist = NULL;
+	user_ssize_t len, resid;
+	int error, dontroute, mlen;
+	int atomic = sosendallatonce(so);
+	int sblocked = 0;
+	struct proc *p = current_proc();
+	u_int uiofirst = 0;
+	u_int uiolast = 0;
+	struct mbuf *top = NULL;
+	uint16_t headroom = 0;
+	boolean_t bigcl;
+
+	KERNEL_DEBUG((DBG_FNC_SOSEND_LIST | DBG_FUNC_START), so, uiocnt,
+	    so->so_snd.sb_cc, so->so_snd.sb_lowat, so->so_snd.sb_hiwat);
+
+	if (so->so_type != SOCK_DGRAM) {
+		error = EINVAL;
+		goto out;
+	}
+	if (atomic == 0) {
+		error = EINVAL;
+		goto out;
+	}
+	if (so->so_proto->pr_usrreqs->pru_send_list == NULL) {
+		error = EPROTONOSUPPORT;
+		goto out;
+	}
+	if (flags & ~(MSG_DONTWAIT | MSG_NBIO)) {
+		error = EINVAL;
+		goto out;
+	}
+	resid = uio_array_resid(uioarray, uiocnt);
+
+	/*
+	 * In theory resid should be unsigned.
+	 * However, space must be signed, as it might be less than 0
+	 * if we over-committed, and we must use a signed comparison
+	 * of space and resid.  On the other hand, a negative resid
+	 * causes us to loop sending 0-length segments to the protocol.
+	 *
+	 * Note: We limit resid to be a positive int value as we use
+	 * imin() to set bytes_to_copy -- radr://14558484
+	 */
+	if (resid < 0 || resid > INT_MAX) {
+		error = EINVAL;
+		goto out;
+	}
+
+	socket_lock(so, 1);
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+#if NECP
+	so_update_necp_policy(so, NULL, NULL);
+#endif /* NECP */
+
+	dontroute = (flags & MSG_DONTROUTE) &&
+	    (so->so_options & SO_DONTROUTE) == 0 &&
+	    (so->so_proto->pr_flags & PR_ATOMIC);
+	OSIncrementAtomicLong(&p->p_stats->p_ru.ru_msgsnd);
+
+	error = sosendcheck(so, NULL, resid, 0, atomic, flags,
+	    &sblocked, NULL);
+	if (error)
+		goto release;
+
+	/*
+	 * Use big 4 KB clusters when the outgoing interface does not prefer
+	 * 2 KB clusters
+	 */
+	bigcl = !(so->so_flags1 & SOF1_IF_2KCL) || sosendbigcl_ignore_capab;
+
+	if (soreserveheadroom != 0)
+		headroom = so->so_pktheadroom;
+
+	do {
+		int i;
+		int num_needed = 0;
+		int chainlength;
+		size_t maxpktlen = 0;
+		int bytes_to_alloc;
+
+		if (sosendminchain > 0)
+			chainlength = 0;
+		else
+			chainlength = sosendmaxchain;
+
+		socket_unlock(so, 0);
+
+		/*
+		 * Find a set of uio that fit in a reasonable number
+		 * of mbuf packets
+		 */
+		for (i = uiofirst; i < uiocnt; i++) {
+			struct uio *auio = uioarray[i];
+
+			len = uio_resid(auio);
+
+			/* Do nothing for empty messages */
+			if (len == 0)
+				continue;
+
+			num_needed += 1;
+			uiolast += 1;
+
+			if (len > maxpktlen)
+				maxpktlen = len;
+
+			chainlength += len;
+			if (chainlength > sosendmaxchain)
+				break;
+		}
+		/*
+		 * Nothing left to send
+		 */
+		if (num_needed == 0) {
+			socket_lock(so, 0);
+			break;
+		}
+		/*
+		 * Allocate buffer large enough to include headroom space for
+		 * network and link header
+		 * 
+		 */
+		bytes_to_alloc = maxpktlen + headroom;
+
+		/*
+		 * Allocate a single contiguous buffer of the smallest available
+		 * size when possible
+		 */
+		if (bytes_to_alloc > MCLBYTES &&
+		    bytes_to_alloc <= MBIGCLBYTES && bigcl) {
+			freelist = m_getpackets_internal(
+			    (unsigned int *)&num_needed,
+			    num_needed, M_WAIT, 1,
+			    MBIGCLBYTES);
+		} else if (bytes_to_alloc > _MHLEN &&
+		    bytes_to_alloc <= MCLBYTES) {
+			freelist = m_getpackets_internal(
+			    (unsigned int *)&num_needed,
+			    num_needed, M_WAIT, 1,
+			    MCLBYTES);
+		} else {
+			freelist = m_allocpacket_internal(
+			    (unsigned int *)&num_needed,
+			    bytes_to_alloc, NULL, M_WAIT, 1, 0);
+		}
+		
+		if (freelist == NULL) {
+			socket_lock(so, 0);
+			error = ENOMEM;
+			goto release;
+		}
+		/*
+		 * Copy each uio of the set into its own mbuf packet
+		 */
+		for (i = uiofirst, m = freelist;
+		    i < uiolast && m != NULL;
+		    i++) {
+			int bytes_to_copy;
+			struct mbuf *n;
+			struct uio *auio = uioarray[i];
+
+			bytes_to_copy = uio_resid(auio);
+
+			/* Do nothing for empty messages */
+			if (bytes_to_copy == 0)
+				continue;
+			/*
+			 * Leave headroom for protocol headers
+			 * in the first mbuf of the chain
+			 */
+			m->m_data += headroom;
+
+			for (n = m; n != NULL; n = n->m_next) {
+				if ((m->m_flags & M_EXT))
+					mlen = m->m_ext.ext_size -
+					    m_leadingspace(m);
+				else if ((m->m_flags & M_PKTHDR))
+					mlen =
+					    MHLEN - m_leadingspace(m);
+				else
+					mlen = MLEN - m_leadingspace(m);
+				len = imin(mlen, bytes_to_copy);
+
+				/*
+				 * Note: uiomove() decrements the iovec
+				 * length
+				 */
+				error = uiomove(mtod(n, caddr_t),
+				    len, auio);
+				if (error != 0)
+					break;
+				n->m_len = len;
+				m->m_pkthdr.len += len;
+
+				VERIFY(m->m_pkthdr.len <= maxpktlen);
+
+				bytes_to_copy -= len;
+				resid -= len;
+			}
+			if (m->m_pkthdr.len == 0) {
+				printf(
+				    "%s:%d so %llx pkt %llx type %u len null\n",
+				    __func__, __LINE__,
+				    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+				    (uint64_t)DEBUG_KERNEL_ADDRPERM(m),
+				    m->m_type);
+			}
+			if (error != 0)
+				break;
+			m = m->m_nextpkt;
+		}
+
+		socket_lock(so, 0);
+
+		if (error)
+			goto release;
+		top = freelist;
+		freelist = NULL;
+
+		if (dontroute)
+			so->so_options |= SO_DONTROUTE;
+
+		if ((flags & MSG_SKIPCFIL) == 0) {
+			struct mbuf **prevnextp = NULL;
+
+			for (i = uiofirst, m = top;
+			    i < uiolast && m != NULL;
+			    i++) {
+				struct mbuf *nextpkt = m->m_nextpkt;
+
+				/*
+				 * Socket filter processing
+				 */
+				error = sflt_data_out(so, NULL, &m,
+				    NULL, 0);
+				if (error != 0 && error != EJUSTRETURN)
+					goto release;
+
+#if CONTENT_FILTER
+				if (error == 0) {
+					/*
+					 * Content filter processing
+					 */
+					error = cfil_sock_data_out(so, NULL, m,
+					    NULL, 0);
+					if (error != 0 && error != EJUSTRETURN)
+						goto release;
+				}
+#endif /* CONTENT_FILTER */
+				/*
+				 * Remove packet from the list when
+				 * swallowed by a filter
+				 */
+				if (error == EJUSTRETURN) {
+					error = 0;
+					if (prevnextp != NULL)
+						*prevnextp = nextpkt;
+					else
+						top = nextpkt;
+				}
+
+				m = nextpkt;
+				if (m != NULL)
+					prevnextp = &m->m_nextpkt;
+			}
+		}
+		if (top != NULL)
+			error = (*so->so_proto->pr_usrreqs->pru_send_list)
+			    (so, 0, top, NULL, NULL, p);
+
+		if (dontroute)
+			so->so_options &= ~SO_DONTROUTE;
+
+		top = NULL;
+		uiofirst = uiolast;
+	} while (resid > 0 && error == 0);
+release:
+	if (sblocked)
+		sbunlock(&so->so_snd, FALSE);	/* will unlock socket */
+	else
+		socket_unlock(so, 1);
+out:
+	if (top != NULL)
+		m_freem(top);
+	if (freelist != NULL)
+		m_freem_list(freelist);
+
+	KERNEL_DEBUG(DBG_FNC_SOSEND_LIST | DBG_FUNC_END, so, resid,
+	    so->so_snd.sb_cc, 0, error);
+
+	return (error);
+}
+
+/*
+ * May return ERESTART when packet is dropped by MAC policy check
+ */
+static int
+soreceive_addr(struct proc *p, struct socket *so, struct sockaddr **psa,
+    int flags, struct mbuf **mp, struct mbuf **nextrecordp, int canwait)
+{
+	int error = 0;
+	struct mbuf *m = *mp;
+	struct mbuf *nextrecord = *nextrecordp;
+
+	KASSERT(m->m_type == MT_SONAME, ("receive 1a"));
+#if CONFIG_MACF_SOCKET_SUBSET
+	/*
+	 * Call the MAC framework for policy checking if we're in
+	 * the user process context and the socket isn't connected.
+	 */
+	if (p != kernproc && !(so->so_state & SS_ISCONNECTED)) {
+		struct mbuf *m0 = m;
+		/*
+		 * Dequeue this record (temporarily) from the receive
+		 * list since we're about to drop the socket's lock
+		 * where a new record may arrive and be appended to
+		 * the list.  Upon MAC policy failure, the record
+		 * will be freed.  Otherwise, we'll add it back to
+		 * the head of the list.  We cannot rely on SB_LOCK
+		 * because append operation uses the socket's lock.
+		 */
+		do {
+			m->m_nextpkt = NULL;
+			sbfree(&so->so_rcv, m);
+			m = m->m_next;
+		} while (m != NULL);
+		m = m0;
+		so->so_rcv.sb_mb = nextrecord;
+		SB_EMPTY_FIXUP(&so->so_rcv);
+		SBLASTRECORDCHK(&so->so_rcv, "soreceive 1a");
+		SBLASTMBUFCHK(&so->so_rcv, "soreceive 1a");
+		socket_unlock(so, 0);
+
+		if (mac_socket_check_received(proc_ucred(p), so,
+		    mtod(m, struct sockaddr *)) != 0) {
+			/*
+			 * MAC policy failure; free this record and
+			 * process the next record (or block until
+			 * one is available).  We have adjusted sb_cc
+			 * and sb_mbcnt above so there is no need to
+			 * call sbfree() again.
+			 */
+			m_freem(m);
+			/*
+			 * Clear SB_LOCK but don't unlock the socket.
+			 * Process the next record or wait for one.
+			 */
+			socket_lock(so, 0);
+			sbunlock(&so->so_rcv, TRUE); /* stay locked */
+			error = ERESTART;
+			goto done;
+		}
+		socket_lock(so, 0);
+		/*
+		 * If the socket has been defunct'd, drop it.
+		 */
+		if (so->so_flags & SOF_DEFUNCT) {
+			m_freem(m);
+			error = ENOTCONN;
+			goto done;
+		}
+		/*
+		 * Re-adjust the socket receive list and re-enqueue
+		 * the record in front of any packets which may have
+		 * been appended while we dropped the lock.
+		 */
+		for (m = m0; m->m_next != NULL; m = m->m_next)
+			sballoc(&so->so_rcv, m);
+		sballoc(&so->so_rcv, m);
+		if (so->so_rcv.sb_mb == NULL) {
+			so->so_rcv.sb_lastrecord = m0;
+			so->so_rcv.sb_mbtail = m;
+		}
+		m = m0;
+		nextrecord = m->m_nextpkt = so->so_rcv.sb_mb;
+		so->so_rcv.sb_mb = m;
+		SBLASTRECORDCHK(&so->so_rcv, "soreceive 1b");
+		SBLASTMBUFCHK(&so->so_rcv, "soreceive 1b");
+	}
+#endif /* CONFIG_MACF_SOCKET_SUBSET */
+	if (psa != NULL) {
+		*psa = dup_sockaddr(mtod(m, struct sockaddr *), canwait);
+		if ((*psa == NULL) && (flags & MSG_NEEDSA)) {
+			error = EWOULDBLOCK;
+			goto done;
+		}
+	}
+	if (flags & MSG_PEEK) {
+		m = m->m_next;
+	} else {
+		sbfree(&so->so_rcv, m);
+		if (m->m_next == NULL && so->so_rcv.sb_cc != 0) {
+			panic("%s: about to create invalid socketbuf",
+			    __func__);
+			/* NOTREACHED */
+		}
+		MFREE(m, so->so_rcv.sb_mb);
+		m = so->so_rcv.sb_mb;
+		if (m != NULL) {
+			m->m_nextpkt = nextrecord;
+		} else {
+			so->so_rcv.sb_mb = nextrecord;
+			SB_EMPTY_FIXUP(&so->so_rcv);
+		}
+	}
+done:
+	*mp = m;
+	*nextrecordp = nextrecord;
+
+	return (error);
+}
+
+/*
+ * Process one or more MT_CONTROL mbufs present before any data mbufs
+ * in the first mbuf chain on the socket buffer.  If MSG_PEEK, we
+ * just copy the data; if !MSG_PEEK, we call into the protocol to
+ * perform externalization.
+ */
+static int
+soreceive_ctl(struct socket *so, struct mbuf **controlp, int flags,
+    struct mbuf **mp, struct mbuf **nextrecordp)
+{
+	int error = 0;
+	struct mbuf *cm = NULL, *cmn;
+	struct mbuf **cme = &cm;
+	struct sockbuf *sb_rcv = &so->so_rcv;
+	struct mbuf **msgpcm = NULL;
+	struct mbuf *m = *mp;
+	struct mbuf *nextrecord = *nextrecordp;
+	struct protosw *pr = so->so_proto;
+
+	/*
+	 * Externalizing the control messages would require us to
+	 * drop the socket's lock below.  Once we re-acquire the
+	 * lock, the mbuf chain might change.  In order to preserve
+	 * consistency, we unlink all control messages from the
+	 * first mbuf chain in one shot and link them separately
+	 * onto a different chain.
+	 */
+	do {
+		if (flags & MSG_PEEK) {
+			if (controlp != NULL) {
+				if (*controlp == NULL) {
+					msgpcm = controlp;
+				}
+				*controlp = m_copy(m, 0, m->m_len);
+
+				/*
+				 * If we failed to allocate an mbuf,
+				 * release any previously allocated
+				 * mbufs for control data. Return
+				 * an error. Keep the mbufs in the
+				 * socket as this is using
+				 * MSG_PEEK flag.
+				 */
+				if (*controlp == NULL) {
+					m_freem(*msgpcm);
+					error = ENOBUFS;
+					goto done;
+				}
+				controlp = &(*controlp)->m_next;
+			}
+			m = m->m_next;
+		} else {
+			m->m_nextpkt = NULL;
+			sbfree(sb_rcv, m);
+			sb_rcv->sb_mb = m->m_next;
+			m->m_next = NULL;
+			*cme = m;
+			cme = &(*cme)->m_next;
+			m = sb_rcv->sb_mb;
+		}
+	} while (m != NULL && m->m_type == MT_CONTROL);
+
+	if (!(flags & MSG_PEEK)) {
+		if (sb_rcv->sb_mb != NULL) {
+			sb_rcv->sb_mb->m_nextpkt = nextrecord;
+		} else {
+			sb_rcv->sb_mb = nextrecord;
+			SB_EMPTY_FIXUP(sb_rcv);
+		}
+		if (nextrecord == NULL)
+			sb_rcv->sb_lastrecord = m;
+	}
+
+	SBLASTRECORDCHK(&so->so_rcv, "soreceive ctl");
+	SBLASTMBUFCHK(&so->so_rcv, "soreceive ctl");
+
+	while (cm != NULL) {
+		int cmsg_type;
+
+		cmn = cm->m_next;
+		cm->m_next = NULL;
+		cmsg_type = mtod(cm, struct cmsghdr *)->cmsg_type;
+
+		/*
+		 * Call the protocol to externalize SCM_RIGHTS message
+		 * and return the modified message to the caller upon
+		 * success.  Otherwise, all other control messages are
+		 * returned unmodified to the caller.  Note that we
+		 * only get into this loop if MSG_PEEK is not set.
+		 */
+		if (pr->pr_domain->dom_externalize != NULL &&
+		    cmsg_type == SCM_RIGHTS) {
+			/*
+			 * Release socket lock: see 3903171.  This
+			 * would also allow more records to be appended
+			 * to the socket buffer.  We still have SB_LOCK
+			 * set on it, so we can be sure that the head
+			 * of the mbuf chain won't change.
+			 */
+			socket_unlock(so, 0);
+			error = (*pr->pr_domain->dom_externalize)(cm);
+			socket_lock(so, 0);
+		} else {
+			error = 0;
+		}
+
+		if (controlp != NULL && error == 0) {
+			*controlp = cm;
+			controlp = &(*controlp)->m_next;
+		} else {
+			(void) m_free(cm);
+		}
+		cm = cmn;
+	}
+	/*
+	 * Update the value of nextrecord in case we received new
+	 * records when the socket was unlocked above for
+	 * externalizing SCM_RIGHTS.
+	 */
+	if (m != NULL)
+		nextrecord = sb_rcv->sb_mb->m_nextpkt;
+	else
+		nextrecord = sb_rcv->sb_mb;
+
+done:
+	*mp = m;
+	*nextrecordp = nextrecord;
+
+	return (error);
+}
+
+/*
+ * Implement receive operations on a socket.
+ * We depend on the way that records are added to the sockbuf
+ * by sbappend*.  In particular, each record (mbufs linked through m_next)
+ * must begin with an address if the protocol so specifies,
+ * followed by an optional mbuf or mbufs containing ancillary data,
+ * and then zero or more mbufs of data.
+ * In order to avoid blocking network interrupts for the entire time here,
+ * we splx() while doing the actual copy to user space.
+ * Although the sockbuf is locked, new data may still be appended,
+ * and thus we must maintain consistency of the sockbuf during that time.
+ *
+ * The caller may receive the data as a single mbuf chain by supplying
+ * an mbuf **mp0 for use in returning the chain.  The uio is then used
+ * only for the count in uio_resid.
+ *
+ * Returns:	0			Success
+ *		ENOBUFS
+ *		ENOTCONN
+ *		EWOULDBLOCK
+ *	uiomove:EFAULT
+ *	sblock:EWOULDBLOCK
+ *	sblock:EINTR
+ *	sbwait:EBADF
+ *	sbwait:EINTR
+ *	sodelayed_copy:EFAULT
+ *	<pru_rcvoob>:EINVAL[TCP]
+ *	<pru_rcvoob>:EWOULDBLOCK[TCP]
+ *	<pru_rcvoob>:???
+ *	<pr_domain->dom_externalize>:EMSGSIZE[AF_UNIX]
+ *	<pr_domain->dom_externalize>:ENOBUFS[AF_UNIX]
+ *	<pr_domain->dom_externalize>:???
+ *
+ * Notes:	Additional return values from calls through <pru_rcvoob> and
+ *		<pr_domain->dom_externalize> depend on protocols other than
+ *		TCP or AF_UNIX, which are documented above.
+ */
+int
+soreceive(struct socket *so, struct sockaddr **psa, struct uio *uio,
+    struct mbuf **mp0, struct mbuf **controlp, int *flagsp)
+{
+	struct mbuf *m, **mp, *ml = NULL;
+	struct mbuf *nextrecord, *free_list;
+	int flags, error, offset;
+	user_ssize_t len;
+	struct protosw *pr = so->so_proto;
+	int moff, type = 0;
+	user_ssize_t orig_resid = uio_resid(uio);
+	user_ssize_t delayed_copy_len;
+	int can_delay;
+	int need_event;
+	struct proc *p = current_proc();
+	boolean_t en_tracing = FALSE;
+
+	/*
+	 * Sanity check on the length passed by caller as we are making 'int'
+	 * comparisons
+	 */
+	if (orig_resid < 0 || orig_resid > INT_MAX)
+		return (EINVAL);
+
+	KERNEL_DEBUG(DBG_FNC_SORECEIVE | DBG_FUNC_START, so,
+	    uio_resid(uio), so->so_rcv.sb_cc, so->so_rcv.sb_lowat,
+	    so->so_rcv.sb_hiwat);
+
+	socket_lock(so, 1);
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+#ifdef MORE_LOCKING_DEBUG
+	if (so->so_usecount == 1) {
+		panic("%s: so=%x no other reference on socket\n", __func__, so);
+		/* NOTREACHED */
+	}
+#endif
+	mp = mp0;
+	if (psa != NULL)
+		*psa = NULL;
+	if (controlp != NULL)
+		*controlp = NULL;
+	if (flagsp != NULL)
+		flags = *flagsp &~ MSG_EOR;
+	else
+		flags = 0;
+
+	/*
+	 * If a recv attempt is made on a previously-accepted socket
+	 * that has been marked as inactive (disconnected), reject
+	 * the request.
+	 */
+	if (so->so_flags & SOF_DEFUNCT) {
+		struct sockbuf *sb = &so->so_rcv;
+
+		error = ENOTCONN;
+		SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] (%d)\n",
+		    __func__, proc_pid(p), (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so), error));
+		/*
+		 * This socket should have been disconnected and flushed
+		 * prior to being returned from sodefunct(); there should
+		 * be no data on its receive list, so panic otherwise.
+		 */
+		if (so->so_state & SS_DEFUNCT)
+			sb_empty_assert(sb, __func__);
+		socket_unlock(so, 1);
+		return (error);
+	}
+
+	if ((so->so_flags1 & SOF1_PRECONNECT_DATA) &&
+	    pr->pr_usrreqs->pru_preconnect) {
+		/*
+		 * A user may set the CONNECT_RESUME_ON_READ_WRITE-flag but not
+		 * calling write() right after this. *If* the app calls a read
+		 * we do not want to block this read indefinetely. Thus,
+		 * we trigger a connect so that the session gets initiated.
+		 */
+		error = (*pr->pr_usrreqs->pru_preconnect)(so);
+
+		if (error) {
+			socket_unlock(so, 1);
+			return (error);
+		}
+	}
+
+	if (ENTR_SHOULDTRACE &&
+	    (SOCK_CHECK_DOM(so, AF_INET) || SOCK_CHECK_DOM(so, AF_INET6))) {
+		/*
+		 * enable energy tracing for inet sockets that go over
+		 * non-loopback interfaces only.
+		 */
+		struct inpcb *inp = sotoinpcb(so);
+		if (inp->inp_last_outifp != NULL &&
+		    !(inp->inp_last_outifp->if_flags & IFF_LOOPBACK)) {
+			en_tracing = TRUE;
+			KERNEL_ENERGYTRACE(kEnTrActKernSockRead, DBG_FUNC_START,
+			    VM_KERNEL_ADDRPERM(so),
+			    ((so->so_state & SS_NBIO) ?
+			    kEnTrFlagNonBlocking : 0),
+			    (int64_t)orig_resid);
+		}
+	}
+
+	/*
+	 * When SO_WANTOOBFLAG is set we try to get out-of-band data
+	 * regardless of the flags argument. Here is the case were
+	 * out-of-band data is not inline.
+	 */
+	if ((flags & MSG_OOB) ||
+	    ((so->so_options & SO_WANTOOBFLAG) != 0 &&
+	    (so->so_options & SO_OOBINLINE) == 0 &&
+	    (so->so_oobmark || (so->so_state & SS_RCVATMARK)))) {
+		m = m_get(M_WAIT, MT_DATA);
+		if (m == NULL) {
+			socket_unlock(so, 1);
+			KERNEL_DEBUG(DBG_FNC_SORECEIVE | DBG_FUNC_END,
+			    ENOBUFS, 0, 0, 0, 0);
+			return (ENOBUFS);
+		}
+		error = (*pr->pr_usrreqs->pru_rcvoob)(so, m, flags & MSG_PEEK);
+		if (error)
+			goto bad;
+		socket_unlock(so, 0);
+		do {
+			error = uiomove(mtod(m, caddr_t),
+			    imin(uio_resid(uio), m->m_len), uio);
+			m = m_free(m);
+		} while (uio_resid(uio) && error == 0 && m != NULL);
+		socket_lock(so, 0);
+bad:
+		if (m != NULL)
+			m_freem(m);
+
+		if ((so->so_options & SO_WANTOOBFLAG) != 0) {
+			if (error == EWOULDBLOCK || error == EINVAL) {
+				/*
+				 * Let's try to get normal data:
+				 * EWOULDBLOCK: out-of-band data not
+				 * receive yet. EINVAL: out-of-band data
+				 * already read.
+				 */
+				error = 0;
+				goto nooob;
+			} else if (error == 0 && flagsp != NULL) {
+				*flagsp |= MSG_OOB;
+			}
+		}
+		socket_unlock(so, 1);
+		if (en_tracing) {
+			KERNEL_ENERGYTRACE(kEnTrActKernSockRead, DBG_FUNC_END,
+			    VM_KERNEL_ADDRPERM(so), 0,
+			    (int64_t)(orig_resid - uio_resid(uio)));
+		}
+		KERNEL_DEBUG(DBG_FNC_SORECEIVE | DBG_FUNC_END, error,
+		    0, 0, 0, 0);
+
+		return (error);
+	}
+nooob:
+	if (mp != NULL)
+		*mp = NULL;
+
+	if (so->so_state & SS_ISCONFIRMING && uio_resid(uio)) {
+		(*pr->pr_usrreqs->pru_rcvd)(so, 0);
+	}
+
+	free_list = NULL;
+	delayed_copy_len = 0;
+restart:
+#ifdef MORE_LOCKING_DEBUG
+	if (so->so_usecount <= 1)
+		printf("soreceive: sblock so=0x%llx ref=%d on socket\n",
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so), so->so_usecount);
+#endif
+	/*
+	 * See if the socket has been closed (SS_NOFDREF|SS_CANTRCVMORE)
+	 * and if so just return to the caller.  This could happen when
+	 * soreceive() is called by a socket upcall function during the
+	 * time the socket is freed.  The socket buffer would have been
+	 * locked across the upcall, therefore we cannot put this thread
+	 * to sleep (else we will deadlock) or return EWOULDBLOCK (else
+	 * we may livelock), because the lock on the socket buffer will
+	 * only be released when the upcall routine returns to its caller.
+	 * Because the socket has been officially closed, there can be
+	 * no further read on it.
+	 *
+	 * A multipath subflow socket would have its SS_NOFDREF set by
+	 * default, so check for SOF_MP_SUBFLOW socket flag; when the
+	 * socket is closed for real, SOF_MP_SUBFLOW would be cleared.
+	 */
+	if ((so->so_state & (SS_NOFDREF | SS_CANTRCVMORE)) ==
+	    (SS_NOFDREF | SS_CANTRCVMORE) && !(so->so_flags & SOF_MP_SUBFLOW)) {
+		socket_unlock(so, 1);
+		return (0);
+	}
+
+	error = sblock(&so->so_rcv, SBLOCKWAIT(flags));
+	if (error) {
+		socket_unlock(so, 1);
+		KERNEL_DEBUG(DBG_FNC_SORECEIVE | DBG_FUNC_END, error,
+		    0, 0, 0, 0);
+		if (en_tracing) {
+			KERNEL_ENERGYTRACE(kEnTrActKernSockRead, DBG_FUNC_END,
+			    VM_KERNEL_ADDRPERM(so), 0,
+			    (int64_t)(orig_resid - uio_resid(uio)));
+		}
+		return (error);
+	}
+
+	m = so->so_rcv.sb_mb;
+	/*
+	 * If we have less data than requested, block awaiting more
+	 * (subject to any timeout) if:
+	 *   1. the current count is less than the low water mark, or
+	 *   2. MSG_WAITALL is set, and it is possible to do the entire
+	 *	receive operation at once if we block (resid <= hiwat).
+	 *   3. MSG_DONTWAIT is not set
+	 * If MSG_WAITALL is set but resid is larger than the receive buffer,
+	 * we have to do the receive in sections, and thus risk returning
+	 * a short count if a timeout or signal occurs after we start.
+	 */
+	if (m == NULL || (((flags & MSG_DONTWAIT) == 0 &&
+	    so->so_rcv.sb_cc < uio_resid(uio)) &&
+	    (so->so_rcv.sb_cc < so->so_rcv.sb_lowat ||
+	    ((flags & MSG_WAITALL) && uio_resid(uio) <= so->so_rcv.sb_hiwat)) &&
+	    m->m_nextpkt == NULL && (pr->pr_flags & PR_ATOMIC) == 0)) {
+		/*
+		 * Panic if we notice inconsistencies in the socket's
+		 * receive list; both sb_mb and sb_cc should correctly
+		 * reflect the contents of the list, otherwise we may
+		 * end up with false positives during select() or poll()
+		 * which could put the application in a bad state.
+		 */
+		SB_MB_CHECK(&so->so_rcv);
+
+		if (so->so_error) {
+			if (m != NULL)
+				goto dontblock;
+			error = so->so_error;
+			if ((flags & MSG_PEEK) == 0)
+				so->so_error = 0;
+			goto release;
+		}
+		if (so->so_state & SS_CANTRCVMORE) {
+#if CONTENT_FILTER
+			/*
+			 * Deal with half closed connections
+			 */
+			if ((so->so_state & SS_ISDISCONNECTED) == 0 &&
+				cfil_sock_data_pending(&so->so_rcv) != 0)
+				CFIL_LOG(LOG_INFO,
+					"so %llx ignore SS_CANTRCVMORE",
+					(uint64_t)DEBUG_KERNEL_ADDRPERM(so));
+			else
+#endif /* CONTENT_FILTER */
+			if (m != NULL)
+				goto dontblock;
+			else
+				goto release;
+		}
+		for (; m != NULL; m = m->m_next)
+			if (m->m_type == MT_OOBDATA || (m->m_flags & M_EOR)) {
+				m = so->so_rcv.sb_mb;
+				goto dontblock;
+			}
+		if ((so->so_state & (SS_ISCONNECTED|SS_ISCONNECTING)) == 0 &&
+		    (so->so_proto->pr_flags & PR_CONNREQUIRED)) {
+			error = ENOTCONN;
+			goto release;
+		}
+		if (uio_resid(uio) == 0)
+			goto release;
+
+		if ((so->so_state & SS_NBIO) ||
+		    (flags & (MSG_DONTWAIT|MSG_NBIO))) {
+			error = EWOULDBLOCK;
+			goto release;
+		}
+		SBLASTRECORDCHK(&so->so_rcv, "soreceive sbwait 1");
+		SBLASTMBUFCHK(&so->so_rcv, "soreceive sbwait 1");
+		sbunlock(&so->so_rcv, TRUE);	/* keep socket locked */
+#if EVEN_MORE_LOCKING_DEBUG
+		if (socket_debug)
+			printf("Waiting for socket data\n");
+#endif
+
+		error = sbwait(&so->so_rcv);
+#if EVEN_MORE_LOCKING_DEBUG
+		if (socket_debug)
+			printf("SORECEIVE - sbwait returned %d\n", error);
+#endif
+		if (so->so_usecount < 1) {
+			panic("%s: after 2nd sblock so=%p ref=%d on socket\n",
+			    __func__, so, so->so_usecount);
+			/* NOTREACHED */
+		}
+		if (error) {
+			socket_unlock(so, 1);
+			KERNEL_DEBUG(DBG_FNC_SORECEIVE | DBG_FUNC_END, error,
+			    0, 0, 0, 0);
+			if (en_tracing) {
+				KERNEL_ENERGYTRACE(kEnTrActKernSockRead, DBG_FUNC_END,
+				    VM_KERNEL_ADDRPERM(so), 0,
+				    (int64_t)(orig_resid - uio_resid(uio)));
+			}
+			return (error);
+		}
+		goto restart;
+	}
+dontblock:
+	OSIncrementAtomicLong(&p->p_stats->p_ru.ru_msgrcv);
+	SBLASTRECORDCHK(&so->so_rcv, "soreceive 1");
+	SBLASTMBUFCHK(&so->so_rcv, "soreceive 1");
+	nextrecord = m->m_nextpkt;
+
+	if ((pr->pr_flags & PR_ADDR) && m->m_type == MT_SONAME) {
+		error = soreceive_addr(p, so, psa, flags, &m, &nextrecord,
+		    mp0 == NULL);
+		if (error == ERESTART)
+			goto restart;
+		else if (error != 0)
+			goto release;
+		orig_resid = 0;
+	}
+
+	/*
+	 * Process one or more MT_CONTROL mbufs present before any data mbufs
+	 * in the first mbuf chain on the socket buffer.  If MSG_PEEK, we
+	 * just copy the data; if !MSG_PEEK, we call into the protocol to
+	 * perform externalization.
+	 */
+	if (m != NULL && m->m_type == MT_CONTROL) {
+		error = soreceive_ctl(so, controlp, flags, &m, &nextrecord);
+		if (error != 0)
+			goto release;
+		orig_resid = 0;
+	}
+
+	/*
+	 * If the socket is a TCP socket with message delivery
+	 * enabled, then create a control msg to deliver the
+	 * relative TCP sequence number for this data. Waiting
+	 * until this point will protect against failures to
+	 * allocate an mbuf for control msgs.
+	 */
+	if (so->so_type == SOCK_STREAM && SOCK_PROTO(so) == IPPROTO_TCP &&
+	    (so->so_flags & SOF_ENABLE_MSGS) && controlp != NULL) {
+		struct mbuf *seq_cm;
+
+		seq_cm = sbcreatecontrol((caddr_t)&m->m_pkthdr.msg_seq,
+		    sizeof (uint32_t), SCM_SEQNUM, SOL_SOCKET);
+		if (seq_cm == NULL) {
+			/* unable to allocate a control mbuf */
+			error = ENOBUFS;
+			goto release;
+		}
+		*controlp = seq_cm;
+		controlp = &seq_cm->m_next;
+	}
+
+	if (m != NULL) {
+		if (!(flags & MSG_PEEK)) {
+			/*
+			 * We get here because m points to an mbuf following
+			 * any MT_SONAME or MT_CONTROL mbufs which have been
+			 * processed above.  In any case, m should be pointing
+			 * to the head of the mbuf chain, and the nextrecord
+			 * should be either NULL or equal to m->m_nextpkt.
+			 * See comments above about SB_LOCK.
+			 */
+			if (m != so->so_rcv.sb_mb ||
+			    m->m_nextpkt != nextrecord) {
+				panic("%s: post-control !sync so=%p m=%p "
+				    "nextrecord=%p\n", __func__, so, m,
+				    nextrecord);
+				/* NOTREACHED */
+			}
+			if (nextrecord == NULL)
+				so->so_rcv.sb_lastrecord = m;
+		}
+		type = m->m_type;
+		if (type == MT_OOBDATA)
+			flags |= MSG_OOB;
+	} else {
+		if (!(flags & MSG_PEEK)) {
+			SB_EMPTY_FIXUP(&so->so_rcv);
+		}
+	}
+	SBLASTRECORDCHK(&so->so_rcv, "soreceive 2");
+	SBLASTMBUFCHK(&so->so_rcv, "soreceive 2");
+
+	moff = 0;
+	offset = 0;
+
+	if (!(flags & MSG_PEEK) && uio_resid(uio) > sorecvmincopy)
+		can_delay = 1;
+	else
+		can_delay = 0;
+
+	need_event = 0;
+
+	while (m != NULL &&
+	    (uio_resid(uio) - delayed_copy_len) > 0 && error == 0) {
+		if (m->m_type == MT_OOBDATA) {
+			if (type != MT_OOBDATA)
+				break;
+		} else if (type == MT_OOBDATA) {
+			break;
+		}
+		/*
+		 * Make sure to allways set MSG_OOB event when getting
+		 * out of band data inline.
+		 */
+		if ((so->so_options & SO_WANTOOBFLAG) != 0 &&
+		    (so->so_options & SO_OOBINLINE) != 0 &&
+		    (so->so_state & SS_RCVATMARK) != 0) {
+			flags |= MSG_OOB;
+		}
+		so->so_state &= ~SS_RCVATMARK;
+		len = uio_resid(uio) - delayed_copy_len;
+		if (so->so_oobmark && len > so->so_oobmark - offset)
+			len = so->so_oobmark - offset;
+		if (len > m->m_len - moff)
+			len = m->m_len - moff;
+		/*
+		 * If mp is set, just pass back the mbufs.
+		 * Otherwise copy them out via the uio, then free.
+		 * Sockbuf must be consistent here (points to current mbuf,
+		 * it points to next record) when we drop priority;
+		 * we must note any additions to the sockbuf when we
+		 * block interrupts again.
+		 */
+		if (mp == NULL) {
+			SBLASTRECORDCHK(&so->so_rcv, "soreceive uiomove");
+			SBLASTMBUFCHK(&so->so_rcv, "soreceive uiomove");
+			if (can_delay && len == m->m_len) {
+				/*
+				 * only delay the copy if we're consuming the
+				 * mbuf and we're NOT in MSG_PEEK mode
+				 * and we have enough data to make it worthwile
+				 * to drop and retake the lock... can_delay
+				 * reflects the state of the 2 latter
+				 * constraints moff should always be zero
+				 * in these cases
+				 */
+				delayed_copy_len += len;
+			} else {
+				if (delayed_copy_len) {
+					error = sodelayed_copy(so, uio,
+					    &free_list, &delayed_copy_len);
+
+					if (error) {
+						goto release;
+					}
+					/*
+					 * can only get here if MSG_PEEK is not
+					 * set therefore, m should point at the
+					 * head of the rcv queue; if it doesn't,
+					 * it means something drastically
+					 * changed while we were out from behind
+					 * the lock in sodelayed_copy. perhaps
+					 * a RST on the stream. in any event,
+					 * the stream has been interrupted. it's
+					 * probably best just to return whatever
+					 * data we've moved and let the caller
+					 * sort it out...
+					 */
+					if (m != so->so_rcv.sb_mb) {
+						break;
+					}
+				}
+				socket_unlock(so, 0);
+				error = uiomove(mtod(m, caddr_t) + moff,
+				    (int)len, uio);
+				socket_lock(so, 0);
+
+				if (error)
+					goto release;
+			}
+		} else {
+			uio_setresid(uio, (uio_resid(uio) - len));
+		}
+		if (len == m->m_len - moff) {
+			if (m->m_flags & M_EOR)
+				flags |= MSG_EOR;
+			if (flags & MSG_PEEK) {
+				m = m->m_next;
+				moff = 0;
+			} else {
+				nextrecord = m->m_nextpkt;
+				sbfree(&so->so_rcv, m);
+				m->m_nextpkt = NULL;
+
+				/*
+				 * If this packet is an unordered packet
+				 * (indicated by M_UNORDERED_DATA flag), remove
+				 * the additional bytes added to the
+				 * receive socket buffer size.
+				 */
+				if ((so->so_flags & SOF_ENABLE_MSGS) &&
+				    m->m_len &&
+				    (m->m_flags & M_UNORDERED_DATA) &&
+				    sbreserve(&so->so_rcv,
+				    so->so_rcv.sb_hiwat - m->m_len)) {
+					if (so->so_msg_state->msg_uno_bytes >
+					    m->m_len) {
+						so->so_msg_state->
+						    msg_uno_bytes -= m->m_len;
+					} else {
+						so->so_msg_state->
+						    msg_uno_bytes = 0;
+					}
+					m->m_flags &= ~M_UNORDERED_DATA;
+				}
+
+				if (mp != NULL) {
+					*mp = m;
+					mp = &m->m_next;
+					so->so_rcv.sb_mb = m = m->m_next;
+					*mp = NULL;
+				} else {
+					if (free_list == NULL)
+						free_list = m;
+					else
+						ml->m_next = m;
+					ml = m;
+					so->so_rcv.sb_mb = m = m->m_next;
+					ml->m_next = NULL;
+				}
+				if (m != NULL) {
+					m->m_nextpkt = nextrecord;
+					if (nextrecord == NULL)
+						so->so_rcv.sb_lastrecord = m;
+				} else {
+					so->so_rcv.sb_mb = nextrecord;
+					SB_EMPTY_FIXUP(&so->so_rcv);
+				}
+				SBLASTRECORDCHK(&so->so_rcv, "soreceive 3");
+				SBLASTMBUFCHK(&so->so_rcv, "soreceive 3");
+			}
+		} else {
+			if (flags & MSG_PEEK) {
+				moff += len;
+			} else {
+				if (mp != NULL) {
+					int copy_flag;
+
+					if (flags & MSG_DONTWAIT)
+						copy_flag = M_DONTWAIT;
+					else
+						copy_flag = M_WAIT;
+					*mp = m_copym(m, 0, len, copy_flag);
+					/*
+					 * Failed to allocate an mbuf?
+					 * Adjust uio_resid back, it was
+					 * adjusted down by len bytes which
+					 * we didn't copy over.
+					 */
+					if (*mp == NULL) {
+						uio_setresid(uio,
+						    (uio_resid(uio) + len));
+						break;
+					}
+				}
+				m->m_data += len;
+				m->m_len -= len;
+				so->so_rcv.sb_cc -= len;
+			}
+		}
+		if (so->so_oobmark) {
+			if ((flags & MSG_PEEK) == 0) {
+				so->so_oobmark -= len;
+				if (so->so_oobmark == 0) {
+					so->so_state |= SS_RCVATMARK;
+					/*
+					 * delay posting the actual event until
+					 * after any delayed copy processing
+					 * has finished
+					 */
+					need_event = 1;
+					break;
+				}
+			} else {
+				offset += len;
+				if (offset == so->so_oobmark)
+					break;
+			}
+		}
+		if (flags & MSG_EOR)
+			break;
+		/*
+		 * If the MSG_WAITALL or MSG_WAITSTREAM flag is set
+		 * (for non-atomic socket), we must not quit until
+		 * "uio->uio_resid == 0" or an error termination.
+		 * If a signal/timeout occurs, return with a short
+		 * count but without error.  Keep sockbuf locked
+		 * against other readers.
+		 */
+		while (flags & (MSG_WAITALL|MSG_WAITSTREAM) && m == NULL &&
+		    (uio_resid(uio) - delayed_copy_len) > 0 &&
+		    !sosendallatonce(so) && !nextrecord) {
+			if (so->so_error || ((so->so_state & SS_CANTRCVMORE)
+#if CONTENT_FILTER
+			    && cfil_sock_data_pending(&so->so_rcv) == 0
+#endif /* CONTENT_FILTER */
+			    ))
+				goto release;
+
+			/*
+			 * Depending on the protocol (e.g. TCP), the following
+			 * might cause the socket lock to be dropped and later
+			 * be reacquired, and more data could have arrived and
+			 * have been appended to the receive socket buffer by
+			 * the time it returns.  Therefore, we only sleep in
+			 * sbwait() below if and only if the socket buffer is
+			 * empty, in order to avoid a false sleep.
+			 */
+			if (pr->pr_flags & PR_WANTRCVD && so->so_pcb &&
+			    (((struct inpcb *)so->so_pcb)->inp_state !=
+			    INPCB_STATE_DEAD))
+				(*pr->pr_usrreqs->pru_rcvd)(so, flags);
+
+			SBLASTRECORDCHK(&so->so_rcv, "soreceive sbwait 2");
+			SBLASTMBUFCHK(&so->so_rcv, "soreceive sbwait 2");
+
+			if (so->so_rcv.sb_mb == NULL && sbwait(&so->so_rcv)) {
+				error = 0;
+				goto release;
+			}
+			/*
+			 * have to wait until after we get back from the sbwait
+			 * to do the copy because we will drop the lock if we
+			 * have enough data that has been delayed... by dropping
+			 * the lock we open up a window allowing the netisr
+			 * thread to process the incoming packets and to change
+			 * the state of this socket... we're issuing the sbwait
+			 * because the socket is empty and we're expecting the
+			 * netisr thread to wake us up when more packets arrive;
+			 * if we allow that processing to happen and then sbwait
+			 * we could stall forever with packets sitting in the
+			 * socket if no further packets arrive from the remote
+			 * side.
+			 *
+			 * we want to copy before we've collected all the data
+			 * to satisfy this request to allow the copy to overlap
+			 * the incoming packet processing on an MP system
+			 */
+			if (delayed_copy_len > sorecvmincopy &&
+			    (delayed_copy_len > (so->so_rcv.sb_hiwat / 2))) {
+				error = sodelayed_copy(so, uio,
+				    &free_list, &delayed_copy_len);
+
+				if (error)
+					goto release;
+			}
+			m = so->so_rcv.sb_mb;
+			if (m != NULL) {
+				nextrecord = m->m_nextpkt;
+			}
+			SB_MB_CHECK(&so->so_rcv);
+		}
+	}
+#ifdef MORE_LOCKING_DEBUG
+	if (so->so_usecount <= 1) {
+		panic("%s: after big while so=%p ref=%d on socket\n",
+		    __func__, so, so->so_usecount);
+		/* NOTREACHED */
+	}
+#endif
+
+	if (m != NULL && pr->pr_flags & PR_ATOMIC) {
+		if (so->so_options & SO_DONTTRUNC) {
+			flags |= MSG_RCVMORE;
+		} else {
+			flags |= MSG_TRUNC;
+			if ((flags & MSG_PEEK) == 0)
+				(void) sbdroprecord(&so->so_rcv);
+		}
+	}
+
+	/*
+	 * pru_rcvd below (for TCP) may cause more data to be received
+	 * if the socket lock is dropped prior to sending the ACK; some
+	 * legacy OpenTransport applications don't handle this well
+	 * (if it receives less data than requested while MSG_HAVEMORE
+	 * is set), and so we set the flag now based on what we know
+	 * prior to calling pru_rcvd.
+	 */
+	if ((so->so_options & SO_WANTMORE) && so->so_rcv.sb_cc > 0)
+		flags |= MSG_HAVEMORE;
+
+	if ((flags & MSG_PEEK) == 0) {
+		if (m == NULL) {
+			so->so_rcv.sb_mb = nextrecord;
+			/*
+			 * First part is an inline SB_EMPTY_FIXUP().  Second
+			 * part makes sure sb_lastrecord is up-to-date if
+			 * there is still data in the socket buffer.
+			 */
+			if (so->so_rcv.sb_mb == NULL) {
+				so->so_rcv.sb_mbtail = NULL;
+				so->so_rcv.sb_lastrecord = NULL;
+			} else if (nextrecord->m_nextpkt == NULL) {
+				so->so_rcv.sb_lastrecord = nextrecord;
+			}
+			SB_MB_CHECK(&so->so_rcv);
+		}
+		SBLASTRECORDCHK(&so->so_rcv, "soreceive 4");
+		SBLASTMBUFCHK(&so->so_rcv, "soreceive 4");
+		if (pr->pr_flags & PR_WANTRCVD && so->so_pcb)
+			(*pr->pr_usrreqs->pru_rcvd)(so, flags);
+	}
+
+	if (delayed_copy_len) {
+		error = sodelayed_copy(so, uio, &free_list, &delayed_copy_len);
+		if (error)
+			goto release;
+	}
+	if (free_list != NULL) {
+		m_freem_list(free_list);
+		free_list = NULL;
+	}
+	if (need_event)
+		postevent(so, 0, EV_OOB);
+
+	if (orig_resid == uio_resid(uio) && orig_resid &&
+	    (flags & MSG_EOR) == 0 && (so->so_state & SS_CANTRCVMORE) == 0) {
+		sbunlock(&so->so_rcv, TRUE);	/* keep socket locked */
+		goto restart;
+	}
+
+	if (flagsp != NULL)
+		*flagsp |= flags;
+release:
+#ifdef MORE_LOCKING_DEBUG
+	if (so->so_usecount <= 1) {
+		panic("%s: release so=%p ref=%d on socket\n", __func__,
+		    so, so->so_usecount);
+		/* NOTREACHED */
+	}
+#endif
+	if (delayed_copy_len)
+		error = sodelayed_copy(so, uio, &free_list, &delayed_copy_len);
+
+	if (free_list != NULL)
+		m_freem_list(free_list);
+
+	sbunlock(&so->so_rcv, FALSE);	/* will unlock socket */
+
+	if (en_tracing) {
+		KERNEL_ENERGYTRACE(kEnTrActKernSockRead, DBG_FUNC_END,
+		    VM_KERNEL_ADDRPERM(so),
+		    ((error == EWOULDBLOCK) ? kEnTrFlagNoWork : 0),
+		    (int64_t)(orig_resid - uio_resid(uio)));
+	}
+	KERNEL_DEBUG(DBG_FNC_SORECEIVE | DBG_FUNC_END, so, uio_resid(uio),
+	    so->so_rcv.sb_cc, 0, error);
+
+	return (error);
+}
+
+/*
+ * Returns:	0			Success
+ *	uiomove:EFAULT
+ */
+static int
+sodelayed_copy(struct socket *so, struct uio *uio, struct mbuf **free_list,
+    user_ssize_t *resid)
+{
+	int error = 0;
+	struct mbuf *m;
+
+	m = *free_list;
+
+	socket_unlock(so, 0);
+
+	while (m != NULL && error == 0) {
+		error = uiomove(mtod(m, caddr_t), (int)m->m_len, uio);
+		m = m->m_next;
+	}
+	m_freem_list(*free_list);
+
+	*free_list = NULL;
+	*resid = 0;
+
+	socket_lock(so, 0);
+
+	return (error);
+}
+
+static int
+sodelayed_copy_list(struct socket *so, struct recv_msg_elem *msgarray,
+    u_int uiocnt, struct mbuf **free_list, user_ssize_t *resid)
+{
+#pragma unused(so)
+	int error = 0;
+	struct mbuf *ml, *m;
+	int i = 0;
+	struct uio *auio;
+
+	for (ml = *free_list, i = 0; ml != NULL && i < uiocnt;
+	    ml = ml->m_nextpkt, i++) {
+		auio = msgarray[i].uio;
+		for (m = ml; m != NULL; m = m->m_next) {
+			error = uiomove(mtod(m, caddr_t), m->m_len, auio);
+			if (error != 0)
+				goto out;
+		}
+	}
+out:
+	m_freem_list(*free_list);
+
+	*free_list = NULL;
+	*resid = 0;
+
+	return (error);
+}
+
+int
+soreceive_list(struct socket *so, struct recv_msg_elem *msgarray, u_int uiocnt,
+    int *flagsp)
+{
+	struct mbuf *m;
+	struct mbuf *nextrecord;
+	struct mbuf *ml = NULL, *free_list = NULL, *free_tail = NULL;
+	int error;
+	user_ssize_t len, pktlen, delayed_copy_len = 0;
+	struct protosw *pr = so->so_proto;
+	user_ssize_t resid;
+	struct proc *p = current_proc();
+	struct uio *auio = NULL;
+	int npkts = 0;
+	int sblocked = 0;
+	struct sockaddr **psa = NULL;
+	struct mbuf **controlp = NULL;
+	int can_delay;
+	int flags;
+	struct mbuf *free_others = NULL;
+
+	KERNEL_DEBUG(DBG_FNC_SORECEIVE_LIST | DBG_FUNC_START,
+	    so, uiocnt,
+	    so->so_rcv.sb_cc, so->so_rcv.sb_lowat, so->so_rcv.sb_hiwat);
+
+	/*
+	 * Sanity checks:
+	 * - Only supports don't wait flags
+	 * - Only support datagram sockets (could be extended to raw)
+	 * - Must be atomic
+	 * - Protocol must support packet chains
+	 * - The uio array is NULL (should we panic?)
+	 */
+	if (flagsp != NULL)
+		flags = *flagsp;
+	else
+		flags = 0;
+	if (flags & ~(MSG_PEEK | MSG_WAITALL | MSG_DONTWAIT | MSG_NEEDSA |
+	    MSG_NBIO)) {
+		printf("%s invalid flags 0x%x\n", __func__, flags);
+		error = EINVAL;
+		goto out;
+	}
+	if (so->so_type != SOCK_DGRAM) {
+		error = EINVAL;
+		goto out;
+	}
+	if (sosendallatonce(so) == 0) {
+		error = EINVAL;
+		goto out;
+	}
+	if (so->so_proto->pr_usrreqs->pru_send_list == NULL) {
+		error = EPROTONOSUPPORT;
+		goto out;
+	}
+	if (msgarray == NULL) {
+		printf("%s uioarray is NULL\n", __func__);
+		error = EINVAL;
+		goto out;
+	}
+	if (uiocnt == 0) {
+		printf("%s uiocnt is 0\n", __func__);
+		error = EINVAL;
+		goto out;
+	}
+	/*
+	 * Sanity check on the length passed by caller as we are making 'int'
+	 * comparisons
+	 */
+	resid = recv_msg_array_resid(msgarray, uiocnt);
+	if (resid < 0 || resid > INT_MAX) {
+		error = EINVAL;
+		goto out;
+	}
+
+	if (!(flags & MSG_PEEK) && sorecvmincopy > 0)
+		can_delay = 1;
+	else
+		can_delay = 0;
+
+	socket_lock(so, 1);
+	so_update_last_owner_locked(so, p);
+	so_update_policy(so);
+
+#if NECP
+	so_update_necp_policy(so, NULL, NULL);
+#endif /* NECP */
+
+	/*
+	 * If a recv attempt is made on a previously-accepted socket
+	 * that has been marked as inactive (disconnected), reject
+	 * the request.
+	 */
+	if (so->so_flags & SOF_DEFUNCT) {
+		struct sockbuf *sb = &so->so_rcv;
+
+		error = ENOTCONN;
+		SODEFUNCTLOG(("%s[%d]: defunct so 0x%llx [%d,%d] (%d)\n",
+		    __func__, proc_pid(p), (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so), error));
+		/*
+		 * This socket should have been disconnected and flushed
+		 * prior to being returned from sodefunct(); there should
+		 * be no data on its receive list, so panic otherwise.
+		 */
+		if (so->so_state & SS_DEFUNCT)
+			sb_empty_assert(sb, __func__);
+		goto release;
+	}
+
+next:
+	/*
+	 * The uio may be empty
+	 */
+	if (npkts >= uiocnt) {
+		error = 0;
+		goto release;
+	}
+restart:
+	/*
+	 * See if the socket has been closed (SS_NOFDREF|SS_CANTRCVMORE)
+	 * and if so just return to the caller.  This could happen when
+	 * soreceive() is called by a socket upcall function during the
+	 * time the socket is freed.  The socket buffer would have been
+	 * locked across the upcall, therefore we cannot put this thread
+	 * to sleep (else we will deadlock) or return EWOULDBLOCK (else
+	 * we may livelock), because the lock on the socket buffer will
+	 * only be released when the upcall routine returns to its caller.
+	 * Because the socket has been officially closed, there can be
+	 * no further read on it.
+	 */
+	if ((so->so_state & (SS_NOFDREF | SS_CANTRCVMORE)) ==
+	    (SS_NOFDREF | SS_CANTRCVMORE)) {
+		error = 0;
+		goto release;
+	}
+
+	error = sblock(&so->so_rcv, SBLOCKWAIT(flags));
+	if (error) {
+		goto release;
+	}
+	sblocked = 1;
+
+	m = so->so_rcv.sb_mb;
+	/*
+	 * Block awaiting more datagram if needed
+	 */
+	if (m == NULL || (((flags & MSG_DONTWAIT) == 0 &&
+	    (so->so_rcv.sb_cc < so->so_rcv.sb_lowat ||
+	    ((flags & MSG_WAITALL) && npkts < uiocnt))))) {
+		/*
+		 * Panic if we notice inconsistencies in the socket's
+		 * receive list; both sb_mb and sb_cc should correctly
+		 * reflect the contents of the list, otherwise we may
+		 * end up with false positives during select() or poll()
+		 * which could put the application in a bad state.
+		 */
+		SB_MB_CHECK(&so->so_rcv);
+
+		if (so->so_error) {
+			error = so->so_error;
+			if ((flags & MSG_PEEK) == 0)
+				so->so_error = 0;
+			goto release;
+		}
+		if (so->so_state & SS_CANTRCVMORE) {
+			goto release;
+		}
+		if ((so->so_state & (SS_ISCONNECTED|SS_ISCONNECTING)) == 0 &&
+		    (so->so_proto->pr_flags & PR_CONNREQUIRED)) {
+			error = ENOTCONN;
+			goto release;
+		}
+		if ((so->so_state & SS_NBIO) ||
+		    (flags & (MSG_DONTWAIT|MSG_NBIO))) {
+			error = EWOULDBLOCK;
+			goto release;
+		}
+		/*
+		 * Do not block if we got some data
+		 */
+		if (free_list != NULL) {
+			error = 0;
+			goto release;
+		}
+
+		SBLASTRECORDCHK(&so->so_rcv, "soreceive sbwait 1");
+		SBLASTMBUFCHK(&so->so_rcv, "soreceive sbwait 1");
+
+		sbunlock(&so->so_rcv, TRUE);	/* keep socket locked */
+		sblocked = 0;
+
+		error = sbwait(&so->so_rcv);
+		if (error) {
+			goto release;
+		}
+		goto restart;
+	}
+
+	OSIncrementAtomicLong(&p->p_stats->p_ru.ru_msgrcv);
+	SBLASTRECORDCHK(&so->so_rcv, "soreceive 1");
+	SBLASTMBUFCHK(&so->so_rcv, "soreceive 1");
+
+	/*
+	 * Consume the current uio index as we have a datagram
+	 */
+	auio = msgarray[npkts].uio;
+	resid = uio_resid(auio);
+	msgarray[npkts].which |= SOCK_MSG_DATA;
+	psa = (msgarray[npkts].which & SOCK_MSG_SA) ?
+	    &msgarray[npkts].psa : NULL;
+	controlp = (msgarray[npkts].which & SOCK_MSG_CONTROL) ?
+	    &msgarray[npkts].controlp : NULL;
+	npkts += 1;
+	nextrecord = m->m_nextpkt;
+
+	if ((pr->pr_flags & PR_ADDR) && m->m_type == MT_SONAME) {
+		error = soreceive_addr(p, so, psa, flags, &m, &nextrecord, 1);
+		if (error == ERESTART)
+			goto restart;
+		else if (error != 0)
+			goto release;
+	}
+
+	if (m != NULL && m->m_type == MT_CONTROL) {
+		error = soreceive_ctl(so, controlp, flags, &m, &nextrecord);
+		if (error != 0)
+			goto release;
+	}
+
+	if (m->m_pkthdr.len == 0) {
+		printf("%s:%d so %llx pkt %llx type %u pktlen null\n",
+		    __func__, __LINE__,
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(m),
+		    m->m_type);
+	}
+
+	/*
+	 * Loop to copy the mbufs of the current record
+	 * Support zero length packets
+	 */
+	ml = NULL;
+	pktlen = 0;
+	while (m != NULL && (len = resid - pktlen) >= 0 && error == 0) {
+		if (m->m_len == 0)
+			panic("%p m_len zero", m);
+		if (m->m_type == 0)
+			panic("%p m_type zero", m);
+		/*
+		 * Clip to the residual length
+		 */
+		if (len > m->m_len)
+			len = m->m_len;
+		pktlen += len;
+		/*
+		 * Copy the mbufs via the uio or delay the copy
+		 * Sockbuf must be consistent here (points to current mbuf,
+		 * it points to next record) when we drop priority;
+		 * we must note any additions to the sockbuf when we
+		 * block interrupts again.
+		 */
+		if (len > 0 && can_delay == 0) {
+			socket_unlock(so, 0);
+			error = uiomove(mtod(m, caddr_t), (int)len, auio);
+			socket_lock(so, 0);
+			if (error)
+				goto release;
+		} else {
+			delayed_copy_len += len;
+		}
+
+		if (len == m->m_len) {
+			/*
+			 * m was entirely copied
+			 */
+			sbfree(&so->so_rcv, m);
+			nextrecord = m->m_nextpkt;
+			m->m_nextpkt = NULL;
+
+			/*
+			 * Set the first packet to the head of the free list
+			 */
+			if (free_list == NULL)
+				free_list = m;
+			/*
+			 * Link current packet to tail of free list
+			 */
+			if (ml == NULL) {
+				if (free_tail != NULL)
+					free_tail->m_nextpkt = m;
+				free_tail = m;
+			}
+			/*
+			 * Link current mbuf to last mbuf of current packet
+			 */
+			if (ml != NULL)
+				ml->m_next = m;
+			ml = m;
+
+			/*
+			 * Move next buf to head of socket buffer
+			 */
+			so->so_rcv.sb_mb = m = ml->m_next;
+			ml->m_next = NULL;
+
+			if (m != NULL) {
+				m->m_nextpkt = nextrecord;
+				if (nextrecord == NULL)
+					so->so_rcv.sb_lastrecord = m;
+			} else {
+				so->so_rcv.sb_mb = nextrecord;
+				SB_EMPTY_FIXUP(&so->so_rcv);
+			}
+			SBLASTRECORDCHK(&so->so_rcv, "soreceive 3");
+			SBLASTMBUFCHK(&so->so_rcv, "soreceive 3");
+		} else {
+			/*
+			 * Stop the loop on partial copy
+			 */
+			break;
+		}
+	}
+#ifdef MORE_LOCKING_DEBUG
+	if (so->so_usecount <= 1) {
+		panic("%s: after big while so=%llx ref=%d on socket\n",
+		    __func__,
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so), so->so_usecount);
+		/* NOTREACHED */
+	}
+#endif
+	/*
+	 * Tell the caller we made a partial copy
+	 */
+	if (m != NULL) {
+		if (so->so_options & SO_DONTTRUNC) {
+			/*
+			 * Copyout first the freelist then the partial mbuf
+			 */
+			socket_unlock(so, 0);
+			if (delayed_copy_len)
+				error = sodelayed_copy_list(so, msgarray,
+				    uiocnt, &free_list, &delayed_copy_len);
+
+			if (error == 0) {
+				error = uiomove(mtod(m, caddr_t), (int)len,
+				    auio);
+			}
+			socket_lock(so, 0);
+			if (error)
+				goto release;
+
+			m->m_data += len;
+			m->m_len -= len;
+			so->so_rcv.sb_cc -= len;
+			flags |= MSG_RCVMORE;
+		} else {
+			(void) sbdroprecord(&so->so_rcv);
+			nextrecord = so->so_rcv.sb_mb;
+			m = NULL;
+			flags |= MSG_TRUNC;
+		}
+	}
+
+	if (m == NULL) {
+		so->so_rcv.sb_mb = nextrecord;
+		/*
+		 * First part is an inline SB_EMPTY_FIXUP().  Second
+		 * part makes sure sb_lastrecord is up-to-date if
+		 * there is still data in the socket buffer.
+		 */
+		if (so->so_rcv.sb_mb == NULL) {
+			so->so_rcv.sb_mbtail = NULL;
+			so->so_rcv.sb_lastrecord = NULL;
+		} else if (nextrecord->m_nextpkt == NULL) {
+			so->so_rcv.sb_lastrecord = nextrecord;
+		}
+		SB_MB_CHECK(&so->so_rcv);
+	}
+	SBLASTRECORDCHK(&so->so_rcv, "soreceive 4");
+	SBLASTMBUFCHK(&so->so_rcv, "soreceive 4");
+
+	/*
+	 * We can continue to the next packet as long as:
+	 * - We haven't exhausted the uio array
+	 * - There was no error
+	 * - A packet was not truncated
+	 * - We can still receive more data
+	 */
+	if (npkts < uiocnt && error == 0 &&
+	    (flags & (MSG_RCVMORE | MSG_TRUNC)) == 0 &&
+	    (so->so_state & SS_CANTRCVMORE) == 0) {
+		sbunlock(&so->so_rcv, TRUE);	/* keep socket locked */
+		sblocked = 0;
+
+		goto next;
+	}
+	if (flagsp != NULL)
+		*flagsp |= flags;
+
+release:
+	/*
+	 * pru_rcvd may cause more data to be received if the socket lock
+	 * is dropped so we set MSG_HAVEMORE now based on what we know.
+	 * That way the caller won't be surprised if it receives less data
+	 * than requested.
+	 */
+	if ((so->so_options & SO_WANTMORE) && so->so_rcv.sb_cc > 0)
+		flags |= MSG_HAVEMORE;
+
+	if (pr->pr_flags & PR_WANTRCVD && so->so_pcb)
+		(*pr->pr_usrreqs->pru_rcvd)(so, flags);
+
+	if (sblocked)
+		sbunlock(&so->so_rcv, FALSE);	/* will unlock socket */
+	else
+		socket_unlock(so, 1);
+
+	if (delayed_copy_len)
+		error = sodelayed_copy_list(so, msgarray, uiocnt,
+		    &free_list, &delayed_copy_len);
+out:
+	/*
+	 * Amortize the cost of freeing the mbufs
+	 */
+	if (free_list != NULL)
+		m_freem_list(free_list);
+	if (free_others != NULL)
+		m_freem_list(free_others);
+
+	KERNEL_DEBUG(DBG_FNC_SORECEIVE_LIST | DBG_FUNC_END, error,
+	    0, 0, 0, 0);
+	return (error);
+}
+
+/*
+ * Returns:	0			Success
+ *		EINVAL
+ *		ENOTCONN
+ *	<pru_shutdown>:EINVAL
+ *	<pru_shutdown>:EADDRNOTAVAIL[TCP]
+ *	<pru_shutdown>:ENOBUFS[TCP]
+ *	<pru_shutdown>:EMSGSIZE[TCP]
+ *	<pru_shutdown>:EHOSTUNREACH[TCP]
+ *	<pru_shutdown>:ENETUNREACH[TCP]
+ *	<pru_shutdown>:ENETDOWN[TCP]
+ *	<pru_shutdown>:ENOMEM[TCP]
+ *	<pru_shutdown>:EACCES[TCP]
+ *	<pru_shutdown>:EMSGSIZE[TCP]
+ *	<pru_shutdown>:ENOBUFS[TCP]
+ *	<pru_shutdown>:???[TCP]		[ignorable: mostly IPSEC/firewall/DLIL]
+ *	<pru_shutdown>:???		[other protocol families]
+ */
+int
+soshutdown(struct socket *so, int how)
+{
+	int error;
+
+	KERNEL_DEBUG(DBG_FNC_SOSHUTDOWN | DBG_FUNC_START, how, 0, 0, 0, 0);
+
+	switch (how) {
+	case SHUT_RD:
+	case SHUT_WR:
+	case SHUT_RDWR:
+		socket_lock(so, 1);
+		if ((so->so_state &
+		    (SS_ISCONNECTED|SS_ISCONNECTING|SS_ISDISCONNECTING)) == 0) {
+			error = ENOTCONN;
+		} else {
+			error = soshutdownlock(so, how);
+		}
+		socket_unlock(so, 1);
+		break;
+	default:
+		error = EINVAL;
+		break;
+	}
+
+	KERNEL_DEBUG(DBG_FNC_SOSHUTDOWN | DBG_FUNC_END, how, error, 0, 0, 0);
+
+	return (error);
+}
+
+int
+soshutdownlock_final(struct socket *so, int how)
+{
+	struct protosw *pr = so->so_proto;
+	int error = 0;
+
+	sflt_notify(so, sock_evt_shutdown, &how);
+
+	if (how != SHUT_WR) {
+		if ((so->so_state & SS_CANTRCVMORE) != 0) {
+			/* read already shut down */
+			error = ENOTCONN;
+			goto done;
+		}
+		sorflush(so);
+		postevent(so, 0, EV_RCLOSED);
+	}
+	if (how != SHUT_RD) {
+		if ((so->so_state & SS_CANTSENDMORE) != 0) {
+			/* write already shut down */
+			error = ENOTCONN;
+			goto done;
+		}
+		error = (*pr->pr_usrreqs->pru_shutdown)(so);
+		postevent(so, 0, EV_WCLOSED);
+	}
+done:
+	KERNEL_DEBUG(DBG_FNC_SOSHUTDOWN, how, 1, 0, 0, 0);
+	return (error);
+}
+
+int
+soshutdownlock(struct socket *so, int how)
+{
+	int error = 0;
+
+#if CONTENT_FILTER
+	/*
+	 * A content filter may delay the actual shutdown until it
+	 * has processed the pending data
+	 */
+	if (so->so_flags & SOF_CONTENT_FILTER) {
+		error = cfil_sock_shutdown(so, &how);
+		if (error == EJUSTRETURN) {
+			error = 0;
+			goto done;
+		} else if (error != 0) {
+			goto done;
+		}
+	}
+#endif /* CONTENT_FILTER */
+
+	error = soshutdownlock_final(so, how);
+
+done:
+	return (error);
+}
+
+void
+sowflush(struct socket *so)
+{
+	struct sockbuf *sb = &so->so_snd;
+#ifdef notyet
+	lck_mtx_t *mutex_held;
+	/*
+	 * XXX: This code is currently commented out, because we may get here
+	 * as part of sofreelastref(), and at that time, pr_getlock() may no
+	 * longer be able to return us the lock; this will be fixed in future.
+	 */
+	if (so->so_proto->pr_getlock != NULL)
+		mutex_held = (*so->so_proto->pr_getlock)(so, 0);
+	else
+		mutex_held = so->so_proto->pr_domain->dom_mtx;
+
+	lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+#endif /* notyet */
+
+	/*
+	 * Obtain lock on the socket buffer (SB_LOCK).  This is required
+	 * to prevent the socket buffer from being unexpectedly altered
+	 * while it is used by another thread in socket send/receive.
+	 *
+	 * sblock() must not fail here, hence the assertion.
+	 */
+	(void) sblock(sb, SBL_WAIT | SBL_NOINTR | SBL_IGNDEFUNCT);
+	VERIFY(sb->sb_flags & SB_LOCK);
+
+	sb->sb_flags		&= ~(SB_SEL|SB_UPCALL);
+	sb->sb_flags		|= SB_DROP;
+	sb->sb_upcall		= NULL;
+	sb->sb_upcallarg	= NULL;
+
+	sbunlock(sb, TRUE);	/* keep socket locked */
+
+	selthreadclear(&sb->sb_sel);
+	sbrelease(sb);
+}
+
+void
+sorflush(struct socket *so)
+{
+	struct sockbuf *sb = &so->so_rcv;
+	struct protosw *pr = so->so_proto;
+	struct sockbuf asb;
+#ifdef notyet
+	lck_mtx_t *mutex_held;
+	/*
+	 * XXX: This code is currently commented out, because we may get here
+	 * as part of sofreelastref(), and at that time, pr_getlock() may no
+	 * longer be able to return us the lock; this will be fixed in future.
+	 */
+	if (so->so_proto->pr_getlock != NULL)
+		mutex_held = (*so->so_proto->pr_getlock)(so, 0);
+	else
+		mutex_held = so->so_proto->pr_domain->dom_mtx;
+
+	lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+#endif /* notyet */
+
+	sflt_notify(so, sock_evt_flush_read, NULL);
+
+	socantrcvmore(so);
+
+	/*
+	 * Obtain lock on the socket buffer (SB_LOCK).  This is required
+	 * to prevent the socket buffer from being unexpectedly altered
+	 * while it is used by another thread in socket send/receive.
+	 *
+	 * sblock() must not fail here, hence the assertion.
+	 */
+	(void) sblock(sb, SBL_WAIT | SBL_NOINTR | SBL_IGNDEFUNCT);
+	VERIFY(sb->sb_flags & SB_LOCK);
+
+	/*
+	 * Copy only the relevant fields from "sb" to "asb" which we
+	 * need for sbrelease() to function.  In particular, skip
+	 * sb_sel as it contains the wait queue linkage, which would
+	 * wreak havoc if we were to issue selthreadclear() on "asb".
+	 * Make sure to not carry over SB_LOCK in "asb", as we need
+	 * to acquire it later as part of sbrelease().
+	 */
+	bzero(&asb, sizeof (asb));
+	asb.sb_cc		= sb->sb_cc;
+	asb.sb_hiwat		= sb->sb_hiwat;
+	asb.sb_mbcnt		= sb->sb_mbcnt;
+	asb.sb_mbmax		= sb->sb_mbmax;
+	asb.sb_ctl		= sb->sb_ctl;
+	asb.sb_lowat		= sb->sb_lowat;
+	asb.sb_mb		= sb->sb_mb;
+	asb.sb_mbtail		= sb->sb_mbtail;
+	asb.sb_lastrecord	= sb->sb_lastrecord;
+	asb.sb_so		= sb->sb_so;
+	asb.sb_flags		= sb->sb_flags;
+	asb.sb_flags		&= ~(SB_LOCK|SB_SEL|SB_KNOTE|SB_UPCALL);
+	asb.sb_flags		|= SB_DROP;
+
+	/*
+	 * Ideally we'd bzero() these and preserve the ones we need;
+	 * but to do that we'd need to shuffle things around in the
+	 * sockbuf, and we can't do it now because there are KEXTS
+	 * that are directly referring to the socket structure.
+	 *
+	 * Setting SB_DROP acts as a barrier to prevent further appends.
+	 * Clearing SB_SEL is done for selthreadclear() below.
+	 */
+	sb->sb_cc		= 0;
+	sb->sb_hiwat		= 0;
+	sb->sb_mbcnt		= 0;
+	sb->sb_mbmax		= 0;
+	sb->sb_ctl		= 0;
+	sb->sb_lowat		= 0;
+	sb->sb_mb		= NULL;
+	sb->sb_mbtail		= NULL;
+	sb->sb_lastrecord	= NULL;
+	sb->sb_timeo.tv_sec	= 0;
+	sb->sb_timeo.tv_usec	= 0;
+	sb->sb_upcall		= NULL;
+	sb->sb_upcallarg	= NULL;
+	sb->sb_flags		&= ~(SB_SEL|SB_UPCALL);
+	sb->sb_flags		|= SB_DROP;
+
+	sbunlock(sb, TRUE);	/* keep socket locked */
+
+	/*
+	 * Note that selthreadclear() is called on the original "sb" and
+	 * not the local "asb" because of the way wait queue linkage is
+	 * implemented.  Given that selwakeup() may be triggered, SB_SEL
+	 * should no longer be set (cleared above.)
+	 */
+	selthreadclear(&sb->sb_sel);
+
+	if ((pr->pr_flags & PR_RIGHTS) && pr->pr_domain->dom_dispose)
+		(*pr->pr_domain->dom_dispose)(asb.sb_mb);
+
+	sbrelease(&asb);
+}
+
+/*
+ * Perhaps this routine, and sooptcopyout(), below, ought to come in
+ * an additional variant to handle the case where the option value needs
+ * to be some kind of integer, but not a specific size.
+ * In addition to their use here, these functions are also called by the
+ * protocol-level pr_ctloutput() routines.
+ *
+ * Returns:	0			Success
+ *		EINVAL
+ *	copyin:EFAULT
+ */
+int
+sooptcopyin(struct sockopt *sopt, void *buf, size_t len, size_t minlen)
+{
+	size_t	valsize;
+
+	/*
+	 * If the user gives us more than we wanted, we ignore it,
+	 * but if we don't get the minimum length the caller
+	 * wants, we return EINVAL.  On success, sopt->sopt_valsize
+	 * is set to however much we actually retrieved.
+	 */
+	if ((valsize = sopt->sopt_valsize) < minlen)
+		return (EINVAL);
+	if (valsize > len)
+		sopt->sopt_valsize = valsize = len;
+
+	if (sopt->sopt_p != kernproc)
+		return (copyin(sopt->sopt_val, buf, valsize));
+
+	bcopy(CAST_DOWN(caddr_t, sopt->sopt_val), buf, valsize);
+	return (0);
+}
+
+/*
+ * sooptcopyin_timeval
+ *   Copy in a timeval value into tv_p, and take into account whether the
+ *   the calling process is 64-bit or 32-bit.  Moved the sanity checking
+ *   code here so that we can verify the 64-bit tv_sec value before we lose
+ *   the top 32-bits assigning tv64.tv_sec to tv_p->tv_sec.
+ */
+static int
+sooptcopyin_timeval(struct sockopt *sopt, struct timeval *tv_p)
+{
+	int			error;
+
+	if (proc_is64bit(sopt->sopt_p)) {
+		struct user64_timeval	tv64;
+
+		if (sopt->sopt_valsize < sizeof (tv64))
+			return (EINVAL);
+
+		sopt->sopt_valsize = sizeof (tv64);
+		if (sopt->sopt_p != kernproc) {
+			error = copyin(sopt->sopt_val, &tv64, sizeof (tv64));
+			if (error != 0)
+				return (error);
+		} else {
+			bcopy(CAST_DOWN(caddr_t, sopt->sopt_val), &tv64,
+			    sizeof (tv64));
+		}
+		if (tv64.tv_sec < 0 || tv64.tv_sec > LONG_MAX ||
+		    tv64.tv_usec < 0 || tv64.tv_usec >= 1000000)
+			return (EDOM);
+
+		tv_p->tv_sec = tv64.tv_sec;
+		tv_p->tv_usec = tv64.tv_usec;
+	} else {
+		struct user32_timeval	tv32;
+
+		if (sopt->sopt_valsize < sizeof (tv32))
+			return (EINVAL);
+
+		sopt->sopt_valsize = sizeof (tv32);
+		if (sopt->sopt_p != kernproc) {
+			error = copyin(sopt->sopt_val, &tv32, sizeof (tv32));
+			if (error != 0) {
+				return (error);
+			}
+		} else {
+			bcopy(CAST_DOWN(caddr_t, sopt->sopt_val), &tv32,
+			    sizeof (tv32));
+		}
+#ifndef __LP64__
+		/*
+		 * K64todo "comparison is always false due to
+		 * limited range of data type"
+		 */
+		if (tv32.tv_sec < 0 || tv32.tv_sec > LONG_MAX ||
+		    tv32.tv_usec < 0 || tv32.tv_usec >= 1000000)
+			return (EDOM);
+#endif
+		tv_p->tv_sec = tv32.tv_sec;
+		tv_p->tv_usec = tv32.tv_usec;
+	}
+	return (0);
+}
+
+/*
+ * Returns:	0			Success
+ *		EINVAL
+ *		ENOPROTOOPT
+ *		ENOBUFS
+ *		EDOM
+ *	sooptcopyin:EINVAL
+ *	sooptcopyin:EFAULT
+ *	sooptcopyin_timeval:EINVAL
+ *	sooptcopyin_timeval:EFAULT
+ *	sooptcopyin_timeval:EDOM
+ *	<pr_ctloutput>:EOPNOTSUPP[AF_UNIX]
+ *	<pr_ctloutput>:???w
+ *	sflt_attach_private:???		[whatever a filter author chooses]
+ *	<sf_setoption>:???		[whatever a filter author chooses]
+ *
+ * Notes:	Other <pru_listen> returns depend on the protocol family; all
+ *		<sf_listen> returns depend on what the filter author causes
+ *		their filter to return.
+ */
+int
+sosetoptlock(struct socket *so, struct sockopt *sopt, int dolock)
+{
+	int	error, optval;
+	struct	linger l;
+	struct	timeval tv;
+#if CONFIG_MACF_SOCKET
+	struct mac extmac;
+#endif /* MAC_SOCKET */
+
+	if (sopt->sopt_dir != SOPT_SET)
+		sopt->sopt_dir = SOPT_SET;
+
+	if (dolock)
+		socket_lock(so, 1);
+
+	if ((so->so_state & (SS_CANTRCVMORE | SS_CANTSENDMORE)) ==
+	    (SS_CANTRCVMORE | SS_CANTSENDMORE) &&
+	    (so->so_flags & SOF_NPX_SETOPTSHUT) == 0) {
+		/* the socket has been shutdown, no more sockopt's */
+		error = EINVAL;
+		goto out;
+	}
+
+	error = sflt_setsockopt(so, sopt);
+	if (error != 0) {
+		if (error == EJUSTRETURN)
+			error = 0;
+		goto out;
+	}
+
+	if (sopt->sopt_level != SOL_SOCKET) {
+		if (so->so_proto != NULL &&
+		    so->so_proto->pr_ctloutput != NULL) {
+			error = (*so->so_proto->pr_ctloutput)(so, sopt);
+			goto out;
+		}
+		error = ENOPROTOOPT;
+	} else {
+		/*
+		 * Allow socket-level (SOL_SOCKET) options to be filtered by
+		 * the protocol layer, if needed.  A zero value returned from
+		 * the handler means use default socket-level processing as
+		 * done by the rest of this routine.  Otherwise, any other
+		 * return value indicates that the option is unsupported.
+		 */
+		if (so->so_proto != NULL && (error = so->so_proto->pr_usrreqs->
+		    pru_socheckopt(so, sopt)) != 0)
+			goto out;
+
+		error = 0;
+		switch (sopt->sopt_name) {
+		case SO_LINGER:
+		case SO_LINGER_SEC:
+			error = sooptcopyin(sopt, &l, sizeof (l), sizeof (l));
+			if (error != 0)
+				goto out;
+
+			so->so_linger = (sopt->sopt_name == SO_LINGER) ?
+			    l.l_linger : l.l_linger * hz;
+			if (l.l_onoff != 0)
+				so->so_options |= SO_LINGER;
+			else
+				so->so_options &= ~SO_LINGER;
+			break;
+
+		case SO_DEBUG:
+		case SO_KEEPALIVE:
+		case SO_DONTROUTE:
+		case SO_USELOOPBACK:
+		case SO_BROADCAST:
+		case SO_REUSEADDR:
+		case SO_REUSEPORT:
+		case SO_OOBINLINE:
+		case SO_TIMESTAMP:
+		case SO_TIMESTAMP_MONOTONIC:
+		case SO_DONTTRUNC:
+		case SO_WANTMORE:
+		case SO_WANTOOBFLAG:
+		case SO_NOWAKEFROMSLEEP:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval)
+				so->so_options |= sopt->sopt_name;
+			else
+				so->so_options &= ~sopt->sopt_name;
+			break;
+
+		case SO_SNDBUF:
+		case SO_RCVBUF:
+		case SO_SNDLOWAT:
+		case SO_RCVLOWAT:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+
+			/*
+			 * Values < 1 make no sense for any of these
+			 * options, so disallow them.
+			 */
+			if (optval < 1) {
+				error = EINVAL;
+				goto out;
+			}
+
+			switch (sopt->sopt_name) {
+			case SO_SNDBUF:
+			case SO_RCVBUF: {
+				struct sockbuf *sb =
+				    (sopt->sopt_name == SO_SNDBUF) ?
+				    &so->so_snd : &so->so_rcv;
+				if (sbreserve(sb, (u_int32_t)optval) == 0) {
+					error = ENOBUFS;
+					goto out;
+				}
+				sb->sb_flags |= SB_USRSIZE;
+				sb->sb_flags &= ~SB_AUTOSIZE;
+				sb->sb_idealsize = (u_int32_t)optval;
+				break;
+			}
+			/*
+			 * Make sure the low-water is never greater than
+			 * the high-water.
+			 */
+			case SO_SNDLOWAT: {
+				int space = sbspace(&so->so_snd);
+				u_int32_t hiwat = so->so_snd.sb_hiwat;
+
+				if (so->so_snd.sb_flags & SB_UNIX) {
+					struct unpcb *unp =
+					    (struct unpcb *)(so->so_pcb);
+					if (unp != NULL &&
+					    unp->unp_conn != NULL) {
+						hiwat += unp->unp_conn->unp_cc;
+					}
+				}
+
+				so->so_snd.sb_lowat =
+				    (optval > hiwat) ?
+				    hiwat : optval;
+
+				if (space >= so->so_snd.sb_lowat) {
+					sowwakeup(so);
+				}
+				break;
+			}
+			case SO_RCVLOWAT: {
+				int64_t data_len;
+				so->so_rcv.sb_lowat =
+				    (optval > so->so_rcv.sb_hiwat) ?
+				    so->so_rcv.sb_hiwat : optval;
+				data_len = so->so_rcv.sb_cc
+				    - so->so_rcv.sb_ctl;
+				if (data_len >= so->so_rcv.sb_lowat)
+				    sorwakeup(so);
+				break;
+			}
+			}
+			break;
+
+		case SO_SNDTIMEO:
+		case SO_RCVTIMEO:
+			error = sooptcopyin_timeval(sopt, &tv);
+			if (error != 0)
+				goto out;
+
+			switch (sopt->sopt_name) {
+			case SO_SNDTIMEO:
+				so->so_snd.sb_timeo = tv;
+				break;
+			case SO_RCVTIMEO:
+				so->so_rcv.sb_timeo = tv;
+				break;
+			}
+			break;
+
+		case SO_NKE: {
+			struct so_nke nke;
+
+			error = sooptcopyin(sopt, &nke, sizeof (nke),
+			    sizeof (nke));
+			if (error != 0)
+				goto out;
+
+			error = sflt_attach_internal(so, nke.nke_handle);
+			break;
+		}
+
+		case SO_NOSIGPIPE:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0)
+				so->so_flags |= SOF_NOSIGPIPE;
+			else
+				so->so_flags &= ~SOF_NOSIGPIPE;
+			break;
+
+		case SO_NOADDRERR:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0)
+				so->so_flags |= SOF_NOADDRAVAIL;
+			else
+				so->so_flags &= ~SOF_NOADDRAVAIL;
+			break;
+
+		case SO_REUSESHAREUID:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0)
+				so->so_flags |= SOF_REUSESHAREUID;
+			else
+				so->so_flags &= ~SOF_REUSESHAREUID;
+			break;
+
+		case SO_NOTIFYCONFLICT:
+			if (kauth_cred_issuser(kauth_cred_get()) == 0) {
+				error = EPERM;
+				goto out;
+			}
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0)
+				so->so_flags |= SOF_NOTIFYCONFLICT;
+			else
+				so->so_flags &= ~SOF_NOTIFYCONFLICT;
+			break;
+
+		case SO_RESTRICTIONS:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+
+			error = so_set_restrictions(so, optval);
+			break;
+
+		case SO_AWDL_UNRESTRICTED:
+			if (SOCK_DOM(so) != PF_INET &&
+			    SOCK_DOM(so) != PF_INET6) {
+				error = EOPNOTSUPP;
+				goto out;
+			}
+			error = sooptcopyin(sopt, &optval, sizeof(optval),
+			    sizeof(optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0) {
+				kauth_cred_t cred =  NULL;
+				proc_t ep = PROC_NULL;
+
+				if (so->so_flags & SOF_DELEGATED) {
+					ep = proc_find(so->e_pid);
+					if (ep)
+						cred = kauth_cred_proc_ref(ep);
+				}
+				error = priv_check_cred(
+				    cred ? cred : so->so_cred,
+				    PRIV_NET_RESTRICTED_AWDL, 0);
+				if (error == 0)
+					inp_set_awdl_unrestricted(
+					    sotoinpcb(so));
+				if (cred)
+					kauth_cred_unref(&cred);
+				if (ep != PROC_NULL)
+					proc_rele(ep);
+			} else
+				inp_clear_awdl_unrestricted(sotoinpcb(so));
+			break;
+
+		case SO_LABEL:
+#if CONFIG_MACF_SOCKET
+			if ((error = sooptcopyin(sopt, &extmac, sizeof (extmac),
+			    sizeof (extmac))) != 0)
+				goto out;
+
+			error = mac_setsockopt_label(proc_ucred(sopt->sopt_p),
+			    so, &extmac);
+#else
+			error = EOPNOTSUPP;
+#endif /* MAC_SOCKET */
+			break;
+
+		case SO_UPCALLCLOSEWAIT:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0)
+				so->so_flags |= SOF_UPCALLCLOSEWAIT;
+			else
+				so->so_flags &= ~SOF_UPCALLCLOSEWAIT;
+			break;
+
+		case SO_RANDOMPORT:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval != 0)
+				so->so_flags |= SOF_BINDRANDOMPORT;
+			else
+				so->so_flags &= ~SOF_BINDRANDOMPORT;
+			break;
+
+		case SO_NP_EXTENSIONS: {
+			struct so_np_extensions sonpx;
+
+			error = sooptcopyin(sopt, &sonpx, sizeof (sonpx),
+			    sizeof (sonpx));
+			if (error != 0)
+				goto out;
+			if (sonpx.npx_mask & ~SONPX_MASK_VALID) {
+				error = EINVAL;
+				goto out;
+			}
+			/*
+			 * Only one bit defined for now
+			 */
+			if ((sonpx.npx_mask & SONPX_SETOPTSHUT)) {
+				if ((sonpx.npx_flags & SONPX_SETOPTSHUT))
+					so->so_flags |= SOF_NPX_SETOPTSHUT;
+				else
+					so->so_flags &= ~SOF_NPX_SETOPTSHUT;
+			}
+			break;
+		}
+
+		case SO_TRAFFIC_CLASS: {
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			error = so_set_traffic_class(so, optval);
+			if (error != 0)
+				goto out;
+			break;
+		}
+
+		case SO_RECV_TRAFFIC_CLASS: {
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval == 0)
+				so->so_flags &= ~SOF_RECV_TRAFFIC_CLASS;
+			else
+				so->so_flags |= SOF_RECV_TRAFFIC_CLASS;
+			break;
+		}
+
+		case SO_TRAFFIC_CLASS_DBG: {
+			struct so_tcdbg so_tcdbg;
+
+			error = sooptcopyin(sopt, &so_tcdbg,
+			    sizeof (struct so_tcdbg), sizeof (struct so_tcdbg));
+			if (error != 0)
+				goto out;
+			error = so_set_tcdbg(so, &so_tcdbg);
+			if (error != 0)
+				goto out;
+			break;
+		}
+
+		case SO_PRIVILEGED_TRAFFIC_CLASS:
+			error = priv_check_cred(kauth_cred_get(),
+			    PRIV_NET_PRIVILEGED_TRAFFIC_CLASS, 0);
+			if (error != 0)
+				goto out;
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval == 0)
+				so->so_flags &= ~SOF_PRIVILEGED_TRAFFIC_CLASS;
+			else
+				so->so_flags |= SOF_PRIVILEGED_TRAFFIC_CLASS;
+			break;
+
+		case SO_DEFUNCTOK:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0 || (so->so_flags & SOF_DEFUNCT)) {
+				if (error == 0)
+					error = EBADF;
+				goto out;
+			}
+			/*
+			 * Any process can set SO_DEFUNCTOK (clear
+			 * SOF_NODEFUNCT), but only root can clear
+			 * SO_DEFUNCTOK (set SOF_NODEFUNCT).
+			 */
+			if (optval == 0 &&
+			    kauth_cred_issuser(kauth_cred_get()) == 0) {
+				error = EPERM;
+				goto out;
+			}
+			if (optval)
+				so->so_flags &= ~SOF_NODEFUNCT;
+			else
+				so->so_flags |= SOF_NODEFUNCT;
+
+			if (SOCK_DOM(so) == PF_INET ||
+			    SOCK_DOM(so) == PF_INET6) {
+				char s[MAX_IPv6_STR_LEN];
+				char d[MAX_IPv6_STR_LEN];
+				struct inpcb *inp = sotoinpcb(so);
+
+				SODEFUNCTLOG(("%s[%d]: so 0x%llx [%s %s:%d -> "
+				    "%s:%d] is now marked as %seligible for "
+				    "defunct\n", __func__, proc_selfpid(),
+				    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+				    (SOCK_TYPE(so) == SOCK_STREAM) ?
+				    "TCP" : "UDP", inet_ntop(SOCK_DOM(so),
+				    ((SOCK_DOM(so) == PF_INET) ?
+				    (void *)&inp->inp_laddr.s_addr :
+				    (void *)&inp->in6p_laddr), s, sizeof (s)),
+				    ntohs(inp->in6p_lport),
+				    inet_ntop(SOCK_DOM(so),
+				    (SOCK_DOM(so) == PF_INET) ?
+				    (void *)&inp->inp_faddr.s_addr :
+				    (void *)&inp->in6p_faddr, d, sizeof (d)),
+				    ntohs(inp->in6p_fport),
+				    (so->so_flags & SOF_NODEFUNCT) ?
+				    "not " : ""));
+			} else {
+				SODEFUNCTLOG(("%s[%d]: so 0x%llx [%d,%d] is "
+				    "now marked as %seligible for defunct\n",
+				    __func__, proc_selfpid(),
+				    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+				    SOCK_DOM(so), SOCK_TYPE(so),
+				    (so->so_flags & SOF_NODEFUNCT) ?
+				    "not " : ""));
+			}
+			break;
+
+		case SO_ISDEFUNCT:
+			/* This option is not settable */
+			error = EINVAL;
+			break;
+
+		case SO_OPPORTUNISTIC:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error == 0)
+				error = so_set_opportunistic(so, optval);
+			break;
+
+		case SO_FLUSH:
+			/* This option is handled by lower layer(s) */
+			error = 0;
+			break;
+
+		case SO_RECV_ANYIF:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error == 0)
+				error = so_set_recv_anyif(so, optval);
+			break;
+
+		case SO_TRAFFIC_MGT_BACKGROUND: {
+			/* This option is handled by lower layer(s) */
+			error = 0;
+			break;
+		}
+
+#if FLOW_DIVERT
+		case SO_FLOW_DIVERT_TOKEN:
+			error = flow_divert_token_set(so, sopt);
+			break;
+#endif	/* FLOW_DIVERT */
+
+
+		case SO_DELEGATED:
+			if ((error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval))) != 0)
+				break;
+
+			error = so_set_effective_pid(so, optval, sopt->sopt_p);
+			break;
+
+		case SO_DELEGATED_UUID: {
+			uuid_t euuid;
+
+			if ((error = sooptcopyin(sopt, &euuid, sizeof (euuid),
+			    sizeof (euuid))) != 0)
+				break;
+
+			error = so_set_effective_uuid(so, euuid, sopt->sopt_p);
+			break;
+		}
+
+#if NECP
+		case SO_NECP_ATTRIBUTES:
+			error = necp_set_socket_attributes(so, sopt);
+			break;
+#endif /* NECP */
+
+#if MPTCP
+		case SO_MPTCP_FASTJOIN:
+			if (!((so->so_flags & SOF_MP_SUBFLOW) ||
+			    ((SOCK_CHECK_DOM(so, PF_MULTIPATH)) &&
+			    (SOCK_CHECK_PROTO(so, IPPROTO_TCP))))) {
+				error = ENOPROTOOPT;
+				break;
+			}
+
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error != 0)
+				goto out;
+			if (optval == 0)
+				so->so_flags &= ~SOF_MPTCP_FASTJOIN;
+			else
+				so->so_flags |= SOF_MPTCP_FASTJOIN;
+			break;
+#endif /* MPTCP */
+
+		case SO_EXTENDED_BK_IDLE:
+			error = sooptcopyin(sopt, &optval, sizeof (optval),
+			    sizeof (optval));
+			if (error == 0)
+				error = so_set_extended_bk_idle(so, optval);
+			break;
+
+		default:
+			error = ENOPROTOOPT;
+			break;
+		}
+		if (error == 0 && so->so_proto != NULL &&
+		    so->so_proto->pr_ctloutput != NULL) {
+			(void) so->so_proto->pr_ctloutput(so, sopt);
+		}
+	}
+out:
+	if (dolock)
+		socket_unlock(so, 1);
+	return (error);
+}
+
+/* Helper routines for getsockopt */
+int
+sooptcopyout(struct sockopt *sopt, void *buf, size_t len)
+{
+	int	error;
+	size_t	valsize;
+
+	error = 0;
+
+	/*
+	 * Documented get behavior is that we always return a value,
+	 * possibly truncated to fit in the user's buffer.
+	 * Traditional behavior is that we always tell the user
+	 * precisely how much we copied, rather than something useful
+	 * like the total amount we had available for her.
+	 * Note that this interface is not idempotent; the entire answer must
+	 * generated ahead of time.
+	 */
+	valsize = min(len, sopt->sopt_valsize);
+	sopt->sopt_valsize = valsize;
+	if (sopt->sopt_val != USER_ADDR_NULL) {
+		if (sopt->sopt_p != kernproc)
+			error = copyout(buf, sopt->sopt_val, valsize);
+		else
+			bcopy(buf, CAST_DOWN(caddr_t, sopt->sopt_val), valsize);
+	}
+	return (error);
+}
+
+static int
+sooptcopyout_timeval(struct sockopt *sopt, const struct timeval *tv_p)
+{
+	int			error;
+	size_t			len;
+	struct user64_timeval	tv64;
+	struct user32_timeval	tv32;
+	const void *		val;
+	size_t			valsize;
+
+	error = 0;
+	if (proc_is64bit(sopt->sopt_p)) {
+		len = sizeof (tv64);
+		tv64.tv_sec = tv_p->tv_sec;
+		tv64.tv_usec = tv_p->tv_usec;
+		val = &tv64;
+	} else {
+		len = sizeof (tv32);
+		tv32.tv_sec = tv_p->tv_sec;
+		tv32.tv_usec = tv_p->tv_usec;
+		val = &tv32;
+	}
+	valsize = min(len, sopt->sopt_valsize);
+	sopt->sopt_valsize = valsize;
+	if (sopt->sopt_val != USER_ADDR_NULL) {
+		if (sopt->sopt_p != kernproc)
+			error = copyout(val, sopt->sopt_val, valsize);
+		else
+			bcopy(val, CAST_DOWN(caddr_t, sopt->sopt_val), valsize);
+	}
+	return (error);
+}
+
+/*
+ * Return:	0			Success
+ *		ENOPROTOOPT
+ *	<pr_ctloutput>:EOPNOTSUPP[AF_UNIX]
+ *	<pr_ctloutput>:???
+ *	<sf_getoption>:???
+ */
+int
+sogetoptlock(struct socket *so, struct sockopt *sopt, int dolock)
+{
+	int	error, optval;
+	struct	linger l;
+	struct	timeval tv;
+#if CONFIG_MACF_SOCKET
+	struct mac extmac;
+#endif /* MAC_SOCKET */
+
+	if (sopt->sopt_dir != SOPT_GET)
+		sopt->sopt_dir = SOPT_GET;
+
+	if (dolock)
+		socket_lock(so, 1);
+
+	error = sflt_getsockopt(so, sopt);
+	if (error != 0) {
+		if (error == EJUSTRETURN)
+			error = 0;
+		goto out;
+	}
+
+	if (sopt->sopt_level != SOL_SOCKET) {
+		if (so->so_proto != NULL &&
+		    so->so_proto->pr_ctloutput != NULL) {
+			error = (*so->so_proto->pr_ctloutput)(so, sopt);
+			goto out;
+		}
+		error = ENOPROTOOPT;
+	} else {
+		/*
+		 * Allow socket-level (SOL_SOCKET) options to be filtered by
+		 * the protocol layer, if needed.  A zero value returned from
+		 * the handler means use default socket-level processing as
+		 * done by the rest of this routine.  Otherwise, any other
+		 * return value indicates that the option is unsupported.
+		 */
+		if (so->so_proto != NULL && (error = so->so_proto->pr_usrreqs->
+		    pru_socheckopt(so, sopt)) != 0)
+			goto out;
+
+		error = 0;
+		switch (sopt->sopt_name) {
+		case SO_LINGER:
+		case SO_LINGER_SEC:
+			l.l_onoff = ((so->so_options & SO_LINGER) ? 1 : 0);
+			l.l_linger = (sopt->sopt_name == SO_LINGER) ?
+			    so->so_linger : so->so_linger / hz;
+			error = sooptcopyout(sopt, &l, sizeof (l));
+			break;
+
+		case SO_USELOOPBACK:
+		case SO_DONTROUTE:
+		case SO_DEBUG:
+		case SO_KEEPALIVE:
+		case SO_REUSEADDR:
+		case SO_REUSEPORT:
+		case SO_BROADCAST:
+		case SO_OOBINLINE:
+		case SO_TIMESTAMP:
+		case SO_TIMESTAMP_MONOTONIC:
+		case SO_DONTTRUNC:
+		case SO_WANTMORE:
+		case SO_WANTOOBFLAG:
+		case SO_NOWAKEFROMSLEEP:
+			optval = so->so_options & sopt->sopt_name;
+integer:
+			error = sooptcopyout(sopt, &optval, sizeof (optval));
+			break;
+
+		case SO_TYPE:
+			optval = so->so_type;
+			goto integer;
+
+		case SO_NREAD:
+			if (so->so_proto->pr_flags & PR_ATOMIC) {
+				int pkt_total;
+				struct mbuf *m1;
+
+				pkt_total = 0;
+				m1 = so->so_rcv.sb_mb;
+				while (m1 != NULL) {
+					if (m1->m_type == MT_DATA ||
+					    m1->m_type == MT_HEADER ||
+					    m1->m_type == MT_OOBDATA)
+						pkt_total += m1->m_len;
+					m1 = m1->m_next;
+				}
+				optval = pkt_total;
+			} else {
+				optval = so->so_rcv.sb_cc - so->so_rcv.sb_ctl;
+			}
+			goto integer;
+
+		case SO_NUMRCVPKT:
+			if (so->so_proto->pr_flags & PR_ATOMIC) {
+				int cnt = 0;
+				struct mbuf *m1;
+
+				m1 = so->so_rcv.sb_mb;
+				while (m1 != NULL) {
+					if (m1->m_type == MT_DATA ||
+					    m1->m_type == MT_HEADER ||
+					    m1->m_type == MT_OOBDATA)
+						cnt += 1;
+					m1 = m1->m_nextpkt;
+				}
+				optval = cnt;
+				goto integer;
+			} else {
+				error = EINVAL;
+				break;
+			}
+
+		case SO_NWRITE:
+			optval = so->so_snd.sb_cc;
+			goto integer;
+
+		case SO_ERROR:
+			optval = so->so_error;
+			so->so_error = 0;
+			goto integer;
+
+		case SO_SNDBUF: {
+			u_int32_t hiwat = so->so_snd.sb_hiwat;
+
+			if (so->so_snd.sb_flags & SB_UNIX) {
+				struct unpcb *unp =
+				    (struct unpcb *)(so->so_pcb);
+				if (unp != NULL && unp->unp_conn != NULL) {
+					hiwat += unp->unp_conn->unp_cc;
+				}
+			}
+
+			optval = hiwat;
+			goto integer;
+		}
+		case SO_RCVBUF:
+			optval = so->so_rcv.sb_hiwat;
+			goto integer;
+
+		case SO_SNDLOWAT:
+			optval = so->so_snd.sb_lowat;
+			goto integer;
+
+		case SO_RCVLOWAT:
+			optval = so->so_rcv.sb_lowat;
+			goto integer;
+
+		case SO_SNDTIMEO:
+		case SO_RCVTIMEO:
+			tv = (sopt->sopt_name == SO_SNDTIMEO ?
+			    so->so_snd.sb_timeo : so->so_rcv.sb_timeo);
+
+			error = sooptcopyout_timeval(sopt, &tv);
+			break;
+
+		case SO_NOSIGPIPE:
+			optval = (so->so_flags & SOF_NOSIGPIPE);
+			goto integer;
+
+		case SO_NOADDRERR:
+			optval = (so->so_flags & SOF_NOADDRAVAIL);
+			goto integer;
+
+		case SO_REUSESHAREUID:
+			optval = (so->so_flags & SOF_REUSESHAREUID);
+			goto integer;
+
+
+		case SO_NOTIFYCONFLICT:
+			optval = (so->so_flags & SOF_NOTIFYCONFLICT);
+			goto integer;
+
+		case SO_RESTRICTIONS:
+			optval = so_get_restrictions(so);
+			goto integer;
+
+		case SO_AWDL_UNRESTRICTED:
+			if (SOCK_DOM(so) == PF_INET ||
+			    SOCK_DOM(so) == PF_INET6) {
+				optval = inp_get_awdl_unrestricted(
+				    sotoinpcb(so));
+				goto integer;
+			} else
+				error = EOPNOTSUPP;
+			break;
+
+		case SO_LABEL:
+#if CONFIG_MACF_SOCKET
+			if ((error = sooptcopyin(sopt, &extmac, sizeof (extmac),
+			    sizeof (extmac))) != 0 ||
+			    (error = mac_socket_label_get(proc_ucred(
+			    sopt->sopt_p), so, &extmac)) != 0)
+				break;
+
+			error = sooptcopyout(sopt, &extmac, sizeof (extmac));
+#else
+			error = EOPNOTSUPP;
+#endif /* MAC_SOCKET */
+			break;
+
+		case SO_PEERLABEL:
+#if CONFIG_MACF_SOCKET
+			if ((error = sooptcopyin(sopt, &extmac, sizeof (extmac),
+			    sizeof (extmac))) != 0 ||
+			    (error = mac_socketpeer_label_get(proc_ucred(
+			    sopt->sopt_p), so, &extmac)) != 0)
+				break;
+
+			error = sooptcopyout(sopt, &extmac, sizeof (extmac));
+#else
+			error = EOPNOTSUPP;
+#endif /* MAC_SOCKET */
+			break;
+
+#ifdef __APPLE_API_PRIVATE
+		case SO_UPCALLCLOSEWAIT:
+			optval = (so->so_flags & SOF_UPCALLCLOSEWAIT);
+			goto integer;
+#endif
+		case SO_RANDOMPORT:
+			optval = (so->so_flags & SOF_BINDRANDOMPORT);
+			goto integer;
+
+		case SO_NP_EXTENSIONS: {
+			struct so_np_extensions sonpx;
+
+			sonpx.npx_flags = (so->so_flags & SOF_NPX_SETOPTSHUT) ?
+			    SONPX_SETOPTSHUT : 0;
+			sonpx.npx_mask = SONPX_MASK_VALID;
+
+			error = sooptcopyout(sopt, &sonpx,
+			    sizeof (struct so_np_extensions));
+			break;
+		}
+
+		case SO_TRAFFIC_CLASS:
+			optval = so->so_traffic_class;
+			goto integer;
+
+		case SO_RECV_TRAFFIC_CLASS:
+			optval = (so->so_flags & SOF_RECV_TRAFFIC_CLASS);
+			goto integer;
+
+		case SO_TRAFFIC_CLASS_STATS:
+			error = sooptcopyout(sopt, &so->so_tc_stats,
+			    sizeof (so->so_tc_stats));
+			break;
+
+		case SO_TRAFFIC_CLASS_DBG:
+			error = sogetopt_tcdbg(so, sopt);
+			break;
+
+		case SO_PRIVILEGED_TRAFFIC_CLASS:
+			optval = (so->so_flags & SOF_PRIVILEGED_TRAFFIC_CLASS);
+			goto integer;
+
+		case SO_DEFUNCTOK:
+			optval = !(so->so_flags & SOF_NODEFUNCT);
+			goto integer;
+
+		case SO_ISDEFUNCT:
+			optval = (so->so_flags & SOF_DEFUNCT);
+			goto integer;
+
+		case SO_OPPORTUNISTIC:
+			optval = so_get_opportunistic(so);
+			goto integer;
+
+		case SO_FLUSH:
+			/* This option is not gettable */
+			error = EINVAL;
+			break;
+
+		case SO_RECV_ANYIF:
+			optval = so_get_recv_anyif(so);
+			goto integer;
+
+		case SO_TRAFFIC_MGT_BACKGROUND:
+			/* This option is handled by lower layer(s) */
+			if (so->so_proto != NULL &&
+			    so->so_proto->pr_ctloutput != NULL) {
+				(void) so->so_proto->pr_ctloutput(so, sopt);
+			}
+			break;
+
+#if FLOW_DIVERT
+		case SO_FLOW_DIVERT_TOKEN:
+			error = flow_divert_token_get(so, sopt);
+			break;
+#endif	/* FLOW_DIVERT */
+
+#if NECP
+		case SO_NECP_ATTRIBUTES:
+			error = necp_get_socket_attributes(so, sopt);
+			break;
+#endif /* NECP */
+
+#if CONTENT_FILTER
+		case SO_CFIL_SOCK_ID: {
+			cfil_sock_id_t sock_id;
+
+			sock_id = cfil_sock_id_from_socket(so);
+
+			error = sooptcopyout(sopt, &sock_id,
+				sizeof(cfil_sock_id_t));
+			break;
+		}
+#endif	/* CONTENT_FILTER */
+
+#if MPTCP
+		case SO_MPTCP_FASTJOIN:
+			if (!((so->so_flags & SOF_MP_SUBFLOW) ||
+			    ((SOCK_CHECK_DOM(so, PF_MULTIPATH)) &&
+			    (SOCK_CHECK_PROTO(so, IPPROTO_TCP))))) {
+				error = ENOPROTOOPT;
+				break;
+			}
+			optval = (so->so_flags & SOF_MPTCP_FASTJOIN);
+			/* Fixed along with rdar://19391339 */
+			goto integer;
+#endif /* MPTCP */
+
+		case SO_EXTENDED_BK_IDLE:
+			optval = (so->so_flags1 & SOF1_EXTEND_BK_IDLE_WANTED);
+			goto integer;
+
+		default:
+			error = ENOPROTOOPT;
+			break;
+		}
+	}
+out:
+	if (dolock)
+		socket_unlock(so, 1);
+	return (error);
+}
+
+/*
+ * The size limits on our soopt_getm is different from that on FreeBSD.
+ * We limit the size of options to MCLBYTES. This will have to change
+ * if we need to define options that need more space than MCLBYTES.
+ */
+int
+soopt_getm(struct sockopt *sopt, struct mbuf **mp)
+{
+	struct mbuf *m, *m_prev;
+	int sopt_size = sopt->sopt_valsize;
+	int how;
+
+	if (sopt_size <= 0 || sopt_size > MCLBYTES)
+		return (EMSGSIZE);
+
+	how = sopt->sopt_p != kernproc ? M_WAIT : M_DONTWAIT;
+	MGET(m, how, MT_DATA);
+	if (m == NULL)
+		return (ENOBUFS);
+	if (sopt_size > MLEN) {
+		MCLGET(m, how);
+		if ((m->m_flags & M_EXT) == 0) {
+			m_free(m);
+			return (ENOBUFS);
+		}
+		m->m_len = min(MCLBYTES, sopt_size);
+	} else {
+		m->m_len = min(MLEN, sopt_size);
+	}
+	sopt_size -= m->m_len;
+	*mp = m;
+	m_prev = m;
+
+	while (sopt_size > 0) {
+		MGET(m, how, MT_DATA);
+		if (m == NULL) {
+			m_freem(*mp);
+			return (ENOBUFS);
+		}
+		if (sopt_size > MLEN) {
+			MCLGET(m, how);
+			if ((m->m_flags & M_EXT) == 0) {
+				m_freem(*mp);
+				m_freem(m);
+				return (ENOBUFS);
+			}
+			m->m_len = min(MCLBYTES, sopt_size);
+		} else {
+			m->m_len = min(MLEN, sopt_size);
+		}
+		sopt_size -= m->m_len;
+		m_prev->m_next = m;
+		m_prev = m;
+	}
+	return (0);
+}
+
+/* copyin sopt data into mbuf chain */
+int
+soopt_mcopyin(struct sockopt *sopt, struct mbuf *m)
+{
+	struct mbuf *m0 = m;
+
+	if (sopt->sopt_val == USER_ADDR_NULL)
+		return (0);
+	while (m != NULL && sopt->sopt_valsize >= m->m_len) {
+		if (sopt->sopt_p != kernproc) {
+			int error;
+
+			error = copyin(sopt->sopt_val, mtod(m, char *),
+			    m->m_len);
+			if (error != 0) {
+				m_freem(m0);
+				return (error);
+			}
+		} else {
+			bcopy(CAST_DOWN(caddr_t, sopt->sopt_val),
+			    mtod(m, char *), m->m_len);
+		}
+		sopt->sopt_valsize -= m->m_len;
+		sopt->sopt_val += m->m_len;
+		m = m->m_next;
+	}
+	/* should be allocated enoughly at ip6_sooptmcopyin() */
+	if (m != NULL) {
+		panic("soopt_mcopyin");
+		/* NOTREACHED */
+	}
+	return (0);
+}
+
+/* copyout mbuf chain data into soopt */
+int
+soopt_mcopyout(struct sockopt *sopt, struct mbuf *m)
+{
+	struct mbuf *m0 = m;
+	size_t valsize = 0;
+
+	if (sopt->sopt_val == USER_ADDR_NULL)
+		return (0);
+	while (m != NULL && sopt->sopt_valsize >= m->m_len) {
+		if (sopt->sopt_p != kernproc) {
+			int error;
+
+			error = copyout(mtod(m, char *), sopt->sopt_val,
+			    m->m_len);
+			if (error != 0) {
+				m_freem(m0);
+				return (error);
+			}
+		} else {
+			bcopy(mtod(m, char *),
+			    CAST_DOWN(caddr_t, sopt->sopt_val), m->m_len);
+		}
+		sopt->sopt_valsize -= m->m_len;
+		sopt->sopt_val += m->m_len;
+		valsize += m->m_len;
+		m = m->m_next;
+	}
+	if (m != NULL) {
+		/* enough soopt buffer should be given from user-land */
+		m_freem(m0);
+		return (EINVAL);
+	}
+	sopt->sopt_valsize = valsize;
+	return (0);
+}
+
+void
+sohasoutofband(struct socket *so)
+{
+	if (so->so_pgid < 0)
+		gsignal(-so->so_pgid, SIGURG);
+	else if (so->so_pgid > 0)
+		proc_signal(so->so_pgid, SIGURG);
+	selwakeup(&so->so_rcv.sb_sel);
+}
+
+int
+sopoll(struct socket *so, int events, kauth_cred_t cred, void * wql)
+{
+#pragma unused(cred)
+	struct proc *p = current_proc();
+	int revents = 0;
+
+	socket_lock(so, 1);
+	so_update_last_owner_locked(so, PROC_NULL);
+	so_update_policy(so);
+
+	if (events & (POLLIN | POLLRDNORM))
+		if (soreadable(so))
+			revents |= events & (POLLIN | POLLRDNORM);
+
+	if (events & (POLLOUT | POLLWRNORM))
+		if (sowriteable(so))
+			revents |= events & (POLLOUT | POLLWRNORM);
+
+	if (events & (POLLPRI | POLLRDBAND))
+		if (so->so_oobmark || (so->so_state & SS_RCVATMARK))
+			revents |= events & (POLLPRI | POLLRDBAND);
+
+	if (revents == 0) {
+		if (events & (POLLIN | POLLPRI | POLLRDNORM | POLLRDBAND)) {
+			/*
+			 * Darwin sets the flag first,
+			 * BSD calls selrecord first
+			 */
+			so->so_rcv.sb_flags |= SB_SEL;
+			selrecord(p, &so->so_rcv.sb_sel, wql);
+		}
+
+		if (events & (POLLOUT | POLLWRNORM)) {
+			/*
+			 * Darwin sets the flag first,
+			 * BSD calls selrecord first
+			 */
+			so->so_snd.sb_flags |= SB_SEL;
+			selrecord(p, &so->so_snd.sb_sel, wql);
+		}
+	}
+
+	socket_unlock(so, 1);
+	return (revents);
+}
+
+int
+soo_kqfilter(struct fileproc *fp, struct knote *kn, vfs_context_t ctx)
+{
+#pragma unused(fp)
+#if !CONFIG_MACF_SOCKET
+#pragma unused(ctx)
+#endif /* MAC_SOCKET */
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+	struct klist *skl;
+
+	socket_lock(so, 1);
+	so_update_last_owner_locked(so, PROC_NULL);
+	so_update_policy(so);
+
+#if CONFIG_MACF_SOCKET
+	if (mac_socket_check_kqfilter(proc_ucred(vfs_context_proc(ctx)),
+	    kn, so) != 0) {
+		socket_unlock(so, 1);
+		return (1);
+	}
+#endif /* MAC_SOCKET */
+
+	switch (kn->kn_filter) {
+	case EVFILT_READ:
+		kn->kn_fop = &soread_filtops;
+		/*
+		 * If the caller explicitly asked for OOB results (e.g. poll()),
+		 * save that off in the hookid field and reserve the kn_flags
+		 * EV_OOBAND bit for output only.
+		 */
+		if (kn->kn_flags & EV_OOBAND) {
+			kn->kn_flags &= ~EV_OOBAND;
+			kn->kn_hookid = EV_OOBAND;
+		} else {
+			kn->kn_hookid = 0;
+		}
+		skl = &so->so_rcv.sb_sel.si_note;
+		break;
+	case EVFILT_WRITE:
+		kn->kn_fop = &sowrite_filtops;
+		skl = &so->so_snd.sb_sel.si_note;
+		break;
+	case EVFILT_SOCK:
+		kn->kn_fop = &sock_filtops;
+		skl = &so->so_klist;
+		kn->kn_hookid = 0;
+		kn->kn_status |= KN_TOUCH;
+		break;
+	default:
+		socket_unlock(so, 1);
+		return (1);
+	}
+
+	if (KNOTE_ATTACH(skl, kn)) {
+		switch (kn->kn_filter) {
+		case EVFILT_READ:
+			so->so_rcv.sb_flags |= SB_KNOTE;
+			break;
+		case EVFILT_WRITE:
+			so->so_snd.sb_flags |= SB_KNOTE;
+			break;
+		case EVFILT_SOCK:
+			so->so_flags |= SOF_KNOTE;
+			break;
+		default:
+			socket_unlock(so, 1);
+			return (1);
+		}
+	}
+	socket_unlock(so, 1);
+	return (0);
+}
+
+static void
+filt_sordetach(struct knote *kn)
+{
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+
+	socket_lock(so, 1);
+	if (so->so_rcv.sb_flags & SB_KNOTE)
+		if (KNOTE_DETACH(&so->so_rcv.sb_sel.si_note, kn))
+			so->so_rcv.sb_flags &= ~SB_KNOTE;
+	socket_unlock(so, 1);
+}
+
+/*ARGSUSED*/
+static int
+filt_soread(struct knote *kn, long hint)
+{
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+
+	if ((hint & SO_FILT_HINT_LOCKED) == 0)
+		socket_lock(so, 1);
+
+	if (so->so_options & SO_ACCEPTCONN) {
+		int isempty;
+
+		/*
+		 * Radar 6615193 handle the listen case dynamically
+		 * for kqueue read filter. This allows to call listen()
+		 * after registering the kqueue EVFILT_READ.
+		 */
+
+		kn->kn_data = so->so_qlen;
+		isempty = ! TAILQ_EMPTY(&so->so_comp);
+
+		if ((hint & SO_FILT_HINT_LOCKED) == 0)
+			socket_unlock(so, 1);
+
+		return (isempty);
+	}
+
+	/* socket isn't a listener */
+	/*
+	 * NOTE_LOWAT specifies new low water mark in data, i.e.
+	 * the bytes of protocol data. We therefore exclude any
+	 * control bytes.
+	 */
+	kn->kn_data = so->so_rcv.sb_cc - so->so_rcv.sb_ctl;
+
+	/*
+	 * Clear out EV_OOBAND that filt_soread may have set in the
+	 * past.
+	 */
+	kn->kn_flags &= ~EV_OOBAND;
+	if ((so->so_oobmark) || (so->so_state & SS_RCVATMARK)) {
+		kn->kn_flags |= EV_OOBAND;
+		/*
+		 * If caller registered explicit interest in OOB data,
+		 * return immediately (data == amount beyond mark, for
+		 * legacy reasons - that should be changed later).
+		 */
+		if (kn->kn_hookid == EV_OOBAND) {
+			/*
+			 * When so_state is SS_RCVATMARK, so_oobmark
+			 * is 0.
+			 */
+			kn->kn_data -= so->so_oobmark;
+			if ((hint & SO_FILT_HINT_LOCKED) == 0)
+				socket_unlock(so, 1);
+			return (1);
+		}
+	}
+
+	if ((so->so_state & SS_CANTRCVMORE)
+#if CONTENT_FILTER
+	    && cfil_sock_data_pending(&so->so_rcv) == 0
+#endif /* CONTENT_FILTER */
+	   ) {
+		kn->kn_flags |= EV_EOF;
+		kn->kn_fflags = so->so_error;
+		if ((hint & SO_FILT_HINT_LOCKED) == 0)
+			socket_unlock(so, 1);
+		return (1);
+	}
+
+	if (so->so_error) {	/* temporary udp error */
+		if ((hint & SO_FILT_HINT_LOCKED) == 0)
+			socket_unlock(so, 1);
+		return (1);
+	}
+
+	int64_t	lowwat = so->so_rcv.sb_lowat;
+	/*
+	 * Ensure that when NOTE_LOWAT is used, the derived
+	 * low water mark is bounded by socket's rcv buf's
+	 * high and low water mark values.
+	 */
+	if (kn->kn_sfflags & NOTE_LOWAT) {
+		if (kn->kn_sdata > so->so_rcv.sb_hiwat)
+			lowwat = so->so_rcv.sb_hiwat;
+		else if (kn->kn_sdata > lowwat)
+			lowwat = kn->kn_sdata;
+	}
+
+	if ((hint & SO_FILT_HINT_LOCKED) == 0)
+		socket_unlock(so, 1);
+
+	/*
+	 * The order below is important. Since NOTE_LOWAT
+	 * overrides sb_lowat, check for NOTE_LOWAT case
+	 * first.
+	 */
+	if (kn->kn_sfflags & NOTE_LOWAT)
+		return (kn->kn_data >= lowwat);
+
+	return (so->so_rcv.sb_cc >= lowwat);
+}
+
+static void
+filt_sowdetach(struct knote *kn)
+{
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+	socket_lock(so, 1);
+
+	if (so->so_snd.sb_flags & SB_KNOTE)
+		if (KNOTE_DETACH(&so->so_snd.sb_sel.si_note, kn))
+			so->so_snd.sb_flags &= ~SB_KNOTE;
+	socket_unlock(so, 1);
+}
+
+int
+so_wait_for_if_feedback(struct socket *so)
+{
+	if ((SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6) &&
+	    (so->so_state & SS_ISCONNECTED)) {
+		struct inpcb *inp = sotoinpcb(so);
+		if (INP_WAIT_FOR_IF_FEEDBACK(inp))
+			return (1);
+	}
+	return (0);
+}
+
+/*ARGSUSED*/
+static int
+filt_sowrite(struct knote *kn, long hint)
+{
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+	int ret = 0;
+
+	if ((hint & SO_FILT_HINT_LOCKED) == 0)
+		socket_lock(so, 1);
+
+	kn->kn_data = sbspace(&so->so_snd);
+	if (so->so_state & SS_CANTSENDMORE) {
+		kn->kn_flags |= EV_EOF;
+		kn->kn_fflags = so->so_error;
+		ret = 1;
+		goto out;
+	}
+	if (so->so_error) {	/* temporary udp error */
+		ret = 1;
+		goto out;
+	}
+	if (!socanwrite(so)) {
+		ret = 0;
+		goto out;
+	}
+	if (so->so_flags1 & SOF1_PRECONNECT_DATA) {
+		ret = 1;
+		goto out;
+	}
+	int64_t	lowwat = so->so_snd.sb_lowat;
+	if (kn->kn_sfflags & NOTE_LOWAT) {
+		if (kn->kn_sdata > so->so_snd.sb_hiwat)
+			lowwat = so->so_snd.sb_hiwat;
+		else if (kn->kn_sdata > lowwat)
+			lowwat = kn->kn_sdata;
+	}
+	if (kn->kn_data >= lowwat) {
+		if (so->so_flags & SOF_NOTSENT_LOWAT) {
+			if ((SOCK_DOM(so) == PF_INET
+			    || SOCK_DOM(so) == PF_INET6)
+			    && so->so_type == SOCK_STREAM) {
+				ret = tcp_notsent_lowat_check(so);
+			}
+#if MPTCP
+			else if ((SOCK_DOM(so) == PF_MULTIPATH) &&
+			    (SOCK_PROTO(so) == IPPROTO_TCP)) {
+				ret = mptcp_notsent_lowat_check(so);
+			}
+#endif
+			else {
+				ret = 1;
+				goto out;
+			}
+		} else {
+			ret = 1;
+		}
+	}
+	if (so_wait_for_if_feedback(so))
+		ret = 0;
+out:
+	if ((hint & SO_FILT_HINT_LOCKED) == 0)
+		socket_unlock(so, 1);
+	return (ret);
+}
+
+static void
+filt_sockdetach(struct knote *kn)
+{
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+	socket_lock(so, 1);
+
+	if ((so->so_flags & SOF_KNOTE) != 0)
+		if (KNOTE_DETACH(&so->so_klist, kn))
+			so->so_flags &= ~SOF_KNOTE;
+	socket_unlock(so, 1);
+}
+
+static int
+filt_sockev(struct knote *kn, long hint)
+{
+	int ret = 0, locked = 0;
+	struct socket *so = (struct socket *)kn->kn_fp->f_fglob->fg_data;
+	long ev_hint = (hint & SO_FILT_HINT_EV);
+	uint32_t level_trigger = 0;
+
+	if ((hint & SO_FILT_HINT_LOCKED) == 0) {
+		socket_lock(so, 1);
+		locked = 1;
+	}
+
+	if (ev_hint & SO_FILT_HINT_CONNRESET) {
+		kn->kn_fflags |= NOTE_CONNRESET;
+	}
+	if (ev_hint & SO_FILT_HINT_TIMEOUT) {
+		kn->kn_fflags |= NOTE_TIMEOUT;
+	}
+	if (ev_hint & SO_FILT_HINT_NOSRCADDR) {
+		kn->kn_fflags |= NOTE_NOSRCADDR;
+	}
+	if (ev_hint & SO_FILT_HINT_IFDENIED) {
+		kn->kn_fflags |= NOTE_IFDENIED;
+	}
+	if (ev_hint & SO_FILT_HINT_KEEPALIVE) {
+		kn->kn_fflags |= NOTE_KEEPALIVE;
+	}
+	if (ev_hint & SO_FILT_HINT_ADAPTIVE_WTIMO) {
+		kn->kn_fflags |= NOTE_ADAPTIVE_WTIMO;
+	}
+	if (ev_hint & SO_FILT_HINT_ADAPTIVE_RTIMO) {
+		kn->kn_fflags |= NOTE_ADAPTIVE_RTIMO;
+	}
+	if ((ev_hint & SO_FILT_HINT_CONNECTED) ||
+	    (so->so_state & SS_ISCONNECTED)) {
+		kn->kn_fflags |= NOTE_CONNECTED;
+		level_trigger |= NOTE_CONNECTED;
+	}
+	if ((ev_hint & SO_FILT_HINT_DISCONNECTED) ||
+	    (so->so_state & SS_ISDISCONNECTED)) {
+		kn->kn_fflags |= NOTE_DISCONNECTED;
+		level_trigger |= NOTE_DISCONNECTED;
+	}
+	if (ev_hint & SO_FILT_HINT_CONNINFO_UPDATED) {
+		if (so->so_proto != NULL &&
+		    (so->so_proto->pr_flags & PR_EVCONNINFO))
+			kn->kn_fflags |= NOTE_CONNINFO_UPDATED;
+	}
+
+	if ((so->so_state & SS_CANTRCVMORE)
+#if CONTENT_FILTER
+	    && cfil_sock_data_pending(&so->so_rcv) == 0
+#endif /* CONTENT_FILTER */
+	    ) {
+		kn->kn_fflags |= NOTE_READCLOSED;
+		level_trigger |= NOTE_READCLOSED;
+	}
+
+	if (so->so_state & SS_CANTSENDMORE) {
+		kn->kn_fflags |= NOTE_WRITECLOSED;
+		level_trigger |= NOTE_WRITECLOSED;
+	}
+
+	if ((ev_hint & SO_FILT_HINT_SUSPEND) ||
+	    (so->so_flags & SOF_SUSPENDED)) {
+		kn->kn_fflags &= ~(NOTE_SUSPEND | NOTE_RESUME);
+
+		/* If resume event was delivered before, reset it */
+		kn->kn_hookid &= ~NOTE_RESUME;
+
+		kn->kn_fflags |= NOTE_SUSPEND;
+		level_trigger |= NOTE_SUSPEND;
+	}
+
+	if ((ev_hint & SO_FILT_HINT_RESUME) ||
+	    (so->so_flags & SOF_SUSPENDED) == 0) {
+		kn->kn_fflags &= ~(NOTE_SUSPEND | NOTE_RESUME);
+
+		/* If suspend event was delivered before, reset it */
+		kn->kn_hookid &= ~NOTE_SUSPEND;
+
+		kn->kn_fflags |= NOTE_RESUME;
+		level_trigger |= NOTE_RESUME;
+	}
+
+	if (so->so_error != 0) {
+		ret = 1;
+		kn->kn_data = so->so_error;
+		kn->kn_flags |= EV_EOF;
+	} else {
+		get_sockev_state(so, (u_int32_t *)&(kn->kn_data));
+	}
+
+	/* Reset any events that are not requested on this knote */
+	kn->kn_fflags &= (kn->kn_sfflags & EVFILT_SOCK_ALL_MASK);
+	level_trigger &= (kn->kn_sfflags & EVFILT_SOCK_ALL_MASK);
+
+	/* Find the level triggerred events that are already delivered */
+	level_trigger &= kn->kn_hookid;
+	level_trigger &= EVFILT_SOCK_LEVEL_TRIGGER_MASK;
+
+	/* Do not deliver level triggerred events more than once */
+	if ((kn->kn_fflags & ~level_trigger) != 0)
+		ret = 1;
+
+	if (locked)
+		socket_unlock(so, 1);
+
+	return (ret);
+}
+
+static void
+filt_socktouch(struct knote *kn, struct kevent_internal_s *kev, long type)
+{
+#pragma unused(kev)
+	switch (type) {
+	case EVENT_REGISTER:
+	{
+		uint32_t changed_flags;
+		changed_flags = (kn->kn_sfflags ^ kn->kn_hookid);
+
+		/*
+		 * Since we keep track of events that are already
+		 * delivered, if any of those events are not requested
+		 * anymore the state related to them can be reset
+		 */
+		kn->kn_hookid &=
+		    ~(changed_flags & EVFILT_SOCK_LEVEL_TRIGGER_MASK);
+		break;
+	}
+	case EVENT_PROCESS:
+		/*
+		 * Store the state of the events being delivered. This
+		 * state can be used to deliver level triggered events
+		 * ateast once and still avoid waking up the application
+		 * multiple times as long as the event is active.
+		 */
+		if (kn->kn_fflags != 0)
+			kn->kn_hookid |= (kn->kn_fflags &
+				EVFILT_SOCK_LEVEL_TRIGGER_MASK);
+
+		/*
+		 * NOTE_RESUME and NOTE_SUSPEND are an exception, deliver
+		 * only one of them and remember the last one that was
+		 * delivered last
+		 */
+		if (kn->kn_fflags & NOTE_SUSPEND)
+			kn->kn_hookid &= ~NOTE_RESUME;
+		if (kn->kn_fflags & NOTE_RESUME)
+			kn->kn_hookid &= ~NOTE_SUSPEND;
+		break;
+	default:
+		break;
+	}
+}
+
+void
+get_sockev_state(struct socket *so, u_int32_t *statep)
+{
+	u_int32_t state = *(statep);
+
+	if (so->so_state & SS_ISCONNECTED)
+		state |= SOCKEV_CONNECTED;
+	else
+		state &= ~(SOCKEV_CONNECTED);
+	state |= ((so->so_state & SS_ISDISCONNECTED) ? SOCKEV_DISCONNECTED : 0);
+	*(statep) = state;
+}
+
+#define	SO_LOCK_HISTORY_STR_LEN \
+	(2 * SO_LCKDBG_MAX * (2 + (2 * sizeof (void *)) + 1) + 1)
+
+__private_extern__ const char *
+solockhistory_nr(struct socket *so)
+{
+	size_t n = 0;
+	int i;
+	static char lock_history_str[SO_LOCK_HISTORY_STR_LEN];
+
+	bzero(lock_history_str, sizeof (lock_history_str));
+	for (i = SO_LCKDBG_MAX - 1; i >= 0; i--) {
+		n += snprintf(lock_history_str + n,
+		    SO_LOCK_HISTORY_STR_LEN - n, "%p:%p ",
+		    so->lock_lr[(so->next_lock_lr + i) % SO_LCKDBG_MAX],
+		    so->unlock_lr[(so->next_unlock_lr + i) % SO_LCKDBG_MAX]);
+	}
+	return (lock_history_str);
+}
+
+int
+socket_lock(struct socket *so, int refcount)
+{
+	int error = 0;
+	void *lr_saved;
+
+	lr_saved = __builtin_return_address(0);
+
+	if (so->so_proto->pr_lock) {
+		error = (*so->so_proto->pr_lock)(so, refcount, lr_saved);
+	} else {
+#ifdef MORE_LOCKING_DEBUG
+		lck_mtx_assert(so->so_proto->pr_domain->dom_mtx,
+		    LCK_MTX_ASSERT_NOTOWNED);
+#endif
+		lck_mtx_lock(so->so_proto->pr_domain->dom_mtx);
+		if (refcount)
+			so->so_usecount++;
+		so->lock_lr[so->next_lock_lr] = lr_saved;
+		so->next_lock_lr = (so->next_lock_lr+1) % SO_LCKDBG_MAX;
+	}
+
+	return (error);
+}
+
+int
+socket_unlock(struct socket *so, int refcount)
+{
+	int error = 0;
+	void *lr_saved;
+	lck_mtx_t *mutex_held;
+
+	lr_saved = __builtin_return_address(0);
+
+	if (so->so_proto == NULL) {
+		panic("%s: null so_proto so=%p\n", __func__, so);
+		/* NOTREACHED */
+	}
+
+	if (so && so->so_proto->pr_unlock) {
+		error = (*so->so_proto->pr_unlock)(so, refcount, lr_saved);
+	} else {
+		mutex_held = so->so_proto->pr_domain->dom_mtx;
+#ifdef MORE_LOCKING_DEBUG
+		lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+#endif
+		so->unlock_lr[so->next_unlock_lr] = lr_saved;
+		so->next_unlock_lr = (so->next_unlock_lr+1) % SO_LCKDBG_MAX;
+
+		if (refcount) {
+			if (so->so_usecount <= 0) {
+				panic("%s: bad refcount=%d so=%p (%d, %d, %d) "
+				    "lrh=%s", __func__, so->so_usecount, so,
+				    SOCK_DOM(so), so->so_type,
+				    SOCK_PROTO(so), solockhistory_nr(so));
+				/* NOTREACHED */
+			}
+
+			so->so_usecount--;
+			if (so->so_usecount == 0)
+				sofreelastref(so, 1);
+		}
+		lck_mtx_unlock(mutex_held);
+	}
+
+	return (error);
+}
+
+/* Called with socket locked, will unlock socket */
+void
+sofree(struct socket *so)
+{
+	lck_mtx_t *mutex_held;
+
+	if (so->so_proto->pr_getlock != NULL)
+		mutex_held = (*so->so_proto->pr_getlock)(so, 0);
+	else
+		mutex_held = so->so_proto->pr_domain->dom_mtx;
+	lck_mtx_assert(mutex_held, LCK_MTX_ASSERT_OWNED);
+
+	sofreelastref(so, 0);
+}
+
+void
+soreference(struct socket *so)
+{
+	socket_lock(so, 1);	/* locks & take one reference on socket */
+	socket_unlock(so, 0);	/* unlock only */
+}
+
+void
+sodereference(struct socket *so)
+{
+	socket_lock(so, 0);
+	socket_unlock(so, 1);
+}
+
+/*
+ * Set or clear SOF_MULTIPAGES on the socket to enable or disable the
+ * possibility of using jumbo clusters.  Caller must ensure to hold
+ * the socket lock.
+ */
+void
+somultipages(struct socket *so, boolean_t set)
+{
+	if (set)
+		so->so_flags |= SOF_MULTIPAGES;
+	else
+		so->so_flags &= ~SOF_MULTIPAGES;
+}
+
+void
+soif2kcl(struct socket *so, boolean_t set)
+{
+	if (set)
+		so->so_flags1 |= SOF1_IF_2KCL;
+	else
+		so->so_flags1 &= ~SOF1_IF_2KCL;
+}
+
+int
+so_isdstlocal(struct socket *so) {
+
+	struct inpcb *inp = (struct inpcb *)so->so_pcb;
+
+	if (SOCK_DOM(so) == PF_INET)
+		return (inaddr_local(inp->inp_faddr));
+	else if (SOCK_DOM(so) == PF_INET6)
+		return (in6addr_local(&inp->in6p_faddr));
+
+	return (0);
+}
+
+int
+sosetdefunct(struct proc *p, struct socket *so, int level, boolean_t noforce)
+{
+	struct sockbuf *rcv, *snd;
+	int err = 0, defunct;
+
+	rcv = &so->so_rcv;
+	snd = &so->so_snd;
+
+	defunct = (so->so_flags & SOF_DEFUNCT);
+	if (defunct) {
+		if (!(snd->sb_flags & rcv->sb_flags & SB_DROP)) {
+			panic("%s: SB_DROP not set", __func__);
+			/* NOTREACHED */
+		}
+		goto done;
+	}
+
+	if (so->so_flags & SOF_NODEFUNCT) {
+		if (noforce) {
+			err = EOPNOTSUPP;
+			SODEFUNCTLOG(("%s[%d]: (target pid %d level %d) "
+			    "so 0x%llx [%d,%d] is not eligible for defunct "
+			    "(%d)\n", __func__, proc_selfpid(), proc_pid(p),
+			    level, (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+			    SOCK_DOM(so), SOCK_TYPE(so), err));
+			return (err);
+		}
+		so->so_flags &= ~SOF_NODEFUNCT;
+		SODEFUNCTLOG(("%s[%d]: (target pid %d level %d) so 0x%llx "
+		    "[%d,%d] defunct by force\n", __func__, proc_selfpid(),
+		    proc_pid(p), level, (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so)));
+	} else if (so->so_flags1 & SOF1_EXTEND_BK_IDLE_WANTED) {
+		struct inpcb *inp = (struct inpcb *)so->so_pcb;
+		struct ifnet *ifp = inp->inp_last_outifp;
+
+		if (ifp && IFNET_IS_CELLULAR(ifp)) {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_nocell);
+		} else if (so->so_flags & SOF_DELEGATED) {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_nodlgtd);
+		} else if (soextbkidlestat.so_xbkidle_time == 0) {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_notime);
+		} else if (noforce) {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_active);
+		
+			so->so_flags1 |= SOF1_EXTEND_BK_IDLE_INPROG;
+			so->so_extended_bk_start = net_uptime();
+			OSBitOrAtomic(P_LXBKIDLEINPROG, &p->p_ladvflag);
+			
+			inpcb_timer_sched(inp->inp_pcbinfo, INPCB_TIMER_LAZY);
+			
+			err = EOPNOTSUPP;
+			SODEFUNCTLOG(("%s[%d]: (target pid %d level %d) "
+			    "extend bk idle "
+			    "so 0x%llx rcv hw %d cc %d\n",
+			    __func__, proc_selfpid(), proc_pid(p),
+			    level, (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+			    so->so_rcv.sb_hiwat, so->so_rcv.sb_cc));
+			return (err);
+		} else {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_forced);
+		}
+	}
+
+	so->so_flags |= SOF_DEFUNCT;
+
+	/* Prevent further data from being appended to the socket buffers */
+	snd->sb_flags |= SB_DROP;
+	rcv->sb_flags |= SB_DROP;
+
+	/* Flush any existing data in the socket buffers */
+	if (rcv->sb_cc != 0) {
+		rcv->sb_flags &= ~SB_SEL;
+		selthreadclear(&rcv->sb_sel);
+		sbrelease(rcv);
+	}
+	if (snd->sb_cc != 0) {
+		snd->sb_flags &= ~SB_SEL;
+		selthreadclear(&snd->sb_sel);
+		sbrelease(snd);
+	}
+
+done:
+	SODEFUNCTLOG(("%s[%d]: (target pid %d level %d) so 0x%llx [%d,%d] %s "
+	    "defunct%s\n", __func__, proc_selfpid(), proc_pid(p), level,
+	    (uint64_t)DEBUG_KERNEL_ADDRPERM(so), SOCK_DOM(so), SOCK_TYPE(so),
+	    defunct ? "is already" : "marked as",
+	    (so->so_flags1 & SOF1_EXTEND_BK_IDLE_WANTED) ? " extbkidle" : ""));
+
+	return (err);
+}
+
+int
+sodefunct(struct proc *p, struct socket *so, int level)
+{
+	struct sockbuf *rcv, *snd;
+
+	if (!(so->so_flags & SOF_DEFUNCT)) {
+		panic("%s improperly called", __func__);
+		/* NOTREACHED */
+	}
+	if (so->so_state & SS_DEFUNCT)
+		goto done;
+
+	rcv = &so->so_rcv;
+	snd = &so->so_snd;
+
+	if (SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6) {
+		char s[MAX_IPv6_STR_LEN];
+		char d[MAX_IPv6_STR_LEN];
+		struct inpcb *inp = sotoinpcb(so);
+
+		SODEFUNCTLOG(("%s[%d]: (target pid %d level %d) so 0x%llx [%s "
+		    "%s:%d -> %s:%d] is now defunct [rcv_si 0x%x, snd_si 0x%x, "
+		    "rcv_fl 0x%x, snd_fl 0x%x]\n", __func__, proc_selfpid(),
+		    proc_pid(p), level, (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    (SOCK_TYPE(so) == SOCK_STREAM) ? "TCP" : "UDP",
+		    inet_ntop(SOCK_DOM(so), ((SOCK_DOM(so) == PF_INET) ?
+		    (void *)&inp->inp_laddr.s_addr : (void *)&inp->in6p_laddr),
+		    s, sizeof (s)), ntohs(inp->in6p_lport),
+		    inet_ntop(SOCK_DOM(so), (SOCK_DOM(so) == PF_INET) ?
+		    (void *)&inp->inp_faddr.s_addr : (void *)&inp->in6p_faddr,
+		    d, sizeof (d)), ntohs(inp->in6p_fport),
+		    (uint32_t)rcv->sb_sel.si_flags,
+		    (uint32_t)snd->sb_sel.si_flags,
+		    rcv->sb_flags, snd->sb_flags));
+	} else {
+		SODEFUNCTLOG(("%s[%d]: (target pid %d level %d) so 0x%llx "
+		    "[%d,%d] is now defunct [rcv_si 0x%x, snd_si 0x%x, "
+		    "rcv_fl 0x%x, snd_fl 0x%x]\n", __func__, proc_selfpid(),
+		    proc_pid(p), level, (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so), (uint32_t)rcv->sb_sel.si_flags,
+		    (uint32_t)snd->sb_sel.si_flags, rcv->sb_flags,
+		    snd->sb_flags));
+	}
+
+	/*
+	 * Unwedge threads blocked on sbwait() and sb_lock().
+	 */
+	sbwakeup(rcv);
+	sbwakeup(snd);
+
+	so->so_flags1 |= SOF1_DEFUNCTINPROG;
+	if (rcv->sb_flags & SB_LOCK)
+		sbunlock(rcv, TRUE);	/* keep socket locked */
+	if (snd->sb_flags & SB_LOCK)
+		sbunlock(snd, TRUE);	/* keep socket locked */
+
+	/*
+	 * Flush the buffers and disconnect.  We explicitly call shutdown
+	 * on both data directions to ensure that SS_CANT{RCV,SEND}MORE
+	 * states are set for the socket.  This would also flush out data
+	 * hanging off the receive list of this socket.
+	 */
+	(void) soshutdownlock_final(so, SHUT_RD);
+	(void) soshutdownlock_final(so, SHUT_WR);
+	(void) sodisconnectlocked(so);
+
+	/*
+	 * Explicitly handle connectionless-protocol disconnection
+	 * and release any remaining data in the socket buffers.
+	 */
+	if (!(so->so_flags & SS_ISDISCONNECTED))
+		(void) soisdisconnected(so);
+
+	if (so->so_error == 0)
+		so->so_error = EBADF;
+
+	if (rcv->sb_cc != 0) {
+		rcv->sb_flags &= ~SB_SEL;
+		selthreadclear(&rcv->sb_sel);
+		sbrelease(rcv);
+	}
+	if (snd->sb_cc != 0) {
+		snd->sb_flags &= ~SB_SEL;
+		selthreadclear(&snd->sb_sel);
+		sbrelease(snd);
+	}
+	so->so_state |= SS_DEFUNCT;
+
+done:
+	return (0);
+}
+
+int
+soresume(struct proc *p, struct socket *so, int locked)
+{
+	if (locked == 0)
+		socket_lock(so, 1);
+
+	if (so->so_flags1 & SOF1_EXTEND_BK_IDLE_INPROG) {
+		SODEFUNCTLOG(("%s[%d]: )target pid %d) so 0x%llx [%d,%d] "
+		    "resumed from bk idle\n",
+		    __func__, proc_selfpid(), proc_pid(p),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so)));
+
+		so->so_flags1 &= ~SOF1_EXTEND_BK_IDLE_INPROG;
+		so->so_extended_bk_start = 0;
+		OSBitAndAtomic(~P_LXBKIDLEINPROG, &p->p_ladvflag);
+
+		OSIncrementAtomic(&soextbkidlestat.so_xbkidle_resumed);
+		OSDecrementAtomic(&soextbkidlestat.so_xbkidle_active);
+		VERIFY(soextbkidlestat.so_xbkidle_active >= 0);
+	}
+	if (locked == 0)
+		socket_unlock(so, 1);
+
+	return (0);
+}
+
+/*
+ * Does not attempt to account for sockets that are delegated from
+ * the current process
+ */
+int
+so_set_extended_bk_idle(struct socket *so, int optval)
+{
+	int error = 0;
+
+	if ((SOCK_DOM(so) != PF_INET && SOCK_DOM(so) != PF_INET6) ||
+	    SOCK_PROTO(so) != IPPROTO_TCP) {
+		OSDecrementAtomic(&soextbkidlestat.so_xbkidle_notsupp);
+		error = EOPNOTSUPP;
+	} else if (optval == 0) {
+		so->so_flags1 &= ~SOF1_EXTEND_BK_IDLE_WANTED;
+
+		soresume(current_proc(), so, 1);
+	} else {
+		struct proc *p = current_proc();
+		int i;
+		struct filedesc *fdp;
+		int count = 0;
+
+		proc_fdlock(p);
+
+		fdp = p->p_fd;
+		for (i = 0; i < fdp->fd_nfiles; i++) {
+			struct fileproc *fp = fdp->fd_ofiles[i];
+			struct socket *so2;
+
+			if (fp == NULL ||
+			    (fdp->fd_ofileflags[i] & UF_RESERVED) != 0 ||
+			    FILEGLOB_DTYPE(fp->f_fglob) != DTYPE_SOCKET)
+				continue;
+
+			so2 = (struct socket *)fp->f_fglob->fg_data;
+			if (so != so2 &&
+			    so2->so_flags1 & SOF1_EXTEND_BK_IDLE_WANTED)
+				count++;
+			if (count >= soextbkidlestat.so_xbkidle_maxperproc)
+				break;
+		}
+		if (count >= soextbkidlestat.so_xbkidle_maxperproc) {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_toomany);
+			error = EBUSY;
+		} else if (so->so_flags & SOF_DELEGATED) {
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_nodlgtd);
+			error = EBUSY;
+		} else {
+			so->so_flags1 |= SOF1_EXTEND_BK_IDLE_WANTED;
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_wantok);
+		}
+		SODEFUNCTLOG(("%s[%d]: so 0x%llx [%d,%d] "
+		    "%s marked for extended bk idle\n",
+		    __func__, proc_selfpid(),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so),
+		    (so->so_flags1 & SOF1_EXTEND_BK_IDLE_WANTED) ?
+		    "is" : "not"));
+
+		proc_fdunlock(p);
+	}
+
+	return (error);
+}
+
+static void
+so_stop_extended_bk_idle(struct socket *so)
+{
+	so->so_flags1 &= ~SOF1_EXTEND_BK_IDLE_INPROG;
+	so->so_extended_bk_start = 0;
+
+	OSDecrementAtomic(&soextbkidlestat.so_xbkidle_active);
+	VERIFY(soextbkidlestat.so_xbkidle_active >= 0);
+	/*
+	 * Force defunct
+	 */
+	sosetdefunct(current_proc(), so,
+	    SHUTDOWN_SOCKET_LEVEL_DISCONNECT_INTERNAL, FALSE);
+	if (so->so_flags & SOF_DEFUNCT) {
+		sodefunct(current_proc(), so,
+		    SHUTDOWN_SOCKET_LEVEL_DISCONNECT_INTERNAL);
+	}
+}
+
+void
+so_drain_extended_bk_idle(struct socket *so)
+{
+	if (so && (so->so_flags1 & SOF1_EXTEND_BK_IDLE_INPROG)) {
+		/*
+		 * Only penalize sockets that have outstanding data
+		 */
+		if (so->so_rcv.sb_cc || so->so_snd.sb_cc) {
+			so_stop_extended_bk_idle(so);
+
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_drained);
+		}
+	}
+}
+
+/*
+ * Return values tells if socket is still in extended background idle
+ */
+int
+so_check_extended_bk_idle_time(struct socket *so)
+{
+	int ret = 1;
+
+	if ((so->so_flags1 & SOF1_EXTEND_BK_IDLE_INPROG)) {
+		SODEFUNCTLOG(("%s[%d]: so 0x%llx [%d,%d]\n",
+		    __func__, proc_selfpid(),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so)));
+		if (net_uptime() - so->so_extended_bk_start >
+		    soextbkidlestat.so_xbkidle_time) {
+			so_stop_extended_bk_idle(so);
+
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_expired);
+
+			ret = 0;
+		} else {
+			struct inpcb *inp = (struct inpcb *)so->so_pcb;
+
+			inpcb_timer_sched(inp->inp_pcbinfo, INPCB_TIMER_LAZY);
+			OSIncrementAtomic(&soextbkidlestat.so_xbkidle_resched);
+		}
+	}
+	
+	return (ret);
+}
+
+void
+resume_proc_sockets(proc_t p)
+{
+	if (p->p_ladvflag & P_LXBKIDLEINPROG) {
+		struct filedesc	*fdp;
+		int i;
+
+		proc_fdlock(p);
+		fdp = p->p_fd;
+		for (i = 0; i < fdp->fd_nfiles; i++) {
+			struct fileproc	*fp;
+			struct socket *so;
+
+			fp = fdp->fd_ofiles[i];
+			if (fp == NULL || 
+			    (fdp->fd_ofileflags[i] & UF_RESERVED) != 0 ||
+			    FILEGLOB_DTYPE(fp->f_fglob) != DTYPE_SOCKET)
+				continue;
+
+			so = (struct socket *)fp->f_fglob->fg_data;
+			(void) soresume(p, so, 0);
+		}
+		proc_fdunlock(p);
+
+		OSBitAndAtomic(~P_LXBKIDLEINPROG, &p->p_ladvflag);
+	}
+}
+
+__private_extern__ int
+so_set_recv_anyif(struct socket *so, int optval)
+{
+	int ret = 0;
+
+#if INET6
+	if (SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6) {
+#else
+	if (SOCK_DOM(so) == PF_INET) {
+#endif /* !INET6 */
+		if (optval)
+			sotoinpcb(so)->inp_flags |= INP_RECV_ANYIF;
+		else
+			sotoinpcb(so)->inp_flags &= ~INP_RECV_ANYIF;
+	}
+
+	return (ret);
+}
+
+__private_extern__ int
+so_get_recv_anyif(struct socket *so)
+{
+	int ret = 0;
+
+#if INET6
+	if (SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6) {
+#else
+	if (SOCK_DOM(so) == PF_INET) {
+#endif /* !INET6 */
+		ret = (sotoinpcb(so)->inp_flags & INP_RECV_ANYIF) ? 1 : 0;
+	}
+
+	return (ret);
+}
+
+int
+so_set_restrictions(struct socket *so, uint32_t vals)
+{
+	int nocell_old, nocell_new;
+	int noexpensive_old, noexpensive_new;
+
+	/*
+	 * Deny-type restrictions are trapdoors; once set they cannot be
+	 * unset for the lifetime of the socket.  This allows them to be
+	 * issued by a framework on behalf of the application without
+	 * having to worry that they can be undone.
+	 *
+	 * Note here that socket-level restrictions overrides any protocol
+	 * level restrictions.  For instance, SO_RESTRICT_DENY_CELLULAR
+	 * socket restriction issued on the socket has a higher precendence
+	 * than INP_NO_IFT_CELLULAR.  The latter is affected by the UUID
+	 * policy PROC_UUID_NO_CELLULAR for unrestricted sockets only,
+	 * i.e. when SO_RESTRICT_DENY_CELLULAR has not been issued.
+	 */
+	nocell_old = (so->so_restrictions & SO_RESTRICT_DENY_CELLULAR);
+	noexpensive_old = (so->so_restrictions & SO_RESTRICT_DENY_EXPENSIVE);
+	so->so_restrictions |= (vals & (SO_RESTRICT_DENY_IN |
+	    SO_RESTRICT_DENY_OUT | SO_RESTRICT_DENY_CELLULAR |
+	    SO_RESTRICT_DENY_EXPENSIVE));
+	nocell_new = (so->so_restrictions & SO_RESTRICT_DENY_CELLULAR);
+	noexpensive_new = (so->so_restrictions & SO_RESTRICT_DENY_EXPENSIVE);
+
+	/* we can only set, not clear restrictions */
+	if ((nocell_new - nocell_old) == 0 &&
+	    (noexpensive_new - noexpensive_old) == 0)
+		return (0);
+#if INET6
+	if (SOCK_DOM(so) == PF_INET || SOCK_DOM(so) == PF_INET6) {
+#else
+	if (SOCK_DOM(so) == PF_INET) {
+#endif /* !INET6 */
+		if (nocell_new - nocell_old != 0) {
+			/*
+			 * if deny cellular is now set, do what's needed
+			 * for INPCB
+			 */
+			inp_set_nocellular(sotoinpcb(so));
+		}
+		if (noexpensive_new - noexpensive_old != 0) {
+			inp_set_noexpensive(sotoinpcb(so));
+		}
+	}
+
+	return (0);
+}
+
+uint32_t
+so_get_restrictions(struct socket *so)
+{
+	return (so->so_restrictions & (SO_RESTRICT_DENY_IN |
+	    SO_RESTRICT_DENY_OUT |
+	    SO_RESTRICT_DENY_CELLULAR | SO_RESTRICT_DENY_EXPENSIVE));
+}
+
+struct sockaddr_entry *
+sockaddrentry_alloc(int how)
+{
+	struct sockaddr_entry *se;
+
+	se = (how == M_WAITOK) ? zalloc(se_zone) : zalloc_noblock(se_zone);
+	if (se != NULL)
+		bzero(se, se_zone_size);
+
+	return (se);
+}
+
+void
+sockaddrentry_free(struct sockaddr_entry *se)
+{
+	if (se->se_addr != NULL) {
+		FREE(se->se_addr, M_SONAME);
+		se->se_addr = NULL;
+	}
+	zfree(se_zone, se);
+}
+
+struct sockaddr_entry *
+sockaddrentry_dup(const struct sockaddr_entry *src_se, int how)
+{
+	struct sockaddr_entry *dst_se;
+
+	dst_se = sockaddrentry_alloc(how);
+	if (dst_se != NULL) {
+		int len = src_se->se_addr->sa_len;
+
+		MALLOC(dst_se->se_addr, struct sockaddr *,
+		    len, M_SONAME, how | M_ZERO);
+		if (dst_se->se_addr != NULL) {
+			bcopy(src_se->se_addr, dst_se->se_addr, len);
+		} else {
+			sockaddrentry_free(dst_se);
+			dst_se = NULL;
+		}
+	}
+
+	return (dst_se);
+}
+
+struct sockaddr_list *
+sockaddrlist_alloc(int how)
+{
+	struct sockaddr_list *sl;
+
+	sl = (how == M_WAITOK) ? zalloc(sl_zone) : zalloc_noblock(sl_zone);
+	if (sl != NULL) {
+		bzero(sl, sl_zone_size);
+		TAILQ_INIT(&sl->sl_head);
+	}
+	return (sl);
+}
+
+void
+sockaddrlist_free(struct sockaddr_list *sl)
+{
+	struct sockaddr_entry *se, *tse;
+
+	TAILQ_FOREACH_SAFE(se, &sl->sl_head, se_link, tse) {
+		sockaddrlist_remove(sl, se);
+		sockaddrentry_free(se);
+	}
+	VERIFY(sl->sl_cnt == 0 && TAILQ_EMPTY(&sl->sl_head));
+	zfree(sl_zone, sl);
+}
+
+void
+sockaddrlist_insert(struct sockaddr_list *sl, struct sockaddr_entry *se)
+{
+	VERIFY(!(se->se_flags & SEF_ATTACHED));
+	se->se_flags |= SEF_ATTACHED;
+	TAILQ_INSERT_TAIL(&sl->sl_head, se, se_link);
+	sl->sl_cnt++;
+	VERIFY(sl->sl_cnt != 0);
+}
+
+void
+sockaddrlist_remove(struct sockaddr_list *sl, struct sockaddr_entry *se)
+{
+	VERIFY(se->se_flags & SEF_ATTACHED);
+	se->se_flags &= ~SEF_ATTACHED;
+	VERIFY(sl->sl_cnt != 0);
+	sl->sl_cnt--;
+	TAILQ_REMOVE(&sl->sl_head, se, se_link);
+}
+
+struct sockaddr_list *
+sockaddrlist_dup(const struct sockaddr_list *src_sl, int how)
+{
+	struct sockaddr_entry *src_se, *tse;
+	struct sockaddr_list *dst_sl;
+
+	dst_sl = sockaddrlist_alloc(how);
+	if (dst_sl == NULL)
+		return (NULL);
+
+	TAILQ_FOREACH_SAFE(src_se, &src_sl->sl_head, se_link, tse) {
+		struct sockaddr_entry *dst_se;
+
+		if (src_se->se_addr == NULL)
+			continue;
+
+		dst_se = sockaddrentry_dup(src_se, how);
+		if (dst_se == NULL) {
+			sockaddrlist_free(dst_sl);
+			return (NULL);
+		}
+
+		sockaddrlist_insert(dst_sl, dst_se);
+	}
+	VERIFY(src_sl->sl_cnt == dst_sl->sl_cnt);
+
+	return (dst_sl);
+}
+
+int
+so_set_effective_pid(struct socket *so, int epid, struct proc *p)
+{
+	struct proc *ep = PROC_NULL;
+	int error = 0;
+
+	/* pid 0 is reserved for kernel */
+	if (epid == 0) {
+		error = EINVAL;
+		goto done;
+	}
+
+	/*
+	 * If this is an in-kernel socket, prevent its delegate
+	 * association from changing unless the socket option is
+	 * coming from within the kernel itself.
+	 */
+	if (so->last_pid == 0 && p != kernproc) {
+		error = EACCES;
+		goto done;
+	}
+
+	/*
+	 * If this is issued by a process that's recorded as the
+	 * real owner of the socket, or if the pid is the same as
+	 * the process's own pid, then proceed.  Otherwise ensure
+	 * that the issuing process has the necessary privileges.
+	 */
+	if (epid != so->last_pid || epid != proc_pid(p)) {
+		if ((error = priv_check_cred(kauth_cred_get(),
+		    PRIV_NET_PRIVILEGED_SOCKET_DELEGATE, 0))) {
+			error = EACCES;
+			goto done;
+		}
+	}
+
+	/* Find the process that corresponds to the effective pid */
+	if ((ep = proc_find(epid)) == PROC_NULL) {
+		error = ESRCH;
+		goto done;
+	}
+
+	/*
+	 * If a process tries to delegate the socket to itself, then
+	 * there's really nothing to do; treat it as a way for the
+	 * delegate association to be cleared.  Note that we check
+	 * the passed-in proc rather than calling proc_selfpid(),
+	 * as we need to check the process issuing the socket option
+	 * which could be kernproc.  Given that we don't allow 0 for
+	 * effective pid, it means that a delegated in-kernel socket
+	 * stays delegated during its lifetime (which is probably OK.)
+	 */
+	if (epid == proc_pid(p)) {
+		so->so_flags &= ~SOF_DELEGATED;
+		so->e_upid = 0;
+		so->e_pid = 0;
+		uuid_clear(so->e_uuid);
+	} else {
+		so->so_flags |= SOF_DELEGATED;
+		so->e_upid = proc_uniqueid(ep);
+		so->e_pid = proc_pid(ep);
+		proc_getexecutableuuid(ep, so->e_uuid, sizeof (so->e_uuid));
+	}
+done:
+	if (error == 0 && net_io_policy_log) {
+		uuid_string_t buf;
+
+		uuid_unparse(so->e_uuid, buf);
+		log(LOG_DEBUG, "%s[%s,%d]: so 0x%llx [%d,%d] epid %d (%s) "
+		    "euuid %s%s\n", __func__, proc_name_address(p),
+		    proc_pid(p), (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so),
+		    so->e_pid, proc_name_address(ep), buf,
+		    ((so->so_flags & SOF_DELEGATED) ? " [delegated]" : ""));
+	} else if (error != 0 && net_io_policy_log) {
+		log(LOG_ERR, "%s[%s,%d]: so 0x%llx [%d,%d] epid %d (%s) "
+		    "ERROR (%d)\n", __func__, proc_name_address(p),
+		    proc_pid(p), (uint64_t)DEBUG_KERNEL_ADDRPERM(so),
+		    SOCK_DOM(so), SOCK_TYPE(so),
+		    epid, (ep == PROC_NULL) ? "PROC_NULL" :
+		    proc_name_address(ep), error);
+	}
+
+	/* Update this socket's policy upon success */
+	if (error == 0) {
+		so->so_policy_gencnt *= -1;
+		so_update_policy(so);
+#if NECP
+		so_update_necp_policy(so, NULL, NULL);
+#endif /* NECP */
+	}
+
+	if (ep != PROC_NULL)
+		proc_rele(ep);
+
+	return (error);
+}
+
+int
+so_set_effective_uuid(struct socket *so, uuid_t euuid, struct proc *p)
+{
+	uuid_string_t buf;
+	uuid_t uuid;
+	int error = 0;
+
+	/* UUID must not be all-zeroes (reserved for kernel) */
+	if (uuid_is_null(euuid)) {
+		error = EINVAL;
+		goto done;
+	}
+
+	/*
+	 * If this is an in-kernel socket, prevent its delegate
+	 * association from changing unless the socket option is
+	 * coming from within the kernel itself.
+	 */
+	if (so->last_pid == 0 && p != kernproc) {
+		error = EACCES;
+		goto done;
+	}
+
+	/* Get the UUID of the issuing process */
+	proc_getexecutableuuid(p, uuid, sizeof (uuid));
+
+	/*
+	 * If this is issued by a process that's recorded as the
+	 * real owner of the socket, or if the uuid is the same as
+	 * the process's own uuid, then proceed.  Otherwise ensure
+	 * that the issuing process has the necessary privileges.
+	 */
+	if (uuid_compare(euuid, so->last_uuid) != 0 ||
+	    uuid_compare(euuid, uuid) != 0) {
+		if ((error = priv_check_cred(kauth_cred_get(),
+		    PRIV_NET_PRIVILEGED_SOCKET_DELEGATE, 0))) {
+			error = EACCES;
+			goto done;
+		}
+	}
+
+	/*
+	 * If a process tries to delegate the socket to itself, then
+	 * there's really nothing to do; treat it as a way for the
+	 * delegate association to be cleared.  Note that we check
+	 * the uuid of the passed-in proc rather than that of the
+	 * current process, as we need to check the process issuing
+	 * the socket option which could be kernproc itself.  Given
+	 * that we don't allow 0 for effective uuid, it means that
+	 * a delegated in-kernel socket stays delegated during its
+	 * lifetime (which is okay.)
+	 */
+	if (uuid_compare(euuid, uuid) == 0) {
+		so->so_flags &= ~SOF_DELEGATED;
+		so->e_upid = 0;
+		so->e_pid = 0;
+		uuid_clear(so->e_uuid);
+	} else {
+		so->so_flags |= SOF_DELEGATED;
+		/*
+		 * Unlike so_set_effective_pid(), we only have the UUID
+		 * here and the process ID is not known.  Inherit the
+		 * real {pid,upid} of the socket.
+		 */
+		so->e_upid = so->last_upid;
+		so->e_pid = so->last_pid;
+		uuid_copy(so->e_uuid, euuid);
+	}
+
+done:
+	if (error == 0 && net_io_policy_log) {
+		uuid_unparse(so->e_uuid, buf);
+		log(LOG_DEBUG, "%s[%s,%d]: so 0x%llx [%d,%d] epid %d "
+		    "euuid %s%s\n", __func__, proc_name_address(p), proc_pid(p),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so), SOCK_DOM(so),
+		    SOCK_TYPE(so), so->e_pid, buf,
+		    ((so->so_flags & SOF_DELEGATED) ? " [delegated]" : ""));
+	} else if (error != 0 && net_io_policy_log) {
+		uuid_unparse(euuid, buf);
+		log(LOG_DEBUG, "%s[%s,%d]: so 0x%llx [%d,%d] euuid %s "
+		    "ERROR (%d)\n", __func__, proc_name_address(p), proc_pid(p),
+		    (uint64_t)DEBUG_KERNEL_ADDRPERM(so), SOCK_DOM(so),
+		    SOCK_TYPE(so), buf, error);
+	}
+
+	/* Update this socket's policy upon success */
+	if (error == 0) {
+		so->so_policy_gencnt *= -1;
+		so_update_policy(so);
+#if NECP
+		so_update_necp_policy(so, NULL, NULL);
+#endif /* NECP */
+	}
+
+	return (error);
+}
+
+void
+netpolicy_post_msg(uint32_t ev_code, struct netpolicy_event_data *ev_data,
+    uint32_t ev_datalen)
+{
+	struct kev_msg ev_msg;
+
+	/*
+	 * A netpolicy event always starts with a netpolicy_event_data
+	 * structure, but the caller can provide for a longer event
+	 * structure to post, depending on the event code.
+	 */
+	VERIFY(ev_data != NULL && ev_datalen >= sizeof (*ev_data));
+
+	bzero(&ev_msg, sizeof (ev_msg));
+	ev_msg.vendor_code	= KEV_VENDOR_APPLE;
+	ev_msg.kev_class	= KEV_NETWORK_CLASS;
+	ev_msg.kev_subclass	= KEV_NETPOLICY_SUBCLASS;
+	ev_msg.event_code	= ev_code;
+
+	ev_msg.dv[0].data_ptr	= ev_data;
+	ev_msg.dv[0].data_length = ev_datalen;
+
+	kev_post_msg(&ev_msg);
+}
+
+void
+socket_post_kev_msg(uint32_t ev_code,
+    struct kev_socket_event_data *ev_data,
+    uint32_t ev_datalen)
+{
+	struct kev_msg ev_msg;
+
+	bzero(&ev_msg, sizeof(ev_msg));
+	ev_msg.vendor_code = KEV_VENDOR_APPLE;
+	ev_msg.kev_class = KEV_NETWORK_CLASS;
+	ev_msg.kev_subclass = KEV_SOCKET_SUBCLASS;
+	ev_msg.event_code = ev_code;
+
+	ev_msg.dv[0].data_ptr = ev_data;
+	ev_msg.dv[0]. data_length = ev_datalen;
+
+	kev_post_msg(&ev_msg);
+}
+
+void
+socket_post_kev_msg_closed(struct socket *so)
+{
+	struct kev_socket_closed ev;
+	struct sockaddr *socksa = NULL, *peersa = NULL;
+	int err;
+	bzero(&ev, sizeof(ev));
+	err = (*so->so_proto->pr_usrreqs->pru_sockaddr)(so, &socksa);
+	if (err == 0) {
+		err = (*so->so_proto->pr_usrreqs->pru_peeraddr)(so,
+		    &peersa);
+		if (err == 0) {
+			memcpy(&ev.ev_data.kev_sockname, socksa,
+			    min(socksa->sa_len,
+			    sizeof (ev.ev_data.kev_sockname)));
+			memcpy(&ev.ev_data.kev_peername, peersa,
+			    min(peersa->sa_len,
+			    sizeof (ev.ev_data.kev_peername)));
+			socket_post_kev_msg(KEV_SOCKET_CLOSED,
+			    &ev.ev_data, sizeof (ev));
+		}
+	}
+	if (socksa != NULL)
+		FREE(socksa, M_SONAME);
+	if (peersa != NULL)
+		FREE(peersa, M_SONAME);
+}
diff -Nur xnu-3247.1.106/bsd/kern/voodoo_assert.h xnu-3247.1.106-AnV/bsd/kern/voodoo_assert.h
--- xnu-3247.1.106/bsd/kern/voodoo_assert.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/kern/voodoo_assert.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+#ifndef _VOODOO_ASSERT_H
+#define _VOODOO_ASSERT_H
+
+#include <kern/debug.h>
+
+#define ASSERT(expr) do { if (!(expr)) panic("%s: failed assertion '%s'", \
+                                                      __FUNCTION__, #expr); } while (0)
+
+#define BUG(msg) panic("%s: %s\n", __FUNCTION__, #msg)
+
+#endif
\ No newline at end of file
diff -Nur xnu-3247.1.106/bsd/netinet/ip_dummynet.c xnu-3247.1.106-AnV/bsd/netinet/ip_dummynet.c
--- xnu-3247.1.106/bsd/netinet/ip_dummynet.c	2015-12-06 01:31:36.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/netinet/ip_dummynet.c	2015-12-13 17:08:10.000000000 +0100
@@ -1932,9 +1932,9 @@
 void
 dn_ipfw_rule_delete(void *r)
 {
-    struct dn_pipe *p ;
-    struct dn_flow_set *fs ;
-    struct dn_pkt_tag *pkt ;
+    struct dn_pipe *p = (struct dn_pipe *)0;
+    struct dn_flow_set *fs = (struct dn_flow_set *)0;
+    struct dn_pkt_tag *pkt = (struct dn_pkt_tag *)0;
     struct mbuf *m ;
     int i;
 
diff -Nur xnu-3247.1.106/bsd/nfs/nfs_vfsops.c xnu-3247.1.106-AnV/bsd/nfs/nfs_vfsops.c
--- xnu-3247.1.106/bsd/nfs/nfs_vfsops.c	2015-12-06 01:31:45.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/nfs/nfs_vfsops.c	2015-12-13 17:08:10.000000000 +0100
@@ -3426,7 +3426,7 @@
 	} while (0)
 #define xb_copy_opaque(E, XBSRC, XBDST) \
 	do { \
-		uint32_t __count, __val; \
+		uint32_t __count=0, __val=0; \
 		xb_copy_32((E), (XBSRC), (XBDST), __count); \
 		if (E) break; \
 		__count = nfsm_rndup(__count); \
diff -Nur xnu-3247.1.106/bsd/nfs/nfs_vfsops.c.orig xnu-3247.1.106-AnV/bsd/nfs/nfs_vfsops.c.orig
--- xnu-3247.1.106/bsd/nfs/nfs_vfsops.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/bsd/nfs/nfs_vfsops.c.orig	2015-12-06 01:31:45.000000000 +0100
@@ -0,0 +1,5770 @@
+/*
+ * Copyright (c) 2000-2014 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/* Copyright (c) 1995 NeXT Computer, Inc. All Rights Reserved */
+/*
+ * Copyright (c) 1989, 1993, 1995
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * Rick Macklem at The University of Guelph.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *	This product includes software developed by the University of
+ *	California, Berkeley and its contributors.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	@(#)nfs_vfsops.c	8.12 (Berkeley) 5/20/95
+ * FreeBSD-Id: nfs_vfsops.c,v 1.52 1997/11/12 05:42:21 julian Exp $
+ */
+/*
+ * NOTICE: This file was modified by SPARTA, Inc. in 2005 to introduce
+ * support for mandatory and extensible security protections.  This notice
+ * is included in support of clause 2.2 (b) of the Apple Public License,
+ * Version 2.0.
+ */
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/conf.h>
+#include <sys/ioctl.h>
+#include <sys/signal.h>
+#include <sys/proc_internal.h> /* for fs rooting to update rootdir in fdp */
+#include <sys/kauth.h>
+#include <sys/vnode_internal.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+#include <sys/sysctl.h>
+#include <sys/mount_internal.h>
+#include <sys/kpi_mbuf.h>
+#include <sys/socket.h>
+#include <sys/socketvar.h>
+#include <sys/fcntl.h>
+#include <sys/quota.h>
+#include <sys/priv.h>
+#include <libkern/OSAtomic.h>
+
+#include <sys/vm.h>
+#include <sys/vmparam.h>
+
+#if !defined(NO_MOUNT_PRIVATE)
+#include <sys/filedesc.h>
+#endif /* NO_MOUNT_PRIVATE */
+
+#include <net/if.h>
+#include <net/route.h>
+#include <netinet/in.h>
+
+#include <nfs/rpcv2.h>
+#include <nfs/krpc.h>
+#include <nfs/nfsproto.h>
+#include <nfs/nfs.h>
+#include <nfs/nfsnode.h>
+#include <nfs/nfs_gss.h>
+#include <nfs/nfsmount.h>
+#include <nfs/xdr_subs.h>
+#include <nfs/nfsm_subs.h>
+#include <nfs/nfsdiskless.h>
+#include <nfs/nfs_lock.h>
+#if CONFIG_MACF
+#include <security/mac_framework.h>
+#endif
+
+#include <pexpert/pexpert.h>
+
+#define NFS_VFS_DBG(...) NFS_DBG(NFS_FAC_VFS, 7, ## __VA_ARGS__)
+
+/*
+ * NFS client globals
+ */
+
+int nfs_ticks;
+static lck_grp_t *nfs_global_grp, *nfs_mount_grp;
+lck_mtx_t *nfs_global_mutex;
+uint32_t nfs_fs_attr_bitmap[NFS_ATTR_BITMAP_LEN];
+uint32_t nfs_object_attr_bitmap[NFS_ATTR_BITMAP_LEN];
+uint32_t nfs_getattr_bitmap[NFS_ATTR_BITMAP_LEN];
+struct nfsclientidlist nfsclientids;
+
+/* NFS requests */
+struct nfs_reqqhead nfs_reqq;
+lck_grp_t *nfs_request_grp;
+lck_mtx_t *nfs_request_mutex;
+thread_call_t nfs_request_timer_call;
+int nfs_request_timer_on;
+u_int32_t nfs_xid = 0;
+u_int32_t nfs_xidwrap = 0;		/* to build a (non-wrapping) 64 bit xid */
+
+thread_call_t nfs_buf_timer_call;
+
+/* NFSv4 */
+lck_grp_t *nfs_open_grp;
+uint32_t nfs_open_owner_seqnum = 0;
+uint32_t nfs_lock_owner_seqnum = 0;
+thread_call_t nfs4_callback_timer_call;
+int nfs4_callback_timer_on = 0;
+
+/* nfsiod */
+lck_grp_t *nfsiod_lck_grp;
+lck_mtx_t *nfsiod_mutex;
+struct nfsiodlist nfsiodfree, nfsiodwork;
+struct nfsiodmountlist nfsiodmounts;
+int nfsiod_thread_count = 0;
+int nfsiod_thread_max = NFS_DEFASYNCTHREAD;
+int nfs_max_async_writes = NFS_DEFMAXASYNCWRITES;
+
+int nfs_iosize = NFS_IOSIZE;
+int nfs_access_cache_timeout = NFS_MAXATTRTIMO;
+int nfs_access_delete = 1; /* too many servers get this wrong - workaround on by default */
+int nfs_access_dotzfs = 1;
+int nfs_access_for_getattr = 0;
+int nfs_allow_async = 0;
+int nfs_statfs_rate_limit = NFS_DEFSTATFSRATELIMIT;
+int nfs_lockd_mounts = 0;
+int nfs_lockd_request_sent = 0;
+int nfs_idmap_ctrl = NFS_IDMAP_CTRL_USE_IDMAP_SERVICE;
+int nfs_callback_port = 0;
+
+int nfs_tprintf_initial_delay = NFS_TPRINTF_INITIAL_DELAY;
+int nfs_tprintf_delay = NFS_TPRINTF_DELAY;
+
+
+int		mountnfs(char *, mount_t, vfs_context_t, vnode_t *);
+static int	nfs_mount_diskless(struct nfs_dlmount *, const char *, int, vnode_t *, mount_t *, vfs_context_t);
+#if !defined(NO_MOUNT_PRIVATE)
+static int	nfs_mount_diskless_private(struct nfs_dlmount *, const char *, int, vnode_t *, mount_t *, vfs_context_t);
+#endif /* NO_MOUNT_PRIVATE */
+int		nfs_mount_connect(struct nfsmount *);
+void		nfs_mount_drain_and_cleanup(struct nfsmount *);
+void		nfs_mount_cleanup(struct nfsmount *);
+int		nfs_mountinfo_assemble(struct nfsmount *, struct xdrbuf *);
+int		nfs4_mount_update_path_with_symlink(struct nfsmount *, struct nfs_fs_path *, uint32_t, fhandle_t *, int *, fhandle_t *, vfs_context_t);
+
+/*
+ * NFS VFS operations.
+ */
+int	nfs_vfs_mount(mount_t, vnode_t, user_addr_t, vfs_context_t);
+int	nfs_vfs_start(mount_t, int, vfs_context_t);
+int	nfs_vfs_unmount(mount_t, int, vfs_context_t);
+int	nfs_vfs_root(mount_t, vnode_t *, vfs_context_t);
+int	nfs_vfs_quotactl(mount_t, int, uid_t, caddr_t, vfs_context_t);
+int	nfs_vfs_getattr(mount_t, struct vfs_attr *, vfs_context_t);
+int	nfs_vfs_sync(mount_t, int, vfs_context_t);
+int	nfs_vfs_vget(mount_t, ino64_t, vnode_t *, vfs_context_t);
+int	nfs_vfs_vptofh(vnode_t, int *, unsigned char *, vfs_context_t);
+int	nfs_vfs_fhtovp(mount_t, int, unsigned char *, vnode_t *, vfs_context_t);
+int	nfs_vfs_init(struct vfsconf *);
+int	nfs_vfs_sysctl(int *, u_int, user_addr_t, size_t *, user_addr_t, size_t, vfs_context_t);
+
+struct vfsops nfs_vfsops = {
+	nfs_vfs_mount,
+	nfs_vfs_start,
+	nfs_vfs_unmount,
+	nfs_vfs_root,
+	nfs_vfs_quotactl,
+	nfs_vfs_getattr,
+	nfs_vfs_sync,
+	nfs_vfs_vget,
+	nfs_vfs_fhtovp,
+	nfs_vfs_vptofh,
+	nfs_vfs_init,
+	nfs_vfs_sysctl,
+	NULL,		/* setattr */
+	{ NULL,		/* reserved */
+	  NULL,		/* reserved */
+	  NULL,		/* reserved */
+	  NULL,		/* reserved */
+	  NULL,		/* reserved */
+	  NULL,		/* reserved */
+	  NULL }	/* reserved */
+};
+
+
+/*
+ * version-specific NFS functions
+ */
+int nfs3_mount(struct nfsmount *, vfs_context_t, nfsnode_t *);
+int nfs4_mount(struct nfsmount *, vfs_context_t, nfsnode_t *);
+int nfs3_fsinfo(struct nfsmount *, nfsnode_t, vfs_context_t);
+int nfs3_update_statfs(struct nfsmount *, vfs_context_t);
+int nfs4_update_statfs(struct nfsmount *, vfs_context_t);
+#if !QUOTA
+#define nfs3_getquota	NULL
+#define nfs4_getquota	NULL
+#else
+int nfs3_getquota(struct nfsmount *, vfs_context_t, uid_t, int, struct dqblk *);
+int nfs4_getquota(struct nfsmount *, vfs_context_t, uid_t, int, struct dqblk *);
+#endif
+
+struct nfs_funcs nfs3_funcs = {
+	nfs3_mount,
+	nfs3_update_statfs,
+	nfs3_getquota,
+	nfs3_access_rpc,
+	nfs3_getattr_rpc,
+	nfs3_setattr_rpc,
+	nfs3_read_rpc_async,
+	nfs3_read_rpc_async_finish,
+	nfs3_readlink_rpc,
+	nfs3_write_rpc_async,
+	nfs3_write_rpc_async_finish,
+	nfs3_commit_rpc,
+	nfs3_lookup_rpc_async,
+	nfs3_lookup_rpc_async_finish,
+	nfs3_remove_rpc,
+	nfs3_rename_rpc,
+	nfs3_setlock_rpc,
+	nfs3_unlock_rpc,
+	nfs3_getlock_rpc
+	};
+struct nfs_funcs nfs4_funcs = {
+	nfs4_mount,
+	nfs4_update_statfs,
+	nfs4_getquota,
+	nfs4_access_rpc,
+	nfs4_getattr_rpc,
+	nfs4_setattr_rpc,
+	nfs4_read_rpc_async,
+	nfs4_read_rpc_async_finish,
+	nfs4_readlink_rpc,
+	nfs4_write_rpc_async,
+	nfs4_write_rpc_async_finish,
+	nfs4_commit_rpc,
+	nfs4_lookup_rpc_async,
+	nfs4_lookup_rpc_async_finish,
+	nfs4_remove_rpc,
+	nfs4_rename_rpc,
+	nfs4_setlock_rpc,
+	nfs4_unlock_rpc,
+	nfs4_getlock_rpc
+	};
+
+/*
+ * Called once to initialize data structures...
+ */
+int
+nfs_vfs_init(__unused struct vfsconf *vfsp)
+{
+	int i;
+
+	/*
+	 * Check to see if major data structures haven't bloated.
+	 */
+	if (sizeof (struct nfsnode) > NFS_NODEALLOC) {
+		printf("struct nfsnode bloated (> %dbytes)\n", NFS_NODEALLOC);
+		printf("Try reducing NFS_SMALLFH\n");
+	}
+	if (sizeof (struct nfsmount) > NFS_MNTALLOC)
+		printf("struct nfsmount bloated (> %dbytes)\n", NFS_MNTALLOC);
+
+	nfs_ticks = (hz * NFS_TICKINTVL + 500) / 1000;
+	if (nfs_ticks < 1)
+		nfs_ticks = 1;
+
+	/* init async I/O thread pool state */
+	TAILQ_INIT(&nfsiodfree);
+	TAILQ_INIT(&nfsiodwork);
+	TAILQ_INIT(&nfsiodmounts);
+	nfsiod_lck_grp = lck_grp_alloc_init("nfsiod", LCK_GRP_ATTR_NULL);
+	nfsiod_mutex = lck_mtx_alloc_init(nfsiod_lck_grp, LCK_ATTR_NULL);
+
+	/* init lock groups, etc. */
+	nfs_mount_grp = lck_grp_alloc_init("nfs_mount", LCK_GRP_ATTR_NULL);
+	nfs_open_grp = lck_grp_alloc_init("nfs_open", LCK_GRP_ATTR_NULL);
+	nfs_global_grp = lck_grp_alloc_init("nfs_global", LCK_GRP_ATTR_NULL);
+
+	nfs_global_mutex = lck_mtx_alloc_init(nfs_global_grp, LCK_ATTR_NULL);
+
+	/* init request list mutex */
+	nfs_request_grp = lck_grp_alloc_init("nfs_request", LCK_GRP_ATTR_NULL);
+	nfs_request_mutex = lck_mtx_alloc_init(nfs_request_grp, LCK_ATTR_NULL);
+
+	/* initialize NFS request list */
+	TAILQ_INIT(&nfs_reqq);
+
+	nfs_nbinit();			/* Init the nfsbuf table */
+	nfs_nhinit();			/* Init the nfsnode table */
+	nfs_lockinit();			/* Init the nfs lock state */
+	nfs_gss_init();			/* Init RPCSEC_GSS security */
+
+	/* NFSv4 stuff */
+	NFS4_PER_FS_ATTRIBUTES(nfs_fs_attr_bitmap);
+	NFS4_PER_OBJECT_ATTRIBUTES(nfs_object_attr_bitmap);
+	NFS4_DEFAULT_ATTRIBUTES(nfs_getattr_bitmap);
+	for (i=0; i < NFS_ATTR_BITMAP_LEN; i++)
+		nfs_getattr_bitmap[i] &= nfs_object_attr_bitmap[i];
+	TAILQ_INIT(&nfsclientids);
+
+	/* initialize NFS timer callouts */
+	nfs_request_timer_call = thread_call_allocate(nfs_request_timer, NULL);
+	nfs_buf_timer_call = thread_call_allocate(nfs_buf_timer, NULL);
+	nfs4_callback_timer_call = thread_call_allocate(nfs4_callback_timer, NULL);
+
+	return (0);
+}
+
+/*
+ * nfs statfs call
+ */
+int
+nfs3_update_statfs(struct nfsmount *nmp, vfs_context_t ctx)
+{
+	nfsnode_t np;
+	int error = 0, lockerror, status, nfsvers;
+	u_int64_t xid;
+	struct nfsm_chain nmreq, nmrep;
+	uint32_t val = 0;
+
+	nfsvers = nmp->nm_vers;
+	np = nmp->nm_dnp;
+	if (!np)
+		return (ENXIO);
+	if ((error = vnode_get(NFSTOV(np))))
+		return (error);
+
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	nfsm_chain_build_alloc_init(error, &nmreq, NFSX_FH(nfsvers));
+	nfsm_chain_add_fh(error, &nmreq, nfsvers, np->n_fhp, np->n_fhsize);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsmout_if(error);
+	error = nfs_request2(np, NULL, &nmreq, NFSPROC_FSSTAT, vfs_context_thread(ctx),
+		vfs_context_ucred(ctx), NULL, R_SOFT, &nmrep, &xid, &status);
+	if (error == ETIMEDOUT)
+		goto nfsmout;
+	if ((lockerror = nfs_node_lock(np)))
+		error = lockerror;
+	if (nfsvers == NFS_VER3)
+		nfsm_chain_postop_attr_update(error, &nmrep, np, &xid);
+	if (!lockerror)
+		nfs_node_unlock(np);
+	if (!error)
+		error = status;
+	nfsm_assert(error, NFSTONMP(np), ENXIO);
+	nfsmout_if(error);
+	lck_mtx_lock(&nmp->nm_lock);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_TOTAL);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_FREE);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_AVAIL);
+	if (nfsvers == NFS_VER3) {
+		NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_FILES_AVAIL);
+		NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_FILES_TOTAL);
+		NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_FILES_FREE);
+		nmp->nm_fsattr.nfsa_bsize = NFS_FABLKSIZE;
+		nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_space_total);
+		nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_space_free);
+		nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_space_avail);
+		nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_files_total);
+		nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_files_free);
+		nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_files_avail);
+		// skip invarsec
+	} else {
+		nfsm_chain_adv(error, &nmrep, NFSX_UNSIGNED); // skip tsize?
+		nfsm_chain_get_32(error, &nmrep, nmp->nm_fsattr.nfsa_bsize);
+		nfsm_chain_get_32(error, &nmrep, val);
+		nfsmout_if(error);
+		if (nmp->nm_fsattr.nfsa_bsize <= 0)
+			nmp->nm_fsattr.nfsa_bsize = NFS_FABLKSIZE;
+		nmp->nm_fsattr.nfsa_space_total = (uint64_t)val * nmp->nm_fsattr.nfsa_bsize;
+		nfsm_chain_get_32(error, &nmrep, val);
+		nfsmout_if(error);
+		nmp->nm_fsattr.nfsa_space_free = (uint64_t)val * nmp->nm_fsattr.nfsa_bsize;
+		nfsm_chain_get_32(error, &nmrep, val);
+		nfsmout_if(error);
+		nmp->nm_fsattr.nfsa_space_avail = (uint64_t)val * nmp->nm_fsattr.nfsa_bsize;
+	}
+	lck_mtx_unlock(&nmp->nm_lock);
+nfsmout:
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	vnode_put(NFSTOV(np));
+	return (error);
+}
+
+int
+nfs4_update_statfs(struct nfsmount *nmp, vfs_context_t ctx)
+{
+	nfsnode_t np;
+	int error = 0, lockerror, status, nfsvers, numops;
+	u_int64_t xid;
+	struct nfsm_chain nmreq, nmrep;
+	uint32_t bitmap[NFS_ATTR_BITMAP_LEN];
+	struct nfs_vattr nvattr;
+	struct nfsreq_secinfo_args si;
+
+	nfsvers = nmp->nm_vers;
+	np = nmp->nm_dnp;
+	if (!np)
+		return (ENXIO);
+	if ((error = vnode_get(NFSTOV(np))))
+		return (error);
+
+	NFSREQ_SECINFO_SET(&si, np, NULL, 0, NULL, 0);
+	NVATTR_INIT(&nvattr);
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	// PUTFH + GETATTR
+	numops = 2;
+	nfsm_chain_build_alloc_init(error, &nmreq, 15 * NFSX_UNSIGNED);
+	nfsm_chain_add_compound_header(error, &nmreq, "statfs", nmp->nm_minor_vers, numops);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTFH);
+	nfsm_chain_add_fh(error, &nmreq, nfsvers, np->n_fhp, np->n_fhsize);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_GETATTR);
+	NFS_COPY_ATTRIBUTES(nfs_getattr_bitmap, bitmap);
+	NFS4_STATFS_ATTRIBUTES(bitmap);
+	nfsm_chain_add_bitmap_supported(error, &nmreq, bitmap, nmp, np);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsm_assert(error, (numops == 0), EPROTO);
+	nfsmout_if(error);
+	error = nfs_request2(np, NULL, &nmreq, NFSPROC4_COMPOUND,
+		vfs_context_thread(ctx), vfs_context_ucred(ctx),
+		NULL, R_SOFT, &nmrep, &xid, &status);
+	nfsm_chain_skip_tag(error, &nmrep);
+	nfsm_chain_get_32(error, &nmrep, numops);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_PUTFH);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_GETATTR);
+	nfsm_assert(error, NFSTONMP(np), ENXIO);
+	nfsmout_if(error);
+	lck_mtx_lock(&nmp->nm_lock);
+	error = nfs4_parsefattr(&nmrep, &nmp->nm_fsattr, &nvattr, NULL, NULL, NULL);
+	lck_mtx_unlock(&nmp->nm_lock);
+	nfsmout_if(error);
+	if ((lockerror = nfs_node_lock(np)))
+		error = lockerror;
+	if (!error)
+		nfs_loadattrcache(np, &nvattr, &xid, 0);
+	if (!lockerror)
+		nfs_node_unlock(np);
+	nfsm_assert(error, NFSTONMP(np), ENXIO);
+	nfsmout_if(error);
+	nmp->nm_fsattr.nfsa_bsize = NFS_FABLKSIZE;
+nfsmout:
+	NVATTR_CLEANUP(&nvattr);
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	vnode_put(NFSTOV(np));
+	return (error);
+}
+
+
+/*
+ * The NFS VFS_GETATTR function: "statfs"-type information is retrieved
+ * using the nf_update_statfs() function, and other attributes are cobbled
+ * together from whatever sources we can (getattr, fsinfo, pathconf).
+ */
+int
+nfs_vfs_getattr(mount_t mp, struct vfs_attr *fsap, vfs_context_t ctx)
+{
+	struct nfsmount *nmp;
+	uint32_t bsize;
+	int error = 0, nfsvers;
+
+	nmp = VFSTONFS(mp);
+	if (nfs_mount_gone(nmp))
+		return (ENXIO);
+	nfsvers = nmp->nm_vers;
+
+	if (VFSATTR_IS_ACTIVE(fsap, f_bsize)  ||
+	    VFSATTR_IS_ACTIVE(fsap, f_iosize) ||
+	    VFSATTR_IS_ACTIVE(fsap, f_blocks) ||
+	    VFSATTR_IS_ACTIVE(fsap, f_bfree)  ||
+	    VFSATTR_IS_ACTIVE(fsap, f_bavail) ||
+	    VFSATTR_IS_ACTIVE(fsap, f_bused)  ||
+	    VFSATTR_IS_ACTIVE(fsap, f_files)  ||
+	    VFSATTR_IS_ACTIVE(fsap, f_ffree)) {
+		int statfsrate = nfs_statfs_rate_limit;
+		int refresh = 1;
+
+		/*
+		 * Are we rate-limiting statfs RPCs?
+		 * (Treat values less than 1 or greater than 1,000,000 as no limit.)
+		 */
+		if ((statfsrate > 0) && (statfsrate < 1000000)) {
+			struct timeval now;
+			uint32_t stamp;
+
+			microuptime(&now);
+			lck_mtx_lock(&nmp->nm_lock);
+			stamp = (now.tv_sec * statfsrate) + (now.tv_usec / (1000000/statfsrate));
+			if (stamp != nmp->nm_fsattrstamp) {
+				refresh = 1;
+				nmp->nm_fsattrstamp = stamp;
+			} else {
+				refresh = 0;
+			}
+			lck_mtx_unlock(&nmp->nm_lock);
+		}
+
+		if (refresh && !nfs_use_cache(nmp))
+			error = nmp->nm_funcs->nf_update_statfs(nmp, ctx);
+		if ((error == ESTALE) || (error == ETIMEDOUT))
+			error = 0;
+		if (error)
+			return (error);
+
+		lck_mtx_lock(&nmp->nm_lock);
+		VFSATTR_RETURN(fsap, f_iosize, nfs_iosize);
+		VFSATTR_RETURN(fsap, f_bsize, nmp->nm_fsattr.nfsa_bsize);
+		bsize = nmp->nm_fsattr.nfsa_bsize;
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_TOTAL))
+			VFSATTR_RETURN(fsap, f_blocks, nmp->nm_fsattr.nfsa_space_total / bsize);
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_FREE))
+			VFSATTR_RETURN(fsap, f_bfree, nmp->nm_fsattr.nfsa_space_free / bsize);
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_AVAIL))
+			VFSATTR_RETURN(fsap, f_bavail, nmp->nm_fsattr.nfsa_space_avail / bsize);
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_TOTAL) &&
+		    NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SPACE_FREE))
+			VFSATTR_RETURN(fsap, f_bused,
+				(nmp->nm_fsattr.nfsa_space_total / bsize) -
+				(nmp->nm_fsattr.nfsa_space_free / bsize));
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_FILES_TOTAL))
+			VFSATTR_RETURN(fsap, f_files, nmp->nm_fsattr.nfsa_files_total);
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_FILES_FREE))
+			VFSATTR_RETURN(fsap, f_ffree, nmp->nm_fsattr.nfsa_files_free);
+		lck_mtx_unlock(&nmp->nm_lock);
+	}
+
+	if (VFSATTR_IS_ACTIVE(fsap, f_capabilities)) {
+		u_int32_t caps, valid;
+		nfsnode_t np = nmp->nm_dnp;
+
+		nfsm_assert(error, VFSTONFS(mp) && np, ENXIO);
+		if (error)
+			return (error);
+		lck_mtx_lock(&nmp->nm_lock);
+
+		/*
+		 * The capabilities[] array defines what this volume supports.
+		 *
+		 * The valid[] array defines which bits this code understands
+		 * the meaning of (whether the volume has that capability or not).
+		 * Any zero bits here means "I don't know what you're asking about"
+		 * and the caller cannot tell whether that capability is
+		 * present or not.
+		 */
+		caps = valid = 0;
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SYMLINK_SUPPORT)) {
+			valid |= VOL_CAP_FMT_SYMBOLICLINKS;
+			if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_SYMLINK)
+				caps |= VOL_CAP_FMT_SYMBOLICLINKS;
+		}
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_LINK_SUPPORT)) {
+			valid |= VOL_CAP_FMT_HARDLINKS;
+			if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_LINK)
+				caps |= VOL_CAP_FMT_HARDLINKS;
+		}
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_CASE_INSENSITIVE)) {
+			valid |= VOL_CAP_FMT_CASE_SENSITIVE;
+			if (!(nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_CASE_INSENSITIVE))
+				caps |= VOL_CAP_FMT_CASE_SENSITIVE;
+		}
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_CASE_PRESERVING)) {
+			valid |= VOL_CAP_FMT_CASE_PRESERVING;
+			if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_CASE_PRESERVING)
+				caps |= VOL_CAP_FMT_CASE_PRESERVING;
+		}
+		/* Note: VOL_CAP_FMT_2TB_FILESIZE is actually used to test for "large file support" */
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXFILESIZE)) {
+			/* Is server's max file size at least 4GB? */
+			if (nmp->nm_fsattr.nfsa_maxfilesize >= 0x100000000ULL)
+				caps |= VOL_CAP_FMT_2TB_FILESIZE;
+		} else if (nfsvers >= NFS_VER3) {
+			/*
+			 * NFSv3 and up supports 64 bits of file size.
+			 * So, we'll just assume maxfilesize >= 4GB
+			 */
+			caps |= VOL_CAP_FMT_2TB_FILESIZE;
+		}
+		if (nfsvers >= NFS_VER4) {
+			caps |= VOL_CAP_FMT_HIDDEN_FILES;
+			valid |= VOL_CAP_FMT_HIDDEN_FILES;
+			// VOL_CAP_FMT_OPENDENYMODES
+//			caps |= VOL_CAP_FMT_OPENDENYMODES;
+//			valid |= VOL_CAP_FMT_OPENDENYMODES;
+		}
+		fsap->f_capabilities.capabilities[VOL_CAPABILITIES_FORMAT] =
+			// VOL_CAP_FMT_PERSISTENTOBJECTIDS |
+			// VOL_CAP_FMT_SYMBOLICLINKS |
+			// VOL_CAP_FMT_HARDLINKS |
+			// VOL_CAP_FMT_JOURNAL |
+			// VOL_CAP_FMT_JOURNAL_ACTIVE |
+			// VOL_CAP_FMT_NO_ROOT_TIMES |
+			// VOL_CAP_FMT_SPARSE_FILES |
+			// VOL_CAP_FMT_ZERO_RUNS |
+			// VOL_CAP_FMT_CASE_SENSITIVE |
+			// VOL_CAP_FMT_CASE_PRESERVING |
+			// VOL_CAP_FMT_FAST_STATFS |
+			// VOL_CAP_FMT_2TB_FILESIZE |
+			// VOL_CAP_FMT_OPENDENYMODES |
+			// VOL_CAP_FMT_HIDDEN_FILES |
+			caps;
+		fsap->f_capabilities.valid[VOL_CAPABILITIES_FORMAT] =
+			VOL_CAP_FMT_PERSISTENTOBJECTIDS |
+			// VOL_CAP_FMT_SYMBOLICLINKS |
+			// VOL_CAP_FMT_HARDLINKS |
+			// VOL_CAP_FMT_JOURNAL |
+			// VOL_CAP_FMT_JOURNAL_ACTIVE |
+			// VOL_CAP_FMT_NO_ROOT_TIMES |
+			// VOL_CAP_FMT_SPARSE_FILES |
+			// VOL_CAP_FMT_ZERO_RUNS |
+			// VOL_CAP_FMT_CASE_SENSITIVE |
+			// VOL_CAP_FMT_CASE_PRESERVING |
+			VOL_CAP_FMT_FAST_STATFS |
+			VOL_CAP_FMT_2TB_FILESIZE |
+			// VOL_CAP_FMT_OPENDENYMODES |
+			// VOL_CAP_FMT_HIDDEN_FILES |
+			valid;
+
+		/*
+		 * We don't support most of the interfaces.
+		 *
+		 * We MAY support locking, but we don't have any easy way of probing.
+		 * We can tell if there's no lockd running or if locks have been
+		 * disabled for a mount, so we can definitely answer NO in that case.
+		 * Any attempt to send a request to lockd to test for locking support
+		 * may cause the lazily-launched locking daemons to be started
+		 * unnecessarily.  So we avoid that.  However, we do record if we ever
+		 * successfully perform a lock operation on a mount point, so if it
+		 * looks like lock ops have worked, we do report that we support them.
+		 */
+		caps = valid = 0;
+		if (nfsvers >= NFS_VER4) {
+			caps = VOL_CAP_INT_ADVLOCK | VOL_CAP_INT_FLOCK;
+			valid = VOL_CAP_INT_ADVLOCK | VOL_CAP_INT_FLOCK;
+			if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_ACL)
+				caps |= VOL_CAP_INT_EXTENDED_SECURITY;
+			valid |= VOL_CAP_INT_EXTENDED_SECURITY;
+			if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_NAMED_ATTR)
+				caps |= VOL_CAP_INT_EXTENDED_ATTR;
+			valid |= VOL_CAP_INT_EXTENDED_ATTR;
+#if NAMEDSTREAMS
+			if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_NAMED_ATTR)
+				caps |= VOL_CAP_INT_NAMEDSTREAMS;
+			valid |= VOL_CAP_INT_NAMEDSTREAMS;
+#endif
+		} else if (nmp->nm_lockmode == NFS_LOCK_MODE_DISABLED) {
+			/* locks disabled on this mount, so they definitely won't work */
+			valid = VOL_CAP_INT_ADVLOCK | VOL_CAP_INT_FLOCK;
+		} else if (nmp->nm_state & NFSSTA_LOCKSWORK) {
+			caps = VOL_CAP_INT_ADVLOCK | VOL_CAP_INT_FLOCK;
+			valid = VOL_CAP_INT_ADVLOCK | VOL_CAP_INT_FLOCK;
+		}
+		fsap->f_capabilities.capabilities[VOL_CAPABILITIES_INTERFACES] =
+			// VOL_CAP_INT_SEARCHFS |
+			// VOL_CAP_INT_ATTRLIST |
+			// VOL_CAP_INT_NFSEXPORT |
+			// VOL_CAP_INT_READDIRATTR |
+			// VOL_CAP_INT_EXCHANGEDATA |
+			// VOL_CAP_INT_COPYFILE |
+			// VOL_CAP_INT_ALLOCATE |
+			// VOL_CAP_INT_VOL_RENAME |
+			// VOL_CAP_INT_ADVLOCK |
+			// VOL_CAP_INT_FLOCK |
+			// VOL_CAP_INT_EXTENDED_SECURITY |
+			// VOL_CAP_INT_USERACCESS |
+			// VOL_CAP_INT_MANLOCK |
+			// VOL_CAP_INT_NAMEDSTREAMS |
+			// VOL_CAP_INT_EXTENDED_ATTR |
+			VOL_CAP_INT_REMOTE_EVENT |
+			caps;
+		fsap->f_capabilities.valid[VOL_CAPABILITIES_INTERFACES] =
+			VOL_CAP_INT_SEARCHFS |
+			VOL_CAP_INT_ATTRLIST |
+			VOL_CAP_INT_NFSEXPORT |
+			VOL_CAP_INT_READDIRATTR |
+			VOL_CAP_INT_EXCHANGEDATA |
+			VOL_CAP_INT_COPYFILE |
+			VOL_CAP_INT_ALLOCATE |
+			VOL_CAP_INT_VOL_RENAME |
+			// VOL_CAP_INT_ADVLOCK |
+			// VOL_CAP_INT_FLOCK |
+			// VOL_CAP_INT_EXTENDED_SECURITY |
+			// VOL_CAP_INT_USERACCESS |
+			// VOL_CAP_INT_MANLOCK |
+			// VOL_CAP_INT_NAMEDSTREAMS |
+			// VOL_CAP_INT_EXTENDED_ATTR |
+			VOL_CAP_INT_REMOTE_EVENT |
+			valid;
+
+		fsap->f_capabilities.capabilities[VOL_CAPABILITIES_RESERVED1] = 0;
+		fsap->f_capabilities.valid[VOL_CAPABILITIES_RESERVED1] = 0;
+
+		fsap->f_capabilities.capabilities[VOL_CAPABILITIES_RESERVED2] = 0;
+		fsap->f_capabilities.valid[VOL_CAPABILITIES_RESERVED2] = 0;
+
+		VFSATTR_SET_SUPPORTED(fsap, f_capabilities);
+		lck_mtx_unlock(&nmp->nm_lock);
+	}
+
+	if (VFSATTR_IS_ACTIVE(fsap, f_attributes)) {
+		fsap->f_attributes.validattr.commonattr = 0;
+		fsap->f_attributes.validattr.volattr =
+			ATTR_VOL_CAPABILITIES | ATTR_VOL_ATTRIBUTES;
+		fsap->f_attributes.validattr.dirattr = 0;
+		fsap->f_attributes.validattr.fileattr = 0;
+		fsap->f_attributes.validattr.forkattr = 0;
+
+		fsap->f_attributes.nativeattr.commonattr = 0;
+		fsap->f_attributes.nativeattr.volattr =
+			ATTR_VOL_CAPABILITIES | ATTR_VOL_ATTRIBUTES;
+		fsap->f_attributes.nativeattr.dirattr = 0;
+		fsap->f_attributes.nativeattr.fileattr = 0;
+		fsap->f_attributes.nativeattr.forkattr = 0;
+
+		VFSATTR_SET_SUPPORTED(fsap, f_attributes);
+	}
+
+	return (error);
+}
+
+/*
+ * nfs version 3 fsinfo rpc call
+ */
+int
+nfs3_fsinfo(struct nfsmount *nmp, nfsnode_t np, vfs_context_t ctx)
+{
+	int error = 0, lockerror, status, nmlocked = 0;
+	u_int64_t xid;
+	uint32_t val, prefsize, maxsize;
+	struct nfsm_chain nmreq, nmrep;
+
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	nfsm_chain_build_alloc_init(error, &nmreq, NFSX_FH(nmp->nm_vers));
+	nfsm_chain_add_fh(error, &nmreq, nmp->nm_vers, np->n_fhp, np->n_fhsize);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsmout_if(error);
+	error = nfs_request(np, NULL, &nmreq, NFSPROC_FSINFO, ctx, NULL, &nmrep, &xid, &status);
+	if ((lockerror = nfs_node_lock(np)))
+		error = lockerror;
+	nfsm_chain_postop_attr_update(error, &nmrep, np, &xid);
+	if (!lockerror)
+		nfs_node_unlock(np);
+	if (!error)
+		error = status;
+	nfsmout_if(error);
+
+	lck_mtx_lock(&nmp->nm_lock);
+	nmlocked = 1;
+
+	nfsm_chain_get_32(error, &nmrep, maxsize);
+	nfsm_chain_get_32(error, &nmrep, prefsize);
+	nfsmout_if(error);
+	nmp->nm_fsattr.nfsa_maxread = maxsize;
+	if (prefsize < nmp->nm_rsize)
+		nmp->nm_rsize = (prefsize + NFS_FABLKSIZE - 1) &
+			~(NFS_FABLKSIZE - 1);
+	if ((maxsize > 0) && (maxsize < nmp->nm_rsize)) {
+		nmp->nm_rsize = maxsize & ~(NFS_FABLKSIZE - 1);
+		if (nmp->nm_rsize == 0)
+			nmp->nm_rsize = maxsize;
+	}
+	nfsm_chain_adv(error, &nmrep, NFSX_UNSIGNED); // skip rtmult
+
+	nfsm_chain_get_32(error, &nmrep, maxsize);
+	nfsm_chain_get_32(error, &nmrep, prefsize);
+	nfsmout_if(error);
+	nmp->nm_fsattr.nfsa_maxwrite = maxsize;
+	if (prefsize < nmp->nm_wsize)
+		nmp->nm_wsize = (prefsize + NFS_FABLKSIZE - 1) &
+			~(NFS_FABLKSIZE - 1);
+	if ((maxsize > 0) && (maxsize < nmp->nm_wsize)) {
+		nmp->nm_wsize = maxsize & ~(NFS_FABLKSIZE - 1);
+		if (nmp->nm_wsize == 0)
+			nmp->nm_wsize = maxsize;
+	}
+	nfsm_chain_adv(error, &nmrep, NFSX_UNSIGNED); // skip wtmult
+
+	nfsm_chain_get_32(error, &nmrep, prefsize);
+	nfsmout_if(error);
+	if ((prefsize > 0) && (prefsize < nmp->nm_readdirsize))
+		nmp->nm_readdirsize = prefsize;
+	if ((nmp->nm_fsattr.nfsa_maxread > 0) &&
+	    (nmp->nm_fsattr.nfsa_maxread < nmp->nm_readdirsize))
+		nmp->nm_readdirsize = nmp->nm_fsattr.nfsa_maxread;
+
+	nfsm_chain_get_64(error, &nmrep, nmp->nm_fsattr.nfsa_maxfilesize);
+
+	nfsm_chain_adv(error, &nmrep, 2 * NFSX_UNSIGNED); // skip time_delta
+
+	/* convert FS properties to our own flags */
+	nfsm_chain_get_32(error, &nmrep, val);
+	nfsmout_if(error);
+	if (val & NFSV3FSINFO_LINK)
+		nmp->nm_fsattr.nfsa_flags |= NFS_FSFLAG_LINK;
+	if (val & NFSV3FSINFO_SYMLINK)
+		nmp->nm_fsattr.nfsa_flags |= NFS_FSFLAG_SYMLINK;
+	if (val & NFSV3FSINFO_HOMOGENEOUS)
+		nmp->nm_fsattr.nfsa_flags |= NFS_FSFLAG_HOMOGENEOUS;
+	if (val & NFSV3FSINFO_CANSETTIME)
+		nmp->nm_fsattr.nfsa_flags |= NFS_FSFLAG_SET_TIME;
+	nmp->nm_state |= NFSSTA_GOTFSINFO;
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXREAD);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXWRITE);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXFILESIZE);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_LINK_SUPPORT);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_SYMLINK_SUPPORT);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_HOMOGENEOUS);
+	NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_CANSETTIME);
+nfsmout:
+	if (nmlocked)
+		lck_mtx_unlock(&nmp->nm_lock);
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	return (error);
+}
+
+/*
+ * Mount a remote root fs via. nfs. This depends on the info in the
+ * nfs_diskless structure that has been filled in properly by some primary
+ * bootstrap.
+ * It goes something like this:
+ * - do enough of "ifconfig" by calling ifioctl() so that the system
+ *   can talk to the server
+ * - If nfs_diskless.mygateway is filled in, use that address as
+ *   a default gateway.
+ * - hand craft the swap nfs vnode hanging off a fake mount point
+ *	if swdevt[0].sw_dev == NODEV
+ * - build the rootfs mount point and call mountnfs() to do the rest.
+ */
+int
+nfs_mountroot(void)
+{
+	struct nfs_diskless nd;
+	mount_t mp = NULL;
+	vnode_t vp = NULL;
+	vfs_context_t ctx;
+	int error;
+#if !defined(NO_MOUNT_PRIVATE)
+	mount_t mppriv = NULL;
+	vnode_t vppriv = NULL;
+#endif /* NO_MOUNT_PRIVATE */
+	int v3, sotype;
+
+	/*
+	 * Call nfs_boot_init() to fill in the nfs_diskless struct.
+	 * Note: networking must already have been configured before
+	 * we're called.
+	 */
+	bzero((caddr_t) &nd, sizeof(nd));
+	error = nfs_boot_init(&nd);
+	if (error)
+		panic("nfs_boot_init: unable to initialize NFS root system information, "
+		      "error %d, check configuration: %s\n", error, PE_boot_args());
+
+	/*
+	 * Try NFSv3 first, then fallback to NFSv2.
+	 * Likewise, try TCP first, then fall back to UDP.
+	 */
+	v3 = 1;
+	sotype = SOCK_STREAM;
+
+tryagain:
+	error = nfs_boot_getfh(&nd, v3, sotype);
+	if (error) {
+		if (error == EHOSTDOWN || error == EHOSTUNREACH) {
+			if (nd.nd_root.ndm_mntfrom)
+				FREE_ZONE(nd.nd_root.ndm_mntfrom,
+					  MAXPATHLEN, M_NAMEI);
+			if (nd.nd_root.ndm_path)
+				FREE_ZONE(nd.nd_root.ndm_path,
+					  MAXPATHLEN, M_NAMEI);
+			if (nd.nd_private.ndm_mntfrom)
+				FREE_ZONE(nd.nd_private.ndm_mntfrom,
+					  MAXPATHLEN, M_NAMEI);
+			if (nd.nd_private.ndm_path)
+				FREE_ZONE(nd.nd_private.ndm_path,
+					  MAXPATHLEN, M_NAMEI);
+			return (error);
+		}
+		if (v3) {
+			if (sotype == SOCK_STREAM) {
+				printf("NFS mount (v3,TCP) failed with error %d, trying UDP...\n", error);
+				sotype = SOCK_DGRAM;
+				goto tryagain;
+			}
+			printf("NFS mount (v3,UDP) failed with error %d, trying v2...\n", error);
+			v3 = 0;
+			sotype = SOCK_STREAM;
+			goto tryagain;
+		} else if (sotype == SOCK_STREAM) {
+			printf("NFS mount (v2,TCP) failed with error %d, trying UDP...\n", error);
+			sotype = SOCK_DGRAM;
+			goto tryagain;
+		} else {
+			printf("NFS mount (v2,UDP) failed with error %d, giving up...\n", error);
+		}
+		switch(error) {
+		case EPROGUNAVAIL:
+			panic("NFS mount failed: NFS server mountd not responding, check server configuration: %s", PE_boot_args());
+		case EACCES:
+		case EPERM:
+			panic("NFS mount failed: NFS server refused mount, check server configuration: %s", PE_boot_args());
+		default:
+			panic("NFS mount failed with error %d, check configuration: %s", error, PE_boot_args());
+		}
+	}
+
+	ctx = vfs_context_kernel();
+
+	/*
+	 * Create the root mount point.
+	 */
+#if !defined(NO_MOUNT_PRIVATE)
+	{
+		//PWC hack until we have a real "mount" tool to remount root rw
+		int rw_root=0;
+		int flags = MNT_ROOTFS|MNT_RDONLY;
+		PE_parse_boot_argn("-rwroot_hack", &rw_root, sizeof (rw_root));
+		if(rw_root)
+		{
+			flags = MNT_ROOTFS;
+			kprintf("-rwroot_hack in effect: mounting root fs read/write\n");
+		}
+				
+	if ((error = nfs_mount_diskless(&nd.nd_root, "/", flags, &vp, &mp, ctx)))
+#else
+	if ((error = nfs_mount_diskless(&nd.nd_root, "/", MNT_ROOTFS, &vp, &mp, ctx)))
+#endif /* NO_MOUNT_PRIVATE */
+	{
+		if (v3) {
+			if (sotype == SOCK_STREAM) {
+				printf("NFS root mount (v3,TCP) failed with %d, trying UDP...\n", error);
+				sotype = SOCK_DGRAM;
+				goto tryagain;
+			}
+			printf("NFS root mount (v3,UDP) failed with %d, trying v2...\n", error);
+			v3 = 0;
+			sotype = SOCK_STREAM;
+			goto tryagain;
+		} else if (sotype == SOCK_STREAM) {
+			printf("NFS root mount (v2,TCP) failed with %d, trying UDP...\n", error);
+			sotype = SOCK_DGRAM;
+			goto tryagain;
+		} else {
+			printf("NFS root mount (v2,UDP) failed with error %d, giving up...\n", error);
+		}
+		panic("NFS root mount failed with error %d, check configuration: %s\n", error, PE_boot_args());
+	}
+	}
+	printf("root on %s\n", nd.nd_root.ndm_mntfrom);
+
+	vfs_unbusy(mp);
+	mount_list_add(mp);
+	rootvp = vp;
+	
+#if !defined(NO_MOUNT_PRIVATE)
+	if (nd.nd_private.ndm_saddr.sin_addr.s_addr) {
+	    error = nfs_mount_diskless_private(&nd.nd_private, "/private",
+					       0, &vppriv, &mppriv, ctx);
+	    if (error)
+		panic("NFS /private mount failed with error %d, check configuration: %s\n", error, PE_boot_args());
+	    printf("private on %s\n", nd.nd_private.ndm_mntfrom);
+
+	    vfs_unbusy(mppriv);
+	    mount_list_add(mppriv);
+	}
+
+#endif /* NO_MOUNT_PRIVATE */
+
+	if (nd.nd_root.ndm_mntfrom)
+		FREE_ZONE(nd.nd_root.ndm_mntfrom, MAXPATHLEN, M_NAMEI);
+	if (nd.nd_root.ndm_path)
+		FREE_ZONE(nd.nd_root.ndm_path, MAXPATHLEN, M_NAMEI);
+	if (nd.nd_private.ndm_mntfrom)
+		FREE_ZONE(nd.nd_private.ndm_mntfrom, MAXPATHLEN, M_NAMEI);
+	if (nd.nd_private.ndm_path)
+		FREE_ZONE(nd.nd_private.ndm_path, MAXPATHLEN, M_NAMEI);
+
+	/* Get root attributes (for the time). */
+	error = nfs_getattr(VTONFS(vp), NULL, ctx, NGA_UNCACHED);
+	if (error)
+		panic("NFS mount: failed to get attributes for root directory, error %d, check server", error);
+	return (0);
+}
+
+/*
+ * Internal version of mount system call for diskless setup.
+ */
+static int
+nfs_mount_diskless(
+	struct nfs_dlmount *ndmntp,
+	const char *mntname,
+	int mntflag,
+	vnode_t *vpp,
+	mount_t *mpp,
+	vfs_context_t ctx)
+{
+	mount_t mp;
+	int error, numcomps;
+	char *xdrbuf, *p, *cp, *frompath, *endserverp;
+	char uaddr[MAX_IPv4_STR_LEN];
+	struct xdrbuf xb;
+	uint32_t mattrs[NFS_MATTR_BITMAP_LEN];
+	uint32_t mflags_mask[NFS_MFLAG_BITMAP_LEN];
+	uint32_t mflags[NFS_MFLAG_BITMAP_LEN];
+	uint32_t argslength_offset, attrslength_offset, end_offset;
+
+	if ((error = vfs_rootmountalloc("nfs", ndmntp->ndm_mntfrom, &mp))) {
+		printf("nfs_mount_diskless: NFS not configured\n");
+		return (error);
+	}
+
+	mp->mnt_flag |= mntflag;
+	if (!(mntflag & MNT_RDONLY))
+		mp->mnt_flag &= ~MNT_RDONLY;
+
+	/* find the server-side path being mounted */
+	frompath = ndmntp->ndm_mntfrom;
+	if (*frompath == '[') {  /* skip IPv6 literal address */
+		while (*frompath && (*frompath != ']'))
+			frompath++;
+		if (*frompath == ']')
+			frompath++;
+	}
+	while (*frompath && (*frompath != ':'))
+		frompath++;
+	endserverp = frompath;
+	while (*frompath && (*frompath == ':'))
+		frompath++;
+	/* count fs location path components */
+	p = frompath;
+	while (*p && (*p == '/'))
+		p++;
+	numcomps = 0;
+	while (*p) {
+		numcomps++;
+		while (*p && (*p != '/'))
+			p++;
+		while (*p && (*p == '/'))
+			p++;
+	}
+
+	/* convert address to universal address string */
+	if (inet_ntop(AF_INET, &ndmntp->ndm_saddr.sin_addr, uaddr, sizeof(uaddr)) != uaddr) {
+		printf("nfs_mount_diskless: bad address\n");
+		return (EINVAL);
+	}
+
+	/* prepare mount attributes */
+	NFS_BITMAP_ZERO(mattrs, NFS_MATTR_BITMAP_LEN);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_VERSION);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_SOCKET_TYPE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_PORT);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FH);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FS_LOCATIONS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_MNTFLAGS);
+
+	/* prepare mount flags */
+	NFS_BITMAP_ZERO(mflags_mask, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_ZERO(mflags, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_RESVPORT);
+	NFS_BITMAP_SET(mflags, NFS_MFLAG_RESVPORT);
+
+	/* build xdr buffer */
+	xb_init_buffer(&xb, NULL, 0);
+	xb_add_32(error, &xb, NFS_ARGSVERSION_XDR);
+	argslength_offset = xb_offset(&xb);
+	xb_add_32(error, &xb, 0); // args length
+	xb_add_32(error, &xb, NFS_XDRARGS_VERSION_0);
+	xb_add_bitmap(error, &xb, mattrs, NFS_MATTR_BITMAP_LEN);
+	attrslength_offset = xb_offset(&xb);
+	xb_add_32(error, &xb, 0); // attrs length
+	xb_add_32(error, &xb, ndmntp->ndm_nfsv3 ? 3 : 2); // NFS version
+	xb_add_string(error, &xb, ((ndmntp->ndm_sotype == SOCK_DGRAM) ? "udp" : "tcp"), 3);
+	xb_add_32(error, &xb, ntohs(ndmntp->ndm_saddr.sin_port)); // NFS port
+	xb_add_fh(error, &xb, &ndmntp->ndm_fh[0], ndmntp->ndm_fhlen);
+	/* fs location */
+	xb_add_32(error, &xb, 1); /* fs location count */
+	xb_add_32(error, &xb, 1); /* server count */
+	xb_add_string(error, &xb, ndmntp->ndm_mntfrom, (endserverp - ndmntp->ndm_mntfrom)); /* server name */
+	xb_add_32(error, &xb, 1); /* address count */
+	xb_add_string(error, &xb, uaddr, strlen(uaddr)); /* address */
+	xb_add_32(error, &xb, 0); /* empty server info */
+	xb_add_32(error, &xb, numcomps); /* pathname component count */
+	p = frompath;
+	while (*p && (*p == '/'))
+		p++;
+	while (*p) {
+		cp = p;
+		while (*p && (*p != '/'))
+			p++;
+		xb_add_string(error, &xb, cp, (p - cp)); /* component */
+		if (error)
+			break;
+		while (*p && (*p == '/'))
+			p++;
+	}
+	xb_add_32(error, &xb, 0); /* empty fsl info */
+	xb_add_32(error, &xb, mntflag); /* MNT flags */
+	xb_build_done(error, &xb);
+
+	/* update opaque counts */
+	end_offset = xb_offset(&xb);
+	if (!error) {
+		error = xb_seek(&xb, argslength_offset);
+		xb_add_32(error, &xb, end_offset - argslength_offset + XDRWORD/*version*/);
+	}
+	if (!error) {
+		error = xb_seek(&xb, attrslength_offset);
+		xb_add_32(error, &xb, end_offset - attrslength_offset - XDRWORD/*don't include length field*/);
+	}
+	if (error) {
+		printf("nfs_mount_diskless: error %d assembling mount args\n", error);
+		xb_cleanup(&xb);
+		return (error);
+	}
+	/* grab the assembled buffer */
+	xdrbuf = xb_buffer_base(&xb);
+	xb.xb_flags &= ~XB_CLEANUP;
+
+	/* do the mount */
+	if ((error = mountnfs(xdrbuf, mp, ctx, vpp))) {
+		printf("nfs_mountroot: mount %s failed: %d\n", mntname, error);
+		// XXX vfs_rootmountfailed(mp);
+		mount_list_lock();
+		mp->mnt_vtable->vfc_refcount--;
+		mount_list_unlock();
+		vfs_unbusy(mp);
+		mount_lock_destroy(mp);
+#if CONFIG_MACF
+		mac_mount_label_destroy(mp);
+#endif
+		FREE_ZONE(mp, sizeof(struct mount), M_MOUNT);
+	} else {
+		*mpp = mp;
+	}
+	xb_cleanup(&xb);
+	return (error);
+}
+
+#if !defined(NO_MOUNT_PRIVATE)
+/*
+ * Internal version of mount system call to mount "/private"
+ * separately in diskless setup
+ */
+static int
+nfs_mount_diskless_private(
+	struct nfs_dlmount *ndmntp,
+	const char *mntname,
+	int mntflag,
+	vnode_t *vpp,
+	mount_t *mpp,
+	vfs_context_t ctx)
+{
+	mount_t mp;
+	int error, numcomps;
+	proc_t procp;
+	struct vfstable *vfsp;
+	struct nameidata nd;
+	vnode_t vp;
+	char *xdrbuf = NULL, *p, *cp, *frompath, *endserverp;
+	char uaddr[MAX_IPv4_STR_LEN];
+	struct xdrbuf xb;
+	uint32_t mattrs[NFS_MATTR_BITMAP_LEN];
+	uint32_t mflags_mask[NFS_MFLAG_BITMAP_LEN], mflags[NFS_MFLAG_BITMAP_LEN];
+	uint32_t argslength_offset, attrslength_offset, end_offset;
+
+	procp = current_proc(); /* XXX */
+	xb_init(&xb, 0);
+
+	{
+	/*
+	 * mimic main()!. Temporarily set up rootvnode and other stuff so
+	 * that namei works. Need to undo this because main() does it, too
+	 */
+		struct filedesc *fdp;	/* pointer to file descriptor state */
+		fdp = procp->p_fd;
+		mountlist.tqh_first->mnt_flag |= MNT_ROOTFS;
+
+		/* Get the vnode for '/'. Set fdp->fd_cdir to reference it. */
+		if (VFS_ROOT(mountlist.tqh_first, &rootvnode, NULL))
+			panic("cannot find root vnode");
+		error = vnode_ref(rootvnode);
+		if (error) {
+			printf("nfs_mountroot: vnode_ref() failed on root vnode!\n");
+			goto out;
+		}
+		fdp->fd_cdir = rootvnode;
+		fdp->fd_rdir = NULL;
+	}
+
+	/*
+	 * Get vnode to be covered
+	 */
+	NDINIT(&nd, LOOKUP, OP_LOOKUP, FOLLOW | LOCKLEAF, UIO_SYSSPACE,
+	    CAST_USER_ADDR_T(mntname), ctx);
+	if ((error = namei(&nd))) {
+		printf("nfs_mountroot: private namei failed!\n");
+		goto out;
+	}
+	{
+		/* undo vnode_ref() in mimic main()! */
+		vnode_rele(rootvnode);
+	}
+	nameidone(&nd);
+	vp = nd.ni_vp;
+
+	if ((error = VNOP_FSYNC(vp, MNT_WAIT, ctx)) ||
+	    (error = buf_invalidateblks(vp, BUF_WRITE_DATA, 0, 0))) {
+		vnode_put(vp);
+		goto out;
+	}
+	if (vnode_vtype(vp) != VDIR) {
+		vnode_put(vp);
+		error = ENOTDIR;
+		goto out;
+	}
+	for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
+		if (!strncmp(vfsp->vfc_name, "nfs", sizeof(vfsp->vfc_name)))
+			break;
+	if (vfsp == NULL) {
+		printf("nfs_mountroot: private NFS not configured\n");
+		vnode_put(vp);
+		error = ENODEV;
+		goto out;
+	}
+	if (vnode_mountedhere(vp) != NULL) {
+		vnode_put(vp);
+		error = EBUSY;
+		goto out;
+	}
+
+	/*
+	 * Allocate and initialize the filesystem.
+	 */
+	mp = _MALLOC_ZONE((u_int32_t)sizeof(struct mount), M_MOUNT, M_WAITOK);
+	if (!mp) {
+		printf("nfs_mountroot: unable to allocate mount structure\n");
+		vnode_put(vp);
+		error = ENOMEM;
+		goto out;
+	}
+	bzero((char *)mp, sizeof(struct mount));
+
+	/* Initialize the default IO constraints */
+	mp->mnt_maxreadcnt = mp->mnt_maxwritecnt = MAXPHYS;
+	mp->mnt_segreadcnt = mp->mnt_segwritecnt = 32;
+	mp->mnt_ioflags = 0;
+	mp->mnt_realrootvp = NULLVP;
+	mp->mnt_authcache_ttl = CACHED_LOOKUP_RIGHT_TTL;
+
+	mount_lock_init(mp);
+	TAILQ_INIT(&mp->mnt_vnodelist);
+	TAILQ_INIT(&mp->mnt_workerqueue);
+	TAILQ_INIT(&mp->mnt_newvnodes);
+	(void)vfs_busy(mp, LK_NOWAIT);
+	TAILQ_INIT(&mp->mnt_vnodelist);
+	mount_list_lock();
+	vfsp->vfc_refcount++;
+	mount_list_unlock();
+	mp->mnt_vtable = vfsp;
+	mp->mnt_op = vfsp->vfc_vfsops;
+	// mp->mnt_stat.f_type = vfsp->vfc_typenum;
+	mp->mnt_flag = mntflag;
+	mp->mnt_flag |= vfsp->vfc_flags & MNT_VISFLAGMASK;
+	strncpy(mp->mnt_vfsstat.f_fstypename, vfsp->vfc_name, MFSNAMELEN-1);
+	vp->v_mountedhere = mp;
+	mp->mnt_vnodecovered = vp;
+	vp = NULLVP;
+	mp->mnt_vfsstat.f_owner = kauth_cred_getuid(kauth_cred_get());
+	(void) copystr(mntname, mp->mnt_vfsstat.f_mntonname, MAXPATHLEN - 1, 0);
+	(void) copystr(ndmntp->ndm_mntfrom, mp->mnt_vfsstat.f_mntfromname, MAXPATHLEN - 1, 0);
+#if CONFIG_MACF
+	mac_mount_label_init(mp);
+	mac_mount_label_associate(ctx, mp);
+#endif
+
+	/* find the server-side path being mounted */
+	frompath = ndmntp->ndm_mntfrom;
+	if (*frompath == '[') {  /* skip IPv6 literal address */
+		while (*frompath && (*frompath != ']'))
+			frompath++;
+		if (*frompath == ']')
+			frompath++;
+	}
+	while (*frompath && (*frompath != ':'))
+		frompath++;
+	endserverp = frompath;
+	while (*frompath && (*frompath == ':'))
+		frompath++;
+	/* count fs location path components */
+	p = frompath;
+	while (*p && (*p == '/'))
+		p++;
+	numcomps = 0;
+	while (*p) {
+		numcomps++;
+		while (*p && (*p != '/'))
+			p++;
+		while (*p && (*p == '/'))
+			p++;
+	}
+
+	/* convert address to universal address string */
+	if (inet_ntop(AF_INET, &ndmntp->ndm_saddr.sin_addr, uaddr, sizeof(uaddr)) != uaddr) {
+		printf("nfs_mountroot: bad address\n");
+		error = EINVAL;
+		goto out;
+	}
+
+	/* prepare mount attributes */
+	NFS_BITMAP_ZERO(mattrs, NFS_MATTR_BITMAP_LEN);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_VERSION);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_SOCKET_TYPE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_PORT);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FH);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FS_LOCATIONS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_MNTFLAGS);
+
+	/* prepare mount flags */
+	NFS_BITMAP_ZERO(mflags_mask, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_ZERO(mflags, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_RESVPORT);
+	NFS_BITMAP_SET(mflags, NFS_MFLAG_RESVPORT);
+
+	/* build xdr buffer */
+	xb_init_buffer(&xb, NULL, 0);
+	xb_add_32(error, &xb, NFS_ARGSVERSION_XDR);
+	argslength_offset = xb_offset(&xb);
+	xb_add_32(error, &xb, 0); // args length
+	xb_add_32(error, &xb, NFS_XDRARGS_VERSION_0);
+	xb_add_bitmap(error, &xb, mattrs, NFS_MATTR_BITMAP_LEN);
+	attrslength_offset = xb_offset(&xb);
+	xb_add_32(error, &xb, 0); // attrs length
+	xb_add_32(error, &xb, ndmntp->ndm_nfsv3 ? 3 : 2); // NFS version
+	xb_add_string(error, &xb, ((ndmntp->ndm_sotype == SOCK_DGRAM) ? "udp" : "tcp"), 3);
+	xb_add_32(error, &xb, ntohs(ndmntp->ndm_saddr.sin_port)); // NFS port
+	xb_add_fh(error, &xb, &ndmntp->ndm_fh[0], ndmntp->ndm_fhlen);
+	/* fs location */
+	xb_add_32(error, &xb, 1); /* fs location count */
+	xb_add_32(error, &xb, 1); /* server count */
+	xb_add_string(error, &xb, ndmntp->ndm_mntfrom, (endserverp - ndmntp->ndm_mntfrom)); /* server name */
+	xb_add_32(error, &xb, 1); /* address count */
+	xb_add_string(error, &xb, uaddr, strlen(uaddr)); /* address */
+	xb_add_32(error, &xb, 0); /* empty server info */
+	xb_add_32(error, &xb, numcomps); /* pathname component count */
+	p = frompath;
+	while (*p && (*p == '/'))
+		p++;
+	while (*p) {
+		cp = p;
+		while (*p && (*p != '/'))
+			p++;
+		xb_add_string(error, &xb, cp, (p - cp)); /* component */
+		if (error)
+			break;
+		while (*p && (*p == '/'))
+			p++;
+	}
+	xb_add_32(error, &xb, 0); /* empty fsl info */
+	xb_add_32(error, &xb, mntflag); /* MNT flags */
+	xb_build_done(error, &xb);
+
+	/* update opaque counts */
+	end_offset = xb_offset(&xb);
+	if (!error) {
+		error = xb_seek(&xb, argslength_offset);
+		xb_add_32(error, &xb, end_offset - argslength_offset + XDRWORD/*version*/);
+	}
+	if (!error) {
+		error = xb_seek(&xb, attrslength_offset);
+		xb_add_32(error, &xb, end_offset - attrslength_offset - XDRWORD/*don't include length field*/);
+	}
+	if (error) {
+		printf("nfs_mountroot: error %d assembling mount args\n", error);
+		goto out;
+	}
+	/* grab the assembled buffer */
+	xdrbuf = xb_buffer_base(&xb);
+	xb.xb_flags &= ~XB_CLEANUP;
+
+	/* do the mount */
+	if ((error = mountnfs(xdrbuf, mp, ctx, &vp))) {
+		printf("nfs_mountroot: mount %s failed: %d\n", mntname, error);
+		vnode_put(mp->mnt_vnodecovered);
+		mount_list_lock();
+		vfsp->vfc_refcount--;
+		mount_list_unlock();
+		vfs_unbusy(mp);
+		mount_lock_destroy(mp);
+#if CONFIG_MACF
+		mac_mount_label_destroy(mp);
+#endif
+		FREE_ZONE(mp, sizeof (struct mount), M_MOUNT);
+		goto out;
+	}
+
+	*mpp = mp;
+	*vpp = vp;
+out:
+	xb_cleanup(&xb);
+	return (error);
+}
+#endif /* NO_MOUNT_PRIVATE */
+
+/*
+ * Convert old style NFS mount args to XDR.
+ */
+static int
+nfs_convert_old_nfs_args(mount_t mp, user_addr_t data, vfs_context_t ctx, int argsversion, int inkernel, char **xdrbufp)
+{
+	int error = 0, args64bit, argsize, numcomps;
+	struct user_nfs_args args;
+	struct nfs_args tempargs;
+	caddr_t argsp;
+	size_t len;
+	u_char nfh[NFS4_FHSIZE];
+	char *mntfrom, *endserverp, *frompath, *p, *cp;
+	struct sockaddr_storage ss;
+	void *sinaddr;
+	char uaddr[MAX_IPv6_STR_LEN];
+	uint32_t mattrs[NFS_MATTR_BITMAP_LEN];
+	uint32_t mflags_mask[NFS_MFLAG_BITMAP_LEN], mflags[NFS_MFLAG_BITMAP_LEN];
+	uint32_t nfsvers, nfslockmode = 0, argslength_offset, attrslength_offset, end_offset;
+	struct xdrbuf xb;
+
+	*xdrbufp = NULL;
+
+	/* allocate a temporary buffer for mntfrom */
+	MALLOC_ZONE(mntfrom, char*, MAXPATHLEN, M_NAMEI, M_WAITOK);
+	if (!mntfrom)
+		return (ENOMEM);
+
+	args64bit = (inkernel || vfs_context_is64bit(ctx));
+	argsp = args64bit ? (void*)&args : (void*)&tempargs;
+
+	argsize = args64bit ? sizeof(args) : sizeof(tempargs);
+	switch (argsversion) {
+	case 3:
+		argsize -= NFS_ARGSVERSION4_INCSIZE;
+	case 4:
+		argsize -= NFS_ARGSVERSION5_INCSIZE;
+	case 5:
+		argsize -= NFS_ARGSVERSION6_INCSIZE;
+	case 6:
+		break;
+	default:
+		error = EPROGMISMATCH;
+		goto nfsmout;
+	}
+
+	/* read in the structure */
+	if (inkernel)
+		bcopy(CAST_DOWN(void *, data), argsp, argsize);
+	else
+		error = copyin(data, argsp, argsize);
+	nfsmout_if(error);
+
+	if (!args64bit) {
+		args.addrlen = tempargs.addrlen;
+		args.sotype = tempargs.sotype;
+		args.proto = tempargs.proto;
+		args.fhsize = tempargs.fhsize;
+		args.flags = tempargs.flags;
+		args.wsize = tempargs.wsize;
+		args.rsize = tempargs.rsize;
+		args.readdirsize = tempargs.readdirsize;
+		args.timeo = tempargs.timeo;
+		args.retrans = tempargs.retrans;
+		args.maxgrouplist = tempargs.maxgrouplist;
+		args.readahead = tempargs.readahead;
+		args.leaseterm = tempargs.leaseterm;
+		args.deadthresh = tempargs.deadthresh;
+		args.addr = CAST_USER_ADDR_T(tempargs.addr);
+		args.fh = CAST_USER_ADDR_T(tempargs.fh);
+		args.hostname = CAST_USER_ADDR_T(tempargs.hostname);
+		if (args.version >= 4) {
+			args.acregmin = tempargs.acregmin;
+			args.acregmax = tempargs.acregmax;
+			args.acdirmin = tempargs.acdirmin;
+			args.acdirmax = tempargs.acdirmax;
+		}
+		if (args.version >= 5)
+			args.auth = tempargs.auth;
+		if (args.version >= 6)
+			args.deadtimeout = tempargs.deadtimeout;
+	}
+
+	if ((args.fhsize < 0) || (args.fhsize > NFS4_FHSIZE)) {
+		error = EINVAL;
+		goto nfsmout;
+	}
+	if (args.fhsize > 0) {
+		if (inkernel)
+			bcopy(CAST_DOWN(void *, args.fh), (caddr_t)nfh, args.fhsize);
+		else
+			error = copyin(args.fh, (caddr_t)nfh, args.fhsize);
+		nfsmout_if(error);
+	}
+
+	if (inkernel)
+		error = copystr(CAST_DOWN(void *, args.hostname), mntfrom, MAXPATHLEN-1, &len);
+	else
+		error = copyinstr(args.hostname, mntfrom, MAXPATHLEN-1, &len);
+	nfsmout_if(error);
+	bzero(&mntfrom[len], MAXPATHLEN - len);
+
+	/* find the server-side path being mounted */
+	frompath = mntfrom;
+	if (*frompath == '[') {  /* skip IPv6 literal address */
+		while (*frompath && (*frompath != ']'))
+			frompath++;
+		if (*frompath == ']')
+			frompath++;
+	}
+	while (*frompath && (*frompath != ':'))
+		frompath++;
+	endserverp = frompath;
+	while (*frompath && (*frompath == ':'))
+		frompath++;
+	/* count fs location path components */
+	p = frompath;
+	while (*p && (*p == '/'))
+		p++;
+	numcomps = 0;
+	while (*p) {
+		numcomps++;
+		while (*p && (*p != '/'))
+			p++;
+		while (*p && (*p == '/'))
+			p++;
+	}
+
+	/* copy socket address */
+	if (inkernel)
+		bcopy(CAST_DOWN(void *, args.addr), &ss, args.addrlen);
+	else {
+		if ((size_t)args.addrlen > sizeof (struct sockaddr_storage))
+			error = EINVAL;
+		else
+			error = copyin(args.addr, &ss, args.addrlen);
+	}
+	nfsmout_if(error);
+	ss.ss_len = args.addrlen;
+
+	/* convert address to universal address string */
+	if (ss.ss_family == AF_INET)
+		sinaddr = &((struct sockaddr_in*)&ss)->sin_addr;
+	else if (ss.ss_family == AF_INET6)
+		sinaddr = &((struct sockaddr_in6*)&ss)->sin6_addr;
+	else
+		sinaddr = NULL;
+	if (!sinaddr || (inet_ntop(ss.ss_family, sinaddr, uaddr, sizeof(uaddr)) != uaddr)) {
+		error = EINVAL;
+		goto nfsmout;
+	}
+
+	/* prepare mount flags */
+	NFS_BITMAP_ZERO(mflags_mask, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_ZERO(mflags, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_SOFT);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_INTR);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_RESVPORT);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NOCONNECT);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_DUMBTIMER);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_CALLUMNT);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_RDIRPLUS);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NONEGNAMECACHE);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_MUTEJUKEBOX);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NOQUOTA);
+	if (args.flags & NFSMNT_SOFT)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_SOFT);
+	if (args.flags & NFSMNT_INT)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_INTR);
+	if (args.flags & NFSMNT_RESVPORT)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_RESVPORT);
+	if (args.flags & NFSMNT_NOCONN)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NOCONNECT);
+	if (args.flags & NFSMNT_DUMBTIMR)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_DUMBTIMER);
+	if (args.flags & NFSMNT_CALLUMNT)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_CALLUMNT);
+	if (args.flags & NFSMNT_RDIRPLUS)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_RDIRPLUS);
+	if (args.flags & NFSMNT_NONEGNAMECACHE)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NONEGNAMECACHE);
+	if (args.flags & NFSMNT_MUTEJUKEBOX)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_MUTEJUKEBOX);
+	if (args.flags & NFSMNT_NOQUOTA)
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NOQUOTA);
+
+	/* prepare mount attributes */
+	NFS_BITMAP_ZERO(mattrs, NFS_MATTR_BITMAP_LEN);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FLAGS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_VERSION);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_SOCKET_TYPE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_PORT);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FH);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FS_LOCATIONS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_MNTFLAGS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_MNTFROM);
+	if (args.flags & NFSMNT_NFSV4)
+		nfsvers = 4;
+	else if (args.flags & NFSMNT_NFSV3)
+		nfsvers = 3;
+	else
+		nfsvers = 2;
+	if ((args.flags & NFSMNT_RSIZE) && (args.rsize > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_READ_SIZE);
+	if ((args.flags & NFSMNT_WSIZE) && (args.wsize > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_WRITE_SIZE);
+	if ((args.flags & NFSMNT_TIMEO) && (args.timeo > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_REQUEST_TIMEOUT);
+	if ((args.flags & NFSMNT_RETRANS) && (args.retrans > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_SOFT_RETRY_COUNT);
+	if ((args.flags & NFSMNT_MAXGRPS) && (args.maxgrouplist > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_MAX_GROUP_LIST);
+	if ((args.flags & NFSMNT_READAHEAD) && (args.readahead > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_READAHEAD);
+	if ((args.flags & NFSMNT_READDIRSIZE) && (args.readdirsize > 0))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_READDIR_SIZE);
+	if ((args.flags & NFSMNT_NOLOCKS) ||
+	    (args.flags & NFSMNT_LOCALLOCKS)) {
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_LOCK_MODE);
+		if (args.flags & NFSMNT_NOLOCKS)
+			nfslockmode = NFS_LOCK_MODE_DISABLED;
+		else if (args.flags & NFSMNT_LOCALLOCKS)
+			nfslockmode = NFS_LOCK_MODE_LOCAL;
+		else
+			nfslockmode = NFS_LOCK_MODE_ENABLED;
+	}
+	if (args.version >= 4) {
+		if ((args.flags & NFSMNT_ACREGMIN) && (args.acregmin > 0))
+			NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_REG_MIN);
+		if ((args.flags & NFSMNT_ACREGMAX) && (args.acregmax > 0))
+			NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_REG_MAX);
+		if ((args.flags & NFSMNT_ACDIRMIN) && (args.acdirmin > 0))
+			NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MIN);
+		if ((args.flags & NFSMNT_ACDIRMAX) && (args.acdirmax > 0))
+			NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MAX);
+	}
+	if (args.version >= 5) {
+		if ((args.flags & NFSMNT_SECFLAVOR) || (args.flags & NFSMNT_SECSYSOK))
+			NFS_BITMAP_SET(mattrs, NFS_MATTR_SECURITY);
+	}
+	if (args.version >= 6) {
+		if ((args.flags & NFSMNT_DEADTIMEOUT) && (args.deadtimeout > 0))
+			NFS_BITMAP_SET(mattrs, NFS_MATTR_DEAD_TIMEOUT);
+	}
+
+	/* build xdr buffer */
+	xb_init_buffer(&xb, NULL, 0);
+	xb_add_32(error, &xb, args.version);
+	argslength_offset = xb_offset(&xb);
+	xb_add_32(error, &xb, 0); // args length
+	xb_add_32(error, &xb, NFS_XDRARGS_VERSION_0);
+	xb_add_bitmap(error, &xb, mattrs, NFS_MATTR_BITMAP_LEN);
+	attrslength_offset = xb_offset(&xb);
+	xb_add_32(error, &xb, 0); // attrs length
+	xb_add_bitmap(error, &xb, mflags_mask, NFS_MFLAG_BITMAP_LEN); /* mask */
+	xb_add_bitmap(error, &xb, mflags, NFS_MFLAG_BITMAP_LEN); /* value */
+	xb_add_32(error, &xb, nfsvers);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READ_SIZE))
+		xb_add_32(error, &xb, args.rsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_WRITE_SIZE))
+		xb_add_32(error, &xb, args.wsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READDIR_SIZE))
+		xb_add_32(error, &xb, args.readdirsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READAHEAD))
+		xb_add_32(error, &xb, args.readahead);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_REG_MIN)) {
+		xb_add_32(error, &xb, args.acregmin);
+		xb_add_32(error, &xb, 0);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_REG_MAX)) {
+		xb_add_32(error, &xb, args.acregmax);
+		xb_add_32(error, &xb, 0);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MIN)) {
+		xb_add_32(error, &xb, args.acdirmin);
+		xb_add_32(error, &xb, 0);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MAX)) {
+		xb_add_32(error, &xb, args.acdirmax);
+		xb_add_32(error, &xb, 0);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_LOCK_MODE))
+		xb_add_32(error, &xb, nfslockmode);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SECURITY)) {
+		uint32_t flavors[2], i=0;
+		if (args.flags & NFSMNT_SECFLAVOR)
+			flavors[i++] = args.auth;
+		if ((args.flags & NFSMNT_SECSYSOK) && ((i == 0) || (flavors[0] != RPCAUTH_SYS)))
+			flavors[i++] = RPCAUTH_SYS;
+		xb_add_word_array(error, &xb, flavors, i);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MAX_GROUP_LIST))
+		xb_add_32(error, &xb, args.maxgrouplist);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SOCKET_TYPE))
+		xb_add_string(error, &xb, ((args.sotype == SOCK_DGRAM) ? "udp" : "tcp"), 3);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_PORT))
+		xb_add_32(error, &xb, ((ss.ss_family == AF_INET) ? 
+			ntohs(((struct sockaddr_in*)&ss)->sin_port) :
+			ntohs(((struct sockaddr_in6*)&ss)->sin6_port)));
+	/* NFS_MATTR_MOUNT_PORT (not available in old args) */
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_REQUEST_TIMEOUT)) {
+		/* convert from .1s increments to time */
+		xb_add_32(error, &xb, args.timeo/10);
+		xb_add_32(error, &xb, (args.timeo%10)*100000000);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SOFT_RETRY_COUNT))
+		xb_add_32(error, &xb, args.retrans);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_DEAD_TIMEOUT)) {
+		xb_add_32(error, &xb, args.deadtimeout);
+		xb_add_32(error, &xb, 0);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FH))
+		xb_add_fh(error, &xb, &nfh[0], args.fhsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FS_LOCATIONS)) {
+		xb_add_32(error, &xb, 1); /* fs location count */
+		xb_add_32(error, &xb, 1); /* server count */
+		xb_add_string(error, &xb, mntfrom, (endserverp - mntfrom)); /* server name */
+		xb_add_32(error, &xb, 1); /* address count */
+		xb_add_string(error, &xb, uaddr, strlen(uaddr)); /* address */
+		xb_add_32(error, &xb, 0); /* empty server info */
+		xb_add_32(error, &xb, numcomps); /* pathname component count */
+		nfsmout_if(error);
+		p = frompath;
+		while (*p && (*p == '/'))
+			p++;
+		while (*p) {
+			cp = p;
+			while (*p && (*p != '/'))
+				p++;
+			xb_add_string(error, &xb, cp, (p - cp)); /* component */
+			nfsmout_if(error);
+			while (*p && (*p == '/'))
+				p++;
+		}
+		xb_add_32(error, &xb, 0); /* empty fsl info */
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MNTFLAGS))
+		xb_add_32(error, &xb, (vfs_flags(mp) & MNT_VISFLAGMASK)); /* VFS MNT_* flags */
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MNTFROM))
+		xb_add_string(error, &xb, mntfrom, strlen(mntfrom)); /* fixed f_mntfromname */
+	xb_build_done(error, &xb);
+
+	/* update opaque counts */
+	end_offset = xb_offset(&xb);
+	error = xb_seek(&xb, argslength_offset);
+	xb_add_32(error, &xb, end_offset - argslength_offset + XDRWORD/*version*/);
+	nfsmout_if(error);
+	error = xb_seek(&xb, attrslength_offset);
+	xb_add_32(error, &xb, end_offset - attrslength_offset - XDRWORD/*don't include length field*/);
+
+	if (!error) {
+		/* grab the assembled buffer */
+		*xdrbufp = xb_buffer_base(&xb);
+		xb.xb_flags &= ~XB_CLEANUP;
+	}
+nfsmout:
+	xb_cleanup(&xb);
+	FREE_ZONE(mntfrom, MAXPATHLEN, M_NAMEI);
+	return (error);
+}
+
+/*
+ * VFS Operations.
+ *
+ * mount system call
+ */
+int
+nfs_vfs_mount(mount_t mp, vnode_t vp, user_addr_t data, vfs_context_t ctx)
+{
+	int error = 0, inkernel = vfs_iskernelmount(mp);
+	uint32_t argsversion, argslength;
+	char *xdrbuf = NULL;
+
+	/* read in version */
+	if (inkernel)
+		bcopy(CAST_DOWN(void *, data), &argsversion, sizeof(argsversion));
+	else if ((error = copyin(data, &argsversion, sizeof(argsversion))))
+		return (error);
+
+	/* If we have XDR args, then all values in the buffer are in network order */
+	if (argsversion == htonl(NFS_ARGSVERSION_XDR))
+		argsversion = NFS_ARGSVERSION_XDR;
+
+	switch (argsversion) {
+	case 3:
+	case 4:
+	case 5:
+	case 6:
+		/* convert old-style args to xdr */
+		error = nfs_convert_old_nfs_args(mp, data, ctx, argsversion, inkernel, &xdrbuf);
+		break;
+	case NFS_ARGSVERSION_XDR:
+		/* copy in xdr buffer */
+		if (inkernel)
+			bcopy(CAST_DOWN(void *, (data + XDRWORD)), &argslength, XDRWORD);
+		else
+			error = copyin((data + XDRWORD), &argslength, XDRWORD);
+		if (error)
+			break;
+		argslength = ntohl(argslength);
+		/* put a reasonable limit on the size of the XDR args */
+		if (argslength > 16*1024) {
+			error = E2BIG;
+			break;
+		}
+		/* allocate xdr buffer */
+		xdrbuf = xb_malloc(xdr_rndup(argslength));
+		if (!xdrbuf) {
+			error = ENOMEM;
+			break;
+		}
+		if (inkernel)
+			bcopy(CAST_DOWN(void *, data), xdrbuf, argslength);
+		else
+			error = copyin(data, xdrbuf, argslength);
+		break;
+	default:
+		error = EPROGMISMATCH;
+	}
+
+	if (error) {
+		if (xdrbuf)
+			xb_free(xdrbuf);
+		return (error);
+	}
+	error = mountnfs(xdrbuf, mp, ctx, &vp);
+	return (error);
+}
+
+/*
+ * Common code for mount and mountroot
+ */
+
+/* Set up an NFSv2/v3 mount */
+int
+nfs3_mount(
+	struct nfsmount *nmp,
+	vfs_context_t ctx,
+	nfsnode_t *npp)
+{
+	int error = 0;
+	struct nfs_vattr nvattr;
+	u_int64_t xid;
+
+	*npp = NULL;
+
+	if (!nmp->nm_fh)
+		return (EINVAL);
+
+	/*
+	 * Get file attributes for the mountpoint.  These are needed
+	 * in order to properly create the root vnode.
+	 */
+	error = nfs3_getattr_rpc(NULL, nmp->nm_mountp, nmp->nm_fh->fh_data, nmp->nm_fh->fh_len, 0,
+			ctx, &nvattr, &xid);
+	if (error)
+		goto out;
+
+	error = nfs_nget(nmp->nm_mountp, NULL, NULL, nmp->nm_fh->fh_data, nmp->nm_fh->fh_len,
+			&nvattr, &xid, RPCAUTH_UNKNOWN, NG_MARKROOT, npp);
+	if (*npp)
+		nfs_node_unlock(*npp);
+	if (error)
+		goto out;
+
+	/*
+	 * Try to make sure we have all the general info from the server.
+	 */
+	if (nmp->nm_vers == NFS_VER2) {
+		NFS_BITMAP_SET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXNAME);
+		nmp->nm_fsattr.nfsa_maxname = NFS_MAXNAMLEN;
+	} else if (nmp->nm_vers == NFS_VER3) {
+		/* get the NFSv3 FSINFO */
+		error = nfs3_fsinfo(nmp, *npp, ctx);
+		if (error)
+			goto out;
+		/* If the server indicates all pathconf info is */
+		/* the same, grab a copy of that info now */
+		if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_HOMOGENEOUS) &&
+		    (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_HOMOGENEOUS)) {
+			struct nfs_fsattr nfsa;
+			if (!nfs3_pathconf_rpc(*npp, &nfsa, ctx)) {
+				/* cache a copy of the results */
+				lck_mtx_lock(&nmp->nm_lock);
+				nfs3_pathconf_cache(nmp, &nfsa);
+				lck_mtx_unlock(&nmp->nm_lock);
+			}
+		}
+	}
+out:
+	if (*npp && error) {
+		vnode_put(NFSTOV(*npp));
+		vnode_recycle(NFSTOV(*npp));
+		*npp = NULL;
+	}
+	return (error);
+}
+
+/*
+ * Update an NFSv4 mount path with the contents of the symlink.
+ *
+ * Read the link for the given file handle.
+ * Insert the link's components into the path.
+ */
+int
+nfs4_mount_update_path_with_symlink(struct nfsmount *nmp, struct nfs_fs_path *nfsp, uint32_t curcomp, fhandle_t *dirfhp, int *depthp, fhandle_t *fhp, vfs_context_t ctx)
+{
+	int error = 0, status, numops;
+	uint32_t len = 0, comp, newcomp, linkcompcount;
+	u_int64_t xid;
+	struct nfsm_chain nmreq, nmrep;
+	struct nfsreq rq, *req = &rq;
+	struct nfsreq_secinfo_args si;
+	char *link = NULL, *p, *q, ch;
+	struct nfs_fs_path nfsp2;
+
+	bzero(&nfsp2, sizeof(nfsp2));
+	if (dirfhp->fh_len)
+		NFSREQ_SECINFO_SET(&si, NULL, dirfhp->fh_data, dirfhp->fh_len, nfsp->np_components[curcomp], 0);
+	else
+		NFSREQ_SECINFO_SET(&si, NULL, NULL, 0, nfsp->np_components[curcomp], 0);
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	MALLOC_ZONE(link, char *, MAXPATHLEN, M_NAMEI, M_WAITOK); 
+	if (!link)
+		error = ENOMEM;
+
+	// PUTFH, READLINK
+	numops = 2;
+	nfsm_chain_build_alloc_init(error, &nmreq, 12 * NFSX_UNSIGNED);
+	nfsm_chain_add_compound_header(error, &nmreq, "readlink", nmp->nm_minor_vers, numops);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTFH);
+	nfsm_chain_add_fh(error, &nmreq, NFS_VER4, fhp->fh_data, fhp->fh_len);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_READLINK);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsm_assert(error, (numops == 0), EPROTO);
+	nfsmout_if(error);
+
+	error = nfs_request_async(NULL, nmp->nm_mountp, &nmreq, NFSPROC4_COMPOUND,
+			vfs_context_thread(ctx), vfs_context_ucred(ctx), &si, 0, NULL, &req);
+	if (!error)
+		error = nfs_request_async_finish(req, &nmrep, &xid, &status);
+
+	nfsm_chain_skip_tag(error, &nmrep);
+	nfsm_chain_get_32(error, &nmrep, numops);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_PUTFH);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_READLINK);
+	nfsm_chain_get_32(error, &nmrep, len);
+	nfsmout_if(error);
+	if (len == 0)
+		error = ENOENT;
+	else if (len >= MAXPATHLEN)
+		len = MAXPATHLEN - 1;
+	nfsm_chain_get_opaque(error, &nmrep, len, link);
+	nfsmout_if(error);
+	/* make sure link string is terminated properly */
+	link[len] = '\0';
+
+	/* count the number of components in link */
+	p = link;
+	while (*p && (*p == '/'))
+		p++;
+	linkcompcount = 0;
+	while (*p) {
+		linkcompcount++;
+		while (*p && (*p != '/'))
+			p++;
+		while (*p && (*p == '/'))
+			p++;
+	}
+
+	/* free up used components */
+	for (comp=0; comp <= curcomp; comp++) {
+		if (nfsp->np_components[comp]) {
+			FREE(nfsp->np_components[comp], M_TEMP);
+			nfsp->np_components[comp] = NULL;
+		}
+	}
+
+	/* set up new path */
+	nfsp2.np_compcount = nfsp->np_compcount - curcomp - 1 + linkcompcount;
+	MALLOC(nfsp2.np_components, char **, nfsp2.np_compcount*sizeof(char*), M_TEMP, M_WAITOK|M_ZERO);
+	if (!nfsp2.np_components) {
+		error = ENOMEM;
+		goto nfsmout;
+	}
+
+	/* add link components */
+	p = link;
+	while (*p && (*p == '/'))
+		p++;
+	for (newcomp=0; newcomp < linkcompcount; newcomp++) {
+		/* find end of component */
+		q = p;
+		while (*q && (*q != '/'))
+			q++;
+		MALLOC(nfsp2.np_components[newcomp], char *, q-p+1, M_TEMP, M_WAITOK|M_ZERO);
+		if (!nfsp2.np_components[newcomp]) {
+			error = ENOMEM;
+			break;
+		}
+		ch = *q;
+		*q = '\0';
+		strlcpy(nfsp2.np_components[newcomp], p, q-p+1);
+		*q = ch;
+		p = q;
+		while (*p && (*p == '/'))
+			p++;
+	}
+	nfsmout_if(error);
+
+	/* add remaining components */
+	for(comp = curcomp + 1; comp < nfsp->np_compcount; comp++,newcomp++) {
+		nfsp2.np_components[newcomp] = nfsp->np_components[comp];
+		nfsp->np_components[comp] = NULL;
+	}
+
+	/* move new path into place */
+	FREE(nfsp->np_components, M_TEMP);
+	nfsp->np_components = nfsp2.np_components;
+	nfsp->np_compcount = nfsp2.np_compcount;
+	nfsp2.np_components = NULL;
+
+	/* for absolute link, let the caller now that the next dirfh is root */
+	if (link[0] == '/') {
+		dirfhp->fh_len = 0;
+		*depthp = 0;
+	}
+nfsmout:
+	if (link)
+		FREE_ZONE(link, MAXPATHLEN, M_NAMEI);
+	if (nfsp2.np_components) {
+		for (comp=0; comp < nfsp2.np_compcount; comp++)
+			if (nfsp2.np_components[comp])
+				FREE(nfsp2.np_components[comp], M_TEMP);
+		FREE(nfsp2.np_components, M_TEMP);
+	}
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	return (error);
+}
+
+/* Set up an NFSv4 mount */
+int
+nfs4_mount(
+	struct nfsmount *nmp,
+	vfs_context_t ctx,
+	nfsnode_t *npp)
+{
+	struct nfsm_chain nmreq, nmrep;
+	int error = 0, numops, status, interval, isdotdot, loopcnt = 0, depth = 0;
+	struct nfs_fs_path fspath, *nfsp, fspath2;
+	uint32_t bitmap[NFS_ATTR_BITMAP_LEN], comp, comp2;
+	fhandle_t fh, dirfh;
+	struct nfs_vattr nvattr;
+	u_int64_t xid;
+	struct nfsreq rq, *req = &rq;
+	struct nfsreq_secinfo_args si;
+	struct nfs_sec sec;
+	struct nfs_fs_locations nfsls;
+
+	*npp = NULL;
+	fh.fh_len = dirfh.fh_len = 0;
+	TAILQ_INIT(&nmp->nm_open_owners);
+	TAILQ_INIT(&nmp->nm_delegations);
+	TAILQ_INIT(&nmp->nm_dreturnq);
+	nmp->nm_stategenid = 1;
+	NVATTR_INIT(&nvattr);
+	bzero(&nfsls, sizeof(nfsls));
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	/*
+	 * If no security flavors were specified we'll want to default to the server's
+	 * preferred flavor.  For NFSv4.0 we need a file handle and name to get that via
+	 * SECINFO, so we'll do that on the last component of the server path we are
+	 * mounting.  If we are mounting the server's root, we'll need to defer the
+	 * SECINFO call to the first successful LOOKUP request.
+	 */
+	if (!nmp->nm_sec.count)
+		nmp->nm_state |= NFSSTA_NEEDSECINFO;
+
+	/* make a copy of the current location's path */
+	nfsp = &nmp->nm_locations.nl_locations[nmp->nm_locations.nl_current.nli_loc]->nl_path;
+	bzero(&fspath, sizeof(fspath));
+	fspath.np_compcount = nfsp->np_compcount;
+	if (fspath.np_compcount > 0) {
+		MALLOC(fspath.np_components, char **, fspath.np_compcount*sizeof(char*), M_TEMP, M_WAITOK|M_ZERO);
+		if (!fspath.np_components) {
+			error = ENOMEM;
+			goto nfsmout;
+		}
+		for (comp=0; comp < nfsp->np_compcount; comp++) {
+			int slen = strlen(nfsp->np_components[comp]);
+			MALLOC(fspath.np_components[comp], char *, slen+1, M_TEMP, M_WAITOK|M_ZERO);
+			if (!fspath.np_components[comp]) {
+				error = ENOMEM;
+				break;
+			}
+			strlcpy(fspath.np_components[comp], nfsp->np_components[comp], slen+1);
+		}
+		if (error)
+			goto nfsmout;
+	}
+
+	/* for mirror mounts, we can just use the file handle passed in */
+	if (nmp->nm_fh) {
+		dirfh.fh_len = nmp->nm_fh->fh_len;
+		bcopy(nmp->nm_fh->fh_data, dirfh.fh_data, dirfh.fh_len);
+		NFSREQ_SECINFO_SET(&si, NULL, dirfh.fh_data, dirfh.fh_len, NULL, 0);
+		goto gotfh;
+	}
+
+	/* otherwise, we need to get the fh for the directory we are mounting */
+
+	/* if no components, just get root */
+	if (fspath.np_compcount == 0) {
+nocomponents:
+		// PUTROOTFH + GETATTR(FH)
+		NFSREQ_SECINFO_SET(&si, NULL, NULL, 0, NULL, 0);
+		numops = 2;
+		nfsm_chain_build_alloc_init(error, &nmreq, 9 * NFSX_UNSIGNED);
+		nfsm_chain_add_compound_header(error, &nmreq, "mount", nmp->nm_minor_vers, numops);
+		numops--;
+		nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTROOTFH);
+		numops--;
+		nfsm_chain_add_32(error, &nmreq, NFS_OP_GETATTR);
+		NFS_CLEAR_ATTRIBUTES(bitmap);
+		NFS4_DEFAULT_ATTRIBUTES(bitmap);
+		NFS_BITMAP_SET(bitmap, NFS_FATTR_FILEHANDLE);
+		nfsm_chain_add_bitmap(error, &nmreq, bitmap, NFS_ATTR_BITMAP_LEN);
+		nfsm_chain_build_done(error, &nmreq);
+		nfsm_assert(error, (numops == 0), EPROTO);
+		nfsmout_if(error);
+		error = nfs_request_async(NULL, nmp->nm_mountp, &nmreq, NFSPROC4_COMPOUND,
+				vfs_context_thread(ctx), vfs_context_ucred(ctx), &si, 0, NULL, &req);
+		if (!error)
+			error = nfs_request_async_finish(req, &nmrep, &xid, &status);
+		nfsm_chain_skip_tag(error, &nmrep);
+		nfsm_chain_get_32(error, &nmrep, numops);
+		nfsm_chain_op_check(error, &nmrep, NFS_OP_PUTROOTFH);
+		nfsm_chain_op_check(error, &nmrep, NFS_OP_GETATTR);
+		nfsmout_if(error);
+		NFS_CLEAR_ATTRIBUTES(nmp->nm_fsattr.nfsa_bitmap);
+		error = nfs4_parsefattr(&nmrep, &nmp->nm_fsattr, &nvattr, &dirfh, NULL, NULL);
+		if (!error && !NFS_BITMAP_ISSET(&nvattr.nva_bitmap, NFS_FATTR_FILEHANDLE)) {
+			printf("nfs: mount didn't return filehandle?\n");
+			error = EBADRPC;
+		}
+		nfsmout_if(error);
+		nfsm_chain_cleanup(&nmrep);
+		nfsm_chain_null(&nmreq);
+		NVATTR_CLEANUP(&nvattr);
+		goto gotfh;
+	}
+
+	/* look up each path component */
+	for (comp=0; comp < fspath.np_compcount; ) {
+		isdotdot = 0;
+		if (fspath.np_components[comp][0] == '.') {
+			if (fspath.np_components[comp][1] == '\0') {
+				/* skip "." */
+				comp++;
+				continue;
+			}
+			/* treat ".." specially */
+			if ((fspath.np_components[comp][1] == '.') &&
+			    (fspath.np_components[comp][2] == '\0'))
+			    	isdotdot = 1;
+			if (isdotdot && (dirfh.fh_len == 0)) {
+				/* ".." in root directory is same as "." */
+				comp++;
+				continue;
+			}
+		}
+		// PUT(ROOT)FH + LOOKUP(P) + GETFH + GETATTR
+		if (dirfh.fh_len == 0)
+			NFSREQ_SECINFO_SET(&si, NULL, NULL, 0, isdotdot ? NULL : fspath.np_components[comp], 0);
+		else
+			NFSREQ_SECINFO_SET(&si, NULL, dirfh.fh_data, dirfh.fh_len, isdotdot ? NULL : fspath.np_components[comp], 0);
+		numops = 4;
+		nfsm_chain_build_alloc_init(error, &nmreq, 18 * NFSX_UNSIGNED);
+		nfsm_chain_add_compound_header(error, &nmreq, "mount", nmp->nm_minor_vers, numops);
+		numops--;
+		if (dirfh.fh_len) {
+			nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTFH);
+			nfsm_chain_add_fh(error, &nmreq, NFS_VER4, dirfh.fh_data, dirfh.fh_len);
+		} else {
+			nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTROOTFH);
+		}
+		numops--;
+		if (isdotdot) {
+			nfsm_chain_add_32(error, &nmreq, NFS_OP_LOOKUPP);
+		} else {
+			nfsm_chain_add_32(error, &nmreq, NFS_OP_LOOKUP);
+			nfsm_chain_add_name(error, &nmreq,
+				fspath.np_components[comp], strlen(fspath.np_components[comp]), nmp);
+		}
+		numops--;
+		nfsm_chain_add_32(error, &nmreq, NFS_OP_GETFH);
+		numops--;
+		nfsm_chain_add_32(error, &nmreq, NFS_OP_GETATTR);
+		NFS_CLEAR_ATTRIBUTES(bitmap);
+		NFS4_DEFAULT_ATTRIBUTES(bitmap);
+		/* if no namedattr support or component is ".zfs", clear NFS_FATTR_NAMED_ATTR */
+		if (NMFLAG(nmp, NONAMEDATTR) || !strcmp(fspath.np_components[comp], ".zfs"))
+			NFS_BITMAP_CLR(bitmap, NFS_FATTR_NAMED_ATTR);
+		nfsm_chain_add_bitmap(error, &nmreq, bitmap, NFS_ATTR_BITMAP_LEN);
+		nfsm_chain_build_done(error, &nmreq);
+		nfsm_assert(error, (numops == 0), EPROTO);
+		nfsmout_if(error);
+		error = nfs_request_async(NULL, nmp->nm_mountp, &nmreq, NFSPROC4_COMPOUND,
+				vfs_context_thread(ctx), vfs_context_ucred(ctx), &si, 0, NULL, &req);
+		if (!error)
+			error = nfs_request_async_finish(req, &nmrep, &xid, &status);
+		nfsm_chain_skip_tag(error, &nmrep);
+		nfsm_chain_get_32(error, &nmrep, numops);
+		nfsm_chain_op_check(error, &nmrep, dirfh.fh_len ? NFS_OP_PUTFH : NFS_OP_PUTROOTFH);
+		nfsm_chain_op_check(error, &nmrep, isdotdot ? NFS_OP_LOOKUPP : NFS_OP_LOOKUP);
+		nfsmout_if(error);
+		nfsm_chain_op_check(error, &nmrep, NFS_OP_GETFH);
+		nfsm_chain_get_32(error, &nmrep, fh.fh_len);
+		nfsm_chain_get_opaque(error, &nmrep, fh.fh_len, fh.fh_data);
+		nfsm_chain_op_check(error, &nmrep, NFS_OP_GETATTR);
+		if (!error) {
+			NFS_CLEAR_ATTRIBUTES(nmp->nm_fsattr.nfsa_bitmap);
+			error = nfs4_parsefattr(&nmrep, &nmp->nm_fsattr, &nvattr, NULL, NULL, &nfsls);
+		}
+		nfsm_chain_cleanup(&nmrep);
+		nfsm_chain_null(&nmreq);
+		if (error) {
+			/* LOOKUP succeeded but GETATTR failed?  This could be a referral. */
+			/* Try the lookup again with a getattr for fs_locations. */
+			nfs_fs_locations_cleanup(&nfsls);
+			error = nfs4_get_fs_locations(nmp, NULL, dirfh.fh_data, dirfh.fh_len, fspath.np_components[comp], ctx, &nfsls);
+			if (!error && (nfsls.nl_numlocs < 1))
+				error = ENOENT;
+			nfsmout_if(error);
+			if (++loopcnt > MAXSYMLINKS) {
+				/* too many symlink/referral redirections */
+				error = ELOOP;
+				goto nfsmout;
+			}
+			/* tear down the current connection */
+			nfs_disconnect(nmp);
+			/* replace fs locations */
+			nfs_fs_locations_cleanup(&nmp->nm_locations);
+			nmp->nm_locations = nfsls;
+			bzero(&nfsls, sizeof(nfsls));
+			/* initiate a connection using the new fs locations */
+			error = nfs_mount_connect(nmp);
+			if (!error && !(nmp->nm_locations.nl_current.nli_flags & NLI_VALID))
+				error = EIO;
+			nfsmout_if(error);
+			/* add new server's remote path to beginning of our path and continue */
+			nfsp = &nmp->nm_locations.nl_locations[nmp->nm_locations.nl_current.nli_loc]->nl_path;
+			bzero(&fspath2, sizeof(fspath2));
+			fspath2.np_compcount = (fspath.np_compcount - comp - 1) + nfsp->np_compcount;
+			if (fspath2.np_compcount > 0) {
+				MALLOC(fspath2.np_components, char **, fspath2.np_compcount*sizeof(char*), M_TEMP, M_WAITOK|M_ZERO);
+				if (!fspath2.np_components) {
+					error = ENOMEM;
+					goto nfsmout;
+				}
+				for (comp2=0; comp2 < nfsp->np_compcount; comp2++) {
+					int slen = strlen(nfsp->np_components[comp2]);
+					MALLOC(fspath2.np_components[comp2], char *, slen+1, M_TEMP, M_WAITOK|M_ZERO);
+					if (!fspath2.np_components[comp2]) {
+						/* clean up fspath2, then error out */
+						while (comp2 > 0) {
+							comp2--;
+							FREE(fspath2.np_components[comp2], M_TEMP);
+						}
+						FREE(fspath2.np_components, M_TEMP);
+						error = ENOMEM;
+						goto nfsmout;
+					}
+					strlcpy(fspath2.np_components[comp2], nfsp->np_components[comp2], slen+1);
+				}
+				if ((fspath.np_compcount - comp - 1) > 0)
+					bcopy(&fspath.np_components[comp+1], &fspath2.np_components[nfsp->np_compcount], (fspath.np_compcount - comp - 1)*sizeof(char*));
+				/* free up unused parts of old path (prior components and component array) */
+				do {
+					FREE(fspath.np_components[comp], M_TEMP);
+				} while (comp-- > 0);
+				FREE(fspath.np_components, M_TEMP);
+				/* put new path in place */
+				fspath = fspath2;
+			}
+			/* reset dirfh and component index */
+			dirfh.fh_len = 0;
+			comp = 0;
+			NVATTR_CLEANUP(&nvattr);
+			if (fspath.np_compcount == 0)
+				goto nocomponents;
+			continue;
+		}
+		nfsmout_if(error);
+		/* if file handle is for a symlink, then update the path with the symlink contents */
+		if (NFS_BITMAP_ISSET(&nvattr.nva_bitmap, NFS_FATTR_TYPE) && (nvattr.nva_type == VLNK)) {
+			if (++loopcnt > MAXSYMLINKS)
+				error = ELOOP;
+			else
+				error = nfs4_mount_update_path_with_symlink(nmp, &fspath, comp, &dirfh, &depth, &fh, ctx);
+			nfsmout_if(error);
+			/* directory file handle is either left the same or reset to root (if link was absolute) */
+			/* path traversal starts at beginning of the path again */
+			comp = 0;
+			NVATTR_CLEANUP(&nvattr);
+			nfs_fs_locations_cleanup(&nfsls);
+			continue;
+		}
+		NVATTR_CLEANUP(&nvattr);
+		nfs_fs_locations_cleanup(&nfsls);
+		/* not a symlink... */
+		if ((nmp->nm_state & NFSSTA_NEEDSECINFO) && (comp == (fspath.np_compcount-1)) && !isdotdot) {
+			/* need to get SECINFO for the directory being mounted */
+			if (dirfh.fh_len == 0)
+				NFSREQ_SECINFO_SET(&si, NULL, NULL, 0, isdotdot ? NULL : fspath.np_components[comp], 0);
+			else
+				NFSREQ_SECINFO_SET(&si, NULL, dirfh.fh_data, dirfh.fh_len, isdotdot ? NULL : fspath.np_components[comp], 0);
+			sec.count = NX_MAX_SEC_FLAVORS;
+			error = nfs4_secinfo_rpc(nmp, &si, vfs_context_ucred(ctx), sec.flavors, &sec.count);
+			/* [sigh] some implementations return "illegal" error for unsupported ops */
+			if (error == NFSERR_OP_ILLEGAL)
+				error = 0;
+			nfsmout_if(error);
+			/* set our default security flavor to the first in the list */
+			if (sec.count)
+				nmp->nm_auth = sec.flavors[0];
+			nmp->nm_state &= ~NFSSTA_NEEDSECINFO;
+		}
+		/* advance directory file handle, component index, & update depth */
+		dirfh = fh;
+		comp++;
+		if (!isdotdot) /* going down the hierarchy */
+			depth++;
+		else if (--depth <= 0)  /* going up the hierarchy */
+			dirfh.fh_len = 0; /* clear dirfh when we hit root */
+	}
+
+gotfh:
+	/* get attrs for mount point root */
+	numops = NMFLAG(nmp, NONAMEDATTR) ? 2 : 3; // PUTFH + GETATTR + OPENATTR
+	nfsm_chain_build_alloc_init(error, &nmreq, 25 * NFSX_UNSIGNED);
+	nfsm_chain_add_compound_header(error, &nmreq, "mount", nmp->nm_minor_vers, numops);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTFH);
+	nfsm_chain_add_fh(error, &nmreq, NFS_VER4, dirfh.fh_data, dirfh.fh_len);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_GETATTR);
+	NFS_CLEAR_ATTRIBUTES(bitmap);
+	NFS4_DEFAULT_ATTRIBUTES(bitmap);
+	/* if no namedattr support or last component is ".zfs", clear NFS_FATTR_NAMED_ATTR */
+	if (NMFLAG(nmp, NONAMEDATTR) || ((fspath.np_compcount > 0) && !strcmp(fspath.np_components[fspath.np_compcount-1], ".zfs")))
+		NFS_BITMAP_CLR(bitmap, NFS_FATTR_NAMED_ATTR);
+	nfsm_chain_add_bitmap(error, &nmreq, bitmap, NFS_ATTR_BITMAP_LEN);
+	if (!NMFLAG(nmp, NONAMEDATTR)) {
+		numops--;
+		nfsm_chain_add_32(error, &nmreq, NFS_OP_OPENATTR);
+		nfsm_chain_add_32(error, &nmreq, 0);
+	}
+	nfsm_chain_build_done(error, &nmreq);
+	nfsm_assert(error, (numops == 0), EPROTO);
+	nfsmout_if(error);
+	error = nfs_request_async(NULL, nmp->nm_mountp, &nmreq, NFSPROC4_COMPOUND,
+			vfs_context_thread(ctx), vfs_context_ucred(ctx), &si, 0, NULL, &req);
+	if (!error)
+		error = nfs_request_async_finish(req, &nmrep, &xid, &status);
+	nfsm_chain_skip_tag(error, &nmrep);
+	nfsm_chain_get_32(error, &nmrep, numops);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_PUTFH);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_GETATTR);
+	nfsmout_if(error);
+	NFS_CLEAR_ATTRIBUTES(nmp->nm_fsattr.nfsa_bitmap);
+	error = nfs4_parsefattr(&nmrep, &nmp->nm_fsattr, &nvattr, NULL, NULL, NULL);
+	nfsmout_if(error);
+	if (!NMFLAG(nmp, NONAMEDATTR)) {
+		nfsm_chain_op_check(error, &nmrep, NFS_OP_OPENATTR);
+		if (error == ENOENT)
+			error = 0;
+		/* [sigh] some implementations return "illegal" error for unsupported ops */
+		if (error || !NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_NAMED_ATTR)) {
+			nmp->nm_fsattr.nfsa_flags &= ~NFS_FSFLAG_NAMED_ATTR;
+		} else {
+			nmp->nm_fsattr.nfsa_flags |= NFS_FSFLAG_NAMED_ATTR;
+		}
+	} else {
+		nmp->nm_fsattr.nfsa_flags &= ~NFS_FSFLAG_NAMED_ATTR;
+	}
+	if (NMFLAG(nmp, NOACL)) /* make sure ACL support is turned off */
+		nmp->nm_fsattr.nfsa_flags &= ~NFS_FSFLAG_ACL;
+	if (NMFLAG(nmp, ACLONLY) && !(nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_ACL))
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_ACLONLY);
+	if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_FH_EXPIRE_TYPE)) {
+		uint32_t fhtype = ((nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_FHTYPE_MASK) >> NFS_FSFLAG_FHTYPE_SHIFT);
+		if (fhtype != NFS_FH_PERSISTENT)
+			printf("nfs: warning: non-persistent file handles! for %s\n", vfs_statfs(nmp->nm_mountp)->f_mntfromname);
+	}
+
+	/* make sure it's a directory */
+	if (!NFS_BITMAP_ISSET(&nvattr.nva_bitmap, NFS_FATTR_TYPE) || (nvattr.nva_type != VDIR)) {
+		error = ENOTDIR;
+		goto nfsmout;
+	}
+
+	/* save the NFS fsid */
+	nmp->nm_fsid = nvattr.nva_fsid;
+
+	/* create the root node */
+	error = nfs_nget(nmp->nm_mountp, NULL, NULL, dirfh.fh_data, dirfh.fh_len, &nvattr, &xid, rq.r_auth, NG_MARKROOT, npp);
+	nfsmout_if(error);
+
+	if (nmp->nm_fsattr.nfsa_flags & NFS_FSFLAG_ACL)
+		vfs_setextendedsecurity(nmp->nm_mountp);
+
+	/* adjust I/O sizes to server limits */
+	if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXREAD) && (nmp->nm_fsattr.nfsa_maxread > 0)) {
+		if (nmp->nm_fsattr.nfsa_maxread < (uint64_t)nmp->nm_rsize) {
+			nmp->nm_rsize = nmp->nm_fsattr.nfsa_maxread & ~(NFS_FABLKSIZE - 1);
+			if (nmp->nm_rsize == 0)
+				nmp->nm_rsize = nmp->nm_fsattr.nfsa_maxread;
+		}
+	}
+	if (NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_bitmap, NFS_FATTR_MAXWRITE) && (nmp->nm_fsattr.nfsa_maxwrite > 0)) {
+		if (nmp->nm_fsattr.nfsa_maxwrite < (uint64_t)nmp->nm_wsize) {
+			nmp->nm_wsize = nmp->nm_fsattr.nfsa_maxwrite & ~(NFS_FABLKSIZE - 1);
+			if (nmp->nm_wsize == 0)
+				nmp->nm_wsize = nmp->nm_fsattr.nfsa_maxwrite;
+		}
+	}
+
+	/* set up lease renew timer */
+	nmp->nm_renew_timer = thread_call_allocate(nfs4_renew_timer, nmp);
+	interval = nmp->nm_fsattr.nfsa_lease / 2;
+	if (interval < 1)
+		interval = 1;
+	nfs_interval_timer_start(nmp->nm_renew_timer, interval * 1000);
+
+nfsmout:
+	if (fspath.np_components) {
+		for (comp=0; comp < fspath.np_compcount; comp++)
+			if (fspath.np_components[comp])
+				FREE(fspath.np_components[comp], M_TEMP);
+		FREE(fspath.np_components, M_TEMP);
+	}
+	NVATTR_CLEANUP(&nvattr);
+	nfs_fs_locations_cleanup(&nfsls);
+	if (*npp)
+		nfs_node_unlock(*npp);
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	return (error);
+}
+
+/*
+ * Thread to handle initial NFS mount connection.
+ */
+void
+nfs_mount_connect_thread(void *arg, __unused wait_result_t wr)
+{
+	struct nfsmount *nmp = arg;
+	int error = 0, savederror = 0, slpflag = (NMFLAG(nmp, INTR) ? PCATCH : 0);
+	int done = 0, timeo, tries, maxtries;
+
+	if (NM_OMFLAG(nmp, MNTQUICK)) {
+		timeo = 8;
+		maxtries = 1;
+	} else {
+		timeo = 30;
+		maxtries = 2;
+	}
+
+	for (tries = 0; tries < maxtries; tries++) {
+		error = nfs_connect(nmp, 1, timeo);
+		switch (error) {
+		case ETIMEDOUT:
+		case EAGAIN:
+		case EPIPE:
+		case EADDRNOTAVAIL:
+		case ENETDOWN:
+		case ENETUNREACH:
+		case ENETRESET:
+		case ECONNABORTED:
+		case ECONNRESET:
+		case EISCONN:
+		case ENOTCONN:
+		case ESHUTDOWN:
+		case ECONNREFUSED:
+		case EHOSTDOWN:
+		case EHOSTUNREACH:
+			/* just keep retrying on any of these errors */
+			break;
+		case 0:
+		default:
+			/* looks like we got an answer... */
+			done = 1;
+			break;
+		}
+
+		/* save the best error */
+		if (nfs_connect_error_class(error) >= nfs_connect_error_class(savederror))
+			savederror = error;
+		if (done) {
+			error = savederror;
+			break;
+		}
+
+		/* pause before next attempt */
+		if ((error = nfs_sigintr(nmp, NULL, current_thread(), 0)))
+			break;
+		error = tsleep(nmp, PSOCK|slpflag, "nfs_mount_connect_retry", 2*hz);
+		if (error && (error != EWOULDBLOCK))
+			break;
+		error = savederror;
+	}
+
+	/* update status of mount connect */
+	lck_mtx_lock(&nmp->nm_lock);
+	if (!nmp->nm_mounterror)
+		nmp->nm_mounterror = error;
+	nmp->nm_state &= ~NFSSTA_MOUNT_THREAD;
+	lck_mtx_unlock(&nmp->nm_lock);
+	wakeup(&nmp->nm_nss);
+}
+
+int
+nfs_mount_connect(struct nfsmount *nmp)
+{
+	int error = 0, slpflag;
+	thread_t thd;
+	struct timespec ts = { 2, 0 };
+
+	/*
+	 * Set up the socket.  Perform initial search for a location/server/address to
+	 * connect to and negotiate any unspecified mount parameters.  This work is
+	 * done on a kernel thread to satisfy reserved port usage needs.
+	 */
+	slpflag = NMFLAG(nmp, INTR) ? PCATCH : 0;
+	lck_mtx_lock(&nmp->nm_lock);
+	/* set flag that the thread is running */
+	nmp->nm_state |= NFSSTA_MOUNT_THREAD;
+	if (kernel_thread_start(nfs_mount_connect_thread, nmp, &thd) != KERN_SUCCESS) {
+		nmp->nm_state &= ~NFSSTA_MOUNT_THREAD;
+		nmp->nm_mounterror = EIO;
+		printf("nfs mount %s start socket connect thread failed\n", vfs_statfs(nmp->nm_mountp)->f_mntfromname);
+	} else {
+		thread_deallocate(thd);
+	}
+
+	/* wait until mount connect thread is finished/gone */
+	while (nmp->nm_state & NFSSTA_MOUNT_THREAD) {
+		error = msleep(&nmp->nm_nss, &nmp->nm_lock, slpflag|PSOCK, "nfsconnectthread", &ts);
+		if ((error && (error != EWOULDBLOCK)) || ((error = nfs_sigintr(nmp, NULL, current_thread(), 1)))) {
+			/* record error */
+			if (!nmp->nm_mounterror)
+				nmp->nm_mounterror = error;
+			/* signal the thread that we are aborting */
+			nmp->nm_sockflags |= NMSOCK_UNMOUNT;
+			if (nmp->nm_nss)
+				wakeup(nmp->nm_nss);
+			/* and continue waiting on it to finish */
+			slpflag = 0;
+		}
+	}
+	lck_mtx_unlock(&nmp->nm_lock);
+
+	/* grab mount connect status */
+	error = nmp->nm_mounterror;
+
+	return (error);
+}
+
+/* Table of maximum minor version for a given version */
+uint32_t maxminorverstab[] = {
+	0, /* Version 0 (does not exist) */
+	0, /* Version 1 (does not exist) */
+	0, /* Version 2 */
+	0, /* Version 3 */
+	0, /* Version 4 */
+};
+
+#define NFS_MAX_SUPPORTED_VERSION  ((long)(sizeof (maxminorverstab) / sizeof (uint32_t) - 1))
+#define NFS_MAX_SUPPORTED_MINOR_VERSION(v) ((long)(maxminorverstab[(v)]))
+
+#define DEFAULT_NFS_MIN_VERS VER2PVER(2, 0)
+#define DEFAULT_NFS_MAX_VERS VER2PVER(3, 0)
+
+/*
+ * Common code to mount an NFS file system.
+ */
+int
+mountnfs(
+	char *xdrbuf,
+	mount_t mp,
+	vfs_context_t ctx,
+	vnode_t *vpp)
+{
+	struct nfsmount *nmp;
+	nfsnode_t np;
+	int error = 0;
+	struct vfsstatfs *sbp;
+	struct xdrbuf xb;
+	uint32_t i, val, maxio, iosize, len;
+	uint32_t *mattrs;
+	uint32_t *mflags_mask;
+	uint32_t *mflags;
+	uint32_t argslength, attrslength;
+	struct nfs_location_index firstloc = { NLI_VALID, 0, 0, 0 };
+
+	/* make sure mbuf constants are set up */
+	if (!nfs_mbuf_mhlen)
+		nfs_mbuf_init();
+
+	if (vfs_flags(mp) & MNT_UPDATE) {
+		nmp = VFSTONFS(mp);
+		/* update paths, file handles, etc, here	XXX */
+		xb_free(xdrbuf);
+		return (0);
+	} else {
+		/* allocate an NFS mount structure for this mount */
+		MALLOC_ZONE(nmp, struct nfsmount *,
+				sizeof (struct nfsmount), M_NFSMNT, M_WAITOK);
+		if (!nmp) {
+			xb_free(xdrbuf);
+			return (ENOMEM);
+		}
+		bzero((caddr_t)nmp, sizeof (struct nfsmount));
+		lck_mtx_init(&nmp->nm_lock, nfs_mount_grp, LCK_ATTR_NULL);
+		TAILQ_INIT(&nmp->nm_resendq);
+		TAILQ_INIT(&nmp->nm_iodq);
+		TAILQ_INIT(&nmp->nm_gsscl);
+		LIST_INIT(&nmp->nm_monlist);
+		vfs_setfsprivate(mp, nmp);
+		vfs_getnewfsid(mp);
+		nmp->nm_mountp = mp;
+		vfs_setauthopaque(mp);
+
+		nfs_nhinit_finish();
+
+		nmp->nm_args = xdrbuf;
+
+		/* set up defaults */
+		nmp->nm_ref = 0;
+		nmp->nm_vers = 0;
+		nmp->nm_min_vers = DEFAULT_NFS_MIN_VERS;
+		nmp->nm_max_vers = DEFAULT_NFS_MAX_VERS;
+		nmp->nm_timeo = NFS_TIMEO;
+		nmp->nm_retry = NFS_RETRANS;
+		nmp->nm_sotype = 0;
+		nmp->nm_sofamily = 0;
+		nmp->nm_nfsport = 0;
+		nmp->nm_wsize = NFS_WSIZE;
+		nmp->nm_rsize = NFS_RSIZE;
+		nmp->nm_readdirsize = NFS_READDIRSIZE;
+		nmp->nm_numgrps = NFS_MAXGRPS;
+		nmp->nm_readahead = NFS_DEFRAHEAD;
+		nmp->nm_tprintf_delay = nfs_tprintf_delay;
+		if (nmp->nm_tprintf_delay < 0)
+			nmp->nm_tprintf_delay = 0;
+		nmp->nm_tprintf_initial_delay = nfs_tprintf_initial_delay;
+		if (nmp->nm_tprintf_initial_delay < 0)
+			nmp->nm_tprintf_initial_delay = 0;
+		nmp->nm_acregmin = NFS_MINATTRTIMO;
+		nmp->nm_acregmax = NFS_MAXATTRTIMO;
+		nmp->nm_acdirmin = NFS_MINDIRATTRTIMO;
+		nmp->nm_acdirmax = NFS_MAXDIRATTRTIMO;
+		nmp->nm_auth = RPCAUTH_SYS;
+		nmp->nm_iodlink.tqe_next = NFSNOLIST;
+		nmp->nm_deadtimeout = 0;
+		nmp->nm_curdeadtimeout = 0;
+		NFS_BITMAP_SET(nmp->nm_flags, NFS_MFLAG_NOACL);
+		nmp->nm_realm = NULL;
+		nmp->nm_principal = NULL;
+		nmp->nm_sprinc = NULL;
+	}
+
+	mattrs = nmp->nm_mattrs;
+	mflags = nmp->nm_mflags;
+	mflags_mask = nmp->nm_mflags_mask;
+
+	/* set up NFS mount with args */
+	xb_init_buffer(&xb, xdrbuf, 2*XDRWORD);
+	xb_get_32(error, &xb, val); /* version */
+	xb_get_32(error, &xb, argslength); /* args length */
+	nfsmerr_if(error);
+	xb_init_buffer(&xb, xdrbuf, argslength);	/* restart parsing with actual buffer length */
+	xb_get_32(error, &xb, val); /* version */
+	xb_get_32(error, &xb, argslength); /* args length */
+	xb_get_32(error, &xb, val); /* XDR args version */
+	if (val != NFS_XDRARGS_VERSION_0)
+		error = EINVAL;
+	len = NFS_MATTR_BITMAP_LEN;
+	xb_get_bitmap(error, &xb, mattrs, len); /* mount attribute bitmap */
+	attrslength = 0;
+	xb_get_32(error, &xb, attrslength); /* attrs length */
+	if (!error && (attrslength > (argslength - ((4+NFS_MATTR_BITMAP_LEN+1)*XDRWORD))))
+		error = EINVAL;
+	nfsmerr_if(error);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FLAGS)) {
+		len = NFS_MFLAG_BITMAP_LEN;
+		xb_get_bitmap(error, &xb, mflags_mask, len); /* mount flag mask */
+		len = NFS_MFLAG_BITMAP_LEN;
+		xb_get_bitmap(error, &xb, mflags, len); /* mount flag values */
+		if (!error) {
+			/* clear all mask bits and OR in all the ones that are set */
+			nmp->nm_flags[0] &= ~mflags_mask[0];
+			nmp->nm_flags[0] |= (mflags_mask[0] & mflags[0]);
+		}
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_VERSION)) {
+		/* Can't specify a single version and a range */
+		if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_VERSION_RANGE))
+			error = EINVAL;
+		xb_get_32(error, &xb, nmp->nm_vers);
+		if (nmp->nm_vers > NFS_MAX_SUPPORTED_VERSION ||
+		    nmp->nm_vers < NFS_VER2)
+			error = EINVAL;
+		if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_MINOR_VERSION))
+			xb_get_32(error, &xb, nmp->nm_minor_vers);
+		else
+			nmp->nm_minor_vers = maxminorverstab[nmp->nm_vers];
+		if (nmp->nm_minor_vers > maxminorverstab[nmp->nm_vers])
+			error = EINVAL;
+		nmp->nm_max_vers = nmp->nm_min_vers = 
+			VER2PVER(nmp->nm_vers, nmp->nm_minor_vers);
+	} 
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_MINOR_VERSION)) {
+		/* should have also gotten NFS version (and already gotten minor version) */
+		if (!NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_VERSION))
+			error = EINVAL;
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_VERSION_RANGE)) {
+		xb_get_32(error, &xb, nmp->nm_min_vers);
+		xb_get_32(error, &xb, nmp->nm_max_vers);
+		if ((nmp->nm_min_vers > nmp->nm_max_vers) ||
+		    (PVER2MAJOR(nmp->nm_max_vers) > NFS_MAX_SUPPORTED_VERSION) ||
+		    (PVER2MINOR(nmp->nm_min_vers) > maxminorverstab[PVER2MAJOR(nmp->nm_min_vers)]) ||
+		    (PVER2MINOR(nmp->nm_max_vers) > maxminorverstab[PVER2MAJOR(nmp->nm_max_vers)]))
+			error = EINVAL;
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READ_SIZE))
+		xb_get_32(error, &xb, nmp->nm_rsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_WRITE_SIZE))
+		xb_get_32(error, &xb, nmp->nm_wsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READDIR_SIZE))
+		xb_get_32(error, &xb, nmp->nm_readdirsize);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READAHEAD))
+		xb_get_32(error, &xb, nmp->nm_readahead);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_REG_MIN)) {
+		xb_get_32(error, &xb, nmp->nm_acregmin);
+		xb_skip(error, &xb, XDRWORD);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_REG_MAX)) {
+		xb_get_32(error, &xb, nmp->nm_acregmax);
+		xb_skip(error, &xb, XDRWORD);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MIN)) {
+		xb_get_32(error, &xb, nmp->nm_acdirmin);
+		xb_skip(error, &xb, XDRWORD);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MAX)) {
+		xb_get_32(error, &xb, nmp->nm_acdirmax);
+		xb_skip(error, &xb, XDRWORD);
+	}
+	nfsmerr_if(error);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_LOCK_MODE)) {
+		xb_get_32(error, &xb, val);
+		switch (val) {
+		case NFS_LOCK_MODE_DISABLED:
+		case NFS_LOCK_MODE_LOCAL:
+			if (nmp->nm_vers >= NFS_VER4) {
+				/* disabled/local lock mode only allowed on v2/v3 */
+				error = EINVAL;
+				break;
+			}
+			/* FALLTHROUGH */
+		case NFS_LOCK_MODE_ENABLED:
+			nmp->nm_lockmode = val;
+			break;
+		default:
+			error = EINVAL;
+		}
+	}
+	nfsmerr_if(error);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SECURITY)) {
+		uint32_t seccnt;
+		xb_get_32(error, &xb, seccnt);
+		if (!error && ((seccnt < 1) || (seccnt > NX_MAX_SEC_FLAVORS)))
+			error = EINVAL;
+		nfsmerr_if(error);
+		nmp->nm_sec.count = seccnt;
+		for (i=0; i < seccnt; i++) {
+			xb_get_32(error, &xb, nmp->nm_sec.flavors[i]);
+			/* Check for valid security flavor */
+			switch (nmp->nm_sec.flavors[i]) {
+			case RPCAUTH_NONE:
+			case RPCAUTH_SYS:
+			case RPCAUTH_KRB5:
+			case RPCAUTH_KRB5I:
+			case RPCAUTH_KRB5P:
+				break;
+			default:
+				error = EINVAL;
+			}
+		}
+		/* start with the first flavor */
+		nmp->nm_auth = nmp->nm_sec.flavors[0];
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MAX_GROUP_LIST))
+		xb_get_32(error, &xb, nmp->nm_numgrps);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SOCKET_TYPE)) {
+		char sotype[6];
+
+		xb_get_32(error, &xb, val);
+		if (!error && ((val < 3) || (val > 5)))
+			error = EINVAL;
+		nfsmerr_if(error);
+		error = xb_get_bytes(&xb, sotype, val, 0);
+		nfsmerr_if(error);
+		sotype[val] = '\0';
+		if (!strcmp(sotype, "tcp")) {
+			nmp->nm_sotype = SOCK_STREAM;
+		} else if (!strcmp(sotype, "udp")) {
+			nmp->nm_sotype = SOCK_DGRAM;
+		} else if (!strcmp(sotype, "tcp4")) {
+			nmp->nm_sotype = SOCK_STREAM;
+			nmp->nm_sofamily = AF_INET;
+		} else if (!strcmp(sotype, "udp4")) {
+			nmp->nm_sotype = SOCK_DGRAM;
+			nmp->nm_sofamily = AF_INET;
+		} else if (!strcmp(sotype, "tcp6")) {
+			nmp->nm_sotype = SOCK_STREAM;
+			nmp->nm_sofamily = AF_INET6;
+		} else if (!strcmp(sotype, "udp6")) {
+			nmp->nm_sotype = SOCK_DGRAM;
+			nmp->nm_sofamily = AF_INET6;
+		} else if (!strcmp(sotype, "inet4")) {
+			nmp->nm_sofamily = AF_INET;
+		} else if (!strcmp(sotype, "inet6")) {
+			nmp->nm_sofamily = AF_INET6;
+		} else if (!strcmp(sotype, "inet")) {
+			nmp->nm_sofamily = 0; /* ok */
+		} else {
+			error = EINVAL;
+		}
+		if (!error && (nmp->nm_vers >= NFS_VER4) && nmp->nm_sotype &&
+		    (nmp->nm_sotype != SOCK_STREAM))
+			error = EINVAL;		/* NFSv4 is only allowed over TCP. */
+		nfsmerr_if(error);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_PORT))
+		xb_get_32(error, &xb, nmp->nm_nfsport);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MOUNT_PORT))
+		xb_get_32(error, &xb, nmp->nm_mountport);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_REQUEST_TIMEOUT)) {
+		/* convert from time to 0.1s units */
+		xb_get_32(error, &xb, nmp->nm_timeo);
+		xb_get_32(error, &xb, val);
+		nfsmerr_if(error);
+		if (val >= 1000000000)
+			error = EINVAL;
+		nfsmerr_if(error);
+		nmp->nm_timeo *= 10;
+		nmp->nm_timeo += (val+100000000-1)/100000000;
+		/* now convert to ticks */
+		nmp->nm_timeo = (nmp->nm_timeo * NFS_HZ + 5) / 10;
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SOFT_RETRY_COUNT)) {
+		xb_get_32(error, &xb, val);
+		if (!error && (val > 1))
+			nmp->nm_retry = val;
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_DEAD_TIMEOUT)) {
+		xb_get_32(error, &xb, nmp->nm_deadtimeout);
+		xb_skip(error, &xb, XDRWORD);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FH)) {
+		nfsmerr_if(error);
+		MALLOC(nmp->nm_fh, fhandle_t *, sizeof(fhandle_t), M_TEMP, M_WAITOK|M_ZERO);
+		if (!nmp->nm_fh)
+			error = ENOMEM;
+		xb_get_32(error, &xb, nmp->nm_fh->fh_len);
+		nfsmerr_if(error);
+		error = xb_get_bytes(&xb, (char*)&nmp->nm_fh->fh_data[0], nmp->nm_fh->fh_len, 0);
+	}
+	nfsmerr_if(error);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FS_LOCATIONS)) {
+		uint32_t loc, serv, addr, comp;
+		struct nfs_fs_location *fsl;
+		struct nfs_fs_server *fss;
+		struct nfs_fs_path *fsp;
+
+		xb_get_32(error, &xb, nmp->nm_locations.nl_numlocs); /* fs location count */
+		/* sanity check location count */
+		if (!error && ((nmp->nm_locations.nl_numlocs < 1) || (nmp->nm_locations.nl_numlocs > 256)))
+			error = EINVAL;
+		nfsmerr_if(error);
+		MALLOC(nmp->nm_locations.nl_locations, struct nfs_fs_location **, nmp->nm_locations.nl_numlocs * sizeof(struct nfs_fs_location*), M_TEMP, M_WAITOK|M_ZERO);
+		if (!nmp->nm_locations.nl_locations)
+			error = ENOMEM;
+		for (loc = 0; loc < nmp->nm_locations.nl_numlocs; loc++) {
+			nfsmerr_if(error);
+			MALLOC(fsl, struct nfs_fs_location *, sizeof(struct nfs_fs_location), M_TEMP, M_WAITOK|M_ZERO);
+			if (!fsl)
+				error = ENOMEM;
+			nmp->nm_locations.nl_locations[loc] = fsl;
+			xb_get_32(error, &xb, fsl->nl_servcount); /* server count */
+			/* sanity check server count */
+			if (!error && ((fsl->nl_servcount < 1) || (fsl->nl_servcount > 256)))
+				error = EINVAL;
+			nfsmerr_if(error);
+			MALLOC(fsl->nl_servers, struct nfs_fs_server **, fsl->nl_servcount * sizeof(struct nfs_fs_server*), M_TEMP, M_WAITOK|M_ZERO);
+			if (!fsl->nl_servers)
+				error = ENOMEM;
+			for (serv = 0; serv < fsl->nl_servcount; serv++) {
+				nfsmerr_if(error);
+				MALLOC(fss, struct nfs_fs_server *, sizeof(struct nfs_fs_server), M_TEMP, M_WAITOK|M_ZERO);
+				if (!fss)
+					error = ENOMEM;
+				fsl->nl_servers[serv] = fss;
+				xb_get_32(error, &xb, val); /* server name length */
+				/* sanity check server name length */
+				if (!error && ((val < 1) || (val > MAXPATHLEN)))
+					error = EINVAL;
+				nfsmerr_if(error);
+				MALLOC(fss->ns_name, char *, val+1, M_TEMP, M_WAITOK|M_ZERO);
+				if (!fss->ns_name)
+					error = ENOMEM;
+				nfsmerr_if(error);
+				error = xb_get_bytes(&xb, fss->ns_name, val, 0); /* server name */
+				xb_get_32(error, &xb, fss->ns_addrcount); /* address count */
+				/* sanity check address count (OK to be zero) */
+				if (!error && (fss->ns_addrcount > 256))
+					error = EINVAL;
+				nfsmerr_if(error);
+				if (fss->ns_addrcount > 0) {
+					MALLOC(fss->ns_addresses, char **, fss->ns_addrcount * sizeof(char *), M_TEMP, M_WAITOK|M_ZERO);
+					if (!fss->ns_addresses)
+						error = ENOMEM;
+					for (addr = 0; addr < fss->ns_addrcount; addr++) {
+						xb_get_32(error, &xb, val); /* address length */
+						/* sanity check address length */
+						if (!error && ((val < 1) || (val > 128)))
+							error = EINVAL;
+						nfsmerr_if(error);
+						MALLOC(fss->ns_addresses[addr], char *, val+1, M_TEMP, M_WAITOK|M_ZERO);
+						if (!fss->ns_addresses[addr])
+							error = ENOMEM;
+						nfsmerr_if(error);
+						error = xb_get_bytes(&xb, fss->ns_addresses[addr], val, 0); /* address */
+					}
+				}
+				xb_get_32(error, &xb, val); /* server info length */
+				xb_skip(error, &xb, val); /* skip server info */
+			}
+			/* get pathname */
+			fsp = &fsl->nl_path;
+			xb_get_32(error, &xb, fsp->np_compcount); /* component count */
+			/* sanity check component count */
+			if (!error && (fsp->np_compcount > MAXPATHLEN))
+				error = EINVAL;
+			nfsmerr_if(error);
+			if (fsp->np_compcount) {
+				MALLOC(fsp->np_components, char **, fsp->np_compcount * sizeof(char*), M_TEMP, M_WAITOK|M_ZERO);
+				if (!fsp->np_components)
+					error = ENOMEM;
+			}
+			for (comp = 0; comp < fsp->np_compcount; comp++) {
+				xb_get_32(error, &xb, val); /* component length */
+				/* sanity check component length */
+				if (!error && (val == 0)) {
+					/*
+					 * Apparently some people think a path with zero components should
+					 * be encoded with one zero-length component.  So, just ignore any
+					 * zero length components.
+					 */
+					comp--;
+					fsp->np_compcount--;
+					if (fsp->np_compcount == 0) {
+						FREE(fsp->np_components, M_TEMP);
+						fsp->np_components = NULL;
+					}
+					continue;
+				}
+				if (!error && ((val < 1) || (val > MAXPATHLEN)))
+					error = EINVAL;
+				nfsmerr_if(error);
+				MALLOC(fsp->np_components[comp], char *, val+1, M_TEMP, M_WAITOK|M_ZERO);
+				if (!fsp->np_components[comp])
+					error = ENOMEM;
+				nfsmerr_if(error);
+				error = xb_get_bytes(&xb, fsp->np_components[comp], val, 0); /* component */
+			}
+			xb_get_32(error, &xb, val); /* fs location info length */
+			xb_skip(error, &xb, val); /* skip fs location info */
+		}
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MNTFLAGS))
+		xb_skip(error, &xb, XDRWORD);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MNTFROM)) {
+		xb_get_32(error, &xb, len);
+		nfsmerr_if(error);
+		val = len;
+		if (val >= sizeof(vfs_statfs(mp)->f_mntfromname))
+			val = sizeof(vfs_statfs(mp)->f_mntfromname) - 1;
+		error = xb_get_bytes(&xb, vfs_statfs(mp)->f_mntfromname, val, 0);
+		if ((len - val) > 0)
+			xb_skip(error, &xb, len - val);
+		nfsmerr_if(error);
+		vfs_statfs(mp)->f_mntfromname[val] = '\0';
+	}
+	nfsmerr_if(error);
+
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_REALM)) {
+		xb_get_32(error, &xb, len);
+		if (!error && ((len < 1) || (len > MAXPATHLEN)))
+			error=EINVAL;
+		nfsmerr_if(error);
+		/* allocate an extra byte for a leading '@' if its not already prepended to the realm */
+		MALLOC(nmp->nm_realm, char *, len+2, M_TEMP, M_WAITOK|M_ZERO);
+		if (!nmp->nm_realm)
+			error = ENOMEM;
+		nfsmerr_if(error);
+		error = xb_get_bytes(&xb, nmp->nm_realm, len, 0);
+		if (error == 0 && *nmp->nm_realm != '@') {
+			bcopy(nmp->nm_realm, &nmp->nm_realm[1], len);
+			nmp->nm_realm[0] = '@';
+		}
+	}
+	nfsmerr_if(error);
+
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_PRINCIPAL)) {
+		xb_get_32(error, &xb, len);
+		if (!error && ((len < 1) || (len > MAXPATHLEN)))
+			error=EINVAL;
+		nfsmerr_if(error);
+		MALLOC(nmp->nm_principal, char *, len+1, M_TEMP, M_WAITOK|M_ZERO);
+		if (!nmp->nm_principal)
+			error = ENOMEM;
+		nfsmerr_if(error);
+		error = xb_get_bytes(&xb, nmp->nm_principal, len, 0);
+	}
+	nfsmerr_if(error);
+
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SVCPRINCIPAL)) {
+		xb_get_32(error, &xb, len);
+		if (!error && ((len < 1) || (len > MAXPATHLEN)))
+			error=EINVAL;
+		nfsmerr_if(error);
+		MALLOC(nmp->nm_sprinc, char *, len+1, M_TEMP, M_WAITOK|M_ZERO);
+		if (!nmp->nm_sprinc)
+			error = ENOMEM;
+		nfsmerr_if(error);
+		error = xb_get_bytes(&xb, nmp->nm_sprinc, len, 0);
+	}
+	nfsmerr_if(error);
+
+	/*
+	 * Sanity check/finalize settings.
+	 */
+
+	if (nmp->nm_timeo < NFS_MINTIMEO)
+		nmp->nm_timeo = NFS_MINTIMEO;
+	else if (nmp->nm_timeo > NFS_MAXTIMEO)
+		nmp->nm_timeo = NFS_MAXTIMEO;
+	if (nmp->nm_retry > NFS_MAXREXMIT)
+		nmp->nm_retry = NFS_MAXREXMIT;
+
+	if (nmp->nm_numgrps > NFS_MAXGRPS)
+		nmp->nm_numgrps = NFS_MAXGRPS;
+	if (nmp->nm_readahead > NFS_MAXRAHEAD)
+		nmp->nm_readahead = NFS_MAXRAHEAD;
+	if (nmp->nm_acregmin > nmp->nm_acregmax)
+		nmp->nm_acregmin = nmp->nm_acregmax;
+	if (nmp->nm_acdirmin > nmp->nm_acdirmax)
+		nmp->nm_acdirmin = nmp->nm_acdirmax;
+
+	/* need at least one fs location */
+	if (nmp->nm_locations.nl_numlocs < 1)
+		error = EINVAL;
+	nfsmerr_if(error);
+
+	/* init mount's mntfromname to first location */
+	if (!NM_OMATTR_GIVEN(nmp, MNTFROM))
+		nfs_location_mntfromname(&nmp->nm_locations, firstloc,
+			vfs_statfs(mp)->f_mntfromname, sizeof(vfs_statfs(mp)->f_mntfromname), 0);
+
+	/* Need to save the mounting credential for v4. */
+	nmp->nm_mcred = vfs_context_ucred(ctx);
+	if (IS_VALID_CRED(nmp->nm_mcred))
+		kauth_cred_ref(nmp->nm_mcred);
+
+	/*
+	 * If a reserved port is required, check for that privilege.
+	 * (Note that mirror mounts are exempt because the privilege was
+	 * already checked for the original mount.)
+	 */
+	if (NMFLAG(nmp, RESVPORT) && !vfs_iskernelmount(mp))
+		error = priv_check_cred(nmp->nm_mcred, PRIV_NETINET_RESERVEDPORT, 0);
+	nfsmerr_if(error);
+
+	/* do mount's initial socket connection */
+	error = nfs_mount_connect(nmp);
+	nfsmerr_if(error);
+
+	/* set up the version-specific function tables */
+	if (nmp->nm_vers < NFS_VER4)
+		nmp->nm_funcs = &nfs3_funcs;
+	else
+		nmp->nm_funcs = &nfs4_funcs;
+
+	/* sanity check settings now that version/connection is set */
+	if (nmp->nm_vers == NFS_VER2)		/* ignore RDIRPLUS on NFSv2 */
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_RDIRPLUS);
+	if (nmp->nm_vers >= NFS_VER4) {
+		if (NFS_BITMAP_ISSET(nmp->nm_flags, NFS_MFLAG_ACLONLY)) /* aclonly trumps noacl */
+			NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_NOACL);
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_CALLUMNT);
+		if (nmp->nm_lockmode != NFS_LOCK_MODE_ENABLED)
+			error = EINVAL; /* disabled/local lock mode only allowed on v2/v3 */
+	} else {
+		/* ignore these if not v4 */
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_NOCALLBACK);
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_NONAMEDATTR);
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_NOACL);
+		NFS_BITMAP_CLR(nmp->nm_flags, NFS_MFLAG_ACLONLY);
+	}
+	nfsmerr_if(error);
+
+	if (nmp->nm_sotype == SOCK_DGRAM) {
+		/* I/O size defaults for UDP are different */
+		if (!NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READ_SIZE))
+			nmp->nm_rsize = NFS_DGRAM_RSIZE;
+		if (!NFS_BITMAP_ISSET(mattrs, NFS_MATTR_WRITE_SIZE))
+			nmp->nm_wsize = NFS_DGRAM_WSIZE;
+	}
+
+	/* round down I/O sizes to multiple of NFS_FABLKSIZE */
+	nmp->nm_rsize &= ~(NFS_FABLKSIZE - 1);
+	if (nmp->nm_rsize <= 0)
+		nmp->nm_rsize = NFS_FABLKSIZE;
+	nmp->nm_wsize &= ~(NFS_FABLKSIZE - 1);
+	if (nmp->nm_wsize <= 0)
+		nmp->nm_wsize = NFS_FABLKSIZE;
+
+	/* and limit I/O sizes to maximum allowed */
+	maxio = (nmp->nm_vers == NFS_VER2) ? NFS_V2MAXDATA :
+		(nmp->nm_sotype == SOCK_DGRAM) ? NFS_MAXDGRAMDATA : NFS_MAXDATA;
+	if (maxio > NFS_MAXBSIZE)
+		maxio = NFS_MAXBSIZE;
+	if (nmp->nm_rsize > maxio)
+		nmp->nm_rsize = maxio;
+	if (nmp->nm_wsize > maxio)
+		nmp->nm_wsize = maxio;
+
+	if (nmp->nm_readdirsize > maxio)
+		nmp->nm_readdirsize = maxio;
+	if (nmp->nm_readdirsize > nmp->nm_rsize)
+		nmp->nm_readdirsize = nmp->nm_rsize;
+
+	/* Set up the sockets and related info */
+	if (nmp->nm_sotype == SOCK_DGRAM)
+		TAILQ_INIT(&nmp->nm_cwndq);
+
+	/*
+	 * Get the root node/attributes from the NFS server and
+	 * do any basic, version-specific setup.
+	 */
+	error = nmp->nm_funcs->nf_mount(nmp, ctx, &np);
+	nfsmerr_if(error);
+
+	/*
+	 * A reference count is needed on the node representing the
+	 * remote root.  If this object is not persistent, then backward
+	 * traversals of the mount point (i.e. "..") will not work if
+	 * the node gets flushed out of the cache.
+	 */
+	nmp->nm_dnp = np;
+	*vpp = NFSTOV(np);
+	/* get usecount and drop iocount */
+	error = vnode_ref(*vpp);
+	vnode_put(*vpp);
+	if (error) {
+		vnode_recycle(*vpp);
+		goto nfsmerr;
+	}
+
+	/*
+	 * Do statfs to ensure static info gets set to reasonable values.
+	 */
+	if ((error = nmp->nm_funcs->nf_update_statfs(nmp, ctx))) {
+		int error2 = vnode_getwithref(*vpp);
+		vnode_rele(*vpp);
+		if (!error2)
+			vnode_put(*vpp);
+		vnode_recycle(*vpp);
+		goto nfsmerr;
+	}
+	sbp = vfs_statfs(mp);
+	sbp->f_bsize = nmp->nm_fsattr.nfsa_bsize;
+	sbp->f_blocks = nmp->nm_fsattr.nfsa_space_total / sbp->f_bsize;
+	sbp->f_bfree = nmp->nm_fsattr.nfsa_space_free / sbp->f_bsize;
+	sbp->f_bavail = nmp->nm_fsattr.nfsa_space_avail / sbp->f_bsize;
+	sbp->f_bused = (nmp->nm_fsattr.nfsa_space_total / sbp->f_bsize) -
+			(nmp->nm_fsattr.nfsa_space_free / sbp->f_bsize);
+	sbp->f_files = nmp->nm_fsattr.nfsa_files_total;
+	sbp->f_ffree = nmp->nm_fsattr.nfsa_files_free;
+	sbp->f_iosize = nfs_iosize;
+
+	/*
+	 * Calculate the size used for I/O buffers.  Use the larger
+	 * of the two sizes to minimise NFS requests but make sure
+	 * that it is at least one VM page to avoid wasting buffer
+	 * space and to allow easy mmapping of I/O buffers.
+	 * The read/write RPC calls handle the splitting up of
+	 * buffers into multiple requests if the buffer size is
+	 * larger than the I/O size.
+	 */
+	iosize = max(nmp->nm_rsize, nmp->nm_wsize);
+	if (iosize < PAGE_SIZE)
+		iosize = PAGE_SIZE;
+	nmp->nm_biosize = trunc_page_32(iosize);
+
+	/* For NFSv3 and greater, there is a (relatively) reliable ACCESS call. */
+	if (nmp->nm_vers > NFS_VER2)
+		vfs_setauthopaqueaccess(mp);
+
+	switch (nmp->nm_lockmode) {
+	case NFS_LOCK_MODE_DISABLED:
+		break;
+	case NFS_LOCK_MODE_LOCAL:
+		vfs_setlocklocal(nmp->nm_mountp);
+		break;
+	case NFS_LOCK_MODE_ENABLED:
+	default:
+		if (nmp->nm_vers <= NFS_VER3)
+			nfs_lockd_mount_register(nmp);
+		break;
+	}
+
+	/* success! */
+	lck_mtx_lock(&nmp->nm_lock);
+	nmp->nm_state |= NFSSTA_MOUNTED;
+	lck_mtx_unlock(&nmp->nm_lock);
+	return (0);
+nfsmerr:
+	nfs_mount_cleanup(nmp);
+	return (error);
+}
+
+#if CONFIG_TRIGGERS
+
+/*
+ * We've detected a file system boundary on the server and
+ * need to mount a new file system so that our file systems
+ * MIRROR the file systems on the server.
+ *
+ * Build the mount arguments for the new mount and call kernel_mount().
+ */
+int
+nfs_mirror_mount_domount(vnode_t dvp, vnode_t vp, vfs_context_t ctx)
+{
+	nfsnode_t np = VTONFS(vp);
+	nfsnode_t dnp = VTONFS(dvp);
+	struct nfsmount *nmp = NFSTONMP(np);
+	char fstype[MFSTYPENAMELEN], *mntfromname = NULL, *path = NULL, *relpath, *p, *cp;
+	int error = 0, pathbuflen = MAXPATHLEN, i, mntflags = 0, referral, skipcopy = 0;
+	size_t nlen;
+	struct xdrbuf xb, xbnew;
+	uint32_t mattrs[NFS_MATTR_BITMAP_LEN];
+	uint32_t newmattrs[NFS_MATTR_BITMAP_LEN];
+	uint32_t newmflags[NFS_MFLAG_BITMAP_LEN];
+	uint32_t newmflags_mask[NFS_MFLAG_BITMAP_LEN];
+	uint32_t argslength = 0, val, count, mlen, mlen2, rlen, relpathcomps;
+	uint32_t argslength_offset, attrslength_offset, end_offset;
+	uint32_t numlocs, loc, numserv, serv, numaddr, addr, numcomp, comp;
+	char buf[XDRWORD];
+	struct nfs_fs_locations nfsls;
+
+	referral = (np->n_vattr.nva_flags & NFS_FFLAG_TRIGGER_REFERRAL);
+	if (referral)
+		bzero(&nfsls, sizeof(nfsls));
+
+	xb_init(&xbnew, 0);
+
+	if (!nmp || (nmp->nm_state & (NFSSTA_FORCE|NFSSTA_DEAD)))
+		return (ENXIO);
+
+	/* allocate a couple path buffers we need */
+	MALLOC_ZONE(mntfromname, char *, pathbuflen, M_NAMEI, M_WAITOK); 
+	if (!mntfromname) {
+		error = ENOMEM;
+		goto nfsmerr;
+	}
+	MALLOC_ZONE(path, char *, pathbuflen, M_NAMEI, M_WAITOK); 
+	if (!path) {
+		error = ENOMEM;
+		goto nfsmerr;
+	}
+
+	/* get the path for the directory being mounted on */
+	error = vn_getpath(vp, path, &pathbuflen);
+	if (error) {
+		error = ENOMEM;
+		goto nfsmerr;
+	}
+
+	/*
+	 * Set up the mntfromname for the new mount based on the
+	 * current mount's mntfromname and the directory's path
+	 * relative to the current mount's mntonname.
+	 * Set up relpath to point at the relative path on the current mount.
+	 * Also, count the number of components in relpath.
+	 * We'll be adding those to each fs location path in the new args.
+	 */
+	nlen = strlcpy(mntfromname, vfs_statfs(nmp->nm_mountp)->f_mntfromname, MAXPATHLEN);
+	if ((nlen > 0) && (mntfromname[nlen-1] == '/')) { /* avoid double '/' in new name */
+		mntfromname[nlen-1] = '\0';
+		nlen--;
+	}
+	relpath = mntfromname + nlen;
+	nlen = strlcat(mntfromname, path + strlen(vfs_statfs(nmp->nm_mountp)->f_mntonname), MAXPATHLEN);
+	if (nlen >= MAXPATHLEN) {
+		error = ENAMETOOLONG;
+		goto nfsmerr;
+	}
+	/* count the number of components in relpath */
+	p = relpath;
+	while (*p && (*p == '/'))
+		p++;
+	relpathcomps = 0;
+	while (*p) {
+		relpathcomps++;
+		while (*p && (*p != '/'))
+			p++;
+		while (*p && (*p == '/'))
+			p++;
+	}
+
+	/* grab a copy of the file system type */
+	vfs_name(vnode_mount(vp), fstype);
+
+	/* for referrals, fetch the fs locations */
+	if (referral) {
+		const char *vname = vnode_getname(NFSTOV(np));
+		if (!vname) {
+			error = ENOENT;
+		} else {
+			error = nfs4_get_fs_locations(nmp, dnp, NULL, 0, vname, ctx, &nfsls);
+			vnode_putname(vname);
+			if (!error && (nfsls.nl_numlocs < 1))
+				error = ENOENT;
+		}
+		nfsmerr_if(error);
+	}
+
+	/* set up NFS mount args based on current mount args */
+
+#define xb_copy_32(E, XBSRC, XBDST, V) \
+	do { \
+		if (E) break; \
+		xb_get_32((E), (XBSRC), (V)); \
+		if (skipcopy) break; \
+		xb_add_32((E), (XBDST), (V)); \
+	} while (0)
+#define xb_copy_opaque(E, XBSRC, XBDST) \
+	do { \
+		uint32_t __count, __val; \
+		xb_copy_32((E), (XBSRC), (XBDST), __count); \
+		if (E) break; \
+		__count = nfsm_rndup(__count); \
+		__count /= XDRWORD; \
+		while (__count-- > 0) \
+			xb_copy_32((E), (XBSRC), (XBDST), __val); \
+	} while (0)
+
+	xb_init_buffer(&xb, nmp->nm_args, 2*XDRWORD);
+	xb_get_32(error, &xb, val); /* version */
+	xb_get_32(error, &xb, argslength); /* args length */
+	xb_init_buffer(&xb, nmp->nm_args, argslength);
+
+	xb_init_buffer(&xbnew, NULL, 0);
+	xb_copy_32(error, &xb, &xbnew, val); /* version */
+	argslength_offset = xb_offset(&xbnew);
+	xb_copy_32(error, &xb, &xbnew, val); /* args length */
+	xb_copy_32(error, &xb, &xbnew, val); /* XDR args version */
+	count = NFS_MATTR_BITMAP_LEN;
+	xb_get_bitmap(error, &xb, mattrs, count); /* mount attribute bitmap */
+	nfsmerr_if(error);
+	for (i = 0; i < NFS_MATTR_BITMAP_LEN; i++)
+		newmattrs[i] = mattrs[i];
+	if (referral)
+		NFS_BITMAP_SET(newmattrs, NFS_MATTR_FS_LOCATIONS);
+	else
+		NFS_BITMAP_SET(newmattrs, NFS_MATTR_FH);
+	NFS_BITMAP_SET(newmattrs, NFS_MATTR_FLAGS);
+	NFS_BITMAP_SET(newmattrs, NFS_MATTR_MNTFLAGS);
+	NFS_BITMAP_CLR(newmattrs, NFS_MATTR_MNTFROM);
+	xb_add_bitmap(error, &xbnew, newmattrs, NFS_MATTR_BITMAP_LEN);
+	attrslength_offset = xb_offset(&xbnew);
+	xb_copy_32(error, &xb, &xbnew, val); /* attrs length */
+	NFS_BITMAP_ZERO(newmflags_mask, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_ZERO(newmflags, NFS_MFLAG_BITMAP_LEN);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FLAGS)) {
+		count = NFS_MFLAG_BITMAP_LEN;
+		xb_get_bitmap(error, &xb, newmflags_mask, count); /* mount flag mask bitmap */
+		count = NFS_MFLAG_BITMAP_LEN;
+		xb_get_bitmap(error, &xb, newmflags, count); /* mount flag bitmap */
+	}
+	NFS_BITMAP_SET(newmflags_mask, NFS_MFLAG_EPHEMERAL);
+	NFS_BITMAP_SET(newmflags, NFS_MFLAG_EPHEMERAL);
+	xb_add_bitmap(error, &xbnew, newmflags_mask, NFS_MFLAG_BITMAP_LEN);
+	xb_add_bitmap(error, &xbnew, newmflags, NFS_MFLAG_BITMAP_LEN);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_VERSION))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_MINOR_VERSION))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_VERSION_RANGE)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READ_SIZE))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_WRITE_SIZE))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READDIR_SIZE))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_READAHEAD))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_REG_MIN)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_REG_MAX)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MIN)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MAX)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_LOCK_MODE))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SECURITY)) {
+		xb_copy_32(error, &xb, &xbnew, count);
+		while (!error && (count-- > 0))
+			xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MAX_GROUP_LIST))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SOCKET_TYPE))
+		xb_copy_opaque(error, &xb, &xbnew);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_NFS_PORT))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MOUNT_PORT))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_REQUEST_TIMEOUT)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SOFT_RETRY_COUNT))
+		xb_copy_32(error, &xb, &xbnew, val);
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_DEAD_TIMEOUT)) {
+		xb_copy_32(error, &xb, &xbnew, val);
+		xb_copy_32(error, &xb, &xbnew, val);
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FH)) {
+		xb_get_32(error, &xb, count);
+		xb_skip(error, &xb, count);
+	}
+	if (!referral) {
+		/* set the initial file handle to the directory's file handle */
+		xb_add_fh(error, &xbnew, np->n_fhp, np->n_fhsize);
+	}
+	/* copy/extend/skip fs locations */
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_FS_LOCATIONS)) {
+		numlocs = numserv = numaddr = numcomp = 0;
+		if (referral) /* don't copy the fs locations for a referral */
+			skipcopy = 1;
+		xb_copy_32(error, &xb, &xbnew, numlocs); /* location count */
+		for (loc = 0; !error && (loc < numlocs); loc++) {
+			xb_copy_32(error, &xb, &xbnew, numserv); /* server count */
+			for (serv = 0; !error && (serv < numserv); serv++) {
+				xb_copy_opaque(error, &xb, &xbnew); /* server name */
+				xb_copy_32(error, &xb, &xbnew, numaddr); /* address count */
+				for (addr = 0; !error && (addr < numaddr); addr++)
+					xb_copy_opaque(error, &xb, &xbnew); /* address */
+				xb_copy_opaque(error, &xb, &xbnew); /* server info */
+			}
+			/* pathname */
+			xb_get_32(error, &xb, numcomp); /* component count */
+			if (!skipcopy)
+				xb_add_32(error, &xbnew, numcomp+relpathcomps); /* new component count */
+			for (comp = 0; !error && (comp < numcomp); comp++)
+				xb_copy_opaque(error, &xb, &xbnew); /* component */
+			/* add additional components */
+			for (comp = 0; !skipcopy && !error && (comp < relpathcomps); comp++) {
+				p = relpath;
+				while (*p && (*p == '/'))
+					p++;
+				while (*p && !error) {
+					cp = p;
+					while (*p && (*p != '/'))
+						p++;
+					xb_add_string(error, &xbnew, cp, (p - cp)); /* component */
+					while (*p && (*p == '/'))
+						p++;
+				}
+			}
+			xb_copy_opaque(error, &xb, &xbnew); /* fs location info */
+		}
+		if (referral)
+			skipcopy = 0;
+	}
+	if (referral) {
+		/* add referral's fs locations */
+		xb_add_32(error, &xbnew, nfsls.nl_numlocs);			/* FS_LOCATIONS */
+		for (loc = 0; !error && (loc < nfsls.nl_numlocs); loc++) {
+			xb_add_32(error, &xbnew, nfsls.nl_locations[loc]->nl_servcount);
+			for (serv = 0; !error && (serv < nfsls.nl_locations[loc]->nl_servcount); serv++) {
+				xb_add_string(error, &xbnew, nfsls.nl_locations[loc]->nl_servers[serv]->ns_name,
+					strlen(nfsls.nl_locations[loc]->nl_servers[serv]->ns_name));
+				xb_add_32(error, &xbnew, nfsls.nl_locations[loc]->nl_servers[serv]->ns_addrcount);
+				for (addr = 0; !error && (addr < nfsls.nl_locations[loc]->nl_servers[serv]->ns_addrcount); addr++)
+					xb_add_string(error, &xbnew, nfsls.nl_locations[loc]->nl_servers[serv]->ns_addresses[addr],
+						strlen(nfsls.nl_locations[loc]->nl_servers[serv]->ns_addresses[addr]));
+				xb_add_32(error, &xbnew, 0); /* empty server info */
+			}
+			xb_add_32(error, &xbnew, nfsls.nl_locations[loc]->nl_path.np_compcount);
+			for (comp = 0; !error && (comp < nfsls.nl_locations[loc]->nl_path.np_compcount); comp++)
+				xb_add_string(error, &xbnew, nfsls.nl_locations[loc]->nl_path.np_components[comp],
+					strlen(nfsls.nl_locations[loc]->nl_path.np_components[comp]));
+			xb_add_32(error, &xbnew, 0); /* empty fs location info */
+		}
+	}
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MNTFLAGS))
+		xb_get_32(error, &xb, mntflags);
+	/*
+	 * We add the following mount flags to the ones for the mounted-on mount:
+	 * MNT_DONTBROWSE - to keep the mount from showing up as a separate volume
+	 * MNT_AUTOMOUNTED - to keep DiskArb from retriggering the mount after
+	 *                   an unmount (looking for /.autodiskmounted)
+	 */
+	mntflags |= (MNT_AUTOMOUNTED | MNT_DONTBROWSE);
+	xb_add_32(error, &xbnew, mntflags);
+	if (!referral && NFS_BITMAP_ISSET(mattrs, NFS_MATTR_MNTFROM)) {
+		/* copy mntfrom string and add relpath */
+		rlen = strlen(relpath);
+		xb_get_32(error, &xb, mlen);
+		nfsmerr_if(error);
+		mlen2 = mlen + ((relpath[0] != '/') ? 1 : 0) + rlen;
+		xb_add_32(error, &xbnew, mlen2);
+		count = mlen/XDRWORD;
+		/* copy the original string */
+		while (count-- > 0)
+			xb_copy_32(error, &xb, &xbnew, val);
+		if (!error && (mlen % XDRWORD)) {
+			error = xb_get_bytes(&xb, buf, mlen%XDRWORD, 0);
+			if (!error)
+				error = xb_add_bytes(&xbnew, buf, mlen%XDRWORD, 1);
+		}
+		/* insert a '/' if the relative path doesn't start with one */
+		if (!error && (relpath[0] != '/')) {
+			buf[0] = '/';
+			error = xb_add_bytes(&xbnew, buf, 1, 1);
+		}
+		/* add the additional relative path */
+		if (!error)
+			error = xb_add_bytes(&xbnew, relpath, rlen, 1);
+		/* make sure the resulting string has the right number of pad bytes */
+		if (!error && (mlen2 != nfsm_rndup(mlen2))) {
+			bzero(buf, sizeof(buf));
+			count = nfsm_rndup(mlen2) - mlen2;
+			error = xb_add_bytes(&xbnew, buf, count, 1);
+		}
+	}
+	xb_build_done(error, &xbnew);
+
+	/* update opaque counts */
+	end_offset = xb_offset(&xbnew);
+	if (!error) {
+		error = xb_seek(&xbnew, argslength_offset);
+		argslength = end_offset - argslength_offset + XDRWORD/*version*/;
+		xb_add_32(error, &xbnew, argslength);
+	}
+	if (!error) {
+		error = xb_seek(&xbnew, attrslength_offset);
+		xb_add_32(error, &xbnew, end_offset - attrslength_offset - XDRWORD/*don't include length field*/);
+	}
+	nfsmerr_if(error);
+
+	/*
+	 * For kernel_mount() call, use the existing mount flags (instead of the
+	 * original flags) because flags like MNT_NOSUID and MNT_NODEV may have
+	 * been silently enforced.
+	 */
+	mntflags = vnode_vfsvisflags(vp);
+	mntflags |= (MNT_AUTOMOUNTED | MNT_DONTBROWSE);
+
+	/* do the mount */
+	error = kernel_mount(fstype, dvp, vp, path, xb_buffer_base(&xbnew), argslength,
+			mntflags, KERNEL_MOUNT_PERMIT_UNMOUNT | KERNEL_MOUNT_NOAUTH, ctx);
+
+nfsmerr:
+	if (error)
+		printf("nfs: mirror mount of %s on %s failed (%d)\n",
+			mntfromname, path, error);
+	/* clean up */
+	xb_cleanup(&xbnew);
+	if (referral)
+		nfs_fs_locations_cleanup(&nfsls);
+	if (path)
+		FREE_ZONE(path, MAXPATHLEN, M_NAMEI);
+	if (mntfromname)
+		FREE_ZONE(mntfromname, MAXPATHLEN, M_NAMEI);
+	if (!error)
+		nfs_ephemeral_mount_harvester_start();
+	return (error);
+}
+
+/*
+ * trigger vnode functions
+ */
+
+resolver_result_t
+nfs_mirror_mount_trigger_resolve(
+	vnode_t vp,
+	const struct componentname *cnp,
+	enum path_operation pop,
+	__unused int flags,
+	__unused void *data,
+	vfs_context_t ctx)
+{
+	nfsnode_t np = VTONFS(vp);
+	vnode_t pvp = NULLVP;
+	int error = 0;
+	resolver_result_t result;
+
+	/*
+	 * We have a trigger node that doesn't have anything mounted on it yet.
+	 * We'll do the mount if either:
+	 * (a) this isn't the last component of the path OR
+	 * (b) this is an op that looks like it should trigger the mount.
+	 */
+	if (cnp->cn_flags & ISLASTCN) {
+		switch (pop) {
+		case OP_MOUNT:
+		case OP_UNMOUNT:
+		case OP_STATFS:
+		case OP_LINK:
+		case OP_UNLINK:
+		case OP_RENAME:
+		case OP_MKNOD:
+		case OP_MKFIFO:
+		case OP_SYMLINK:
+		case OP_ACCESS:
+		case OP_GETATTR:
+		case OP_MKDIR:
+		case OP_RMDIR:
+		case OP_REVOKE:
+		case OP_GETXATTR:
+		case OP_LISTXATTR:
+			/* don't perform the mount for these operations */
+			result = vfs_resolver_result(np->n_trigseq, RESOLVER_NOCHANGE, 0);
+#ifdef NFS_TRIGGER_DEBUG
+			NP(np, "nfs trigger RESOLVE: no change, last %d nameiop %d, seq %d",
+				(cnp->cn_flags & ISLASTCN) ? 1 : 0, cnp->cn_nameiop, np->n_trigseq);
+#endif
+			return (result);
+		case OP_OPEN:
+		case OP_CHDIR:
+		case OP_CHROOT:
+		case OP_TRUNCATE:
+		case OP_COPYFILE:
+		case OP_PATHCONF:
+		case OP_READLINK:
+		case OP_SETATTR:
+		case OP_EXCHANGEDATA:
+		case OP_SEARCHFS:
+		case OP_FSCTL:
+		case OP_SETXATTR:
+		case OP_REMOVEXATTR:
+		default:
+			/* go ahead and do the mount */
+			break;
+		}
+	}
+
+	if (vnode_mountedhere(vp) != NULL) {
+		/*
+		 * Um... there's already something mounted.
+		 * Been there.  Done that.  Let's just say it succeeded.
+		 */
+		error = 0;
+		goto skipmount;
+	}
+
+	if ((error = nfs_node_set_busy(np, vfs_context_thread(ctx)))) {
+		result = vfs_resolver_result(np->n_trigseq, RESOLVER_ERROR, error);
+#ifdef NFS_TRIGGER_DEBUG
+		NP(np, "nfs trigger RESOLVE: busy error %d, last %d nameiop %d, seq %d",
+			error, (cnp->cn_flags & ISLASTCN) ? 1 : 0, cnp->cn_nameiop, np->n_trigseq);
+#endif
+		return (result);
+	}
+
+	pvp = vnode_getparent(vp);
+	if (pvp == NULLVP)
+		error = EINVAL;
+	if (!error)
+		error = nfs_mirror_mount_domount(pvp, vp, ctx);
+skipmount:
+	if (!error)
+		np->n_trigseq++;
+	result = vfs_resolver_result(np->n_trigseq, error ? RESOLVER_ERROR : RESOLVER_RESOLVED, error);
+#ifdef NFS_TRIGGER_DEBUG
+	NP(np, "nfs trigger RESOLVE: %s %d, last %d nameiop %d, seq %d",
+		error ? "error" : "resolved", error,
+		(cnp->cn_flags & ISLASTCN) ? 1 : 0, cnp->cn_nameiop, np->n_trigseq);
+#endif
+
+	if (pvp != NULLVP)
+		vnode_put(pvp);
+	nfs_node_clear_busy(np);
+	return (result);
+}
+
+resolver_result_t
+nfs_mirror_mount_trigger_unresolve(
+	vnode_t vp,
+	int flags,
+	__unused void *data,
+	vfs_context_t ctx)
+{
+	nfsnode_t np = VTONFS(vp);
+	mount_t mp;
+	int error;
+	resolver_result_t result;
+
+	if ((error = nfs_node_set_busy(np, vfs_context_thread(ctx)))) {
+		result = vfs_resolver_result(np->n_trigseq, RESOLVER_ERROR, error);
+#ifdef NFS_TRIGGER_DEBUG
+		NP(np, "nfs trigger UNRESOLVE: busy error %d, seq %d", error, np->n_trigseq);
+#endif
+		return (result);
+	}
+
+	mp = vnode_mountedhere(vp);
+	if (!mp)
+		error = EINVAL;
+	if (!error)
+		error = vfs_unmountbyfsid(&(vfs_statfs(mp)->f_fsid), flags, ctx);
+	if (!error)
+		np->n_trigseq++;
+	result = vfs_resolver_result(np->n_trigseq, error ? RESOLVER_ERROR : RESOLVER_UNRESOLVED, error);
+#ifdef NFS_TRIGGER_DEBUG
+	NP(np, "nfs trigger UNRESOLVE: %s %d, seq %d",
+		error ? "error" : "unresolved", error, np->n_trigseq);
+#endif
+	nfs_node_clear_busy(np);
+	return (result);
+}
+
+resolver_result_t
+nfs_mirror_mount_trigger_rearm(
+	vnode_t vp,
+	__unused int flags,
+	__unused void *data,
+	vfs_context_t ctx)
+{
+	nfsnode_t np = VTONFS(vp);
+	int error;
+	resolver_result_t result;
+
+	if ((error = nfs_node_set_busy(np, vfs_context_thread(ctx)))) {
+		result = vfs_resolver_result(np->n_trigseq, RESOLVER_ERROR, error);
+#ifdef NFS_TRIGGER_DEBUG
+		NP(np, "nfs trigger REARM: busy error %d, seq %d", error, np->n_trigseq);
+#endif
+		return (result);
+	}
+
+	np->n_trigseq++;
+	result = vfs_resolver_result(np->n_trigseq,
+			vnode_mountedhere(vp) ? RESOLVER_RESOLVED : RESOLVER_UNRESOLVED, 0);
+#ifdef NFS_TRIGGER_DEBUG
+	NP(np, "nfs trigger REARM: %s, seq %d",
+		vnode_mountedhere(vp) ? "resolved" : "unresolved", np->n_trigseq);
+#endif
+	nfs_node_clear_busy(np);
+	return (result);
+}
+
+/*
+ * Periodically attempt to unmount ephemeral (mirror) mounts in an attempt to limit
+ * the number of unused mounts.
+ */
+
+#define NFS_EPHEMERAL_MOUNT_HARVEST_INTERVAL	120	/* how often the harvester runs */
+struct nfs_ephemeral_mount_harvester_info {
+	fsid_t		fsid;		/* FSID that we need to try to unmount */
+	uint32_t	mountcount;	/* count of ephemeral mounts seen in scan */
+ };
+/* various globals for the harvester */
+static thread_call_t nfs_ephemeral_mount_harvester_timer = NULL;
+static int nfs_ephemeral_mount_harvester_on = 0;
+
+kern_return_t thread_terminate(thread_t);
+
+static int
+nfs_ephemeral_mount_harvester_callback(mount_t mp, void *arg)
+{
+	struct nfs_ephemeral_mount_harvester_info *hinfo = arg;
+	struct nfsmount *nmp;
+	struct timeval now;
+
+	if (strcmp(mp->mnt_vfsstat.f_fstypename, "nfs"))
+		return (VFS_RETURNED);
+	nmp = VFSTONFS(mp);
+	if (!nmp || !NMFLAG(nmp, EPHEMERAL))
+		return (VFS_RETURNED);
+	hinfo->mountcount++;
+
+	/* avoid unmounting mounts that have been triggered within the last harvest interval */
+	microtime(&now);
+	if ((nmp->nm_mounttime >> 32) > ((uint32_t)now.tv_sec - NFS_EPHEMERAL_MOUNT_HARVEST_INTERVAL))
+		return (VFS_RETURNED);
+
+	if (hinfo->fsid.val[0] || hinfo->fsid.val[1]) {
+		/* attempt to unmount previously-found ephemeral mount */
+		vfs_unmountbyfsid(&hinfo->fsid, 0, vfs_context_kernel());
+		hinfo->fsid.val[0] = hinfo->fsid.val[1] = 0;
+	}
+
+	/*
+	 * We can't call unmount here since we hold a mount iter ref
+	 * on mp so save its fsid for the next call iteration to unmount.
+	 */
+	hinfo->fsid.val[0] = mp->mnt_vfsstat.f_fsid.val[0];
+	hinfo->fsid.val[1] = mp->mnt_vfsstat.f_fsid.val[1];
+
+	return (VFS_RETURNED);
+}
+
+/*
+ * Spawn a thread to do the ephemeral mount harvesting.
+ */
+static void
+nfs_ephemeral_mount_harvester_timer_func(void)
+{
+	thread_t thd;
+
+	if (kernel_thread_start(nfs_ephemeral_mount_harvester, NULL, &thd) == KERN_SUCCESS)
+		thread_deallocate(thd);
+}
+
+/*
+ * Iterate all mounts looking for NFS ephemeral mounts to try to unmount.
+ */
+void
+nfs_ephemeral_mount_harvester(__unused void *arg, __unused wait_result_t wr)
+{
+	struct nfs_ephemeral_mount_harvester_info hinfo;
+	uint64_t deadline;
+
+	hinfo.mountcount = 0;
+	hinfo.fsid.val[0] = hinfo.fsid.val[1] = 0;
+	vfs_iterate(VFS_ITERATE_TAIL_FIRST, nfs_ephemeral_mount_harvester_callback, &hinfo);
+	if (hinfo.fsid.val[0] || hinfo.fsid.val[1]) {
+		/* attempt to unmount last found ephemeral mount */
+		vfs_unmountbyfsid(&hinfo.fsid, 0, vfs_context_kernel());
+	}
+
+	lck_mtx_lock(nfs_global_mutex);
+	if (!hinfo.mountcount) {
+		/* no more ephemeral mounts - don't need timer */
+		nfs_ephemeral_mount_harvester_on = 0;
+	} else {
+		/* re-arm the timer */
+		clock_interval_to_deadline(NFS_EPHEMERAL_MOUNT_HARVEST_INTERVAL, NSEC_PER_SEC, &deadline);
+		thread_call_enter_delayed(nfs_ephemeral_mount_harvester_timer, deadline);
+		nfs_ephemeral_mount_harvester_on = 1;
+	}
+	lck_mtx_unlock(nfs_global_mutex);
+
+	/* thread done */
+	thread_terminate(current_thread());
+}
+
+/*
+ * Make sure the NFS ephemeral mount harvester timer is running.
+ */
+void
+nfs_ephemeral_mount_harvester_start(void)
+{
+	uint64_t deadline;
+
+	lck_mtx_lock(nfs_global_mutex);
+	if (nfs_ephemeral_mount_harvester_on) {
+		lck_mtx_unlock(nfs_global_mutex);
+		return;
+	}
+	if (nfs_ephemeral_mount_harvester_timer == NULL)
+		nfs_ephemeral_mount_harvester_timer = thread_call_allocate((thread_call_func_t)nfs_ephemeral_mount_harvester_timer_func, NULL);
+	clock_interval_to_deadline(NFS_EPHEMERAL_MOUNT_HARVEST_INTERVAL, NSEC_PER_SEC, &deadline);
+	thread_call_enter_delayed(nfs_ephemeral_mount_harvester_timer, deadline);
+	nfs_ephemeral_mount_harvester_on = 1;
+	lck_mtx_unlock(nfs_global_mutex);
+}
+
+#endif
+
+/*
+ * Send a MOUNT protocol MOUNT request to the server to get the initial file handle (and security).
+ */
+int
+nfs3_mount_rpc(struct nfsmount *nmp, struct sockaddr *sa, int sotype, int nfsvers, char *path, vfs_context_t ctx, int timeo, fhandle_t *fh, struct nfs_sec *sec)
+{
+	int error = 0, slen, mntproto;
+	thread_t thd = vfs_context_thread(ctx);
+	kauth_cred_t cred = vfs_context_ucred(ctx);
+	uint64_t xid = 0;
+	struct nfsm_chain nmreq, nmrep;
+	mbuf_t mreq;
+	uint32_t mntvers, mntport, val;
+	struct sockaddr_storage ss;
+	struct sockaddr *saddr = (struct sockaddr*)&ss;
+
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	mntvers = (nfsvers == NFS_VER2) ? RPCMNT_VER1 : RPCMNT_VER3;
+	mntproto = (NM_OMFLAG(nmp, MNTUDP) || (sotype == SOCK_DGRAM)) ? IPPROTO_UDP : IPPROTO_TCP;
+	sec->count = 0;
+
+	bcopy(sa, saddr, min(sizeof(ss), sa->sa_len));
+	if (saddr->sa_family == AF_INET) {
+		if (nmp->nm_mountport)
+			((struct sockaddr_in*)saddr)->sin_port = htons(nmp->nm_mountport);
+		mntport = ntohs(((struct sockaddr_in*)saddr)->sin_port);
+	} else {
+		if (nmp->nm_mountport)
+			((struct sockaddr_in6*)saddr)->sin6_port = htons(nmp->nm_mountport);
+		mntport = ntohs(((struct sockaddr_in6*)saddr)->sin6_port);
+	}
+
+	while (!mntport) {
+		error = nfs_portmap_lookup(nmp, ctx, saddr, NULL, RPCPROG_MNT, mntvers, mntproto, timeo);
+		nfsmout_if(error);
+		if (saddr->sa_family == AF_INET)
+			mntport = ntohs(((struct sockaddr_in*)saddr)->sin_port);
+		else
+			mntport = ntohs(((struct sockaddr_in6*)saddr)->sin6_port);
+		if (!mntport) {
+			/* if not found and TCP, then retry with UDP */
+			if (mntproto == IPPROTO_UDP) {
+				error = EPROGUNAVAIL;
+				break;
+			}
+			mntproto = IPPROTO_UDP;
+			bcopy(sa, saddr, min(sizeof(ss), sa->sa_len));
+		}
+	}
+	nfsmout_if(error || !mntport);
+
+	/* MOUNT protocol MOUNT request */
+	slen = strlen(path);
+	nfsm_chain_build_alloc_init(error, &nmreq, NFSX_UNSIGNED + nfsm_rndup(slen));
+	nfsm_chain_add_name(error, &nmreq, path, slen, nmp);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsmout_if(error);
+	error = nfsm_rpchead2(nmp, (mntproto == IPPROTO_UDP) ? SOCK_DGRAM : SOCK_STREAM,
+			RPCPROG_MNT, mntvers, RPCMNT_MOUNT,
+			RPCAUTH_SYS, cred, NULL, nmreq.nmc_mhead, &xid, &mreq);
+	nfsmout_if(error);
+	nmreq.nmc_mhead = NULL;
+	error = nfs_aux_request(nmp, thd, saddr, NULL,
+			((mntproto == IPPROTO_UDP) ? SOCK_DGRAM : SOCK_STREAM),
+			mreq, R_XID32(xid), 1, timeo, &nmrep);
+	nfsmout_if(error);
+	nfsm_chain_get_32(error, &nmrep, val);
+	if (!error && val)
+		error = val;
+	nfsm_chain_get_fh(error, &nmrep, nfsvers, fh);
+	if (!error && (nfsvers > NFS_VER2)) {
+		sec->count = NX_MAX_SEC_FLAVORS;
+		error = nfsm_chain_get_secinfo(&nmrep, &sec->flavors[0], &sec->count);
+	}
+nfsmout:
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	return (error);
+}
+
+
+/*
+ * Send a MOUNT protocol UNMOUNT request to tell the server we've unmounted it.
+ */
+void
+nfs3_umount_rpc(struct nfsmount *nmp, vfs_context_t ctx, int timeo)
+{
+	int error = 0, slen, mntproto;
+	thread_t thd = vfs_context_thread(ctx);
+	kauth_cred_t cred = vfs_context_ucred(ctx);
+	char *path;
+	uint64_t xid = 0;
+	struct nfsm_chain nmreq, nmrep;
+	mbuf_t mreq;
+	uint32_t mntvers, mntport;
+	struct sockaddr_storage ss;
+	struct sockaddr *saddr = (struct sockaddr*)&ss;
+
+	if (!nmp->nm_saddr)
+		return;
+
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	mntvers = (nmp->nm_vers == NFS_VER2) ? RPCMNT_VER1 : RPCMNT_VER3;
+	mntproto = (NM_OMFLAG(nmp, MNTUDP) || (nmp->nm_sotype == SOCK_DGRAM)) ? IPPROTO_UDP : IPPROTO_TCP;
+	mntport = nmp->nm_mountport;
+
+	bcopy(nmp->nm_saddr, saddr, min(sizeof(ss), nmp->nm_saddr->sa_len));
+	if (saddr->sa_family == AF_INET)
+		((struct sockaddr_in*)saddr)->sin_port = htons(mntport);
+	else
+		((struct sockaddr_in6*)saddr)->sin6_port = htons(mntport);
+
+	while (!mntport) {
+		error = nfs_portmap_lookup(nmp, ctx, saddr, NULL, RPCPROG_MNT, mntvers, mntproto, timeo);
+  		nfsmout_if(error);
+		if (saddr->sa_family == AF_INET)
+			mntport = ntohs(((struct sockaddr_in*)saddr)->sin_port);
+		else
+			mntport = ntohs(((struct sockaddr_in6*)saddr)->sin6_port);
+		/* if not found and mntvers > VER1, then retry with VER1 */
+		if (!mntport) {
+			if (mntvers > RPCMNT_VER1) {
+				mntvers = RPCMNT_VER1;
+			} else if (mntproto == IPPROTO_TCP) {
+				mntproto = IPPROTO_UDP;
+				mntvers = (nmp->nm_vers == NFS_VER2) ? RPCMNT_VER1 : RPCMNT_VER3;
+			} else {
+				break;
+			}
+			bcopy(nmp->nm_saddr, saddr, min(sizeof(ss), nmp->nm_saddr->sa_len));
+		}
+	}
+	nfsmout_if(!mntport);
+
+	/* MOUNT protocol UNMOUNT request */
+	path = &vfs_statfs(nmp->nm_mountp)->f_mntfromname[0];
+	while (*path && (*path != '/'))
+		path++;
+	slen = strlen(path);
+	nfsm_chain_build_alloc_init(error, &nmreq, NFSX_UNSIGNED + nfsm_rndup(slen));
+	nfsm_chain_add_name(error, &nmreq, path, slen, nmp);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsmout_if(error);
+	error = nfsm_rpchead2(nmp, (mntproto == IPPROTO_UDP) ? SOCK_DGRAM : SOCK_STREAM,
+			RPCPROG_MNT, RPCMNT_VER1, RPCMNT_UMOUNT,
+			RPCAUTH_SYS, cred, NULL, nmreq.nmc_mhead, &xid, &mreq);
+	nfsmout_if(error);
+	nmreq.nmc_mhead = NULL;
+	error = nfs_aux_request(nmp, thd, saddr, NULL,
+		((mntproto == IPPROTO_UDP) ? SOCK_DGRAM : SOCK_STREAM),
+		mreq, R_XID32(xid), 1, timeo, &nmrep);
+nfsmout:
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+}
+
+/*
+ * unmount system call
+ */
+int
+nfs_vfs_unmount(
+	mount_t mp,
+	int mntflags,
+	__unused vfs_context_t ctx)
+{
+	struct nfsmount *nmp;
+	vnode_t vp;
+	int error, flags = 0;
+	struct timespec ts = { 1, 0 };
+
+	nmp = VFSTONFS(mp);
+	lck_mtx_lock(&nmp->nm_lock);
+	/*
+	 * Set the flag indicating that an unmount attempt is in progress.
+	 */
+	nmp->nm_state |= NFSSTA_UNMOUNTING;
+	/*
+	 * During a force unmount we want to...
+	 *   Mark that we are doing a force unmount.
+	 *   Make the mountpoint soft.
+	 */
+	if (mntflags & MNT_FORCE) {
+		flags |= FORCECLOSE;
+		nmp->nm_state |= NFSSTA_FORCE;
+		NFS_BITMAP_SET(nmp->nm_flags, NFS_MFLAG_SOFT);
+	}
+	/*
+	 * Wait for any in-progress monitored node scan to complete.
+	 */
+	while (nmp->nm_state & NFSSTA_MONITOR_SCAN)
+		msleep(&nmp->nm_state, &nmp->nm_lock, PZERO-1, "nfswaitmonscan", &ts);
+	/*
+	 * Goes something like this..
+	 * - Call vflush() to clear out vnodes for this file system,
+	 *   except for the swap files. Deal with them in 2nd pass.
+	 * - Decrement reference on the vnode representing remote root.
+	 * - Clean up the NFS mount structure.
+	 */
+	vp = NFSTOV(nmp->nm_dnp);
+	lck_mtx_unlock(&nmp->nm_lock);
+
+	/*
+	 * vflush will check for busy vnodes on mountpoint.
+	 * Will do the right thing for MNT_FORCE. That is, we should
+	 * not get EBUSY back.
+	 */
+	error = vflush(mp, vp, SKIPSWAP | flags);
+	if (mntflags & MNT_FORCE) {
+		error = vflush(mp, NULLVP, flags); /* locks vp in the process */
+	} else {
+		if (vnode_isinuse(vp, 1))
+			error = EBUSY;
+		else
+			error = vflush(mp, vp, flags);
+	}
+	if (error) {
+		lck_mtx_lock(&nmp->nm_lock);
+		nmp->nm_state &= ~NFSSTA_UNMOUNTING;
+		lck_mtx_unlock(&nmp->nm_lock);
+		return (error);
+	}
+
+	lck_mtx_lock(&nmp->nm_lock);
+	nmp->nm_dnp = NULL;
+	lck_mtx_unlock(&nmp->nm_lock);
+
+	/*
+	 * Release the root vnode reference held by mountnfs()
+	 */
+	error = vnode_get(vp);
+	vnode_rele(vp);
+	if (!error)
+		vnode_put(vp);
+
+	vflush(mp, NULLVP, FORCECLOSE);
+
+	/* Wait for all other references to be released and free the mount */
+	nfs_mount_drain_and_cleanup(nmp);
+	
+	return (0);
+}
+
+/*
+ * cleanup/destroy NFS fs locations structure
+ */
+void
+nfs_fs_locations_cleanup(struct nfs_fs_locations *nfslsp)
+{
+	struct nfs_fs_location *fsl;
+	struct nfs_fs_server *fss;
+	struct nfs_fs_path *fsp;
+	uint32_t loc, serv, addr, comp;
+
+	/* free up fs locations */
+	if (!nfslsp->nl_numlocs || !nfslsp->nl_locations)
+		return;
+
+	for (loc = 0; loc < nfslsp->nl_numlocs; loc++) {
+		fsl = nfslsp->nl_locations[loc];
+		if (!fsl)
+			continue;
+		if ((fsl->nl_servcount > 0) && fsl->nl_servers) {
+			for (serv = 0; serv < fsl->nl_servcount; serv++) {
+				fss = fsl->nl_servers[serv];
+				if (!fss)
+					continue;
+				if ((fss->ns_addrcount > 0) && fss->ns_addresses) {
+					for (addr = 0; addr < fss->ns_addrcount; addr++)
+						FREE(fss->ns_addresses[addr], M_TEMP);
+					FREE(fss->ns_addresses, M_TEMP);
+				}
+				FREE(fss->ns_name, M_TEMP);
+				FREE(fss, M_TEMP);
+			}
+			FREE(fsl->nl_servers, M_TEMP);
+		}
+		fsp = &fsl->nl_path;
+		if (fsp->np_compcount && fsp->np_components) {
+			for (comp = 0; comp < fsp->np_compcount; comp++)
+				if (fsp->np_components[comp])
+					FREE(fsp->np_components[comp], M_TEMP);
+			FREE(fsp->np_components, M_TEMP);
+		}
+		FREE(fsl, M_TEMP);
+	}
+	FREE(nfslsp->nl_locations, M_TEMP);
+	nfslsp->nl_numlocs = 0;
+	nfslsp->nl_locations = NULL;
+}
+
+void
+nfs_mount_rele(struct nfsmount *nmp)
+{
+	int wup = 0;
+
+	lck_mtx_lock(&nmp->nm_lock);
+	if (nmp->nm_ref < 1)
+		panic("nfs zombie mount underflow\n");
+	nmp->nm_ref--;
+	if (nmp->nm_ref == 0)
+		wup = nmp->nm_state & NFSSTA_MOUNT_DRAIN;
+	lck_mtx_unlock(&nmp->nm_lock);
+	if (wup)
+		wakeup(&nmp->nm_ref);
+}
+
+void
+nfs_mount_drain_and_cleanup(struct nfsmount *nmp)
+{
+	lck_mtx_lock(&nmp->nm_lock);
+	nmp->nm_state |= NFSSTA_MOUNT_DRAIN;
+	while (nmp->nm_ref > 0) {
+		msleep(&nmp->nm_ref, &nmp->nm_lock, PZERO-1, "nfs_mount_drain", NULL);
+	}
+	assert(nmp->nm_ref == 0);
+	lck_mtx_unlock(&nmp->nm_lock);
+	nfs_mount_cleanup(nmp);
+}
+
+/*
+ * nfs_mount_zombie
+ */
+void
+nfs_mount_zombie(struct nfsmount *nmp, int nm_state_flags)
+{
+	struct nfsreq *req, *treq;
+	struct nfs_reqqhead iodq, resendq;
+	struct timespec ts = { 1, 0 };
+	struct nfs_open_owner *noop, *nextnoop;
+	nfsnode_t np;
+	int docallback;
+
+	lck_mtx_lock(&nmp->nm_lock);
+	nmp->nm_state |= nm_state_flags;
+	nmp->nm_ref++;
+	lck_mtx_unlock(&nmp->nm_lock);
+	
+	/* stop callbacks */
+	if ((nmp->nm_vers >= NFS_VER4) && !NMFLAG(nmp, NOCALLBACK) && nmp->nm_cbid)
+		nfs4_mount_callback_shutdown(nmp);
+
+	/* Destroy any RPCSEC_GSS contexts */
+	nfs_gss_clnt_ctx_unmount(nmp);
+
+	/* mark the socket for termination */
+	lck_mtx_lock(&nmp->nm_lock);
+	nmp->nm_sockflags |= NMSOCK_UNMOUNT;
+
+	/* Have the socket thread send the unmount RPC, if requested/appropriate. */
+	if ((nmp->nm_vers < NFS_VER4) && (nmp->nm_state & NFSSTA_MOUNTED) &&
+	    !(nmp->nm_state & (NFSSTA_FORCE|NFSSTA_DEAD)) && NMFLAG(nmp, CALLUMNT))
+		nfs_mount_sock_thread_wake(nmp);
+
+	/* wait for the socket thread to terminate */
+	while (nmp->nm_sockthd && current_thread() != nmp->nm_sockthd) {
+		wakeup(&nmp->nm_sockthd);
+		msleep(&nmp->nm_sockthd, &nmp->nm_lock, PZERO-1, "nfswaitsockthd", &ts);
+	}
+	lck_mtx_unlock(&nmp->nm_lock);
+
+	/* tear down the socket */
+	nfs_disconnect(nmp);
+
+	lck_mtx_lock(&nmp->nm_lock);
+
+	if ((nmp->nm_vers >= NFS_VER4) && !NMFLAG(nmp, NOCALLBACK) && nmp->nm_cbid) {
+		/* clear out any pending delegation return requests */
+		while ((np = TAILQ_FIRST(&nmp->nm_dreturnq))) {
+			TAILQ_REMOVE(&nmp->nm_dreturnq, np, n_dreturn);
+			np->n_dreturn.tqe_next = NFSNOLIST;
+		}
+	}
+
+	/* cancel any renew timer */
+	if ((nmp->nm_vers >= NFS_VER4) && nmp->nm_renew_timer) {
+		thread_call_cancel(nmp->nm_renew_timer);
+		thread_call_free(nmp->nm_renew_timer);
+	}
+
+	lck_mtx_unlock(&nmp->nm_lock);
+
+	if (nmp->nm_state & NFSSTA_MOUNTED)
+		switch (nmp->nm_lockmode) {
+		case NFS_LOCK_MODE_DISABLED:
+		case NFS_LOCK_MODE_LOCAL:
+			break;
+		case NFS_LOCK_MODE_ENABLED:
+		default:
+			if (nmp->nm_vers <= NFS_VER3) {
+				nfs_lockd_mount_unregister(nmp);
+				nmp->nm_lockmode = NFS_LOCK_MODE_DISABLED;
+			}
+			break;
+		}
+
+	if ((nmp->nm_vers >= NFS_VER4) && nmp->nm_longid) {
+		/* remove/deallocate the client ID data */
+		lck_mtx_lock(nfs_global_mutex);
+		TAILQ_REMOVE(&nfsclientids, nmp->nm_longid, nci_link);
+		if (nmp->nm_longid->nci_id)
+			FREE(nmp->nm_longid->nci_id, M_TEMP);
+		FREE(nmp->nm_longid, M_TEMP);
+		lck_mtx_unlock(nfs_global_mutex);
+	}
+
+	/*
+	 * Be sure all requests for this mount are completed
+	 * and removed from the resend queue.
+	 */
+	TAILQ_INIT(&resendq);
+	lck_mtx_lock(nfs_request_mutex);
+	TAILQ_FOREACH(req, &nfs_reqq, r_chain) {
+		if (req->r_nmp == nmp) {
+			lck_mtx_lock(&req->r_mtx);
+			if (!req->r_error && req->r_nmrep.nmc_mhead == NULL)
+				req->r_error = EIO;
+			if (req->r_flags & R_RESENDQ) {
+				lck_mtx_lock(&nmp->nm_lock);
+				req->r_flags &= ~R_RESENDQ;
+				if (req->r_rchain.tqe_next != NFSREQNOLIST) {
+					TAILQ_REMOVE(&nmp->nm_resendq, req, r_rchain);
+					/*
+					 * Queue up the request so that we can unreference them 
+					 * with out holding nfs_request_mutex
+					 */
+					TAILQ_INSERT_TAIL(&resendq, req, r_rchain);
+				}
+				lck_mtx_unlock(&nmp->nm_lock);
+			}
+			wakeup(req);
+			lck_mtx_unlock(&req->r_mtx);
+		}
+	}
+	lck_mtx_unlock(nfs_request_mutex);
+
+	/* Since we've drop the request mutex we can now safely unreference the request */
+	TAILQ_FOREACH_SAFE(req, &resendq, r_rchain, treq) {
+		TAILQ_REMOVE(&resendq, req, r_rchain);
+		nfs_request_rele(req);
+	}
+
+	/*
+	 * Now handle and outstanding async requests. We need to walk the
+	 * request queue again this time with the nfsiod_mutex held. No
+	 * other iods can grab our requests until we've put them on our own
+	 * local iod queue for processing.
+	 */
+	TAILQ_INIT(&iodq);
+	lck_mtx_lock(nfs_request_mutex);
+	lck_mtx_lock(nfsiod_mutex);
+	TAILQ_FOREACH(req, &nfs_reqq, r_chain) {
+		if (req->r_nmp == nmp) {
+			lck_mtx_lock(&req->r_mtx);
+			if (req->r_callback.rcb_func
+			    && !(req->r_flags & R_WAITSENT) && !(req->r_flags & R_IOD)) {
+				/* 
+				 * Since R_IOD is not set then we need to handle it. If
+				 * we're not on a list add it to our iod queue. Otherwise
+				 * we must already be on nm_iodq which is added to our
+				 * local queue below.
+				 * %%% We should really keep a back pointer to our iod queue
+				 * that we're on.
+				 */
+				req->r_flags |= R_IOD;
+				if (req->r_achain.tqe_next == NFSREQNOLIST) {
+					TAILQ_INSERT_TAIL(&iodq, req, r_achain);
+				}
+			}
+			lck_mtx_unlock(&req->r_mtx);
+		}
+	}
+
+	/* finish any async I/O RPCs queued up */
+	if (nmp->nm_iodlink.tqe_next != NFSNOLIST)
+		TAILQ_REMOVE(&nfsiodmounts, nmp, nm_iodlink);
+	TAILQ_CONCAT(&iodq, &nmp->nm_iodq, r_achain);
+	lck_mtx_unlock(nfsiod_mutex);
+	lck_mtx_unlock(nfs_request_mutex);
+
+	TAILQ_FOREACH_SAFE(req, &iodq, r_achain, treq) {
+		TAILQ_REMOVE(&iodq, req, r_achain);
+		req->r_achain.tqe_next = NFSREQNOLIST;
+		lck_mtx_lock(&req->r_mtx);
+		docallback = !(req->r_flags & R_WAITSENT);
+		lck_mtx_unlock(&req->r_mtx);
+		if (docallback)
+			req->r_callback.rcb_func(req);
+	}
+
+	/* clean up common state */
+	lck_mtx_lock(&nmp->nm_lock);
+ 	while ((np = LIST_FIRST(&nmp->nm_monlist))) {
+ 		LIST_REMOVE(np, n_monlink);
+ 		np->n_monlink.le_next = NFSNOLIST;
+ 	}
+	TAILQ_FOREACH_SAFE(noop, &nmp->nm_open_owners, noo_link, nextnoop) {
+		TAILQ_REMOVE(&nmp->nm_open_owners, noop, noo_link);
+		noop->noo_flags &= ~NFS_OPEN_OWNER_LINK;
+		if (noop->noo_refcnt)
+			continue;
+		nfs_open_owner_destroy(noop);
+	}
+	lck_mtx_unlock(&nmp->nm_lock);
+
+	/* clean up NFSv4 state */
+	if (nmp->nm_vers >= NFS_VER4) {
+		lck_mtx_lock(&nmp->nm_lock);
+		while ((np = TAILQ_FIRST(&nmp->nm_delegations))) {
+			TAILQ_REMOVE(&nmp->nm_delegations, np, n_dlink);
+			np->n_dlink.tqe_next = NFSNOLIST;
+		}
+		lck_mtx_unlock(&nmp->nm_lock);
+	}
+
+	nfs_mount_rele(nmp);
+}
+
+/*
+ * cleanup/destroy an nfsmount
+ */
+void
+nfs_mount_cleanup(struct nfsmount *nmp)
+{
+	if (!nmp)
+		return;
+
+	nfs_mount_zombie(nmp, 0);
+
+	NFS_VFS_DBG("Unmounting %s from %s\n",
+		    vfs_statfs(nmp->nm_mountp)->f_mntfromname,
+		    vfs_statfs(nmp->nm_mountp)->f_mntonname);
+	NFS_VFS_DBG("nfs state = %x\n", nmp->nm_state);
+	NFS_VFS_DBG("nfs socket flags = %x\n", nmp->nm_sockflags);
+	NFS_VFS_DBG("nfs mount ref count is %d\n", nmp->nm_ref);
+	NFS_VFS_DBG("mount ref count is %d\n", nmp->nm_mountp->mnt_count);
+	
+	if (nmp->nm_mountp)
+		vfs_setfsprivate(nmp->nm_mountp, NULL);
+
+	lck_mtx_lock(&nmp->nm_lock);
+	if (nmp->nm_ref)
+		panic("Some one has grabbed a ref %d\n", nmp->nm_ref);
+
+	if (nmp->nm_saddr)
+		FREE(nmp->nm_saddr, M_SONAME);
+	if ((nmp->nm_vers < NFS_VER4) && nmp->nm_rqsaddr)
+		FREE(nmp->nm_rqsaddr, M_SONAME);
+
+	if (IS_VALID_CRED(nmp->nm_mcred))
+		kauth_cred_unref(&nmp->nm_mcred);
+
+	nfs_fs_locations_cleanup(&nmp->nm_locations);
+
+	if (nmp->nm_realm)
+		FREE(nmp->nm_realm, M_TEMP);
+	if (nmp->nm_principal)
+		FREE(nmp->nm_principal, M_TEMP);
+	if (nmp->nm_sprinc)
+		FREE(nmp->nm_sprinc, M_TEMP);
+	
+	if (nmp->nm_args)
+		xb_free(nmp->nm_args);
+
+	lck_mtx_unlock(&nmp->nm_lock);
+	
+	lck_mtx_destroy(&nmp->nm_lock, nfs_mount_grp);
+	if (nmp->nm_fh)
+		FREE(nmp->nm_fh, M_TEMP);
+	FREE_ZONE((caddr_t)nmp, sizeof (struct nfsmount), M_NFSMNT);
+}
+
+/*
+ * Return root of a filesystem
+ */
+int
+nfs_vfs_root(mount_t mp, vnode_t *vpp, __unused vfs_context_t ctx)
+{
+	vnode_t vp;
+	struct nfsmount *nmp;
+	int error;
+	u_int32_t vpid;
+
+	nmp = VFSTONFS(mp);
+	if (!nmp || !nmp->nm_dnp)
+		return (ENXIO);
+	vp = NFSTOV(nmp->nm_dnp);
+	vpid = vnode_vid(vp);
+	while ((error = vnode_getwithvid(vp, vpid))) {
+		/* vnode_get() may return ENOENT if the dir changes. */
+		/* If that happens, just try it again, else return the error. */
+		if ((error != ENOENT) || (vnode_vid(vp) == vpid))
+			return (error);
+		vpid = vnode_vid(vp);
+	}
+	*vpp = vp;
+	return (0);
+}
+
+/*
+ * Do operations associated with quotas
+ */
+#if !QUOTA
+int
+nfs_vfs_quotactl(
+	__unused mount_t mp,
+	__unused int cmds,
+	__unused uid_t uid,
+	__unused caddr_t datap,
+	__unused vfs_context_t context)
+{
+	return (ENOTSUP);
+}
+#else
+
+int
+nfs3_getquota(struct nfsmount *nmp, vfs_context_t ctx, uid_t id, int type, struct dqblk *dqb)
+{
+	int error = 0, slen, timeo;
+	int rqport = 0, rqproto, rqvers = (type == GRPQUOTA) ? RPCRQUOTA_EXT_VER : RPCRQUOTA_VER;
+	thread_t thd = vfs_context_thread(ctx);
+	kauth_cred_t cred = vfs_context_ucred(ctx);
+	char *path;
+	uint64_t xid = 0;
+	struct nfsm_chain nmreq, nmrep;
+	mbuf_t mreq;
+	uint32_t val = 0, bsize = 0;
+	struct sockaddr *rqsaddr;
+	struct timeval now;
+
+	if (!nmp->nm_saddr)
+		return (ENXIO);
+
+	if (NMFLAG(nmp, NOQUOTA))
+		return (ENOTSUP);
+
+	if (!nmp->nm_rqsaddr)
+		MALLOC(nmp->nm_rqsaddr, struct sockaddr *, sizeof(struct sockaddr_storage), M_SONAME, M_WAITOK|M_ZERO);
+	if (!nmp->nm_rqsaddr)
+		return (ENOMEM);
+	rqsaddr = nmp->nm_rqsaddr;
+	if (rqsaddr->sa_family == AF_INET6)
+		rqport = ntohs(((struct sockaddr_in6*)rqsaddr)->sin6_port);
+	else if (rqsaddr->sa_family == AF_INET)
+		rqport = ntohs(((struct sockaddr_in*)rqsaddr)->sin_port);
+
+	timeo = NMFLAG(nmp, SOFT) ? 10 : 60;
+	rqproto = IPPROTO_UDP; /* XXX should prefer TCP if mount is TCP */
+
+	/* check if we have a recently cached rquota port */
+	microuptime(&now);
+	if (!rqport || ((nmp->nm_rqsaddrstamp + 60) >= (uint32_t)now.tv_sec)) {
+		/* send portmap request to get rquota port */
+		bcopy(nmp->nm_saddr, rqsaddr, min(sizeof(struct sockaddr_storage), nmp->nm_saddr->sa_len));
+		error = nfs_portmap_lookup(nmp, ctx, rqsaddr, NULL, RPCPROG_RQUOTA, rqvers, rqproto, timeo);
+		if (error)
+			return (error);
+		if (rqsaddr->sa_family == AF_INET6)
+			rqport = ntohs(((struct sockaddr_in6*)rqsaddr)->sin6_port);
+		else if (rqsaddr->sa_family == AF_INET)
+			rqport = ntohs(((struct sockaddr_in*)rqsaddr)->sin_port);
+		else
+			return (EIO);
+		if (!rqport)
+			return (ENOTSUP);
+		microuptime(&now);
+		nmp->nm_rqsaddrstamp = now.tv_sec;
+	}
+
+	/* rquota request */
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+	path = &vfs_statfs(nmp->nm_mountp)->f_mntfromname[0];
+	while (*path && (*path != '/'))
+		path++;
+	slen = strlen(path);
+	nfsm_chain_build_alloc_init(error, &nmreq, 3 * NFSX_UNSIGNED + nfsm_rndup(slen));
+	nfsm_chain_add_name(error, &nmreq, path, slen, nmp);
+	if (type == GRPQUOTA)
+		nfsm_chain_add_32(error, &nmreq, type);
+	nfsm_chain_add_32(error, &nmreq, id);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsmout_if(error);
+	error = nfsm_rpchead2(nmp, (rqproto == IPPROTO_UDP) ? SOCK_DGRAM : SOCK_STREAM,
+			RPCPROG_RQUOTA, rqvers, RPCRQUOTA_GET,
+			RPCAUTH_SYS, cred, NULL, nmreq.nmc_mhead, &xid, &mreq);
+	nfsmout_if(error);
+	nmreq.nmc_mhead = NULL;
+	error = nfs_aux_request(nmp, thd, rqsaddr, NULL,
+			(rqproto == IPPROTO_UDP) ? SOCK_DGRAM : SOCK_STREAM,
+			mreq, R_XID32(xid), 0, timeo, &nmrep);
+	nfsmout_if(error);
+
+	/* parse rquota response */
+	nfsm_chain_get_32(error, &nmrep, val);
+	if (!error && (val != RQUOTA_STAT_OK)) {
+		if (val == RQUOTA_STAT_NOQUOTA)
+			error = ENOENT;
+		else if (val == RQUOTA_STAT_EPERM)
+			error = EPERM;
+		else
+			error = EIO;
+	}
+	nfsm_chain_get_32(error, &nmrep, bsize);
+	nfsm_chain_adv(error, &nmrep, NFSX_UNSIGNED);
+	nfsm_chain_get_32(error, &nmrep, val);
+	nfsmout_if(error);
+	dqb->dqb_bhardlimit = (uint64_t)val * bsize;
+	nfsm_chain_get_32(error, &nmrep, val);
+	nfsmout_if(error);
+	dqb->dqb_bsoftlimit = (uint64_t)val * bsize;
+	nfsm_chain_get_32(error, &nmrep, val);
+	nfsmout_if(error);
+	dqb->dqb_curbytes = (uint64_t)val * bsize;
+	nfsm_chain_get_32(error, &nmrep, dqb->dqb_ihardlimit);
+	nfsm_chain_get_32(error, &nmrep, dqb->dqb_isoftlimit);
+	nfsm_chain_get_32(error, &nmrep, dqb->dqb_curinodes);
+	nfsm_chain_get_32(error, &nmrep, dqb->dqb_btime);
+	nfsm_chain_get_32(error, &nmrep, dqb->dqb_itime);
+	nfsmout_if(error);
+	dqb->dqb_id = id;
+nfsmout:
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	return (error);
+}
+
+int
+nfs4_getquota(struct nfsmount *nmp, vfs_context_t ctx, uid_t id, int type, struct dqblk *dqb)
+{
+	nfsnode_t np;
+	int error = 0, status, nfsvers, numops;
+	u_int64_t xid;
+	struct nfsm_chain nmreq, nmrep;
+	uint32_t bitmap[NFS_ATTR_BITMAP_LEN];
+	thread_t thd = vfs_context_thread(ctx);
+	kauth_cred_t cred = vfs_context_ucred(ctx);
+	struct nfsreq_secinfo_args si;
+
+	if (type != USRQUOTA)  /* NFSv4 only supports user quotas */
+		return (ENOTSUP);
+
+	/* first check that the server supports any of the quota attributes */
+	if (!NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_QUOTA_AVAIL_HARD) &&
+	    !NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_QUOTA_AVAIL_SOFT) &&
+	    !NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_QUOTA_USED))
+		return (ENOTSUP);
+
+	/*
+	 * The credential passed to the server needs to have
+	 * an effective uid that matches the given uid.
+	 */
+	if (id != kauth_cred_getuid(cred)) {
+		struct posix_cred temp_pcred;
+		posix_cred_t pcred = posix_cred_get(cred);
+		bzero(&temp_pcred, sizeof(temp_pcred));
+		temp_pcred.cr_uid = id;
+		temp_pcred.cr_ngroups = pcred->cr_ngroups;
+		bcopy(pcred->cr_groups, temp_pcred.cr_groups, sizeof(temp_pcred.cr_groups));
+		cred = posix_cred_create(&temp_pcred);
+		if (!IS_VALID_CRED(cred))
+			return (ENOMEM);
+	} else {
+		kauth_cred_ref(cred);
+	}
+
+	nfsvers = nmp->nm_vers;
+	np = nmp->nm_dnp;
+	if (!np)
+		error = ENXIO;
+	if (error || ((error = vnode_get(NFSTOV(np))))) {
+		kauth_cred_unref(&cred);
+		return(error);
+	}
+
+	NFSREQ_SECINFO_SET(&si, np, NULL, 0, NULL, 0);
+	nfsm_chain_null(&nmreq);
+	nfsm_chain_null(&nmrep);
+
+	// PUTFH + GETATTR
+	numops = 2;
+	nfsm_chain_build_alloc_init(error, &nmreq, 15 * NFSX_UNSIGNED);
+	nfsm_chain_add_compound_header(error, &nmreq, "quota", nmp->nm_minor_vers, numops);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_PUTFH);
+	nfsm_chain_add_fh(error, &nmreq, nfsvers, np->n_fhp, np->n_fhsize);
+	numops--;
+	nfsm_chain_add_32(error, &nmreq, NFS_OP_GETATTR);
+	NFS_CLEAR_ATTRIBUTES(bitmap);
+	NFS_BITMAP_SET(bitmap, NFS_FATTR_QUOTA_AVAIL_HARD);
+	NFS_BITMAP_SET(bitmap, NFS_FATTR_QUOTA_AVAIL_SOFT);
+	NFS_BITMAP_SET(bitmap, NFS_FATTR_QUOTA_USED);
+	nfsm_chain_add_bitmap_supported(error, &nmreq, bitmap, nmp, NULL);
+	nfsm_chain_build_done(error, &nmreq);
+	nfsm_assert(error, (numops == 0), EPROTO);
+	nfsmout_if(error);
+	error = nfs_request2(np, NULL, &nmreq, NFSPROC4_COMPOUND, thd, cred, &si, 0, &nmrep, &xid, &status);
+	nfsm_chain_skip_tag(error, &nmrep);
+	nfsm_chain_get_32(error, &nmrep, numops);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_PUTFH);
+	nfsm_chain_op_check(error, &nmrep, NFS_OP_GETATTR);
+	nfsm_assert(error, NFSTONMP(np), ENXIO);
+	nfsmout_if(error);
+	error = nfs4_parsefattr(&nmrep, NULL, NULL, NULL, dqb, NULL);
+	nfsmout_if(error);
+	nfsm_assert(error, NFSTONMP(np), ENXIO);
+nfsmout:
+	nfsm_chain_cleanup(&nmreq);
+	nfsm_chain_cleanup(&nmrep);
+	vnode_put(NFSTOV(np));
+	kauth_cred_unref(&cred);
+	return (error);
+}
+
+int
+nfs_vfs_quotactl(mount_t mp, int cmds, uid_t uid, caddr_t datap, vfs_context_t ctx)
+{
+	struct nfsmount *nmp;
+	int cmd, type, error, nfsvers;
+	uid_t euid = kauth_cred_getuid(vfs_context_ucred(ctx));
+	struct dqblk *dqb = (struct dqblk*)datap;
+
+	nmp = VFSTONFS(mp);
+	if (nfs_mount_gone(nmp))
+		return (ENXIO);
+	nfsvers = nmp->nm_vers;
+
+	if (uid == ~0U)
+		uid = euid;
+
+	/* we can only support Q_GETQUOTA */
+	cmd = cmds >> SUBCMDSHIFT;
+	switch (cmd) {
+	case Q_GETQUOTA:
+		break;
+	case Q_QUOTAON:
+	case Q_QUOTAOFF:
+	case Q_SETQUOTA:
+	case Q_SETUSE:
+	case Q_SYNC:
+	case Q_QUOTASTAT:
+		return (ENOTSUP);
+	default:
+		return (EINVAL);
+	}
+
+	type = cmds & SUBCMDMASK;
+	if ((u_int)type >= MAXQUOTAS)
+		return (EINVAL);
+	if ((uid != euid) && ((error = vfs_context_suser(ctx))))
+		return (error);
+
+	if (vfs_busy(mp, LK_NOWAIT))
+		return (0);
+	bzero(dqb, sizeof(*dqb));
+	error = nmp->nm_funcs->nf_getquota(nmp, ctx, uid, type, dqb);
+	vfs_unbusy(mp);
+	return (error);
+}
+#endif
+
+/*
+ * Flush out the buffer cache
+ */
+int nfs_sync_callout(vnode_t, void *);
+
+struct nfs_sync_cargs {
+	vfs_context_t	ctx;
+	int		waitfor;
+	int		error;
+};
+
+int
+nfs_sync_callout(vnode_t vp, void *arg)
+{
+	struct nfs_sync_cargs *cargs = (struct nfs_sync_cargs*)arg;
+	nfsnode_t np = VTONFS(vp);
+	int error;
+
+	if (np->n_flag & NREVOKE) {
+		vn_revoke(vp, REVOKEALL, cargs->ctx);
+		return (VNODE_RETURNED);
+	}
+
+	if (LIST_EMPTY(&np->n_dirtyblkhd))
+		return (VNODE_RETURNED);
+	if (np->n_wrbusy > 0)
+		return (VNODE_RETURNED);
+	if (np->n_bflag & (NBFLUSHINPROG|NBINVALINPROG))
+		return (VNODE_RETURNED);
+
+	error = nfs_flush(np, cargs->waitfor, vfs_context_thread(cargs->ctx), 0);
+	if (error)
+		cargs->error = error;
+
+	return (VNODE_RETURNED);
+}
+
+int
+nfs_vfs_sync(mount_t mp, int waitfor, vfs_context_t ctx)
+{
+	struct nfs_sync_cargs cargs;
+
+	cargs.waitfor = waitfor;
+	cargs.ctx = ctx;
+	cargs.error = 0;
+
+	vnode_iterate(mp, 0, nfs_sync_callout, &cargs);
+
+	return (cargs.error);
+}
+
+/*
+ * NFS flat namespace lookup.
+ * Currently unsupported.
+ */
+/*ARGSUSED*/
+int
+nfs_vfs_vget(
+	__unused mount_t mp,
+	__unused ino64_t ino,
+	__unused vnode_t *vpp,
+	__unused vfs_context_t ctx)
+{
+
+	return (ENOTSUP);
+}
+
+/*
+ * At this point, this should never happen
+ */
+/*ARGSUSED*/
+int
+nfs_vfs_fhtovp(
+	__unused mount_t mp,
+	__unused int fhlen,
+	__unused unsigned char *fhp,
+	__unused vnode_t *vpp,
+	__unused vfs_context_t ctx)
+{
+
+	return (ENOTSUP);
+}
+
+/*
+ * Vnode pointer to File handle, should never happen either
+ */
+/*ARGSUSED*/
+int
+nfs_vfs_vptofh(
+	__unused vnode_t vp,
+	__unused int *fhlenp,
+	__unused unsigned char *fhp,
+	__unused vfs_context_t ctx)
+{
+
+	return (ENOTSUP);
+}
+
+/*
+ * Vfs start routine, a no-op.
+ */
+/*ARGSUSED*/
+int
+nfs_vfs_start(
+	__unused mount_t mp,
+	__unused int flags,
+	__unused vfs_context_t ctx)
+{
+
+	return (0);
+}
+
+/*
+ * Build the mount info buffer for NFS_MOUNTINFO.
+ */
+int
+nfs_mountinfo_assemble(struct nfsmount *nmp, struct xdrbuf *xb)
+{
+	struct xdrbuf xbinfo, xborig;
+	char sotype[6];
+	uint32_t origargsvers, origargslength;
+	uint32_t infolength_offset, curargsopaquelength_offset, curargslength_offset, attrslength_offset, curargs_end_offset, end_offset;
+	uint32_t miattrs[NFS_MIATTR_BITMAP_LEN];
+	uint32_t miflags_mask[NFS_MIFLAG_BITMAP_LEN];
+	uint32_t miflags[NFS_MIFLAG_BITMAP_LEN];
+	uint32_t mattrs[NFS_MATTR_BITMAP_LEN];
+	uint32_t mflags_mask[NFS_MFLAG_BITMAP_LEN];
+	uint32_t mflags[NFS_MFLAG_BITMAP_LEN];
+	uint32_t loc, serv, addr, comp;
+	int i, timeo, error = 0;
+
+	/* set up mount info attr and flag bitmaps */
+	NFS_BITMAP_ZERO(miattrs, NFS_MIATTR_BITMAP_LEN);
+	NFS_BITMAP_SET(miattrs, NFS_MIATTR_FLAGS);
+	NFS_BITMAP_SET(miattrs, NFS_MIATTR_ORIG_ARGS);
+	NFS_BITMAP_SET(miattrs, NFS_MIATTR_CUR_ARGS);
+	NFS_BITMAP_SET(miattrs, NFS_MIATTR_CUR_LOC_INDEX);
+	NFS_BITMAP_ZERO(miflags_mask, NFS_MIFLAG_BITMAP_LEN);
+	NFS_BITMAP_ZERO(miflags, NFS_MIFLAG_BITMAP_LEN);
+	NFS_BITMAP_SET(miflags_mask, NFS_MIFLAG_DEAD);
+	NFS_BITMAP_SET(miflags_mask, NFS_MIFLAG_NOTRESP);
+	NFS_BITMAP_SET(miflags_mask, NFS_MIFLAG_RECOVERY);
+	if (nmp->nm_state & NFSSTA_DEAD)
+		NFS_BITMAP_SET(miflags, NFS_MIFLAG_DEAD);
+	if ((nmp->nm_state & (NFSSTA_TIMEO|NFSSTA_JUKEBOXTIMEO)) ||
+	    ((nmp->nm_state & NFSSTA_LOCKTIMEO) && (nmp->nm_lockmode == NFS_LOCK_MODE_ENABLED)))
+		NFS_BITMAP_SET(miflags, NFS_MIFLAG_NOTRESP);
+	if (nmp->nm_state & NFSSTA_RECOVER)
+		NFS_BITMAP_SET(miflags, NFS_MIFLAG_RECOVERY);
+
+	/* get original mount args length */
+	xb_init_buffer(&xborig, nmp->nm_args, 2*XDRWORD);
+	xb_get_32(error, &xborig, origargsvers); /* version */
+	xb_get_32(error, &xborig, origargslength); /* args length */
+	nfsmerr_if(error);
+
+	/* set up current mount attributes bitmap */
+	NFS_BITMAP_ZERO(mattrs, NFS_MATTR_BITMAP_LEN);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FLAGS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_VERSION);
+	if (nmp->nm_vers >= NFS_VER4)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_MINOR_VERSION);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_READ_SIZE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_WRITE_SIZE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_READDIR_SIZE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_READAHEAD);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_REG_MIN);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_REG_MAX);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MIN);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_ATTRCACHE_DIR_MAX);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_LOCK_MODE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_SECURITY);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_MAX_GROUP_LIST);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_SOCKET_TYPE);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_NFS_PORT);
+	if ((nmp->nm_vers < NFS_VER4) && nmp->nm_mountport)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_MOUNT_PORT);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_REQUEST_TIMEOUT);
+	if (NMFLAG(nmp, SOFT))
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_SOFT_RETRY_COUNT);
+	if (nmp->nm_deadtimeout)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_DEAD_TIMEOUT);
+	if (nmp->nm_fh)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_FH);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_FS_LOCATIONS);
+	NFS_BITMAP_SET(mattrs, NFS_MATTR_MNTFLAGS);
+	if (origargsvers < NFS_ARGSVERSION_XDR)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_MNTFROM);
+	if (nmp->nm_realm)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_REALM);
+	if (nmp->nm_principal)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_PRINCIPAL);
+	if (nmp->nm_sprinc)
+		NFS_BITMAP_SET(mattrs, NFS_MATTR_SVCPRINCIPAL);
+	
+	/* set up current mount flags bitmap */
+	/* first set the flags that we will be setting - either on OR off */
+	NFS_BITMAP_ZERO(mflags_mask, NFS_MFLAG_BITMAP_LEN);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_SOFT);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_INTR);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_RESVPORT);
+	if (nmp->nm_sotype == SOCK_DGRAM)
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NOCONNECT);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_DUMBTIMER);
+	if (nmp->nm_vers < NFS_VER4)
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_CALLUMNT);
+	if (nmp->nm_vers >= NFS_VER3)
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_RDIRPLUS);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NONEGNAMECACHE);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_MUTEJUKEBOX);
+	if (nmp->nm_vers >= NFS_VER4) {
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_EPHEMERAL);
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NOCALLBACK);
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NONAMEDATTR);
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NOACL);
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_ACLONLY);
+	}
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NFC);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_NOQUOTA);
+	if (nmp->nm_vers < NFS_VER4)
+		NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_MNTUDP);
+	NFS_BITMAP_SET(mflags_mask, NFS_MFLAG_MNTQUICK);
+	/* now set the flags that should be set */
+	NFS_BITMAP_ZERO(mflags, NFS_MFLAG_BITMAP_LEN);
+	if (NMFLAG(nmp, SOFT))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_SOFT);
+	if (NMFLAG(nmp, INTR))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_INTR);
+	if (NMFLAG(nmp, RESVPORT))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_RESVPORT);
+	if ((nmp->nm_sotype == SOCK_DGRAM) && NMFLAG(nmp, NOCONNECT))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NOCONNECT);
+	if (NMFLAG(nmp, DUMBTIMER))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_DUMBTIMER);
+	if ((nmp->nm_vers < NFS_VER4) && NMFLAG(nmp, CALLUMNT))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_CALLUMNT);
+	if ((nmp->nm_vers >= NFS_VER3) && NMFLAG(nmp, RDIRPLUS))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_RDIRPLUS);
+	if (NMFLAG(nmp, NONEGNAMECACHE))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NONEGNAMECACHE);
+	if (NMFLAG(nmp, MUTEJUKEBOX))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_MUTEJUKEBOX);
+	if (nmp->nm_vers >= NFS_VER4) {
+		if (NMFLAG(nmp, EPHEMERAL))
+			NFS_BITMAP_SET(mflags, NFS_MFLAG_EPHEMERAL);
+		if (NMFLAG(nmp, NOCALLBACK))
+			NFS_BITMAP_SET(mflags, NFS_MFLAG_NOCALLBACK);
+		if (NMFLAG(nmp, NONAMEDATTR))
+			NFS_BITMAP_SET(mflags, NFS_MFLAG_NONAMEDATTR);
+		if (NMFLAG(nmp, NOACL))
+			NFS_BITMAP_SET(mflags, NFS_MFLAG_NOACL);
+		if (NMFLAG(nmp, ACLONLY))
+			NFS_BITMAP_SET(mflags, NFS_MFLAG_ACLONLY);
+	}
+	if (NMFLAG(nmp, NFC))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NFC);
+	if (NMFLAG(nmp, NOQUOTA) || ((nmp->nm_vers >= NFS_VER4) &&
+	    !NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_QUOTA_AVAIL_HARD) &&
+	    !NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_QUOTA_AVAIL_SOFT) &&
+	    !NFS_BITMAP_ISSET(nmp->nm_fsattr.nfsa_supp_attr, NFS_FATTR_QUOTA_USED)))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_NOQUOTA);
+	if ((nmp->nm_vers < NFS_VER4) && NMFLAG(nmp, MNTUDP))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_MNTUDP);
+	if (NMFLAG(nmp, MNTQUICK))
+		NFS_BITMAP_SET(mflags, NFS_MFLAG_MNTQUICK);
+
+	/* assemble info buffer: */
+	xb_init_buffer(&xbinfo, NULL, 0);
+	xb_add_32(error, &xbinfo, NFS_MOUNT_INFO_VERSION);
+	infolength_offset = xb_offset(&xbinfo);
+	xb_add_32(error, &xbinfo, 0);
+	xb_add_bitmap(error, &xbinfo, miattrs, NFS_MIATTR_BITMAP_LEN);
+	xb_add_bitmap(error, &xbinfo, miflags, NFS_MIFLAG_BITMAP_LEN);
+	xb_add_32(error, &xbinfo, origargslength);
+	if (!error)
+		error = xb_add_bytes(&xbinfo, nmp->nm_args, origargslength, 0);
+
+	/* the opaque byte count for the current mount args values: */
+	curargsopaquelength_offset = xb_offset(&xbinfo);
+	xb_add_32(error, &xbinfo, 0);
+
+	/* Encode current mount args values */
+	xb_add_32(error, &xbinfo, NFS_ARGSVERSION_XDR);
+	curargslength_offset = xb_offset(&xbinfo);
+	xb_add_32(error, &xbinfo, 0);
+	xb_add_32(error, &xbinfo, NFS_XDRARGS_VERSION_0);
+	xb_add_bitmap(error, &xbinfo, mattrs, NFS_MATTR_BITMAP_LEN);
+	attrslength_offset = xb_offset(&xbinfo);
+	xb_add_32(error, &xbinfo, 0);
+	xb_add_bitmap(error, &xbinfo, mflags_mask, NFS_MFLAG_BITMAP_LEN);
+	xb_add_bitmap(error, &xbinfo, mflags, NFS_MFLAG_BITMAP_LEN);
+	xb_add_32(error, &xbinfo, nmp->nm_vers);		/* NFS_VERSION */
+	if (nmp->nm_vers >= NFS_VER4)
+		xb_add_32(error, &xbinfo, nmp->nm_minor_vers);	/* NFS_MINOR_VERSION */
+	xb_add_32(error, &xbinfo, nmp->nm_rsize);		/* READ_SIZE */
+	xb_add_32(error, &xbinfo, nmp->nm_wsize);		/* WRITE_SIZE */
+	xb_add_32(error, &xbinfo, nmp->nm_readdirsize);		/* READDIR_SIZE */
+	xb_add_32(error, &xbinfo, nmp->nm_readahead);		/* READAHEAD */
+	xb_add_32(error, &xbinfo, nmp->nm_acregmin);		/* ATTRCACHE_REG_MIN */
+	xb_add_32(error, &xbinfo, 0);				/* ATTRCACHE_REG_MIN */
+	xb_add_32(error, &xbinfo, nmp->nm_acregmax);		/* ATTRCACHE_REG_MAX */
+	xb_add_32(error, &xbinfo, 0);				/* ATTRCACHE_REG_MAX */
+	xb_add_32(error, &xbinfo, nmp->nm_acdirmin);		/* ATTRCACHE_DIR_MIN */
+	xb_add_32(error, &xbinfo, 0);				/* ATTRCACHE_DIR_MIN */
+	xb_add_32(error, &xbinfo, nmp->nm_acdirmax);		/* ATTRCACHE_DIR_MAX */
+	xb_add_32(error, &xbinfo, 0);				/* ATTRCACHE_DIR_MAX */
+	xb_add_32(error, &xbinfo, nmp->nm_lockmode);		/* LOCK_MODE */
+	if (nmp->nm_sec.count) {
+		xb_add_32(error, &xbinfo, nmp->nm_sec.count);		/* SECURITY */
+		nfsmerr_if(error);
+		for (i=0; i < nmp->nm_sec.count; i++)
+			xb_add_32(error, &xbinfo, nmp->nm_sec.flavors[i]);
+	} else if (nmp->nm_servsec.count) {
+		xb_add_32(error, &xbinfo, nmp->nm_servsec.count);	/* SECURITY */
+		nfsmerr_if(error);
+		for (i=0; i < nmp->nm_servsec.count; i++)
+			xb_add_32(error, &xbinfo, nmp->nm_servsec.flavors[i]);
+	} else {
+		xb_add_32(error, &xbinfo, 1);				/* SECURITY */
+		xb_add_32(error, &xbinfo, nmp->nm_auth);
+	}
+	xb_add_32(error, &xbinfo, nmp->nm_numgrps);		/* MAX_GROUP_LIST */
+	nfsmerr_if(error);
+	snprintf(sotype, sizeof(sotype), "%s%s", (nmp->nm_sotype == SOCK_DGRAM) ? "udp" : "tcp",
+		nmp->nm_sofamily ? (nmp->nm_sofamily == AF_INET) ? "4" : "6" : "");
+	xb_add_string(error, &xbinfo, sotype, strlen(sotype));	/* SOCKET_TYPE */
+	xb_add_32(error, &xbinfo, ntohs(((struct sockaddr_in*)nmp->nm_saddr)->sin_port)); /* NFS_PORT */
+	if ((nmp->nm_vers < NFS_VER4) && nmp->nm_mountport)
+		xb_add_32(error, &xbinfo, nmp->nm_mountport);	/* MOUNT_PORT */
+	timeo = (nmp->nm_timeo * 10) / NFS_HZ;
+	xb_add_32(error, &xbinfo, timeo/10);			/* REQUEST_TIMEOUT */
+	xb_add_32(error, &xbinfo, (timeo%10)*100000000);	/* REQUEST_TIMEOUT */
+	if (NMFLAG(nmp, SOFT))
+		xb_add_32(error, &xbinfo, nmp->nm_retry);	/* SOFT_RETRY_COUNT */
+	if (nmp->nm_deadtimeout) {
+		xb_add_32(error, &xbinfo, nmp->nm_deadtimeout);	/* DEAD_TIMEOUT */
+		xb_add_32(error, &xbinfo, 0);			/* DEAD_TIMEOUT */
+	}
+	if (nmp->nm_fh)
+		xb_add_fh(error, &xbinfo, &nmp->nm_fh->fh_data[0], nmp->nm_fh->fh_len); /* FH */
+	xb_add_32(error, &xbinfo, nmp->nm_locations.nl_numlocs);			/* FS_LOCATIONS */
+	for (loc = 0; !error && (loc < nmp->nm_locations.nl_numlocs); loc++) {
+		xb_add_32(error, &xbinfo, nmp->nm_locations.nl_locations[loc]->nl_servcount);
+		for (serv = 0; !error && (serv < nmp->nm_locations.nl_locations[loc]->nl_servcount); serv++) {
+			xb_add_string(error, &xbinfo, nmp->nm_locations.nl_locations[loc]->nl_servers[serv]->ns_name,
+				strlen(nmp->nm_locations.nl_locations[loc]->nl_servers[serv]->ns_name));
+			xb_add_32(error, &xbinfo, nmp->nm_locations.nl_locations[loc]->nl_servers[serv]->ns_addrcount);
+			for (addr = 0; !error && (addr < nmp->nm_locations.nl_locations[loc]->nl_servers[serv]->ns_addrcount); addr++)
+				xb_add_string(error, &xbinfo, nmp->nm_locations.nl_locations[loc]->nl_servers[serv]->ns_addresses[addr],
+					strlen(nmp->nm_locations.nl_locations[loc]->nl_servers[serv]->ns_addresses[addr]));
+			xb_add_32(error, &xbinfo, 0); /* empty server info */
+		}
+		xb_add_32(error, &xbinfo, nmp->nm_locations.nl_locations[loc]->nl_path.np_compcount);
+		for (comp = 0; !error && (comp < nmp->nm_locations.nl_locations[loc]->nl_path.np_compcount); comp++)
+			xb_add_string(error, &xbinfo, nmp->nm_locations.nl_locations[loc]->nl_path.np_components[comp],
+				strlen(nmp->nm_locations.nl_locations[loc]->nl_path.np_components[comp]));
+		xb_add_32(error, &xbinfo, 0); /* empty fs location info */
+	}
+	xb_add_32(error, &xbinfo, vfs_flags(nmp->nm_mountp));		/* MNTFLAGS */
+	if (origargsvers < NFS_ARGSVERSION_XDR)
+		xb_add_string(error, &xbinfo, vfs_statfs(nmp->nm_mountp)->f_mntfromname,
+			strlen(vfs_statfs(nmp->nm_mountp)->f_mntfromname));	/* MNTFROM */
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_REALM))
+		xb_add_string(error, &xbinfo, nmp->nm_realm, strlen(nmp->nm_realm));
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_PRINCIPAL))
+		xb_add_string(error, &xbinfo, nmp->nm_principal, strlen(nmp->nm_principal));
+	if (NFS_BITMAP_ISSET(mattrs, NFS_MATTR_SVCPRINCIPAL))
+		xb_add_string(error, &xbinfo, nmp->nm_sprinc, strlen(nmp->nm_sprinc));
+
+	curargs_end_offset = xb_offset(&xbinfo);
+
+	/* NFS_MIATTR_CUR_LOC_INDEX */
+	xb_add_32(error, &xbinfo, nmp->nm_locations.nl_current.nli_flags);
+	xb_add_32(error, &xbinfo, nmp->nm_locations.nl_current.nli_loc);
+	xb_add_32(error, &xbinfo, nmp->nm_locations.nl_current.nli_serv);
+	xb_add_32(error, &xbinfo, nmp->nm_locations.nl_current.nli_addr);
+
+	xb_build_done(error, &xbinfo);
+
+	/* update opaque counts */
+	end_offset = xb_offset(&xbinfo);
+	if (!error) {
+		error = xb_seek(&xbinfo, attrslength_offset);
+		xb_add_32(error, &xbinfo, curargs_end_offset - attrslength_offset - XDRWORD/*don't include length field*/);
+	}
+	if (!error) {
+		error = xb_seek(&xbinfo, curargslength_offset);
+		xb_add_32(error, &xbinfo, curargs_end_offset - curargslength_offset + XDRWORD/*version*/);
+	}
+	if (!error) {
+		error = xb_seek(&xbinfo, curargsopaquelength_offset);
+		xb_add_32(error, &xbinfo, curargs_end_offset - curargslength_offset + XDRWORD/*version*/);
+	}
+	if (!error) {
+		error = xb_seek(&xbinfo, infolength_offset);
+		xb_add_32(error, &xbinfo, end_offset - infolength_offset + XDRWORD/*version*/);
+	}
+	nfsmerr_if(error);
+
+	/* copy result xdrbuf to caller */
+	*xb = xbinfo;
+
+	/* and mark the local copy as not needing cleanup */
+	xbinfo.xb_flags &= ~XB_CLEANUP;
+nfsmerr:
+	xb_cleanup(&xbinfo);
+	return (error);
+}
+
+/*
+ * Do that sysctl thang...
+ */
+int
+nfs_vfs_sysctl(int *name, u_int namelen, user_addr_t oldp, size_t *oldlenp,
+           user_addr_t newp, size_t newlen, vfs_context_t ctx)
+{
+	int error = 0, val;
+	int softnobrowse;
+	struct sysctl_req *req = NULL;
+	union union_vfsidctl vc;
+	mount_t mp;
+	struct nfsmount *nmp = NULL;
+	struct vfsquery vq;
+	struct nfsreq *rq;
+	boolean_t is_64_bit;
+	fsid_t fsid;
+	struct xdrbuf xb;
+	struct netfs_status *nsp = NULL;
+	int timeoutmask;
+	uint pos, totlen, count, numThreads;
+#if NFSSERVER
+	struct nfs_exportfs *nxfs;
+	struct nfs_export *nx;
+	struct nfs_active_user_list *ulist;
+	struct nfs_export_stat_desc stat_desc;
+	struct nfs_export_stat_rec statrec;
+	struct nfs_user_stat_node *unode, *unode_next;
+	struct nfs_user_stat_desc ustat_desc;
+	struct nfs_user_stat_user_rec ustat_rec;
+	struct nfs_user_stat_path_rec upath_rec;
+	uint bytes_avail, bytes_total, recs_copied;
+	uint numExports, numRecs;
+#endif /* NFSSERVER */
+
+	/*
+	 * All names at this level are terminal.
+	 */
+	if (namelen > 1)
+		return (ENOTDIR);	/* overloaded */
+
+	is_64_bit = vfs_context_is64bit(ctx);
+
+	/* common code for "new style" VFS_CTL sysctl, get the mount. */
+	switch (name[0]) {
+	case VFS_CTL_TIMEO:
+	case VFS_CTL_NOLOCKS:
+	case VFS_CTL_NSTATUS:
+	case VFS_CTL_QUERY:
+		req = CAST_DOWN(struct sysctl_req *, oldp);
+		if (req == NULL) {
+			return EFAULT;
+		}
+		error = SYSCTL_IN(req, &vc, is_64_bit? sizeof(vc.vc64):sizeof(vc.vc32));
+		if (error)
+			return (error);
+		mp = vfs_getvfs(&vc.vc32.vc_fsid); /* works for 32 and 64 */
+		if (mp == NULL)
+			return (ENOENT);
+		nmp = VFSTONFS(mp);
+		if (!nmp)
+			return (ENOENT);
+		bzero(&vq, sizeof(vq));
+		req->newidx = 0;
+		if (is_64_bit) {
+			req->newptr = vc.vc64.vc_ptr;
+			req->newlen = (size_t)vc.vc64.vc_len;
+		} else {
+			req->newptr = CAST_USER_ADDR_T(vc.vc32.vc_ptr);
+			req->newlen = vc.vc32.vc_len;
+		}
+		break;
+	}
+
+	switch(name[0]) {
+	case NFS_NFSSTATS:
+		if (!oldp) {
+			*oldlenp = sizeof nfsstats;
+			return (0);
+		}
+
+		if (*oldlenp < sizeof nfsstats) {
+			*oldlenp = sizeof nfsstats;
+			return (ENOMEM);
+		}
+
+		error = copyout(&nfsstats, oldp, sizeof nfsstats);
+		if (error)
+			return (error);
+
+		if (newp && newlen != sizeof nfsstats)
+			return (EINVAL);
+
+		if (newp)
+			return copyin(newp, &nfsstats, sizeof nfsstats);
+		return (0);
+	case NFS_MOUNTINFO:
+		/* read in the fsid */
+		if (*oldlenp < sizeof(fsid))
+			return (EINVAL);
+		if ((error = copyin(oldp, &fsid, sizeof(fsid))))
+			return (error);
+		/* swizzle it back to host order */
+		fsid.val[0] = ntohl(fsid.val[0]);
+		fsid.val[1] = ntohl(fsid.val[1]);
+		/* find mount and make sure it's NFS */
+		if (((mp = vfs_getvfs(&fsid))) == NULL)
+			return (ENOENT);
+		if (strcmp(mp->mnt_vfsstat.f_fstypename, "nfs"))
+			return (EINVAL);
+		if (((nmp = VFSTONFS(mp))) == NULL)
+			return (ENOENT);
+		xb_init(&xb, 0);
+		if ((error = nfs_mountinfo_assemble(nmp, &xb)))
+			return (error);
+		if (*oldlenp < xb.xb_u.xb_buffer.xbb_len)
+			error = ENOMEM;
+		else
+			error = copyout(xb_buffer_base(&xb), oldp, xb.xb_u.xb_buffer.xbb_len);
+		*oldlenp = xb.xb_u.xb_buffer.xbb_len;
+		xb_cleanup(&xb);
+		break;
+#if NFSSERVER
+	case NFS_EXPORTSTATS:
+		/* setup export stat descriptor */
+		stat_desc.rec_vers = NFS_EXPORT_STAT_REC_VERSION;
+
+		if (!nfsrv_is_initialized()) {
+			stat_desc.rec_count = 0;
+			if (oldp && (*oldlenp >= sizeof(struct nfs_export_stat_desc)))
+				error = copyout(&stat_desc, oldp, sizeof(struct nfs_export_stat_desc));
+			*oldlenp = sizeof(struct nfs_export_stat_desc);
+			return (error);
+		}
+
+		/* Count the number of exported directories */
+		lck_rw_lock_shared(&nfsrv_export_rwlock);
+		numExports = 0;
+		LIST_FOREACH(nxfs, &nfsrv_exports, nxfs_next)
+			LIST_FOREACH(nx, &nxfs->nxfs_exports, nx_next)
+					numExports += 1;
+
+		/* update stat descriptor's export record count */
+		stat_desc.rec_count = numExports;
+
+		/* calculate total size of required buffer */
+		totlen = sizeof(struct nfs_export_stat_desc) + (numExports * sizeof(struct nfs_export_stat_rec));
+
+		/* Check caller's buffer */
+		if (oldp == 0) {
+			lck_rw_done(&nfsrv_export_rwlock);
+			/* indicate required buffer len */
+			*oldlenp = totlen;
+			return (0);
+		}
+
+		/* We require the caller's buffer to be at least large enough to hold the descriptor */
+		if (*oldlenp < sizeof(struct nfs_export_stat_desc)) {
+			lck_rw_done(&nfsrv_export_rwlock);
+			/* indicate required buffer len */
+			*oldlenp = totlen;
+			return (ENOMEM);
+		}
+
+		/* indicate required buffer len */
+		*oldlenp = totlen;
+
+		/* check if export table is empty */
+		if (!numExports) {
+			lck_rw_done(&nfsrv_export_rwlock);
+			error = copyout(&stat_desc, oldp, sizeof(struct nfs_export_stat_desc));
+			return (error);
+		}
+
+		/* calculate how many actual export stat records fit into caller's buffer */
+		numRecs = (*oldlenp - sizeof(struct nfs_export_stat_desc)) / sizeof(struct nfs_export_stat_rec);
+
+		if (!numRecs) {
+			/* caller's buffer can only accomodate descriptor */
+			lck_rw_done(&nfsrv_export_rwlock);
+			stat_desc.rec_count = 0;
+			error = copyout(&stat_desc, oldp, sizeof(struct nfs_export_stat_desc));
+			return (error);
+		}
+
+		/* adjust to actual number of records to copyout to caller's buffer */
+		if (numRecs > numExports)
+			numRecs = numExports;
+
+		/* set actual number of records we are returning */
+		stat_desc.rec_count = numRecs;
+
+		/* first copy out the stat descriptor */
+		pos = 0;
+		error = copyout(&stat_desc, oldp + pos, sizeof(struct nfs_export_stat_desc));
+		if (error) {
+			lck_rw_done(&nfsrv_export_rwlock);
+			return (error);
+		}
+		pos += sizeof(struct nfs_export_stat_desc);
+
+		/* Loop through exported directories */
+		count = 0;
+		LIST_FOREACH(nxfs, &nfsrv_exports, nxfs_next) {
+			LIST_FOREACH(nx, &nxfs->nxfs_exports, nx_next) {
+
+				if (count >= numRecs)
+					break;
+
+				/* build exported filesystem path */
+				snprintf(statrec.path, sizeof(statrec.path), "%s%s%s",
+					nxfs->nxfs_path, ((nxfs->nxfs_path[1] && nx->nx_path[0]) ? "/" : ""),
+					nx->nx_path);
+
+				/* build the 64-bit export stat counters */
+				statrec.ops = ((uint64_t)nx->nx_stats.ops.hi << 32) |
+						nx->nx_stats.ops.lo;
+				statrec.bytes_read = ((uint64_t)nx->nx_stats.bytes_read.hi << 32) |
+						nx->nx_stats.bytes_read.lo;
+				statrec.bytes_written = ((uint64_t)nx->nx_stats.bytes_written.hi << 32) |
+						nx->nx_stats.bytes_written.lo;
+				error = copyout(&statrec, oldp + pos, sizeof(statrec));
+				if (error) {
+					lck_rw_done(&nfsrv_export_rwlock);
+					return (error);
+				}
+				/* advance buffer position */
+				pos += sizeof(statrec);
+			}
+		}
+		lck_rw_done(&nfsrv_export_rwlock);
+		break;
+	case NFS_USERSTATS:
+		/* init structures used for copying out of kernel */
+		ustat_desc.rec_vers = NFS_USER_STAT_REC_VERSION;
+		ustat_rec.rec_type = NFS_USER_STAT_USER_REC;
+		upath_rec.rec_type = NFS_USER_STAT_PATH_REC;
+
+		/* initialize counters */
+		bytes_total = sizeof(struct nfs_user_stat_desc);
+		bytes_avail  = *oldlenp;
+		recs_copied = 0;
+
+		if (!nfsrv_is_initialized()) /* NFS server not initialized, so no stats */
+			goto ustat_skip;
+
+		/* reclaim old expired user nodes */
+		nfsrv_active_user_list_reclaim();
+
+		/* reserve space for the buffer descriptor */
+		if (bytes_avail >= sizeof(struct nfs_user_stat_desc))
+			bytes_avail -= sizeof(struct nfs_user_stat_desc);
+		else
+			bytes_avail = 0;
+
+		/* put buffer position past the buffer descriptor */
+		pos = sizeof(struct nfs_user_stat_desc);
+
+		/* Loop through exported directories */
+		lck_rw_lock_shared(&nfsrv_export_rwlock);
+		LIST_FOREACH(nxfs, &nfsrv_exports, nxfs_next) {
+			LIST_FOREACH(nx, &nxfs->nxfs_exports, nx_next) {
+				/* copy out path */
+				if (bytes_avail >= sizeof(struct nfs_user_stat_path_rec)) {
+					snprintf(upath_rec.path, sizeof(upath_rec.path), "%s%s%s",
+					    nxfs->nxfs_path, ((nxfs->nxfs_path[1] && nx->nx_path[0]) ? "/" : ""),
+					    nx->nx_path);
+
+					error = copyout(&upath_rec, oldp + pos, sizeof(struct nfs_user_stat_path_rec));
+					if (error) {
+						/* punt */
+						goto ustat_done;
+					}
+
+					pos += sizeof(struct nfs_user_stat_path_rec);
+					bytes_avail -= sizeof(struct nfs_user_stat_path_rec);
+					recs_copied++;
+				}
+				else {
+					/* Caller's buffer is exhausted */
+					bytes_avail = 0;
+				}
+
+				bytes_total += sizeof(struct nfs_user_stat_path_rec);
+
+				/* Scan through all user nodes of this export */
+				ulist = &nx->nx_user_list;
+				lck_mtx_lock(&ulist->user_mutex);
+				for (unode = TAILQ_FIRST(&ulist->user_lru); unode; unode = unode_next) {
+					unode_next = TAILQ_NEXT(unode, lru_link);
+
+					/* copy out node if there is space */
+					if (bytes_avail >= sizeof(struct nfs_user_stat_user_rec)) {
+						/* prepare a user stat rec for copying out */
+						ustat_rec.uid = unode->uid;
+						bcopy(&unode->sock, &ustat_rec.sock, unode->sock.ss_len);
+						ustat_rec.ops = unode->ops;
+						ustat_rec.bytes_read = unode->bytes_read;
+						ustat_rec.bytes_written = unode->bytes_written;
+						ustat_rec.tm_start = unode->tm_start;
+						ustat_rec.tm_last = unode->tm_last;
+
+						error = copyout(&ustat_rec, oldp + pos, sizeof(struct nfs_user_stat_user_rec));
+
+						if (error) {
+							/* punt */
+							lck_mtx_unlock(&ulist->user_mutex);
+							goto ustat_done;
+						}
+
+						pos += sizeof(struct nfs_user_stat_user_rec);
+						bytes_avail -= sizeof(struct nfs_user_stat_user_rec);
+						recs_copied++;
+					}
+					else {
+						/* Caller's buffer is exhausted */
+						bytes_avail = 0;
+					}
+					bytes_total += sizeof(struct nfs_user_stat_user_rec);
+				}
+				/* can unlock this export's list now */
+				lck_mtx_unlock(&ulist->user_mutex);
+			}
+		}
+
+ustat_done:
+		/* unlock the export table */
+		lck_rw_done(&nfsrv_export_rwlock);
+
+ustat_skip:
+		/* indicate number of actual records copied */
+		ustat_desc.rec_count = recs_copied;
+
+		if (!error) {
+			/* check if there was enough room for the buffer descriptor */
+			if (*oldlenp >= sizeof(struct nfs_user_stat_desc))
+				error = copyout(&ustat_desc, oldp, sizeof(struct nfs_user_stat_desc));
+			else
+				error = ENOMEM;
+
+			/* always indicate required buffer size */
+			*oldlenp = bytes_total;
+		}
+		break;
+	case NFS_USERCOUNT:
+		if (!oldp) {
+			*oldlenp = sizeof(nfsrv_user_stat_node_count);
+			return (0);
+		}
+
+		if (*oldlenp < sizeof(nfsrv_user_stat_node_count)) {
+			*oldlenp = sizeof(nfsrv_user_stat_node_count);
+			return (ENOMEM);
+		}
+
+		if (nfsrv_is_initialized()) {
+			/* reclaim old expired user nodes */
+			nfsrv_active_user_list_reclaim();
+		}
+
+		error = copyout(&nfsrv_user_stat_node_count, oldp, sizeof(nfsrv_user_stat_node_count));
+		break;
+#endif /* NFSSERVER */
+	case VFS_CTL_NOLOCKS:
+ 		if (req->oldptr != USER_ADDR_NULL) {
+			lck_mtx_lock(&nmp->nm_lock);
+			val = (nmp->nm_lockmode == NFS_LOCK_MODE_DISABLED) ? 1 : 0;
+			lck_mtx_unlock(&nmp->nm_lock);
+ 			error = SYSCTL_OUT(req, &val, sizeof(val));
+ 			if (error)
+ 				return (error);
+ 		}
+ 		if (req->newptr != USER_ADDR_NULL) {
+ 			error = SYSCTL_IN(req, &val, sizeof(val));
+ 			if (error)
+ 				return (error);
+			lck_mtx_lock(&nmp->nm_lock);
+			if (nmp->nm_lockmode == NFS_LOCK_MODE_LOCAL) {
+				/* can't toggle locks when using local locks */
+				error = EINVAL;
+			} else if ((nmp->nm_vers >= NFS_VER4) && val) {
+				/* can't disable locks for NFSv4 */
+				error = EINVAL;
+			} else if (val) {
+				if ((nmp->nm_vers <= NFS_VER3) && (nmp->nm_lockmode == NFS_LOCK_MODE_ENABLED))
+					nfs_lockd_mount_unregister(nmp);
+				nmp->nm_lockmode = NFS_LOCK_MODE_DISABLED;
+				nmp->nm_state &= ~NFSSTA_LOCKTIMEO;
+			} else {
+				if ((nmp->nm_vers <= NFS_VER3) && (nmp->nm_lockmode == NFS_LOCK_MODE_DISABLED))
+					nfs_lockd_mount_register(nmp);
+				nmp->nm_lockmode = NFS_LOCK_MODE_ENABLED;
+			}
+			lck_mtx_unlock(&nmp->nm_lock);
+ 		}
+		break;
+	case VFS_CTL_QUERY:
+		lck_mtx_lock(&nmp->nm_lock);
+		/* XXX don't allow users to know about/disconnect unresponsive, soft, nobrowse mounts */
+		softnobrowse = (NMFLAG(nmp, SOFT) && (vfs_flags(nmp->nm_mountp) & MNT_DONTBROWSE));
+		if (!softnobrowse && (nmp->nm_state & NFSSTA_TIMEO))
+			vq.vq_flags |= VQ_NOTRESP;
+		if (!softnobrowse && (nmp->nm_state & NFSSTA_JUKEBOXTIMEO) && !NMFLAG(nmp, MUTEJUKEBOX))
+			vq.vq_flags |= VQ_NOTRESP;
+		if (!softnobrowse && (nmp->nm_state & NFSSTA_LOCKTIMEO) &&
+		    (nmp->nm_lockmode == NFS_LOCK_MODE_ENABLED))
+			vq.vq_flags |= VQ_NOTRESP;
+		if (nmp->nm_state & NFSSTA_DEAD)
+			vq.vq_flags |= VQ_DEAD;
+		lck_mtx_unlock(&nmp->nm_lock);
+		error = SYSCTL_OUT(req, &vq, sizeof(vq));
+		break;
+ 	case VFS_CTL_TIMEO:
+ 		if (req->oldptr != USER_ADDR_NULL) {
+			lck_mtx_lock(&nmp->nm_lock);
+			val = nmp->nm_tprintf_initial_delay;
+			lck_mtx_unlock(&nmp->nm_lock);
+ 			error = SYSCTL_OUT(req, &val, sizeof(val));
+ 			if (error)
+ 				return (error);
+ 		}
+ 		if (req->newptr != USER_ADDR_NULL) {
+ 			error = SYSCTL_IN(req, &val, sizeof(val));
+ 			if (error)
+ 				return (error);
+			lck_mtx_lock(&nmp->nm_lock);
+ 			if (val < 0)
+ 				nmp->nm_tprintf_initial_delay = 0;
+			else
+				nmp->nm_tprintf_initial_delay = val;
+			lck_mtx_unlock(&nmp->nm_lock);
+ 		}
+		break;
+	case VFS_CTL_NSTATUS:
+		/*
+		 * Return the status of this mount.  This is much more
+		 * information than VFS_CTL_QUERY.  In addition to the
+		 * vq_flags return the significant mount options along
+		 * with the list of threads blocked on the mount and
+		 * how long the threads have been waiting.
+		 */
+
+		lck_mtx_lock(nfs_request_mutex);
+		lck_mtx_lock(&nmp->nm_lock);
+
+		/*
+		 * Count the number of requests waiting for a reply.
+		 * Note: there could be multiple requests from the same thread.
+		 */
+		numThreads = 0;
+		TAILQ_FOREACH(rq, &nfs_reqq, r_chain) {
+			if (rq->r_nmp == nmp)
+				numThreads++;
+		}
+
+		/* Calculate total size of result buffer */
+		totlen = sizeof(struct netfs_status) + (numThreads * sizeof(uint64_t));
+
+		if (req->oldptr == USER_ADDR_NULL) {		// Caller is querying buffer size
+			lck_mtx_unlock(&nmp->nm_lock);
+			lck_mtx_unlock(nfs_request_mutex);
+			return SYSCTL_OUT(req, NULL, totlen);
+		}
+		if (req->oldlen < totlen) {	// Check if caller's buffer is big enough
+			lck_mtx_unlock(&nmp->nm_lock);
+			lck_mtx_unlock(nfs_request_mutex);
+			return (ERANGE);
+		}
+
+		MALLOC(nsp, struct netfs_status *, totlen, M_TEMP, M_WAITOK|M_ZERO);
+		if (nsp == NULL) {
+			lck_mtx_unlock(&nmp->nm_lock);
+			lck_mtx_unlock(nfs_request_mutex);
+			return (ENOMEM);
+		}
+		timeoutmask = NFSSTA_TIMEO | NFSSTA_LOCKTIMEO | NFSSTA_JUKEBOXTIMEO;
+		if (nmp->nm_state & timeoutmask)
+			nsp->ns_status |= VQ_NOTRESP;
+		if (nmp->nm_state & NFSSTA_DEAD)
+			nsp->ns_status |= VQ_DEAD;
+
+		(void) nfs_mountopts(nmp, nsp->ns_mountopts, sizeof(nsp->ns_mountopts));
+		nsp->ns_threadcount = numThreads;
+		
+		/*
+		 * Get the thread ids of threads waiting for a reply
+		 * and find the longest wait time.
+		 */
+		if (numThreads > 0) {
+			struct timeval now;
+			time_t sendtime;
+
+			microuptime(&now);
+			count = 0;
+			sendtime = now.tv_sec;
+			TAILQ_FOREACH(rq, &nfs_reqq, r_chain) {
+				if (rq->r_nmp == nmp) {
+					if (rq->r_start < sendtime)
+						sendtime = rq->r_start;
+		 			// A thread_id of zero is used to represent an async I/O request.
+					nsp->ns_threadids[count] =
+						rq->r_thread ? thread_tid(rq->r_thread) : 0;
+					if (++count >= numThreads)
+						break;
+				}
+			}
+			nsp->ns_waittime = now.tv_sec - sendtime;
+		}
+
+		lck_mtx_unlock(&nmp->nm_lock);
+		lck_mtx_unlock(nfs_request_mutex);
+
+ 		error = SYSCTL_OUT(req, nsp, totlen);
+		FREE(nsp, M_TEMP);
+		break;
+	default:
+		return (ENOTSUP);
+	}
+	return (error);
+}
diff -Nur xnu-3247.1.106/config/MASTER xnu-3247.1.106-AnV/config/MASTER
--- xnu-3247.1.106/config/MASTER	2015-12-06 01:31:59.000000000 +0100
+++ xnu-3247.1.106-AnV/config/MASTER	2015-12-13 17:08:10.000000000 +0100
@@ -108,7 +108,7 @@
 options		CONTENT_FILTER	#						# <content_filter>
 options 	PACKET_MANGLER	#						# <packet_mangler>
 # secure_kernel - secure kernel from user programs
-options     SECURE_KERNEL       # <secure_kernel> 
+# options     SECURE_KERNEL       # <secure_kernel> 
 
 options     OLD_SEMWAIT_SIGNAL  # old semwait_signal handler
 
diff -Nur xnu-3247.1.106/config/MasterVersion.orig xnu-3247.1.106-AnV/config/MasterVersion.orig
--- xnu-3247.1.106/config/MasterVersion.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/config/MasterVersion.orig	2015-12-06 01:31:59.000000000 +0100
@@ -0,0 +1,19 @@
+15.0.0
+
+# The first line of this file contains the master version number for the kernel.
+# All other instances of the kernel version in xnu are derived from this file.
+#
+# The format of the version number must conform to the version resource format
+# as described in TN1132: http://developer.apple.com/technotes/tn/tn1132.html
+#
+# In particular, the string is formatted as: J[.N[.R[S[L]]]], where:
+#  J represents the kernel major version number (integer)
+#  N represents the kernel minor version number (integer)
+#  R represents the kernel revision number (integer)
+#  S represents the kernel build stage (one of "d", "a", "b", or "r")
+#  L represents the kernel pre-release level (integer)
+#
+# The correct way to make use of the kernel version within kernel code or a
+# kext is to include libkern/verison.h.  version.h contains defines that can
+# be used for build-time version logic and prototypes for variables that can
+# be used for run-time version logic.
diff -Nur xnu-3247.1.106/config/Unsupported.x86_64.exports xnu-3247.1.106-AnV/config/Unsupported.x86_64.exports
--- xnu-3247.1.106/config/Unsupported.x86_64.exports	2015-12-06 01:32:00.000000000 +0100
+++ xnu-3247.1.106-AnV/config/Unsupported.x86_64.exports	2015-12-13 17:08:10.000000000 +0100
@@ -34,6 +34,8 @@
 _rdmsr_carefully
 _real_ncpus
 _rtc_clock_napped
+_rtc_clock_stepped
+_rtc_clock_stepping
 _serial_getc
 _serial_init
 _serial_putc
diff -Nur xnu-3247.1.106/iokit/Drivers/platform/drvAppleIntelClock/AppleIntelClock.h xnu-3247.1.106-AnV/iokit/Drivers/platform/drvAppleIntelClock/AppleIntelClock.h
--- xnu-3247.1.106/iokit/Drivers/platform/drvAppleIntelClock/AppleIntelClock.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/Drivers/platform/drvAppleIntelClock/AppleIntelClock.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 1998-2000 Apple Computer, Inc. All rights reserved.
+ *
+ * @APPLE_LICENSE_HEADER_START@
+ * 
+ * The contents of this file constitute Original Code as defined in and
+ * are subject to the Apple Public Source License Version 1.1 (the
+ * "License").  You may not use this file except in compliance with the
+ * License.  Please obtain a copy of the License at
+ * http://www.apple.com/publicsource and read it before using this file.
+ * 
+ * This Original Code and all software distributed under the License are
+ * distributed on an "AS IS" basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE OR NON-INFRINGEMENT.  Please see the
+ * License for the specific language governing rights and limitations
+ * under the License.
+ * 
+ * @APPLE_LICENSE_HEADER_END@
+ */
+
+#ifndef _APPLEINTELCLOCK_H
+#define _APPLEINTELCLOCK_H
+
+#include <IOKit/IOService.h>
+
+class AppleIntelClock : public IOService
+{
+  OSDeclareDefaultStructors(AppleIntelClock);
+
+public:
+  virtual bool start(IOService * provider);
+};
+
+#endif /* _APPLEINTELCLOCK_H */
diff -Nur xnu-3247.1.106/iokit/Drivers/platform/drvAppleIntelClock/IntelClock.cpp xnu-3247.1.106-AnV/iokit/Drivers/platform/drvAppleIntelClock/IntelClock.cpp
--- xnu-3247.1.106/iokit/Drivers/platform/drvAppleIntelClock/IntelClock.cpp	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/Drivers/platform/drvAppleIntelClock/IntelClock.cpp	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 1998-2000 Apple Computer, Inc. All rights reserved.
+ *
+ * @APPLE_LICENSE_HEADER_START@
+ * 
+ * The contents of this file constitute Original Code as defined in and
+ * are subject to the Apple Public Source License Version 1.1 (the
+ * "License").  You may not use this file except in compliance with the
+ * License.  Please obtain a copy of the License at
+ * http://www.apple.com/publicsource and read it before using this file.
+ * 
+ * This Original Code and all software distributed under the License are
+ * distributed on an "AS IS" basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE OR NON-INFRINGEMENT.  Please see the
+ * License for the specific language governing rights and limitations
+ * under the License.
+ * 
+ * @APPLE_LICENSE_HEADER_END@
+ */
+//
+// Backdoor hack for Intel Clock.
+//
+//
+
+#include "AppleIntelClock.h"
+
+#define super IOService
+OSDefineMetaClassAndStructors(AppleIntelClock, IOService);
+
+bool
+AppleIntelClock::start(IOService *provider)
+{
+	if (!super::start(provider))
+		return false;
+
+	/*
+	 * The clock is already provided by the kernel, so all we need 
+	 * here is publish its availability for any IOKit client to use.
+	 */
+	publishResource("IORTC", this);
+	return true;
+}
diff -Nur xnu-3247.1.106/iokit/IOKit/IOCatalogue.h xnu-3247.1.106-AnV/iokit/IOKit/IOCatalogue.h
--- xnu-3247.1.106/iokit/IOKit/IOCatalogue.h	2015-12-06 01:32:03.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/IOKit/IOCatalogue.h	2015-12-13 17:08:10.000000000 +0100
@@ -240,4 +240,23 @@
 extern const OSSymbol * gIOProbeScoreKey;
 extern IOCatalogue    * gIOCatalogue;
 
+extern "C" {
+/* kaitek: see ::addDrivers() and StartIOKit() for more information about the built-in kernel
+ * kext blacklist. */
+/* AnV - Added configurable blacklist mods */
+    typedef struct {
+        const char *name;
+        uint32_t hits;
+    } blacklist_mod_t;
+    typedef struct {
+        char name[256];
+        uint32_t hits;
+    } blacklist_confmod_t;
+    extern boolean_t blacklistEnabled;
+    extern boolean_t confblacklistEnabled;
+    extern blacklist_mod_t blacklistMods[];
+    extern blacklist_confmod_t confblacklistMods[16];
+    extern uint32_t confblacklistCount;
+};
+
 #endif /* ! _IOKIT_IOCATALOGUE_H */
diff -Nur xnu-3247.1.106/iokit/IOKit/IOService.h xnu-3247.1.106-AnV/iokit/IOKit/IOService.h
--- xnu-3247.1.106/iokit/IOKit/IOService.h	2015-12-06 01:32:05.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/IOKit/IOService.h	2015-12-13 17:08:10.000000000 +0100
@@ -1397,6 +1397,7 @@
     static void actionStop( IOService * client, IOService * provider,
                             void *, void *, void *);
 
+public:
     APPLE_KEXT_COMPATIBILITY_VIRTUAL
         IOReturn resolveInterrupt(IOService *nub, int source);
     APPLE_KEXT_COMPATIBILITY_VIRTUAL
@@ -1407,7 +1408,6 @@
 #endif
 
     /* power management */
-public:
 
 /*! @function PMinit
     @abstract Initializes power management for a driver.
diff -Nur xnu-3247.1.106/iokit/IOKit/IOService.h.orig xnu-3247.1.106-AnV/iokit/IOKit/IOService.h.orig
--- xnu-3247.1.106/iokit/IOKit/IOService.h.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/IOKit/IOService.h.orig	2015-12-06 01:32:05.000000000 +0100
@@ -0,0 +1,1956 @@
+/*
+ * Copyright (c) 1998-2014 Apple Computer, Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * Copyright (c) 1998,1999 Apple Computer, Inc.  All rights reserved. 
+ *
+ * HISTORY
+ *
+ */
+/*!
+    @header
+    This header contains the definition of the IOService class.  IOService is the sole direct subclass of IORegistryEntry and is the base class of almost all I/O Kit family superclasses.  IOService defines methods that support the life cycle of I/O Kit drivers.  For more information on IOService, see {@linkdoc //apple_ref/doc/uid/TP0000011 I/O Kit Fundamentals}.
+
+    @seealso //apple_ref/doc/header/IORegistryEntry.h IORegistryEntry
+*/
+
+#ifndef _IOKIT_IOSERVICE_H
+#define _IOKIT_IOSERVICE_H
+
+#include <IOKit/IORegistryEntry.h>
+#include <IOKit/IOReturn.h>
+#include <IOKit/IODeviceMemory.h>
+#include <IOKit/IONotifier.h>
+#include <IOKit/IOLocks.h>
+
+#include <IOKit/IOKitDebug.h>
+#include <IOKit/IOInterrupts.h>
+
+#include <IOKit/pwr_mgt/IOPMpowerState.h>
+#include <IOKit/IOServicePM.h>
+#include <IOKit/IOReportTypes.h>
+
+extern "C" {
+#include <kern/thread_call.h>
+}
+
+#ifndef UINT64_MAX
+#define UINT64_MAX        18446744073709551615ULL
+#endif
+
+
+enum {
+    kIODefaultProbeScore    = 0
+};
+
+// masks for getState()
+enum {
+    kIOServiceInactiveState = 0x00000001,
+    kIOServiceRegisteredState   = 0x00000002,
+    kIOServiceMatchedState  = 0x00000004,
+    kIOServiceFirstPublishState = 0x00000008,
+    kIOServiceFirstMatchState   = 0x00000010
+};
+
+enum {
+    // options for registerService()
+    kIOServiceExclusive     = 0x00000001,
+
+    // options for terminate()
+    kIOServiceRequired      = 0x00000001,
+    kIOServiceTerminate     = 0x00000004,
+
+    // options for registerService() & terminate()
+    kIOServiceSynchronous   = 0x00000002,
+    // options for registerService()
+    kIOServiceAsynchronous  = 0x00000008
+};
+
+// options for open()
+enum {
+    kIOServiceSeize     = 0x00000001,
+    kIOServiceFamilyOpenOptions = 0xffff0000
+};
+
+// options for close()
+enum {
+    kIOServiceFamilyCloseOptions = 0xffff0000
+};
+
+typedef void * IONotificationRef;
+
+extern const IORegistryPlane *  gIOServicePlane;
+extern const IORegistryPlane *  gIOPowerPlane;
+
+extern const OSSymbol *     gIOResourcesKey;
+extern const OSSymbol *     gIOResourceMatchKey;
+extern const OSSymbol *     gIOProviderClassKey;
+extern const OSSymbol *     gIONameMatchKey;
+extern const OSSymbol *     gIONameMatchedKey;
+extern const OSSymbol *     gIOPropertyMatchKey;
+extern const OSSymbol *     gIOLocationMatchKey;
+extern const OSSymbol *     gIOParentMatchKey;
+extern const OSSymbol *     gIOPathMatchKey;
+extern const OSSymbol *     gIOMatchCategoryKey;
+extern const OSSymbol *     gIODefaultMatchCategoryKey;
+extern const OSSymbol *     gIOMatchedServiceCountKey;
+
+extern const OSSymbol *     gIOUserClientClassKey;
+extern const OSSymbol *     gIOKitDebugKey;
+extern const OSSymbol *     gIOServiceKey;
+
+extern const OSSymbol *     gIOCommandPoolSizeKey;
+
+extern const OSSymbol *     gIOPublishNotification;
+extern const OSSymbol *     gIOFirstPublishNotification;
+extern const OSSymbol *     gIOMatchedNotification;
+extern const OSSymbol *     gIOFirstMatchNotification;
+extern const OSSymbol *     gIOTerminatedNotification;
+
+extern const OSSymbol *     gIOGeneralInterest;
+extern const OSSymbol *     gIOBusyInterest;
+extern const OSSymbol *     gIOOpenInterest;
+extern const OSSymbol *     gIOAppPowerStateInterest;
+extern const OSSymbol *     gIOPriorityPowerStateInterest;
+extern const OSSymbol *     gIOConsoleSecurityInterest;
+
+extern const OSSymbol *     gIODeviceMemoryKey;
+extern const OSSymbol *     gIOInterruptControllersKey;
+extern const OSSymbol *     gIOInterruptSpecifiersKey;
+
+extern SInt32 IOServiceOrdering( const OSMetaClassBase * inObj1, const OSMetaClassBase * inObj2, void * ref );
+
+typedef void (*IOInterruptAction)( OSObject * target, void * refCon,
+                   IOService * nub, int source );
+
+/*! @typedef IOServiceNotificationHandler
+    @param target Reference supplied when the notification was registered.
+    @param refCon Reference constant supplied when the notification was registered.
+    @param newService The IOService object the notification is delivering. It is retained for the duration of the handler's invocation and doesn't need to be released by the handler. */
+
+typedef bool (*IOServiceNotificationHandler)( void * target, void * refCon,
+                                  IOService * newService );
+
+typedef bool (*IOServiceMatchingNotificationHandler)( void * target, void * refCon,
+                                  IOService * newService,
+                                  IONotifier * notifier );
+
+/*! @typedef IOServiceInterestHandler
+    @param target Reference supplied when the notification was registered.
+    @param refCon Reference constant supplied when the notification was registered.
+    @param messageType Type of the message - IOKit defined in IOKit/IOMessage.h or family specific.
+    @param provider The IOService object who is delivering the notification. It is retained for the duration of the handler's invocation and doesn't need to be released by the handler.
+    @param messageArgument An argument for message, dependent on its type.
+    @param argSize Non zero if the argument represents a struct of that size, used when delivering messages outside the kernel. */
+
+typedef IOReturn (*IOServiceInterestHandler)( void * target, void * refCon,
+                                              UInt32 messageType, IOService * provider,
+                                              void * messageArgument, vm_size_t argSize );
+
+typedef void (*IOServiceApplierFunction)(IOService * service, void * context);
+typedef void (*OSObjectApplierFunction)(OSObject * object, void * context);
+
+class IOUserClient;
+class IOPlatformExpert;
+
+/*! @class IOService
+    @abstract The base class for most I/O Kit families, devices, and drivers.
+    @discussion The IOService base class defines APIs used to publish services, instantiate other services based on the existance of a providing service (ie. driver stacking), destroy a service and its dependent stack, notify interested parties of service state changes, and general utility functions useful across all families. 
+
+Types of service are specified with a matching dictionary that describes properties of the service. For example, a matching dictionary might describe any IOUSBDevice (or subclass), an IOUSBDevice with a certain class code, or a IOPCIDevice with a set of matching names or device & vendor IDs. Since the matching dictionary is interpreted by the family which created the service, as well as generically by IOService, the list of properties considered for matching depends on the familiy.
+
+Matching dictionaries are associated with IOService classes by the catalogue, as driver property tables, and also supplied by clients of the notification APIs.
+
+IOService provides matching based on C++ class (via OSMetaClass dynamic casting), registry entry name, a registry path to the service (which includes device tree paths), a name assigned by BSD, or by its location (its point of attachment).
+
+<br><br>Driver Instantiation by IOService<br><br>
+
+Drivers are subclasses of IOService, and their availability is managed through the catalogue. They are instantiated based on the publication of an IOService they use (for example, an IOPCIDevice or IOUSBDevice), or when they are added to  the catalogue and the IOService(s) they use are already available.
+
+When an IOService (the "provider") is published with the @link registerService registerService@/link method, the matching and probing process begins, which is always single threaded per provider. A list of matching dictionaries from the catalog and installed publish notification requests, that successfully match the IOService, is constructed, with ordering supplied by <code>kIOProbeScoreKey</code> ("IOProbeScore") property in the dictionary, or supplied with the notification. 
+
+Each entry in the list is then processed in order - for notifications, the notification is delivered, for driver property tables a lot more happens.
+
+The driver class is instantiated and <code>init()</code> called with its property table. The new driver instance is then attached to the provider, and has its @link probe probe@/link method called with the provider as an argument. The default <code>probe</code> method does nothing but return success, but a driver may implement this method to interrogate the provider to make sure it can work with it. It may also modify its probe score at this time. After probe, the driver is detached and the next in the list is considered (ie. attached, probed, and detached).
+
+When the probing phase is complete, the list consists of successfully probed drivers, in order of their probe score (after adjustment during the @link probe probe@/link call). The list is then divided into categories based on the <code>kIOMatchCategoryKey</code> property ("IOMatchCategory"); drivers without a match category are all considered in one default category. Match categories allow multiple clients of a provider to be attached and started, though the provider may also enforce open/close semantics to gain active access to it.
+
+For each category, the highest scoring driver in that category is attached to the provider, and its @link start start@/link method called. If <code>start</code> is successful, the rest of the drivers in the same match category are discarded, otherwise the next highest scoring driver is started, and so on.
+
+The driver should only consider itself in action when the start method is called, meaning it has been selected for use on the provider, and consuming that particular match category. It should also be prepared to be allocated, probed and freed even if the probe was successful.
+
+After the drivers have all synchronously been started, the installed "matched" notifications that match the registered IOService are delivered.
+
+<br><br>Properties used by IOService<br><br>
+
+    <code>kIOClassKey, extern const OSSymbol * gIOClassKey, "IOClass"</code>
+<br>
+<br>
+Class of the driver to instantiate on matching providers.
+<br>
+<br>
+    <code>kIOProviderClassKey, extern const OSSymbol * gIOProviderClassKey, "IOProviderClass"</code>
+<br>
+<br>
+Class of the provider(s) to be considered for matching, checked with OSDynamicCast so subclasses will also match.
+<br>
+<br>
+    <code>kIOProbeScoreKey, extern const OSSymbol * gIOProbeScoreKey, "IOProbeScore"</code>
+<br>
+<br>
+The probe score initially used to order multiple matching drivers.
+<br>
+<br>
+    <code>kIOMatchCategoryKey, extern const OSSymbol * gIOMatchCategoryKey, "IOMatchCategory"</code>
+<br>
+<br>
+A string defining the driver category for matching purposes. All drivers with no <code>IOMatchCategory</code> property are considered to be in the same default category. Only one driver in a category can be started on each provider.
+<br>
+<br>
+    <code>kIONameMatchKey, extern const OSSymbol * gIONameMatchKey, "IONameMatch"</code>
+<br>
+A string or collection of strings that match the provider's name. The comparison is implemented with the @link //apple_ref/cpp/instm/IORegistryEntry/compareNames/virtualbool/(OSObject*,OSString**) IORegistryEntry::compareNames@/link method, which supports a single string, or any collection (OSArray, OSSet, OSDictionary etc.) of strings. IOService objects with device tree properties (eg. IOPCIDevice) will also be matched based on that standard's "compatible", "name", "device_type" properties. The matching name will be left in the driver's property table in the <code>kIONameMatchedKey</code> property.
+<br>
+Examples
+<pre>
+@textblock
+    <key>IONameMatch</key>
+    <string>pci106b,7</string>
+@/textblock
+</pre>
+
+For a list of possible matching names, a serialized array of strings should used, eg.
+<pre>
+@textblock
+    <key>IONameMatch</key>
+    <array>
+        <string>APPL,happy16</string>
+        <string>pci106b,7</string>
+    </array>
+@/textblock
+</pre>
+
+<br>
+    <code>kIONameMatchedKey, extern const OSSymbol * gIONameMatchedKey, "IONameMatched"</code>
+<br>
+The name successfully matched name from the <code>kIONameMatchKey</code> property will be left in the driver's property table as the <code>kIONameMatchedKey</code> property.
+<br>
+<br>
+    <code>kIOPropertyMatchKey, extern const OSSymbol * gIOPropertyMatchKey, "IOPropertyMatch"</code>
+<br>
+A dictionary of properties that each must exist in the matching IOService and compare successfully with the <code>isEqualTo</code> method.
+
+<pre>
+@textblock
+    <key>IOPropertyMatch</key>
+    <dictionary>
+        <key>APPL,happy16</key>
+        <string>APPL,meek8</string>
+    </dictionary>
+@/textblock
+</pre>
+
+<br>
+    <code>kIOUserClientClassKey, extern const OSSymbol * gIOUserClientClassKey, "IOUserClientClass"</code>
+<br>
+The class name that the service will attempt to allocate when a user client connection is requested.  First the device nub is queried, then the nub's provider is queried by default.
+<br>
+<br>
+    <code>kIOKitDebugKey, extern const OSSymbol * gIOKitDebugKey, "IOKitDebug"</code>
+<br>
+Set some debug flags for logging the driver loading process. Flags are defined in <code>IOKit/IOKitDebug.h</code>, but <code>65535</code> works well.*/
+
+struct IOInterruptAccountingData;
+struct IOInterruptAccountingReporter;
+
+class IOService : public IORegistryEntry
+{
+    OSDeclareDefaultStructors(IOService)
+
+protected:
+/*! @struct ExpansionData
+    @discussion This structure will be used to expand the capablilties of this class in the future.
+    */    
+    struct ExpansionData {
+        uint64_t authorizationID;
+        /*
+         * Variables associated with interrupt accounting.  Consists of an array
+         * (that pairs reporters with opaque "statistics" objects), the count for
+         * the array, and a lock to guard both of the former variables.  The lock
+         * is necessary as IOReporting will not update reports in a manner that is
+         * synchonized with the service (i.e, on a workloop).
+         */
+        IOLock * interruptStatisticsLock;
+        IOInterruptAccountingReporter * interruptStatisticsArray;
+        int interruptStatisticsArrayCount;
+    };
+
+/*! @var reserved
+    Reserved for future use.  (Internal use only)  */
+    ExpansionData * reserved;
+
+private:
+    IOService *     __provider;
+    SInt32      __providerGeneration;
+    IOService *     __owner;
+    IOOptionBits    __state[2];
+    uint64_t        __timeBusy;
+    uint64_t        __accumBusy;
+    IOServicePM *   pwrMgt;
+
+protected:
+    // TRUE once PMinit has been called
+    bool            initialized;
+
+public:
+    // DEPRECATED
+    void *          pm_vars;
+
+public:
+    /* methods available in Mac OS X 10.1 or later */
+/*! @function requestTerminate
+    @abstract Passes a termination up the stack.
+    @discussion When an IOService is made inactive the default behavior is to also make any of its clients that have it as their only provider also inactive, in this way recursing the termination up the driver stack. This method allows an IOService object to override this behavior. Returning <code>true</code> from this method when passed a just terminated provider will cause the client to also be terminated.
+    @param provider The terminated provider of this object.
+    @param options Options originally passed to terminate, plus <code>kIOServiceRecursing</code>.
+    @result <code>true</code> if this object should be terminated now that its provider has been. */
+
+    virtual bool requestTerminate( IOService * provider, IOOptionBits options );
+
+/*! @function willTerminate
+    @abstract Passes a termination up the stack.
+    @discussion Notification that a provider has been terminated, sent before recursing up the stack, in root-to-leaf order.
+    @param provider The terminated provider of this object.
+    @param options Options originally passed to terminate.
+    @result <code>true</code>. */
+
+    virtual bool willTerminate( IOService * provider, IOOptionBits options );
+
+/*! @function didTerminate
+    @abstract Passes a termination up the stack.
+    @discussion Notification that a provider has been terminated, sent after recursing up the stack, in leaf-to-root order.
+    @param provider The terminated provider of this object.
+    @param options Options originally passed to terminate.
+    @param defer If there is pending I/O that requires this object to persist, and the provider is not opened by this object set <code>defer</code> to <code>true</code> and call the <code>IOService::didTerminate()</code> implementation when the I/O completes. Otherwise, leave <code>defer</code> set to its default value of <code>false</code>.
+    @result <code>true</code>. */
+
+    virtual bool didTerminate( IOService * provider, IOOptionBits options, bool * defer );
+
+/*! @function nextIdleTimeout
+    @availability Mac OS X v10.4 and later
+    @abstract Allows subclasses to customize idle power management behavior.
+    @discussion Returns the next time that the device should idle into its next lower power state. Subclasses may override for custom idle behavior.
+    
+    A power managed driver might override this method to provide a more sophisticated idle power off algorithm than the one defined by power management.
+    @param currentTime The current time
+    @param lastActivity The time of last activity on this device
+    @param powerState The device's current power state.
+    @result Returns the next time the device should idle off (in seconds, relative to the current time). */
+
+    virtual SInt32 nextIdleTimeout(AbsoluteTime currentTime, 
+        AbsoluteTime lastActivity, unsigned int powerState);
+
+/*! @function systemWillShutdown
+    @availability Mac OS X v10.5 and later
+    @abstract Notifies members of the power plane of system shutdown and restart.
+    @discussion This function is called for all members of the power plane in leaf-to-root order. If a subclass needs to wait for a pending I/O, then the call to <code>systemWillShutdown</code> should be postponed until the I/O completes.
+    
+    Any power managed driver (which has called @link joinPMtree joinPMtree@/link to join the power plane) interested in taking action at system shutdown or restart should override this method.
+    @param specifier <code>kIOMessageSystemWillPowerOff</code> or <code>kIOMessageSystemWillRestart</code>. */
+
+    virtual void systemWillShutdown( IOOptionBits specifier );
+
+/*! @function copyClientWithCategory
+    @availability Mac OS X v10.6 and later
+    @param category An OSSymbol corresponding to an IOMatchCategory matching property.
+    @result Returns a reference to the IOService child with the given category. The result should be released by the caller.
+*/
+
+    virtual IOService * copyClientWithCategory( const OSSymbol * category );
+
+public:
+/*! @function       configureReport
+    @abstract       configure IOReporting channels
+    @availability   SPI on OS X v10.9 / iOS 7 and later
+
+    @param  channels    - channels to configure
+    @param  action      - enable/disable/size, etc
+    @param  result      - action-specific returned value
+    @param  destination - action-specific default destination
+*/
+virtual IOReturn configureReport(IOReportChannelList   *channels,
+                                 IOReportConfigureAction action,
+                                 void                  *result,
+                                 void                  *destination);
+
+/*! @function       updateReport
+    @abstract       request current data for the specified channels
+    @availability   SPI on OS X 10.9 / iOS 7 and later
+
+    @param channels     - channels to be updated
+    @param action       - type/style of update
+    @param result       - returned details about what was updated
+    @param destination  - destination for this update (action-specific)
+*/
+virtual IOReturn updateReport(IOReportChannelList      *channels,
+                              IOReportUpdateAction      action,
+                              void                     *result,
+                              void                     *destination);
+
+private:
+#if __LP64__
+    OSMetaClassDeclareReservedUsed(IOService, 0);
+    OSMetaClassDeclareReservedUsed(IOService, 1);
+    OSMetaClassDeclareReservedUnused(IOService, 2);
+    OSMetaClassDeclareReservedUnused(IOService, 3);
+    OSMetaClassDeclareReservedUnused(IOService, 4);
+    OSMetaClassDeclareReservedUnused(IOService, 5);
+    OSMetaClassDeclareReservedUnused(IOService, 6);
+    OSMetaClassDeclareReservedUnused(IOService, 7);
+#else
+    OSMetaClassDeclareReservedUsed(IOService, 0);
+    OSMetaClassDeclareReservedUsed(IOService, 1);
+    OSMetaClassDeclareReservedUsed(IOService, 2);
+    OSMetaClassDeclareReservedUsed(IOService, 3);
+    OSMetaClassDeclareReservedUsed(IOService, 4);
+    OSMetaClassDeclareReservedUsed(IOService, 5);
+    OSMetaClassDeclareReservedUsed(IOService, 6);
+    OSMetaClassDeclareReservedUsed(IOService, 7);
+#endif
+
+    OSMetaClassDeclareReservedUnused(IOService, 8);
+    OSMetaClassDeclareReservedUnused(IOService, 9);
+    OSMetaClassDeclareReservedUnused(IOService, 10);
+    OSMetaClassDeclareReservedUnused(IOService, 11);
+    OSMetaClassDeclareReservedUnused(IOService, 12);
+    OSMetaClassDeclareReservedUnused(IOService, 13);
+    OSMetaClassDeclareReservedUnused(IOService, 14);
+    OSMetaClassDeclareReservedUnused(IOService, 15);
+    OSMetaClassDeclareReservedUnused(IOService, 16);
+    OSMetaClassDeclareReservedUnused(IOService, 17);
+    OSMetaClassDeclareReservedUnused(IOService, 18);
+    OSMetaClassDeclareReservedUnused(IOService, 19);
+    OSMetaClassDeclareReservedUnused(IOService, 20);
+    OSMetaClassDeclareReservedUnused(IOService, 21);
+    OSMetaClassDeclareReservedUnused(IOService, 22);
+    OSMetaClassDeclareReservedUnused(IOService, 23);
+    OSMetaClassDeclareReservedUnused(IOService, 24);
+    OSMetaClassDeclareReservedUnused(IOService, 25);
+    OSMetaClassDeclareReservedUnused(IOService, 26);
+    OSMetaClassDeclareReservedUnused(IOService, 27);
+    OSMetaClassDeclareReservedUnused(IOService, 28);
+    OSMetaClassDeclareReservedUnused(IOService, 29);
+    OSMetaClassDeclareReservedUnused(IOService, 30);
+    OSMetaClassDeclareReservedUnused(IOService, 31);
+    OSMetaClassDeclareReservedUnused(IOService, 32);
+    OSMetaClassDeclareReservedUnused(IOService, 33);
+    OSMetaClassDeclareReservedUnused(IOService, 34);
+    OSMetaClassDeclareReservedUnused(IOService, 35);
+    OSMetaClassDeclareReservedUnused(IOService, 36);
+    OSMetaClassDeclareReservedUnused(IOService, 37);
+    OSMetaClassDeclareReservedUnused(IOService, 38);
+    OSMetaClassDeclareReservedUnused(IOService, 39);
+    OSMetaClassDeclareReservedUnused(IOService, 40);
+    OSMetaClassDeclareReservedUnused(IOService, 41);
+    OSMetaClassDeclareReservedUnused(IOService, 42);
+    OSMetaClassDeclareReservedUnused(IOService, 43);
+    OSMetaClassDeclareReservedUnused(IOService, 44);
+    OSMetaClassDeclareReservedUnused(IOService, 45);
+    OSMetaClassDeclareReservedUnused(IOService, 46);
+    OSMetaClassDeclareReservedUnused(IOService, 47);
+
+public:
+/*! @function getState
+    @abstract Accessor for IOService state bits, not normally needed or used outside IOService.
+    @result State bits for the IOService, eg. <code>kIOServiceInactiveState</code>, <code>kIOServiceRegisteredState</code>. */
+
+    virtual IOOptionBits getState( void ) const;
+
+/*! @function isInactive
+    @abstract Checks if the IOService object has been terminated, and is in the process of being destroyed.
+    @discussion When an IOService object is successfully terminated, it is immediately made inactive, which blocks further attach()es, matching or notifications occuring on the object. It remains inactive until the last client closes, and is then finalized and destroyed.
+    @result <code>true</code> if the IOService object has been terminated. */
+
+    bool isInactive( void ) const;
+
+    /* Stack creation */
+
+/*! @function registerService
+    @abstract Starts the registration process for a newly discovered IOService object.
+    @discussion This function allows an IOService subclass to be published and made available to possible clients, by starting the registration process and delivering notifications to registered clients. The object should be completely setup and ready to field requests from clients before <code>registerService</code> is called.
+    @param options The default zero options mask is recommended and should be used in most cases. The registration process is usually asynchronous, with possible driver probing and notification occurring some time later. <code>kIOServiceSynchronous</code> may be passed to carry out the matching and notification process for currently registered clients before returning to the caller. */
+
+    virtual void registerService( IOOptionBits options = 0 );
+
+/*! @function probe
+    @abstract During an IOService object's instantiation, probes a matched service to see if it can be used.
+    @discussion The registration process for an IOService object (the provider) includes instantiating possible driver clients. The <code>probe</code> method is called in the client instance to check the matched service can be used before the driver is considered to be started. Since matching screens many possible providers, in many cases the <code>probe</code> method can be left unimplemented by IOService subclasses. The client is already attached to the provider when <code>probe</code> is called.
+    @param provider The registered IOService object that matches a driver personality's matching dictionary.
+    @param score Pointer to the current driver's probe score, which is used to order multiple matching drivers in the same match category. It defaults to the value of the <code>IOProbeScore</code> property in the drivers property table, or <code>kIODefaultProbeScore</code> if none is specified. The <code>probe</code> method may alter the score to affect start order.
+    @result An IOService instance or zero when the probe is unsuccessful. In almost all cases the value of <code>this</code> is returned on success. If another IOService object is returned, the probed instance is detached and freed, and the returned instance is used in its stead for <code>start</code>. */
+    
+    virtual IOService * probe(  IOService *     provider,
+                                SInt32    *     score );
+
+/*! @function start
+    @abstract During an IOService object's instantiation, starts the IOService object that has been selected to run on the provider.
+    @discussion The <code>start</code> method of an IOService instance is called by its provider when it has been selected (due to its probe score and match category) as the winning client. The client is already attached to the provider when <code>start</code> is called.<br>Implementations of <code>start</code> must call <code>start</code> on their superclass at an appropriate point. If an implementation of <code>start</code> has already called <code>super::start</code> but subsequently determines that it will fail, it must call <code>super::stop</code> to balance the prior call to <code>super::start</code> and prevent reference leaks.
+    @result <code>true</code> if the start was successful; <code>false</code> otherwise (which will cause the instance to be detached and usually freed). */
+    
+    virtual bool start( IOService * provider );
+    
+/*! @function stop
+    @abstract During an IOService termination, the stop method is called in its clients before they are detached & it is destroyed.
+    @discussion The termination process for an IOService (the provider) will call stop in each of its clients, after they have closed the provider if they had it open, or immediately on termination. */
+
+    virtual void stop( IOService * provider );
+
+    /* Open / Close */
+
+/*! @function open
+    @abstract Requests active access to a provider.
+    @discussion IOService provides generic open and close semantics to track clients of a provider that have established an active datapath. The use of <code>open</code> and @link close close@/link, and rules regarding ownership are family defined, and defined by the @link handleOpen handleOpen@/link and @link handleClose handleClose@/link methods in the provider. Some families will limit access to a provider based on its open state.
+    @param forClient Designates the client of the provider requesting the open.
+    @param options Options for the open. The provider family may implement options for open; IOService defines only <code>kIOServiceSeize</code> to request the device be withdrawn from its current owner.
+    @result <code>true</code> if the open was successful; <code>false</code> otherwise. */
+
+    virtual bool open(   IOService *       forClient,
+                         IOOptionBits      options = 0,
+                         void *        arg = 0 );
+
+/*! @function close
+    @abstract Releases active access to a provider.
+    @discussion IOService provides generic open and close semantics to track clients of a provider that have established an active datapath. The use of @link open open@/link and <code>close</code>, and rules regarding ownership are family defined, and defined by the @link handleOpen handleOpen@/link and @link handleClose handleClose@/link methods in the provider.
+    @param forClient Designates the client of the provider requesting the close.
+    @param options Options available for the close. The provider family may implement options for close; IOService defines none.
+    @param arg Family specific arguments which are ignored by IOService. */
+    
+    virtual void close(  IOService *       forClient,
+                         IOOptionBits      options = 0 );
+                         
+/*! @function isOpen
+    @abstract Determines whether a specific, or any, client has an IOService object open.
+    @discussion Returns the open state of an IOService object with respect to the specified client, or when it is open by any client.
+    @param forClient If non-zero, <codeisOpen</code returns the open state for that client. If zero is passed, <codeisOpen</code returns the open state for all clients.
+    @result <codetrue</code if the specific, or any, client has the IOService object open. */
+
+    virtual bool isOpen( const IOService * forClient = 0 ) const;
+
+/*! @function handleOpen
+    @abstract Controls the open / close behavior of an IOService object (overrideable by subclasses).
+    @discussion IOService calls this method in its subclasses in response to the @link open open@/link method, so the subclass may implement the request. The default implementation provides single owner access to an IOService object via <code>open</code>. The object is locked via @link lockForArbitration lockForArbitration@/link before <code>handleOpen</code> is called.
+    @param forClient Designates the client of the provider requesting the open.
+    @param options Options for the open, may be interpreted by the implementor of <code>handleOpen</code>.
+    @result <code>true</code>if the open was successful; <code>false</code> otherwise. */
+
+    virtual bool handleOpen(    IOService *   forClient,
+                                IOOptionBits      options,
+                                void *        arg );
+                                
+/*! @function handleClose
+    @abstract Controls the open / close behavior of an IOService object (overrideable by subclasses).
+    @discussion IOService calls this method in its subclasses in response to the @link close close@/link method, so the subclass may implement the request. The default implementation provides single owner access to an IOService object via @link open open@/link. The object is locked via @link lockForArbitration lockForArbitration@/link before <code>handleClose</code> is called.
+    @param forClient Designates the client of the provider requesting the close.
+    @param options Options for the close, may be interpreted by the implementor of @link handleOpen handleOpen@/link. */
+
+    virtual void handleClose(   IOService *       forClient,
+                                IOOptionBits      options );
+                                
+/*! @function handleIsOpen
+    @abstract Controls the open / close behavior of an IOService object (overrideable by subclasses).
+    @discussion IOService calls this method in its subclasses in response to the @link open open@/link method, so the subclass may implement the request. The default implementation provides single owner access to an IOService object via @link open open@/link. The object is locked via @link lockForArbitration lockForArbitration@/link before <code>handleIsOpen</code> is called.
+    @param forClient If non-zero, <code>isOpen</code> returns the open state for that client. If zero is passed, <code>isOpen</code> returns the open state for all clients.
+    @result <code>true</code> if the specific, or any, client has the IOService object open. */
+
+    virtual bool handleIsOpen(  const IOService * forClient ) const;
+
+    /* Stacking change */
+
+/*! @function terminate
+    @abstract Makes an IOService object inactive and begins its destruction.
+    @discussion Registering an IOService object informs possible clients of its existance and instantiates drivers that may be used with it; <code>terminate</code> involves the opposite process of informing clients that an IOService object is no longer able to be used and will be destroyed. By default, if any client has the service open, <code>terminate</code> fails. If the <code>kIOServiceRequired</code> flag is passed however, <code>terminate</code> will be successful though further progress in the destruction of the IOService object will not proceed until the last client has closed it. The service will be made inactive immediately upon successful termination, and all its clients will be notified via their @link message message@/link method with a message of type <code>kIOMessageServiceIsTerminated</code>. Both these actions take place on the caller's thread. After the IOService object is made inactive, further matching or attach calls will fail on it. Each client has its @link stop stop@/link method called upon their close of an inactive IOService object , or on its termination if they do not have it open. After <code>stop</code>, @link detach detach@/link is called in each client. When all clients have been detached, the @link finalize finalize@/link method is called in the inactive service. The termination process is inherently asynchronous because it will be deferred until all clients have chosen to close.
+    @param options In most cases no options are needed. <code>kIOServiceSynchronous</code> may be passed to cause <code>terminate</code> to not return until the service is finalized. */
+
+    virtual bool terminate( IOOptionBits options = 0 );
+
+/*! @function finalize
+    @abstract Finalizes the destruction of an IOService object.
+    @discussion The <code>finalize</code> method is called in an inactive (ie. terminated) IOService object after the last client has detached. IOService's implementation will call @link stop stop@/link, @link close close@/link, and @link detach detach@/link on each provider. When <code>finalize</code> returns, the object's retain count will have no references generated by IOService's registration process.
+    @param options The options passed to the @link terminate terminate@/link method of the IOService object are passed on to <code>finalize</code>.
+    @result <code>true</code>. */
+    
+    virtual bool finalize( IOOptionBits options );
+
+/*! @function init
+    @abstract Initializes generic IOService data structures (expansion data, etc). */
+    virtual bool init( OSDictionary * dictionary = 0 ) APPLE_KEXT_OVERRIDE;
+
+/*! @function init
+    @abstract Initializes generic IOService data structures (expansion data, etc). */
+    virtual bool init( IORegistryEntry * from,
+                       const IORegistryPlane * inPlane ) APPLE_KEXT_OVERRIDE;
+
+/*! @function free
+    @abstract Frees data structures that were allocated when power management was initialized on this service. */
+    
+    virtual void free( void ) APPLE_KEXT_OVERRIDE;
+
+/*! @function lockForArbitration
+    @abstract Locks an IOService object against changes in state or ownership.
+    @discussion The registration, termination and open / close functions of IOService use <code>lockForArbtration</code> to single-thread access to an IOService object. <code>lockForArbitration</code> grants recursive access to the same thread.
+    @param isSuccessRequired If a request for access to an IOService object should be denied if it is terminated, pass <code>false</code>, otherwise pass <code>true</code>. */
+    
+    virtual bool lockForArbitration( bool isSuccessRequired = true );
+    
+/*! @function unlockForArbitration
+    @abstract Unlocks an IOService obkect after a successful @link lockForArbitration lockForArbitration@/link.
+    @discussion A thread granted exclusive access to an IOService object should release it with <code>unlockForArbitration</code>. */
+    
+    virtual void unlockForArbitration( void );
+
+/*! @function terminateClient
+    @abstract Passes a termination up the stack.
+    @discussion When an IOService object is made inactive the default behavior is to also make any of its clients that have it as their only provider inactive, in this way recursing the termination up the driver stack. This method allows a terminated  IOService object to override this behavior. Note the client may also override this behavior by overriding its @link terminate terminate@/link method.
+    @param client The client of the terminated provider.
+    @param options Options originally passed to @link terminate terminate@/link, plus <code>kIOServiceRecursing</code>.
+    @result result of the terminate request on the client. */
+
+    virtual bool terminateClient( IOService * client, IOOptionBits options );
+
+    /* Busy state indicates discovery, matching or termination is in progress */
+
+/*! @function getBusyState
+    @abstract Returns the <code>busyState</code> of an IOService object.
+    @discussion Many activities in IOService are asynchronous. When registration, matching, or termination is in progress on an IOService object, its <code>busyState</code> is increased by one. Change in <code>busyState</code> to or from zero also changes the IOService object's provider's <code>busyState</code> by one, which means that an IOService object is marked busy when any of the above activities is ocurring on it or any of its clients.
+    @result The <code>busyState</code> value. */
+
+    virtual UInt32 getBusyState( void );
+    
+/*! @function adjustBusy
+    @abstract Adjusts the <code>busyState</code> of an IOService object.
+    @discussion Applies a delta to an IOService object's <code>busyState</code>. A change in the <code>busyState</code> to or from zero will change the IOService object's provider's <code>busyState</code> by one (in the same direction). 
+    @param delta The delta to be applied to the IOService object's <code>busyState</code>. */
+
+    virtual void adjustBusy( SInt32 delta );
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        IOReturn waitQuiet(mach_timespec_t * timeout)
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function waitQuiet
+    @abstract Waits for an IOService object's <code>busyState</code> to be zero.
+    @discussion Blocks the caller until an IOService object is non busy.
+    @param timeout The maximum time to wait in nanoseconds. Default is to wait forever.
+    @result Returns an error code if Mach synchronization primitives fail, <code>kIOReturnTimeout</code>, or <code>kIOReturnSuccess</code>. */
+    
+    IOReturn waitQuiet(uint64_t timeout = UINT64_MAX);
+
+    /* Matching */
+
+/*! @function matchPropertyTable
+    @abstract Allows a registered IOService object to implement family specific matching.
+    @discussion All matching on an IOService object will call this method to allow a family writer to implement matching in addition to the generic methods provided by IOService. The implementer should examine the matching dictionary passed to see if it contains properties the family understands for matching, and use them to match with the IOService object if so. Note that since matching is also carried out by other parts of the I/O Kit, the matching dictionary may contain properties the family does not understand - these should not be considered matching failures.
+    @param table The dictionary of properties to be matched against.
+    @param score Pointer to the current driver's probe score, which is used to order multiple matching drivers in the same match category. It defaults to the value of the <code>IOProbeScore</code> property in the drivers property table, or <code>kIODefaultProbeScore</code> if none is specified.
+    @result <code>false</code> if the family considers the matching dictionary does not match in properties it understands; <code>true</code> otherwise. */
+
+    virtual bool matchPropertyTable( OSDictionary * table,
+                                     SInt32       * score );
+
+    virtual bool matchPropertyTable( OSDictionary * table );
+
+/*! @function matchLocation
+    @abstract Allows a registered IOService object to direct location matching.
+    @discussion By default, a location matching property will be applied to an IOService object's provider. This method allows that behavior to be overridden by families.
+    @param client The IOService object at which matching is taking place.
+    @result Returns the IOService instance to be used for location matching. */
+
+    virtual IOService * matchLocation( IOService * client );
+
+    /* Resource service */
+
+/*! @function publishResource
+    @abstract Uses the resource service to publish a property.
+    @discussion The resource service uses IOService's matching and notification to allow objects to be published and found by any I/O Kit client by a global name. <code>publishResource</code> makes an object available to anyone waiting for it or looking for it in the future.
+    @param key An OSSymbol key that globally identifies the object.
+    @param The object to be published. */
+
+    static void publishResource( const OSSymbol * key, OSObject * value = 0 );
+
+/*! @function publishResource
+    @abstract Uses the resource service to publish a property.
+    @discussion The resource service uses IOService object's matching and notification to allow objects to be published and found by any I/O Kit client by a global name. <code>publishResource</code> makes an object available to anyone waiting for it or looking for it in the future.
+    @param key A C string key that globally identifies the object.
+    @param The object to be published. */
+
+    static void publishResource( const char * key, OSObject * value = 0 );
+    virtual bool addNeededResource( const char * key );
+
+    /* Notifications */
+
+/*! @function addNotification
+    @abstract Deprecated use addMatchingNotification(). Adds a persistant notification handler to be notified of IOService events.
+    @discussion IOService will deliver notifications of changes in state of an IOService object to registered clients. The type of notification is specified by a symbol, for example <code>gIOMatchedNotification</code> or <code>gIOTerminatedNotification</code>, and notifications will only include IOService objects that match the supplied matching dictionary. Notifications are ordered by a priority set with <code>addNotification</code>. When the notification is installed, its handler will be called with each of any currently existing IOService objects that are in the correct state (eg. registered) and match the supplied matching dictionary, avoiding races between finding preexisting and new IOService events. The notification request is identified by an instance of an IONotifier object, through which it can be enabled, disabled, or removed. <code>addNotification</code> consumes a retain count on the matching dictionary when the notification is removed.
+    @param type An OSSymbol identifying the type of notification and IOService state:
+<br>    <code>gIOPublishNotification</code> Delivered when an IOService object is registered.
+<br>    <code>gIOFirstPublishNotification</code> Delivered when an IOService object is registered, but only once per IOService instance. Some IOService objects may be reregistered when their state is changed.
+<br>    <code>gIOMatchedNotification</code> Delivered when an IOService object has been matched with all client drivers, and they have been probed and started.
+<br>    <code>gIOFirstMatchNotification</code> Delivered when an IOService object has been matched with all client drivers, but only once per IOService instance. Some IOService objects may be reregistered when their state is changed.
+<br>    <code>gIOTerminatedNotification</code> Delivered after an IOService object has been terminated, during its finalize stage.
+    @param matching A matching dictionary to restrict notifications to only matching IOService objects. The dictionary will be released when the notification is removed, consuming the passed-in reference.
+    @param handler A C function callback to deliver notifications.
+    @param target An instance reference for the callback's use.
+    @param ref A reference constant for the callback's use.
+    @param priority A constant ordering all notifications of a each type.
+    @result An instance of an IONotifier object that can be used to control or destroy the notification request. */
+
+    static IONotifier * addNotification(
+                            const OSSymbol * type, OSDictionary * matching,
+                            IOServiceNotificationHandler handler,
+                            void * target, void * ref = 0,
+                            SInt32 priority = 0 )
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function addMatchingNotification
+    @abstract Adds a persistant notification handler to be notified of IOService events.
+    @discussion IOService will deliver notifications of changes in state of an IOService object to registered clients. The type of notification is specified by a symbol, for example <code>gIOMatchedNotification</code> or <code>gIOTerminatedNotification</code>, and notifications will only include IOService objects that match the supplied matching dictionary. Notifications are ordered by a priority set with <code>addNotification</code>. When the notification is installed, its handler will be called with each of any currently existing IOService objects that are in the correct state (eg. registered) and match the supplied matching dictionary, avoiding races between finding preexisting and new IOService events. The notification request is identified by an instance of an IONotifier object, through which it can be enabled, disabled, or removed. <code>addMatchingNotification</code> does not consume a reference on the matching dictionary when the notification is removed, unlike addNotification.
+    @param type An OSSymbol identifying the type of notification and IOService state:
+<br>    <code>gIOPublishNotification</code> Delivered when an IOService object is registered.
+<br>    <code>gIOFirstPublishNotification</code> Delivered when an IOService object is registered, but only once per IOService instance. Some IOService objects may be reregistered when their state is changed.
+<br>    <code>gIOMatchedNotification</code> Delivered when an IOService object has been matched with all client drivers, and they have been probed and started.
+<br>    <code>gIOFirstMatchNotification</code> Delivered when an IOService object has been matched with all client drivers, but only once per IOService instance. Some IOService objects may be reregistered when their state is changed.
+<br>    <code>gIOTerminatedNotification</code> Delivered after an IOService object has been terminated, during its finalize stage.
+    @param matching A matching dictionary to restrict notifications to only matching IOService objects. The dictionary is retained while the notification is installed. (Differs from addNotification).
+    @param handler A C function callback to deliver notifications.
+    @param target An instance reference for the callback's use.
+    @param ref A reference constant for the callback's use.
+    @param priority A constant ordering all notifications of a each type.
+    @result An instance of an IONotifier object that can be used to control or destroy the notification request. */
+
+    static IONotifier * addMatchingNotification(
+                            const OSSymbol * type, OSDictionary * matching,
+                            IOServiceMatchingNotificationHandler handler,
+                            void * target, void * ref = 0,
+                            SInt32 priority = 0 );
+
+/*! @function waitForService
+    @abstract Deprecated use waitForMatchingService(). Waits for a matching to service to be published.
+    @discussion Provides a method of waiting for an IOService object matching the supplied matching dictionary to be registered and fully matched. 
+    @param matching The matching dictionary describing the desired IOService object. <code>waitForService</code> consumes one reference of the matching dictionary.
+    @param timeout The maximum time to wait.
+    @result A published IOService object matching the supplied dictionary. */
+
+    static IOService * waitForService( OSDictionary * matching,
+                            mach_timespec_t * timeout = 0);
+
+/*! @function waitForMatchingService
+    @abstract Waits for a matching to service to be published.
+    @discussion Provides a method of waiting for an IOService object matching the supplied matching dictionary to be registered and fully matched. 
+    @param matching The matching dictionary describing the desired IOService object. (Does not consume a reference of the matching dictionary - differs from waitForService() which does consume a reference on the matching dictionary.)
+    @param timeout The maximum time to wait in nanoseconds. Default is to wait forever.
+    @result A published IOService object matching the supplied dictionary. waitForMatchingService returns a reference to the IOService which should be released by the caller. (Differs from waitForService() which does not retain the returned object.) */
+
+    static IOService * waitForMatchingService( OSDictionary * matching,
+                            uint64_t timeout = UINT64_MAX);
+
+/*! @function getMatchingServices
+    @abstract Finds the set of current published IOService objects matching a matching dictionary.
+    @discussion Provides a method of finding the current set of published IOService objects matching the supplied matching dictionary.   
+    @param matching The matching dictionary describing the desired IOService objects.
+    @result An instance of an iterator over a set of IOService objects. To be released by the caller. */
+
+    static OSIterator * getMatchingServices( OSDictionary * matching );
+
+/*! @function copyMatchingService
+    @abstract Finds one of the current published IOService objects matching a matching dictionary.
+    @discussion Provides a method to find one member of the set of published IOService objects matching the supplied matching dictionary.   
+    @param matching The matching dictionary describing the desired IOService object.
+    @result The IOService object or NULL. To be released by the caller. */
+
+    static IOService * copyMatchingService( OSDictionary * matching );
+
+public:
+    /* Helpers to make matching dictionaries for simple cases,
+     * they add keys to an existing dictionary, or create one. */
+
+/*! @function serviceMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify an IOService class match.
+    @discussion A very common matching criteria for IOService object is based on its class. <code>serviceMatching</code> creates a matching dictionary that specifies any IOService object of a class, or its subclasses. The class is specified by name, and an existing dictionary may be passed in, in which case the matching properties will be added to that dictionary rather than creating a new one.
+    @param className The class name, as a const C string. Class matching is successful on IOService objects of this class or any subclass.
+    @param table If zero, <code>serviceMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * serviceMatching( const char * className,
+                                           OSDictionary * table = 0 );
+
+/*! @function serviceMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify an IOService class match.
+    @discussion A very common matching criteria for IOService object is based on its class. <code>serviceMatching</code> creates a matching dictionary that specifies any IOService of a class, or its subclasses. The class is specified by name, and an existing dictionary may be passed in, in which case the matching properties will be added to that dictionary rather than creating a new one.
+    @param className The class name, as an OSString (which includes OSSymbol). Class matching is successful on IOService objects of this class or any subclass.
+    @param table If zero, <code>serviceMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * serviceMatching( const OSString * className,
+                                           OSDictionary * table = 0 );
+
+/*! @function nameMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify an IOService name match.
+    @discussion A very common matching criteria for IOService object is based on its name. <code>nameMatching</code> creates a matching dictionary that specifies any IOService object which responds successfully to the @link //apple_ref/cpp/instm/IORegistryEntry/compareName/virtualbool/(OSString*,OSString**) IORegistryEntry::compareName@/link method. An existing dictionary may be passed in, in which case the matching properties will be added to that dictionary rather than creating a new one.
+    @param name The service's name, as a const C string. Name matching is successful on IOService objects that respond successfully to the <code>IORegistryEntry::compareName</code> method.
+    @param table If zero, <code>nameMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * nameMatching( const char * name,
+                                        OSDictionary * table = 0 );
+
+/*! @function nameMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify an IOService name match.
+    @discussion A very common matching criteria for IOService object is based on its name. <code>nameMatching</code> creates a matching dictionary that specifies any IOService object which responds successfully to the @link //apple_ref/cpp/instm/IORegistryEntry/compareName/virtualbool/(OSString*,OSString**) IORegistryEntry::compareName@/link method. An existing dictionary may be passed in, in which case the matching properties will be added to that dictionary rather than creating a new one.
+    @param name The service's name, as an OSString (which includes OSSymbol). Name matching is successful on IOService objects that respond successfully to the <code>IORegistryEntry::compareName</code> method.
+    @param table If zero, <code>nameMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * nameMatching( const OSString* name,
+                                        OSDictionary * table = 0 );
+
+/*! @function resourceMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify a resource service match.
+    @discussion IOService maintains a resource service IOResources that allows objects to be published and found globally in the I/O Kit based on a name, using the standard IOService matching and notification calls.
+    @param name The resource name, as a const C string. Resource matching is successful when an object by that name has been published with the <code>publishResource</code> method.
+    @param table If zero, <code>resourceMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * resourceMatching( const char * name,
+                                            OSDictionary * table = 0 );
+
+/*! @function resourceMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify a resource service match.
+    @discussion IOService maintains a resource service IOResources that allows objects to be published and found globally in the I/O Kit based on a name, using the standard IOService matching and notification calls.
+    @param name The resource name, as an OSString (which includes OSSymbol). Resource matching is successful when an object by that name has been published with the <code>publishResource</code> method.
+    @param table If zero, <code>resourceMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * resourceMatching( const OSString * name,
+                                            OSDictionary * table = 0 );
+
+
+/*! @function propertyMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify an IOService phandle match.
+    @discussion TODO A very common matching criteria for IOService is based on its name. nameMatching will create a matching dictionary that specifies any IOService which respond successfully to the IORegistryEntry method compareName. An existing dictionary may be passed in, in which case the matching properties will be added to that dictionary rather than creating a new one.
+    @param key The service's phandle, as a const UInt32. PHandle matching is successful on IOService objects that respond successfully to the IORegistryEntry method compareName.
+    @param value The service's phandle, as a const UInt32. PHandle matching is successful on IOService's which respond successfully to the IORegistryEntry method compareName.
+    @param table If zero, nameMatching will create a matching dictionary and return a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * propertyMatching( const OSSymbol * key, const OSObject * value,
+                                            OSDictionary * table = 0 );
+
+/*! @function registryEntryIDMatching
+    @abstract Creates a matching dictionary, or adds matching properties to an existing dictionary, that specify a IORegistryEntryID match.
+    @discussion <code>registryEntryIDMatching</code> creates a matching dictionary that specifies the IOService object with the assigned registry entry ID (returned by <code>IORegistryEntry::getRegistryEntryID()</code>). An existing dictionary may be passed in, in which case the matching properties will be added to that dictionary rather than creating a new one.
+    @param name The service's ID. Matching is successful on the IOService object that return that ID from the <code>IORegistryEntry::getRegistryEntryID()</code> method.
+    @param table If zero, <code>registryEntryIDMatching</code> creates a matching dictionary and returns a reference to it, otherwise the matching properties are added to the specified dictionary.
+    @result The matching dictionary created, or passed in, is returned on success, or zero on failure. */
+
+    static OSDictionary * registryEntryIDMatching( uint64_t entryID,
+                                                   OSDictionary * table = 0 );
+
+
+/*! @function addLocation
+    @abstract Adds a location matching property to an existing dictionary.
+    @discussion This function creates matching properties that specify the location of a IOService object, as an embedded matching dictionary. This matching will be successful on an IOService object that attached to an IOService object which matches this location matching dictionary.
+    @param table The matching properties are added to the specified dictionary, which must be non-zero.
+    @result The location matching dictionary created is returned on success, or zero on failure. */
+
+    static OSDictionary * addLocation( OSDictionary * table );
+
+    /* Helpers for matching dictionaries. */
+
+/*! @function compareProperty
+    @abstract Compares a property in a matching dictionary with an IOService object's property table.
+    @discussion This is a helper function to aid in implementing @link matchPropertyTable matchPropertyTable@/link. If the property specified by <code>key</code> exists in the matching dictionary, it is compared with a property of the same name in the IOService object's property table. The comparison is performed with the <code>isEqualTo</code> method. If the property does not exist in the matching table, success is returned. If the property exists in the matching dictionary but not the IOService property table, failure is returned.
+    @param matching The matching dictionary, which must be non-zero.
+    @param key The dictionary key specifying the property to be compared, as a C string.
+    @result <code>true</code> if the property does not exist in the matching table. If the property exists in the matching dictionary but not the IOService property table, failure is returned. Otherwise the result of calling the property from the matching dictionary's <code>isEqualTo</code> method with the IOService property as an argument is returned. */
+
+    virtual bool compareProperty(   OSDictionary   * matching,
+                                    const char     * key );
+/*! @function compareProperty
+    @abstract Compares a property in a matching dictionary with an IOService object's property table.
+    @discussion This is a helper function to aid in implementing @link matchPropertyTable matchPropertyTable@/link. If the property specified by <code>key</code> exists in the matching dictionary, it is compared with a property of the same name in the IOService object's property table. The comparison is performed with the <code>isEqualTo</code> method. If the property does not exist in the matching table, success is returned. If the property exists in the matching dictionary but not the IOService property table, failure is returned.
+    @param matching The matching dictionary, which must be non-zero.
+    @param key The dictionary key specifying the property to be compared, as an OSString (which includes OSSymbol).
+    @result <code>true</code> if the property does not exist in the matching table. If the property exists in the matching dictionary but not the IOService property table, failure is returned. Otherwise the result of calling the property from the matching dictionary's <code>isEqualTo</code> method with the IOService property as an argument is returned. */
+
+    virtual bool compareProperty(   OSDictionary   * matching,
+                                    const OSString * key );
+
+/*! @function compareProperties
+    @abstract Compares a set of properties in a matching dictionary with an IOService object's property table.
+    @discussion This is a helper function to aid in implementing @link matchPropertyTable matchPropertyTable@/link. A collection of dictionary keys specifies properties in a matching dictionary to be compared, with <code>compareProperty</code>, with an IOService object's property table, if <code>compareProperty</code> returns <code>true</code> for each key, success is returned; otherwise failure.
+    @param matching The matching dictionary, which must be non-zero.
+    @param keys A collection (eg. OSSet, OSArray, OSDictionary) which should contain OSStrings (or OSSymbols) that specify the property keys to be compared.
+    @result Success if <code>compareProperty</code> returns <code>true</code> for each key in the collection; otherwise failure. */
+
+    virtual bool compareProperties( OSDictionary   * matching,
+                                    OSCollection   * keys );
+
+    /* Client / provider accessors */
+
+/*! @function attach
+    @abstract Attaches an IOService client to a provider in the I/O Registry.
+    @discussion This function called in an IOService client enters the client into the I/O Registry as a child of the provider in the service plane. The provider must be active or the attach will fail. Multiple attach calls to the same provider are no-ops and return success. A client may be attached to multiple providers. Entering an object into the I/O Registry retains both the client and provider until they are detached.
+    @param provider The IOService object which will serve as this object's provider.
+    @result <code>false</code> if the provider is inactive or on a resource failure; otherwise <code>true</code>. */
+
+    virtual bool attach( IOService * provider );
+    
+/*! @function detach
+    @abstract Detaches an IOService client from a provider in the I/O Registry.
+    @discussion This function called in an IOService client removes the client as a child of the provider in the service plane of the I/O Registry. If the provider is not a parent of the client this is a no-op, otherwise the I/O Registry releases both the client and provider.
+    @param provider The IOService object to detach from. */
+
+    virtual void detach( IOService * provider );
+
+/*! @function getProvider
+    @abstract Returns an IOService object's primary provider.
+    @discussion This function called in an IOService client will return the provider to which it was first attached. Because the majority of IOService objects have only one provider, this is a useful simplification and also supports caching of the provider when the I/O Registry is unchanged.
+    @result The first provider of the client, or zero if the IOService object is not attached into the I/O Registry. The provider is retained while the client is attached, and should not be released by the caller. */
+
+    virtual IOService * getProvider( void ) const;
+
+/*! @function getWorkLoop
+    @abstract Returns the current work loop or <code>provider->getWorkLoop</code>.
+    @discussion This function returns a valid work loop that a client can use to add an IOCommandGate to. The intention is that an IOService client has data that needs to be protected but doesn't want to pay the cost of a dedicated thread. This data has to be accessed from a provider's call-out context as well. So to achieve both of these goals the client creates an IOCommandGate to lock access to his data but he registers it with the provider's work loop, i.e. the work loop which will make the completion call-outs. This avoids a potential deadlock because the work loop gate uses a recursive lock, which allows the same lock to be held multiple times by a single thread.
+    @result A work loop, either the current work loop or it walks up the @link getProvider getProvider@/link chain calling <code>getWorkLoop</code>. Eventually it will reach a valid work loop-based driver or the root of the I/O tree, where it will return a system-wide work loop. Returns 0 if it fails to find (or create) a work loop.*/
+
+    virtual IOWorkLoop * getWorkLoop() const;
+
+/*! @function getProviderIterator
+    @abstract Returns an iterator over an IOService object's providers.
+    @discussion For those few IOService objects that obtain service from multiple providers, this method supplies an iterator over a client's providers. 
+    @result An iterator over the providers of the client, or zero if there is a resource failure. The iterator must be released when the iteration is finished. All objects returned by the iteration are retained while the iterator is valid, though they may no longer be attached during the iteration. */
+
+    virtual OSIterator * getProviderIterator( void ) const;
+
+/*! @function getOpenProviderIterator
+    @abstract Returns an iterator over an client's providers that are currently opened by the client.
+    @discussion For those few IOService objects that obtain service from multiple providers, this method supplies an iterator over a client's providers, locking each in turn with @link lockForArbitration lockForArbitration@/link and returning those that have been opened by the client. 
+    @result An iterator over the providers the client has open, or zero if there is a resource failure. The iterator must be released when the iteration is finished. All objects returned by the iteration are retained while the iterator is valid, and the current entry in the iteration is locked with <code>lockForArbitration</code>, protecting it from state changes. */
+
+    virtual OSIterator * getOpenProviderIterator( void ) const;
+
+/*! @function getClient
+    @abstract Returns an IOService object's primary client.
+    @discussion This function called in an IOService provider will return the first client to attach to it. For IOService objects which have only only one client, this may be a useful simplification.
+    @result The first client of the provider, or zero if the IOService object is not attached into the I/O Registry. The client is retained while it is attached, and should not be released by the caller. */
+
+    virtual IOService * getClient( void ) const;
+
+/*! @function getClientIterator
+    @abstract Returns an iterator over an IOService object's clients.
+    @discussion For IOService objects that may have multiple clients, this method supplies an iterator over a provider's clients. 
+    @result An iterator over the clients of the provider, or zero if there is a resource failure. The iterator must be released when the iteration is finished. All objects returned by the iteration are retained while the iterator is valid, though they may no longer be attached during the iteration. */
+
+    virtual OSIterator * getClientIterator( void ) const;
+
+/*! @function getOpenClientIterator
+    @abstract Returns an iterator over a provider's clients that currently have opened the provider.
+    @discussion For IOService objects that may have multiple clients, this method supplies an iterator over a provider's clients, locking each in turn with @link lockForArbitration lockForArbitration@/link and returning those that have opened the provider. 
+    @result An iterator over the clients that have opened the provider, or zero if there is a resource failure. The iterator must be released when the iteration is finished. All objects returned by the iteration are retained while the iterator is valid, and the current entry in the iteration is locked with <code>lockForArbitration</code>, protecting it from state changes. */
+
+    virtual OSIterator * getOpenClientIterator( void ) const;
+
+/*! @function callPlatformFunction
+    @abstract Calls the platform function with the given name.
+    @discussion The platform expert or other drivers may implement various functions to control hardware features.  <code>callPlatformFunction</code> allows any IOService object to access these functions. Normally <code>callPlatformFunction</code> is called on a service's provider. The provider services the request or passes it to its provider. The system's IOPlatformExpert subclass catches functions it knows about and redirects them into other parts of the service plane. If the IOPlatformExpert subclass cannot execute the function, the base class is called. The IOPlatformExpert base class attempts to find a service to execute the function by looking up the function name in an IOResources name space. A service may publish a service using <code>publishResource(functionName, this)</code>. If no service can be found to execute the function an error is returned.
+    @param functionName Name of the function to be called. When <code>functionName</code> is a C string, <code>callPlatformFunction</code> converts the C string to an OSSymbol and calls the OSSymbol version of <code>callPlatformFunction</code>. This process can block and should not be used from an interrupt context.
+    @param waitForFunction If <code>true</code>, <code>callPlatformFunction</code> will not return until the function has been called.
+    @result An IOReturn code; <code>kIOReturnSuccess</code> if the function was successfully executed, <code>kIOReturnUnsupported</code> if a service to execute the function could not be found. Other return codes may be returned by the function.*/
+
+    virtual IOReturn callPlatformFunction( const OSSymbol * functionName,
+                                           bool waitForFunction,
+                                           void *param1, void *param2,
+                                           void *param3, void *param4 );
+
+    virtual IOReturn callPlatformFunction( const char * functionName,
+                                           bool waitForFunction,
+                                           void *param1, void *param2,
+                                           void *param3, void *param4 );
+
+
+    /* Some accessors */
+
+/*! @function getPlatform
+    @abstract Returns a pointer to the platform expert instance for the computer.
+    @discussion This method provides an accessor to the platform expert instance for the computer. 
+    @result A pointer to the IOPlatformExpert instance. It should not be released by the caller. */
+
+    static IOPlatformExpert * getPlatform( void );
+
+/*! @function getPMRootDomain
+    @abstract Returns a pointer to the power management root domain instance for the computer.
+    @discussion This method provides an accessor to the power management root domain instance for the computer. 
+    @result A pointer to the power management root domain instance. It should not be released by the caller. */
+
+    static class IOPMrootDomain * getPMRootDomain( void );
+
+/*! @function getServiceRoot
+    @abstract Returns a pointer to the root of the service plane.
+    @discussion This method provides an accessor to the root of the service plane for the computer. 
+    @result A pointer to the IOService instance at the root of the service plane. It should not be released by the caller. */
+
+    static IOService * getServiceRoot( void );
+
+/*! @function getResourceService
+    @abstract Returns a pointer to the IOResources service.
+    @discussion IOService maintains a resource service IOResources that allows objects to be published and found globally in the I/O Kit based on a name, using the standard IOService matching and notification calls.
+    @result A pointer to the IOResources instance. It should not be released by the caller. */
+
+    static IOService * getResourceService( void );
+
+    /* Allocate resources for a matched service */
+
+/*! @function getResources
+    @abstract Allocates any needed resources for a published IOService object before clients attach.
+    @discussion This method is called during the registration process for an IOService object if there are successful driver matches, before any clients attach. It allows for lazy allocation of resources to an IOService object when a matching driver is found.
+    @result An IOReturn code; <code>kIOReturnSuccess</code> is necessary for the IOService object to be successfully used, otherwise the registration process for the object is halted. */
+    
+    virtual IOReturn getResources( void );
+
+    /* Device memory accessors */
+
+/*! @function getDeviceMemoryCount
+    @abstract Returns a count of the physical memory ranges available for a device.
+    @discussion This method returns the count of physical memory ranges, each represented by an IODeviceMemory instance, that have been allocated for a memory mapped device.
+    @result An integer count of the number of ranges available. */
+
+    virtual IOItemCount getDeviceMemoryCount( void );
+
+/*! @function getDeviceMemoryWithIndex
+    @abstract Returns an instance of IODeviceMemory representing one of a device's memory mapped ranges.
+    @discussion This method returns a pointer to an instance of IODeviceMemory for the physical memory range at the given index for a memory mapped device.
+    @param index An index into the array of ranges assigned to the device.
+    @result A pointer to an instance of IODeviceMemory, or zero if the index is beyond the count available. The IODeviceMemory is retained by the provider, so is valid while attached, or while any mappings to it exist. It should not be released by the caller. See also @link mapDeviceMemoryWithIndex mapDeviceMemoryWithIndex@/link, which creates a device memory mapping. */
+
+    virtual IODeviceMemory * getDeviceMemoryWithIndex( unsigned int index );
+
+/*! @function mapDeviceMemoryWithIndex
+    @abstract Maps a physical range of a device.
+    @discussion This method creates a mapping for the IODeviceMemory at the given index, with <code>IODeviceMemory::map(options)</code>. The mapping is represented by the returned instance of IOMemoryMap, which should not be released until the mapping is no longer required.
+    @param index An index into the array of ranges assigned to the device.
+    @result An instance of IOMemoryMap, or zero if the index is beyond the count available. The mapping should be released only when access to it is no longer required. */
+
+    virtual IOMemoryMap * mapDeviceMemoryWithIndex( unsigned int index,
+                                                    IOOptionBits options = 0 );
+
+/*! @function getDeviceMemory
+    @abstract Returns the array of IODeviceMemory objects representing a device's memory mapped ranges.
+    @discussion This method returns an array of IODeviceMemory objects representing the physical memory ranges allocated to a memory mapped device.
+    @result An OSArray of IODeviceMemory objects, or zero if none are available. The array is retained by the provider, so is valid while attached. */
+
+    virtual OSArray * getDeviceMemory( void );
+
+/*! @function setDeviceMemory
+    @abstract Sets the array of IODeviceMemory objects representing a device's memory mapped ranges.
+    @discussion This method sets an array of IODeviceMemory objects representing the physical memory ranges allocated to a memory mapped device.
+    @param array An OSArray of IODeviceMemory objects, or zero if none are available. The array will be retained by the object. */
+
+    virtual void setDeviceMemory( OSArray * array );
+
+    /* Interrupt accessors */
+
+/*! @function registerInterrupt
+    @abstract Registers a C function interrupt handler for a device supplying interrupts.
+    @discussion This method installs a C function interrupt handler to be called at primary interrupt time for a device's interrupt. Only one handler may be installed per interrupt source. IOInterruptEventSource provides a work loop based abstraction for interrupt delivery that may be more appropriate for work loop based drivers.
+    @param source The index of the interrupt source in the device.
+    @param target An object instance to be passed to the interrupt handler.
+    @param handler The C function to be called at primary interrupt time when the interrupt occurs. The handler should process the interrupt by clearing the interrupt, or by disabling the source.
+    @param refCon A reference constant for the handler's use.
+    @result An IOReturn code.<br><code>kIOReturnNoInterrupt</code> is returned if the source is not valid; <code>kIOReturnNoResources</code> is returned if the interrupt already has an installed handler. */
+
+    virtual IOReturn registerInterrupt(int source, OSObject *target,
+                                       IOInterruptAction handler,
+                                       void *refCon = 0);
+                                       
+/*! @function unregisterInterrupt
+    @abstract Removes a C function interrupt handler for a device supplying hardware interrupts.
+    @discussion This method removes a C function interrupt handler previously installed with @link registerInterrupt registerInterrupt@/link.
+    @param source The index of the interrupt source in the device.
+    @result An IOReturn code (<code>kIOReturnNoInterrupt</code> is returned if the source is not valid). */
+
+    virtual IOReturn unregisterInterrupt(int source);
+
+/*! @function addInterruptStatistics
+    @abstract Adds a statistics object to the IOService for the given interrupt.
+    @discussion This method associates a set of statistics and a reporter for those statistics with an interrupt for this IOService, so that we can interrogate the IOService for statistics pertaining to that interrupt.
+    @param statistics The IOInterruptAccountingData container we wish to associate the IOService with.
+    @param source The index of the interrupt source in the device. */
+    IOReturn addInterruptStatistics(IOInterruptAccountingData * statistics, int source);
+
+/*! @function removeInterruptStatistics
+    @abstract Removes any statistics from the IOService for the given interrupt.
+    @discussion This method disassociates any IOInterruptAccountingData container we may have for the given interrupt from the IOService; this indicates that the the interrupt target (at the moment, likely an IOInterruptEventSource) is being destroyed.
+    @param source The index of the interrupt source in the device. */
+    IOReturn removeInterruptStatistics(int source);
+
+/*! @function getInterruptType
+    @abstract Returns the type of interrupt used for a device supplying hardware interrupts.
+    @param source The index of the interrupt source in the device.
+    @param interruptType The interrupt type for the interrupt source will be stored here by <code>getInterruptType</code>.<br> <code>kIOInterruptTypeEdge</code> will be returned for edge-trigggered sources.<br><code>kIOInterruptTypeLevel</code> will be returned for level-trigggered sources.
+    @result An IOReturn code (<code>kIOReturnNoInterrupt</code> is returned if the source is not valid). */
+
+    virtual IOReturn getInterruptType(int source, int *interruptType);
+
+/*! @function enableInterrupt
+    @abstract Enables a device interrupt.
+    @discussion It is the caller's responsiblity to keep track of the enable state of the interrupt source.
+    @param source The index of the interrupt source in the device.
+    @result An IOReturn code (<code>kIOReturnNoInterrupt</code> is returned if the source is not valid). */
+
+    virtual IOReturn enableInterrupt(int source);
+
+/*! @function disableInterrupt
+    @abstract Synchronously disables a device interrupt.
+    @discussion If the interrupt routine is running, the call will block until the routine completes. It is the caller's responsiblity to keep track of the enable state of the interrupt source.
+    @param source The index of the interrupt source in the device.
+    @result An IOReturn code (<code>kIOReturnNoInterrupt</code> is returned if the source is not valid). */
+
+    virtual IOReturn disableInterrupt(int source);
+
+/*! @function causeInterrupt
+    @abstract Causes a device interrupt to occur.
+    @discussion Emulates a hardware interrupt, to be called from task level.
+    @param source The index of the interrupt source in the device.
+    @result An IOReturn code (<code>kIOReturnNoInterrupt</code> is returned if the source is not valid). */
+
+    virtual IOReturn causeInterrupt(int source);
+
+/*! @function requestProbe
+    @abstract Requests that hardware be re-scanned for devices.
+    @discussion For bus families that do not usually detect device addition or removal, this method represents an external request (eg. from a utility application) to rescan and publish or remove found devices.
+    @param options Family defined options, not interpreted by IOService.
+    @result An IOReturn code. */
+
+    virtual IOReturn requestProbe( IOOptionBits options );
+
+    /* Generic API for non-data-path upstream calls */
+
+/*! @function message
+    @abstract Receives a generic message delivered from an attached provider.
+    @discussion A provider may deliver messages via the <code>message</code> method to its clients informing them of state changes, such as <code>kIOMessageServiceIsTerminated</code> or <code>kIOMessageServiceIsSuspended</code>. Certain messages are defined by the I/O Kit in <code>IOMessage.h</code> while others may be family dependent. This method is implemented in the client to receive messages.
+    @param type A type defined in <code>IOMessage.h</code> or defined by the provider family.
+    @param provider The provider from which the message originates.
+    @param argument An argument defined by the provider family, not used by IOService.
+    @result An IOReturn code defined by the message type. */
+
+    virtual IOReturn message( UInt32 type, IOService * provider,
+                              void * argument = 0 );
+                                
+/*! @function messageClient
+    @abstract Sends a generic message to an attached client.
+    @discussion A provider may deliver messages via the @link message message@/link method to its clients informing them of state changes, such as <code>kIOMessageServiceIsTerminated</code> or <code>kIOMessageServiceIsSuspended</code>. Certain messages are defined by the I/O Kit in <code>IOMessage.h</code> while others may be family dependent. This method may be called in the provider to send a message to the specified client, which may be useful for overrides.
+    @param messageType A type defined in <code>IOMessage.h</code> or defined by the provider family.
+    @param client A client of the IOService to send the message.
+    @param messageArgument An argument defined by the provider family, not used by IOService.
+    @param argSize Specifies the size of messageArgument, in bytes.  If argSize is non-zero, messageArgument is treated as a pointer to argSize bytes of data.  If argSize is 0 (the default), messageArgument is treated as an ordinal and passed by value.
+    @result The return code from the client message call. */
+    
+    virtual IOReturn messageClient( UInt32 messageType, OSObject * client,
+                                    void * messageArgument = 0, vm_size_t argSize = 0 );
+
+/*! @function messageClients
+    @abstract Sends a generic message to all attached clients.
+    @discussion A provider may deliver messages via the @link message message@/link method to its clients informing them of state changes, such as <code>kIOMessageServiceIsTerminated</code> or <code>kIOMessageServiceIsSuspended</code>. Certain messages are defined by the I/O Kit in <code>IOMessage.h</code> while others may be family dependent. This method may be called in the provider to send a message to all the attached clients, via the @link messageClient messageClient@/link method.
+    @param type A type defined in <code>IOMessage.h</code> or defined by the provider family.
+    @param argument An argument defined by the provider family, not used by IOService.
+    @param argSize Specifies the size of argument, in bytes.  If argSize is non-zero, argument is treated as a pointer to argSize bytes of data.  If argSize is 0 (the default), argument is treated as an ordinal and passed by value.
+    @result Any non-<code>kIOReturnSuccess</code> return codes returned by the clients, or <code>kIOReturnSuccess</code> if all return <code>kIOReturnSuccess</code>. */
+
+    virtual IOReturn messageClients( UInt32 type,
+                                     void * argument = 0, vm_size_t argSize = 0 );
+
+    virtual IONotifier * registerInterest( const OSSymbol * typeOfInterest,
+                                           IOServiceInterestHandler handler,
+                                           void * target, void * ref = 0 );
+
+    virtual void applyToProviders( IOServiceApplierFunction applier,
+                                   void * context );
+
+    virtual void applyToClients( IOServiceApplierFunction applier,
+                                 void * context );
+
+    virtual void applyToInterested( const OSSymbol * typeOfInterest,
+                                    OSObjectApplierFunction applier,
+                                    void * context );
+
+    virtual IOReturn acknowledgeNotification( IONotificationRef notification,
+                                              IOOptionBits response );
+
+    /* User client create */
+
+/*! @function newUserClient
+    @abstract Creates a connection for a non kernel client.
+    @discussion A non kernel client may request a connection be opened via the @link //apple_ref/c/func/IOServiceOpen IOServiceOpen@/link library function, which will call this method in an IOService object. The rules and capabilities of user level clients are family dependent, and use the functions of the IOUserClient class for support. IOService's implementation returns <code>kIOReturnUnsupported</code>, so any family supporting user clients must implement this method.
+    @param owningTask The Mach task of the client thread in the process of opening the user client. Note that in Mac OS X, each process is based on a Mach task and one or more Mach threads. For more information on the composition of a Mach task and its relationship with Mach threads, see {@linkdoc //apple_ref/doc/uid/TP30000905-CH209-TPXREF103 "Tasks and Threads"}.
+    @param securityID A token representing the access level for the task.
+    @param type A constant specifying the type of connection to be created, specified by the caller of @link //apple_ref/c/func/IOServiceOpen IOServiceOpen@/link and interpreted only by the family.
+    @param handler An instance of an IOUserClient object to represent the connection, which will be released when the connection is closed, or zero if the connection was not opened.    
+    @param properties A dictionary of additional properties for the connection.
+    @result A return code to be passed back to the caller of <code>IOServiceOpen</code>. */
+
+    virtual IOReturn newUserClient( task_t owningTask, void * securityID,
+                                    UInt32 type, OSDictionary * properties,
+                                    IOUserClient ** handler );
+
+    virtual IOReturn newUserClient( task_t owningTask, void * securityID,
+                                    UInt32 type, IOUserClient ** handler );
+
+    /* Return code utilities */
+
+/*! @function stringFromReturn
+    @abstract Supplies a programmer-friendly string from an IOReturn code.
+    @discussion Strings are available for the standard return codes in <code>IOReturn.h</code> in IOService, while subclasses may implement this method to interpret family dependent return codes.
+    @param rtn The IOReturn code.
+    @result A pointer to a constant string, or zero if the return code is unknown. */
+    
+    virtual const char * stringFromReturn( IOReturn rtn );
+
+/*! @function errnoFromReturn
+    @abstract Translates an IOReturn code to a BSD <code>errno</code>.
+    @discussion BSD defines its own return codes for its functions in <code>sys/errno.h</code>, and I/O Kit families may need to supply compliant results in BSD shims. Results are available for the standard return codes in <code>IOReturn.h</code> in IOService, while subclasses may implement this method to interpret family dependent return codes.
+    @param rtn The IOReturn code.
+    @result The BSD <code>errno</code> or <code>EIO</code> if unknown. */
+    
+    virtual int errnoFromReturn( IOReturn rtn );
+
+    /* * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+    /* * * * * * * * * * end of IOService API  * * * * * * * */
+    /* * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+    /* for IOInterruptController implementors */
+
+    int               _numInterruptSources;
+    IOInterruptSource *_interruptSources;
+
+    /* overrides */
+    virtual bool serializeProperties( OSSerialize * s ) const APPLE_KEXT_OVERRIDE;
+
+#ifdef KERNEL_PRIVATE
+    /* Apple only SPI to control CPU low power modes */
+    void   setCPUSnoopDelay(UInt32 ns);
+    UInt32 getCPUSnoopDelay();
+#endif
+    void   requireMaxBusStall(UInt32 ns);
+    void   requireMaxInterruptDelay(uint32_t ns);
+
+    /* * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+    /* * * * * * * * * * * * Internals * * * * * * * * * * * */
+    /* * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+#ifdef XNU_KERNEL_PRIVATE
+public:
+    // called from other xnu components
+    static void initialize( void );
+    static void setPlatform( IOPlatformExpert * platform);
+    static void setPMRootDomain( class IOPMrootDomain * rootDomain );
+    static IOReturn catalogNewDrivers( OSOrderedSet * newTables );
+    uint64_t getAccumulatedBusyTime( void );
+    static void updateConsoleUsers(OSArray * consoleUsers, IOMessage systemMessage);
+    static void consoleLockTimer(thread_call_param_t p0, thread_call_param_t p1);
+    void setTerminateDefer(IOService * provider, bool defer);
+    uint64_t getAuthorizationID( void );
+    IOReturn setAuthorizationID( uint64_t authorizationID );
+    void cpusRunning(void);
+
+private:
+    static IOReturn waitMatchIdle( UInt32 ms );
+    static IONotifier * installNotification(
+                            const OSSymbol * type, OSDictionary * matching,
+                            IOServiceMatchingNotificationHandler handler,
+                            void * target, void * ref,
+                            SInt32 priority, OSIterator ** existing );
+#if !defined(__LP64__)
+    static IONotifier * installNotification(
+                            const OSSymbol * type, OSDictionary * matching,
+                            IOServiceNotificationHandler handler,
+                            void * target, void * ref,
+                            SInt32 priority, OSIterator ** existing);
+#endif /* !defined(__LP64__) */
+#endif
+
+private:
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        bool checkResources( void );
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        bool checkResource( OSObject * matching );
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        void probeCandidates( OSOrderedSet * matches );
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        bool startCandidate( IOService * candidate );
+
+public:
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        IOService * getClientWithCategory( const OSSymbol * category )
+    APPLE_KEXT_DEPRECATED;
+        // copyClientWithCategory is the public replacement
+
+#ifdef XNU_KERNEL_PRIVATE
+    /* Callable within xnu source only - but require vtable entries to be visible */
+public:
+#else
+private:
+#endif
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    bool passiveMatch( OSDictionary * matching, bool changesOK = false);
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    void startMatching( IOOptionBits options = 0 );
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    void doServiceMatch( IOOptionBits options );
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    void doServiceTerminate( IOOptionBits options );
+
+private:
+
+    bool matchPassive(OSDictionary * table, uint32_t options);
+    bool matchInternal(OSDictionary * table, uint32_t options, unsigned int * did);
+    static bool instanceMatch(const OSObject * entry, void * context);
+
+    static OSObject * copyExistingServices( OSDictionary * matching,
+                                            IOOptionBits inState, IOOptionBits options = 0 );
+
+    static IONotifier * setNotification(
+                            const OSSymbol * type, OSDictionary * matching,
+                            IOServiceMatchingNotificationHandler handler,
+                            void * target, void * ref,
+                            SInt32 priority = 0 );
+
+    static IONotifier * doInstallNotification(
+                            const OSSymbol * type, OSDictionary * matching,
+                            IOServiceMatchingNotificationHandler handler,
+                            void * target, void * ref,
+                            SInt32 priority, OSIterator ** existing );
+
+    static bool syncNotificationHandler( void * target, void * ref,
+                            IOService * newService, IONotifier * notifier  );
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    void deliverNotification( const OSSymbol * type,
+                              IOOptionBits orNewState, IOOptionBits andNewState );
+
+    bool invokeNotifer( class _IOServiceNotifier * notify );
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        void unregisterAllInterest( void );
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        IOReturn waitForState( UInt32 mask, UInt32 value,
+                               mach_timespec_t * timeout = 0 );
+
+    IOReturn waitForState( UInt32 mask, UInt32 value, uint64_t timeout );
+
+    UInt32 _adjustBusy( SInt32 delta );
+
+    bool terminatePhase1( IOOptionBits options = 0 );
+    void scheduleTerminatePhase2( IOOptionBits options = 0 );
+    void scheduleStop( IOService * provider );
+    void scheduleFinalize( void );
+    static void terminateThread( void * arg, wait_result_t unused );
+    static void terminateWorker( IOOptionBits options );
+    static void actionWillTerminate( IOService * victim, IOOptionBits options, 
+                                     OSArray * doPhase2List, void*, void * );
+    static void actionDidTerminate( IOService * victim, IOOptionBits options,
+                                    void *, void *, void *);
+
+    static void actionWillStop( IOService * victim, IOOptionBits options, 
+				void *, void *, void *);
+    static void actionDidStop( IOService * victim, IOOptionBits options,
+				void *, void *, void *);
+
+    static void actionFinalize( IOService * victim, IOOptionBits options,
+                                void *, void *, void *);
+    static void actionStop( IOService * client, IOService * provider,
+                            void *, void *, void *);
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        IOReturn resolveInterrupt(IOService *nub, int source);
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+        IOReturn lookupInterrupt(int source, bool resolve, IOInterruptController **interruptController);
+
+#ifdef XNU_KERNEL_PRIVATE
+    /* end xnu internals */
+#endif
+
+    /* power management */
+public:
+
+/*! @function PMinit
+    @abstract Initializes power management for a driver.
+    @discussion <code>PMinit</code> allocates and initializes the power management instance variables, and it should be called before accessing those variables or calling the power management methods. This method should be called inside the driver's <code>start</code> routine and must be paired with a call to @link PMstop PMstop@/link.
+    Most calls to <code>PMinit</code> are followed by calls to @link joinPMtree joinPMtree@/link and @link registerPowerDriver registerPowerDriver@/link. */
+
+    virtual void PMinit( void );
+
+/*! @function PMstop
+    @abstract Stop power managing the driver.
+    @discussion Removes the driver from the power plane and stop its power management. This method is synchronous against any power management method invocations (e.g. <code>setPowerState</code> or <code>setAggressiveness</code>), so when this method returns it is guaranteed those power management methods will not be entered. Driver should not call any power management methods after this call.
+    Calling <code>PMstop</code> cleans up for the three power management initialization calls: @link PMinit PMinit@/link, @link joinPMtree joinPMtree@/link, and @link registerPowerDriver registerPowerDriver@/link. */
+
+    virtual void PMstop( void );
+
+/*! @function joinPMtree
+    @abstract Joins the driver into the power plane of the I/O Registry.
+    @discussion A driver uses this method to call its nub when initializing (usually in its <code>start</code> routine after calling @link PMinit PMinit@/link), to be attached into the power management hierarchy (i.e., the power plane). A driver usually calls this method on the driver for the device that provides it power (this is frequently the nub).    
+    Before this call returns, the caller will probably be called at @link setPowerParent setPowerParent@/link and @link setAggressiveness setAggressiveness@/link and possibly at @link addPowerChild addPowerChild@/link as it is added to the hierarchy. This method may be overridden by a nub subclass.
+    @param driver The driver to be added to the power plane, usually <code>this</code>. */
+
+    virtual void joinPMtree( IOService * driver );
+
+/*! @function registerPowerDriver
+    @abstract Registers a set of power states that the driver supports.
+    @discussion A driver defines its array of supported power states with power management in its power management initialization (its <code>start</code> routine). If successful, power management will call the driver to instruct it to change its power state through @link setPowerState setPowerState@/link.
+    Most drivers do not need to override <code>registerPowerDriver</code>. A nub may override <code>registerPowerDriver</code> if it needs to arrange its children in the power plane differently than the default placement, but this is uncommon.
+    @param controllingDriver A pointer to the calling driver, usually <code>this</code>.
+    @param powerStates A driver-defined array of power states that the driver and device support. Power states are defined in <code>pwr_mgt/IOPMpowerState.h</code>.
+    @param numberOfStates The number of power states in the array.
+    @result </code>IOPMNoErr</code>. All errors are logged via <code>kprintf</code>. */
+
+    virtual IOReturn registerPowerDriver(
+                        IOService *      controllingDriver,
+                        IOPMPowerState * powerStates,
+                        unsigned long    numberOfStates );
+
+/*! @function registerInterestedDriver
+    @abstract Allows an IOService object to register interest in the changing power state of a power-managed IOService object.
+    @discussion Call <code>registerInterestedDriver</code> on the IOService object you are interested in receiving power state messages from, and pass a pointer to the interested driver (<code>this</code>) as an argument.
+    The interested driver is retained until the power interest is removed by calling <code>deRegisterInterestedDriver</code>.
+    The interested driver should override @link powerStateWillChangeTo powerStateWillChangeTo@/link and @link powerStateDidChangeTo powerStateDidChangeTo@/link to receive these power change messages.
+    Interested drivers must acknowledge power changes in <code>powerStateWillChangeTo</code> or <code>powerStateDidChangeTo</code>, either via return value or later calls to @link acknowledgePowerChange acknowledgePowerChange@/link.
+    @param theDriver The driver of interest adds this pointer to the list of interested drivers. It informs drivers on this list before and after the power change.
+    @result Flags describing the capability of the device in its current power state. If the current power state is not yet defined, zero is returned (this is the case when the driver is not yet in the power domain hierarchy or hasn't fully registered with power management yet). */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOPMPowerFlags registerInterestedDriver( IOService * theDriver );
+
+/*! @function deRegisterInterestedDriver
+    @abstract De-registers power state interest from a previous call to <code>registerInterestedDriver</code>.
+    @discussion The retain from <code>registerInterestedDriver</code> is released. This method is synchronous against any <code>powerStateWillChangeTo</code> or <code>powerStateDidChangeTo</code> call targeting the interested driver, so when this method returns it is guaranteed those interest handlers will not be entered.
+    Most drivers do not need to override <code>deRegisterInterestedDriver</code>.
+    @param theDriver The interested driver previously passed into @link registerInterestedDriver registerInterestedDriver@/link.
+    @result A return code that can be ignored by the caller. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOReturn deRegisterInterestedDriver( IOService * theDriver );
+
+/*! @function acknowledgePowerChange
+    @abstract Acknowledges an in-progress power state change.
+    @discussion When power management informs an interested object (via @link powerStateWillChangeTo powerStateWillChangeTo@/link or @link powerStateDidChangeTo powerStateDidChangeTo@/link), the object can return an immediate acknowledgement via a return code, or it may return an indication that it will acknowledge later by calling <code>acknowledgePowerChange</code>.
+    Interested objects are those that have registered as interested drivers, as well as power plane children of the power changing driver. A driver that calls @link registerInterestedDriver registerInterestedDriver@/link must call <code>acknowledgePowerChange</code>, or use an immediate acknowledgement return from <code>powerStateWillChangeTo</code> or <code>powerStateDidChangeTo</code>.
+    @param whichDriver A pointer to the calling driver. The called object tracks all interested parties to ensure that all have acknowledged the power state change.
+    @result <code>IOPMNoErr</code>. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOReturn acknowledgePowerChange( IOService * whichDriver );
+
+/*! @function acknowledgeSetPowerState
+    @abstract Acknowledges the belated completion of a driver's <code>setPowerState</code> power state change.
+    @discussion After power management instructs a driver to change its state via @link setPowerState setPowerState@/link, that driver must acknowledge the change when its device has completed its transition. The acknowledgement may be immediate, via a return code from <code>setPowerState</code>, or delayed, via this call to <code>acknowledgeSetPowerState</code>.
+    Any driver that does not return <code>kIOPMAckImplied</code> from its <code>setPowerState</code> implementation must later call <code>acknowledgeSetPowerState</code>.
+    @result <code>IOPMNoErr</code>. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOReturn acknowledgeSetPowerState( void );
+
+/*! @function requestPowerDomainState
+    @abstract Tells a driver to adjust its power state.
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual IOReturn requestPowerDomainState(
+                        IOPMPowerFlags desiredState,
+                        IOPowerConnection * whichChild,
+                        unsigned long specificationFlags );
+
+/*! @function makeUsable
+    @abstract Requests that a device become usable.
+    @discussion This method is called when some client of a device (or the device's own driver) is asking for the device to become usable. Power management responds by telling the object upon which this method is called to change to its highest power state.
+    <code>makeUsable</code> is implemented using @link changePowerStateToPriv changePowerStateToPriv@/link. Subsequent requests for lower power, such as from <code>changePowerStateToPriv</code>, will pre-empt this request.
+    @result A return code that can be ignored by the caller. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOReturn makeUsable( void );
+
+/*! @function temporaryPowerClampOn
+    @abstract A driver calls this method to hold itself in the highest power state until it has children.
+    @discussion Use <code>temporaryPowerClampOn</code> to hold your driver in its highest power state while waiting for child devices to attach. After children have attached, the clamp is released and the device's power state is controlled by the children's requirements.
+    @result A return code that can be ignored by the caller. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOReturn temporaryPowerClampOn( void );
+
+/*! @function changePowerStateTo
+    @abstract Sets a driver's power state.
+    @discussion This function is one of several that are used to set a driver's power state. In most circumstances, however, you should call @link changePowerStateToPriv changePowerStateToPriv@/link instead.
+    Calls to <code>changePowerStateTo</code>, <code>changePowerStateToPriv</code>, and a driver's power children all affect the power state of a driver. For legacy design reasons, they have overlapping functionality. Although you should call <code>changePowerStateToPriv</code> to change your device's power state, you might need to call <code>changePowerStateTo</code> in the following circumstances:
+    <ul><li>If a driver will be using <code>changePowerStateToPriv</code> to change its power state, it should call <code>changePowerStateTo(0)</code> in its <code>start</code> routine to eliminate the influence <code>changePowerStateTo</code> has on power state calculations.
+    <li>Call <code>changePowerStateTo</code> in conjunction with @link setIdleTimerPeriod setIdleTimerPeriod@/link and @link activityTickle activityTickle@/link to idle a driver into a low power state. For a driver with 3 power states, for example, <code>changePowerStateTo(1)</code> sets a minimum level of power state 1, such that the idle timer period may not set your device's power any lower than state 1.</ul>
+    @param ordinal The number of the desired power state in the power state array. 
+    @result A return code that can be ignored by the caller. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOReturn changePowerStateTo( unsigned long ordinal );
+
+/*! @function currentCapability
+    @abstract Finds out the capability of a device's current power state.
+    @result A copy of the <code>capabilityFlags</code> field for the current power state in the power state array. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    IOPMPowerFlags currentCapability( void );
+
+/*! @function currentPowerConsumption
+    @abstract Finds out the current power consumption of a device.
+    @discussion Most Mac OS X power managed drivers do not report their power consumption via the <code>staticPower</code> field. Thus this call will not accurately reflect power consumption for most drivers.
+    @result A copy of the <code>staticPower</code> field for the current power state in the power state array. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    unsigned long currentPowerConsumption( void );
+
+/*! @function activityTickle
+    @abstract Informs power management when a power-managed device is in use, so that power management can track when it is idle and adjust its power state accordingly.
+    @discussion The <code>activityTickle</code> method is provided for objects in the system (or for the driver itself) to tell a driver that its device is being used.
+    The IOService superclass can manage idleness determination with a simple idle timer mechanism and this <code>activityTickle</code> call. To start this up, the driver calls its superclass's <code>setIdleTimerPeriod</code>. This starts a timer for the time interval specified in the call. When the timer expires, the superclass checks to see if there has been any activity since the last timer expiration. (It checks to see if <code>activityTickle</code> has been called). If there has been activity, it restarts the timer, and this process continues. When the timer expires, and there has been no device activity, the superclass lowers the device power state to the next lower state. This can continue until the device is in state zero.
+    After the device has been powered down by at least one power state, a subsequent call to <code>activityTickle</code> causes the device to be switched to a higher state required for the activity.    
+    If the driver is managing the idleness determination totally on its own, the value of the <code>type</code> parameter should be <code>kIOPMSubclassPolicy</code>, and the driver should override the <code>activityTickle</code> method. The superclass IOService implementation of <code>activityTickle</code> does nothing with the <code>kIOPMSubclassPolicy</code> argument.
+    @param type When <code>type</code> is <code>kIOPMSubclassPolicy</code>, <code>activityTickle</code> is not handled in IOService and should be intercepted by the subclass. When <code>type</code> is <code>kIOPMSuperclassPolicy1</code>, an activity flag is set and the device state is checked. If the device has been powered down, it is powered up again.
+    @param stateNumber When <code>type</code> is <code>kIOPMSuperclassPolicy1</code>, <code>stateNumber</code> contains the desired power state ordinal for the activity. If the device is in a lower state, the superclass will switch it to this state. This is for devices that can handle some accesses in lower power states; the device is powered up only as far as it needs to be for the activity.
+    @result When <code>type</code> is <code>kIOPMSuperclassPolicy1</code>, the superclass returns <code>true</code> if the device is currently in the state specified by <code>stateNumber</code>. If the device is in a lower state and must be powered up, the superclass returns <code>false</code>; in this case the superclass will initiate a power change to power the device up. */
+
+    virtual bool activityTickle(
+                        unsigned long type,
+                        unsigned long stateNumber = 0 );
+
+/*! @function setAggressiveness
+    @abstract Broadcasts an aggressiveness factor from the parent of a driver to the driver.
+    @discussion Implement <code>setAggressiveness</code> to receive a notification when an "aggressiveness Aggressiveness factors are a loose set of power management variables that contain values for system sleep timeout, display sleep timeout, whether the system is on battery or AC, and other power management features. There are several aggressiveness factors that can be broadcast and a driver may take action on whichever factors apply to it.
+    A driver that has joined the power plane via @link joinPMtree joinPMtree@/link will receive <code>setAgressiveness</code> calls when aggressiveness factors change.
+    A driver may override this call if it needs to do something with the new factor (such as change its idle timeout). If overridden, the driver must  call its superclass's <code>setAgressiveness</code> method in its own <code>setAgressiveness</code> implementation.
+    Most drivers do not need to implement <code>setAgressiveness</code>.
+    @param type The aggressiveness factor type, such as <code>kPMMinutesToDim</code>, <code>kPMMinutesToSpinDown</code>, <code>kPMMinutesToSleep</code>, and <code>kPMPowerSource</code>. (Aggressiveness factors are defined in <code>pwr_mgt/IOPM.h</code>.)
+    @param newLevel The aggressiveness factor's new value.
+    @result <code>IOPMNoErr</code>. */
+
+    virtual IOReturn setAggressiveness(
+                        unsigned long type,
+                        unsigned long newLevel );
+
+/*! @function getAggressiveness
+    @abstract Returns the current aggressiveness value for the given type.
+    @param type The aggressiveness factor to query.
+    @param currentLevel Upon successful return, contains the value of aggressiveness factor <code>type</code>.
+    @result <code>kIOReturnSuccess</code> upon success; an I/O Kit error code otherwise. */
+
+    virtual IOReturn getAggressiveness(
+                        unsigned long type,
+                        unsigned long * currentLevel );
+
+#ifndef __LP64__
+/*! @function systemWake
+    @abstract Tells every driver in the power plane that the system is waking up.
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual IOReturn systemWake( void )
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function temperatureCriticalForZone
+    @abstract Alerts a driver to a critical temperature in some thermal zone.
+    @discussion This call is unused by power management. It is not intended to be called or overridden. */
+
+    virtual IOReturn temperatureCriticalForZone( IOService * whichZone )
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function youAreRoot
+    @abstract Informs power management which IOService object is the power plane root.
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual IOReturn youAreRoot( void )
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function setPowerParent
+    @abstract This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual IOReturn setPowerParent(
+                        IOPowerConnection * parent,
+                        bool stateKnown,
+                        IOPMPowerFlags currentState )
+    APPLE_KEXT_DEPRECATED;
+#endif /* !__LP64__ */
+
+/*! @function addPowerChild
+    @abstract Informs a driver that it has a new child.
+    @discussion The Platform Expert uses this method to call a driver and introduce it to a new child. This call is handled internally by power management. It is not intended to be overridden or called by drivers.
+    @param theChild A pointer to the child IOService object. */
+
+    virtual IOReturn addPowerChild( IOService * theChild );
+
+/*! @function removePowerChild
+    @abstract Informs a power managed driver that one of its power plane childen is disappearing.
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual IOReturn removePowerChild( IOPowerConnection * theChild );
+
+#ifndef __LP64__
+/*! @function command_received
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual void command_received( void *, void * , void * , void * );
+#endif
+
+/*! @function start_PM_idle_timer
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    APPLE_KEXT_COMPATIBILITY_VIRTUAL
+    void start_PM_idle_timer( void );
+
+#ifndef __LP64__
+/*! @function PM_idle_timer_expiration
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual void PM_idle_timer_expiration( void )
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function PM_Clamp_Timer_Expired
+    @discussion This call is handled internally by power management. It is not intended to be overridden or called by drivers. */
+
+    virtual void PM_Clamp_Timer_Expired( void )
+    APPLE_KEXT_DEPRECATED;
+#endif
+
+/*! @function setIdleTimerPeriod
+    @abstract Sets or changes the idle timer period.
+    @discussion A driver using the idleness determination provided by IOService calls its superclass with this method to set or change the idle timer period. See @link activityTickle activityTickle@/link for a description of this type of idleness determination.
+    @param period The desired idle timer period in seconds.
+    @result <code>kIOReturnSuccess</code> upon success; an I/O Kit error code otherwise. */
+
+    virtual IOReturn setIdleTimerPeriod( unsigned long );
+
+#ifndef __LP64__
+/*! @function getPMworkloop
+    @abstract Returns a pointer to the system-wide power management work loop.
+    @availability Deprecated in Mac OS X version 10.6.
+    @discussion Most drivers should create their own work loops to synchronize their code; drivers should not run arbitrary code on the power management work loop. */
+
+    virtual IOWorkLoop * getPMworkloop( void )
+    APPLE_KEXT_DEPRECATED;
+#endif
+
+/*! @function getPowerState
+    @abstract Determines a device's power state.
+    @discussion A device's "current power state" is updated at the end of each power state transition (e.g. transition from state 1 to state 0, or state 0 to state 2). This transition includes the time spent powering on or off any power plane children. Thus, if a child calls <code>getPowerState</code> on its power parent during system wake from sleep, the call will return the index to the device's off state rather than its on state.
+    @result The current power state's index into the device's power state array. */
+
+    UInt32 getPowerState( void );
+
+/*! @function setPowerState
+    @abstract Requests a power managed driver to change the power state of its device.
+    @discussion A power managed driver must override <code>setPowerState</code> to take part in system power management. After a driver is registered with power management, the system uses <code>setPowerState</code> to power the device off and on for system sleep and wake.
+    Calls to @link PMinit PMinit@/link and @link registerPowerDriver registerPowerDriver@/link enable power management to change a device's power state using <code>setPowerState</code>. <code>setPowerState</code> is called in a clean and separate thread context.
+    @param powerStateOrdinal The number in the power state array of the state the driver is being instructed to switch to. 
+    @param whatDevice A pointer to the power management object which registered to manage power for this device. In most cases, <code>whatDevice</code> will be equal to your driver's own <code>this</code> pointer.
+    @result The driver must return <code>IOPMAckImplied</code> if it has complied with the request when it returns. Otherwise if it has started the process of changing power state but not finished it, the driver should return a number of microseconds which is an upper limit of the time it will need to finish. Then, when it has completed the power switch, it should call @link acknowledgeSetPowerState acknowledgeSetPowerState@/link. */
+
+    virtual IOReturn setPowerState(
+                        unsigned long powerStateOrdinal,
+                        IOService *   whatDevice );
+
+#ifndef __LP64__
+/*! @function clampPowerOn
+    @abstract Deprecated. Do not use. */
+
+    virtual void clampPowerOn( unsigned long duration );
+#endif
+
+/*! @function maxCapabilityForDomainState
+    @abstract Determines a driver's highest power state possible for a given power domain state.
+    @discussion This happens when the power domain is changing state and power management needs to determine which state the device is capable of in the new domain state.
+    Most drivers do not need to implement this method, and can rely upon the default IOService implementation. The IOService implementation scans the power state array looking for the highest state whose <code>inputPowerRequirement</code> field exactly matches the value of the <code>domainState</code> parameter. If more intelligent determination is required, the driver itself should implement the method and override the superclass's implementation.
+    @param domainState Flags that describe the character of "domain power"; they represent the <code>outputPowerCharacter</code> field of a state in the power domain's power state array.
+    @result A state number. */
+
+    virtual unsigned long maxCapabilityForDomainState( IOPMPowerFlags domainState );
+
+/*! @function initialPowerStateForDomainState
+    @abstract Determines which power state a device is in, given the current power domain state.
+    @discussion Power management calls this method once, when the driver is initializing power management.
+    Most drivers do not need to implement this method, and can rely upon the default IOService implementation. The IOService implementation scans the power state array looking for the highest state whose <code>inputPowerRequirement</code> field exactly matches the value of the <code>domainState</code> parameter. If more intelligent determination is required, the power managed driver should implement the method and override the superclass's implementation.
+    @param domainState Flags that describe the character of "domain power"; they represent the <code>outputPowerCharacter</code> field of a state in the power domain's power state array.
+    @result A state number. */
+
+    virtual unsigned long initialPowerStateForDomainState( IOPMPowerFlags domainState );
+
+/*! @function powerStateForDomainState
+    @abstract Determines what power state the device would be in for a given power domain state.
+    @discussion This call is unused by power management. Drivers should override <code>initialPowerStateForDomainState</code> and/or <code>maxCapabilityForDomainState</code> instead to change the default mapping of domain state to driver power state.
+    @param domainState Flags that describe the character of "domain power"; they represent the <code>outputPowerCharacter</code> field of a state in the power domain's power state array.
+    @result A state number. */
+
+    virtual unsigned long powerStateForDomainState( IOPMPowerFlags domainState );
+
+/*! @function powerStateWillChangeTo
+    @abstract Informs interested parties that a device is about to change its power state.
+    @discussion Power management informs interested parties that a device is about to change to a different power state. Interested parties are those that have registered for this notification via @link registerInterestedDriver registerInterestedDriver@/link. If you have called <code>registerInterestedDriver</code> on a power managed driver, you must implement <code>powerStateWillChangeTo</code> and @link powerStateDidChangeTo powerStateDidChangeTo@/link to receive the notifications.
+    <code>powerStateWillChangeTo</code> is called in a clean and separate thread context. <code>powerStateWillChangeTo</code> is called before a power state transition takes place; <code>powerStateDidChangeTo</code> is called after the transition has completed.
+    @param capabilities Flags that describe the capability of the device in the new power state (they come from the <code>capabilityFlags</code> field of the new state in the power state array).
+    @param stateNumber The number of the state in the state array that the device is switching to.
+    @param whatDevice A pointer to the driver that is changing. It can be used by a driver that is receiving power state change notifications for multiple devices to distinguish between them.
+    @result The driver returns <code>IOPMAckImplied</code> if it has prepared for the power change when it returns. If it has started preparing but not finished, it should return a number of microseconds which is an upper limit of the time it will need to finish preparing. Then, when it has completed its preparations, it should call @link acknowledgePowerChange acknowledgePowerChange@/link. */
+
+    virtual IOReturn powerStateWillChangeTo(
+                        IOPMPowerFlags  capabilities,
+                        unsigned long   stateNumber,
+                        IOService *     whatDevice );
+
+/*! @function powerStateDidChangeTo
+    @abstract Informs interested parties that a device has changed to a different power state.
+    @discussion Power management informs interested parties that a device has changed to a different power state. Interested parties are those that have registered for this notification via @link registerInterestedDriver registerInterestedDriver@/link. If you have called <code>registerInterestedDriver</code> on a power managed driver, you must implemnt @link powerStateWillChangeTo powerStateWillChangeTo@/link and <code>powerStateDidChangeTo</code> to receive the notifications.
+    <code>powerStateDidChangeTo</code> is called in a clean and separate thread context. <code>powerStateWillChangeTo</code> is called before a power state transition takes place; <code>powerStateDidChangeTo</code> is called after the transition has completed.
+    @param capabilities Flags that describe the capability of the device in the new power state (they come from the <code>capabilityFlags</code> field of the new state in the power state array).
+    @param stateNumber The number of the state in the state array that the device is switching to.
+    @param whatDevice A pointer to the driver that is changing. It can be used by a driver that is receiving power state change notifications for multiple devices to distinguish between them.
+    @result The driver returns <code>IOPMAckImplied</code> if it has prepared for the power change when it returns. If it has started preparing but not finished, it should return a number of microseconds which is an upper limit of the time it will need to finish preparing. Then, when it has completed its preparations, it should call @link acknowledgePowerChange acknowledgePowerChange@/link. */
+
+    virtual IOReturn powerStateDidChangeTo(
+                        IOPMPowerFlags  capabilities,
+                        unsigned long   stateNumber,
+                        IOService *     whatDevice );
+
+#ifndef __LP64__
+/*! @function didYouWakeSystem
+    @abstract Asks a driver if its device is the one that just woke the system from sleep.
+    @availability Deprecated in Mac OS X version 10.6.
+    @discussion Power management calls a power managed driver with this method to ask if its device is the one that just woke the system from sleep. If a device is capable of waking the system from sleep, its driver should implement <code>didYouWakeSystem</code> and return <code>true</code> if its device was responsible for waking the system.
+    @result <code>true</code> if the driver's device woke the system and <code>false</code> otherwise. */
+
+    virtual bool didYouWakeSystem( void )
+    APPLE_KEXT_DEPRECATED;
+
+/*! @function newTemperature
+    @abstract Tells a power managed driver that the temperature in the thermal zone has changed.
+    @discussion This call is unused by power management. It is not intended to be called or overridden. */
+
+    virtual IOReturn newTemperature( long currentTemp, IOService * whichZone )
+    APPLE_KEXT_DEPRECATED;
+#endif
+
+    virtual bool askChangeDown( unsigned long );
+    virtual bool tellChangeDown( unsigned long );
+    virtual void tellNoChangeDown ( unsigned long );
+    virtual void tellChangeUp( unsigned long );
+    virtual IOReturn allowPowerChange( unsigned long refcon );
+    virtual IOReturn cancelPowerChange( unsigned long refcon );
+
+protected:
+/*! @function changePowerStateToPriv 
+    @abstract Tells a driver's superclass to change the power state of its device.
+    @discussion A driver uses this method to tell its superclass to change the power state of the device. This is the recommended way to change the power state of a device.
+    Three things affect driver power state: @link changePowerStateTo changePowerStateTo@/link, <code>changePowerStateToPriv</code>, and the desires of the driver's power plane children. Power management puts the device into the maximum state governed by those three entities.
+    Drivers may eliminate the influence of the <code>changePowerStateTo</code> method on power state one of two ways. See @link powerOverrideOnPriv powerOverrideOnPriv@/link to ignore the method's influence, or call <code>changePowerStateTo(0)</code> in the driver's <code>start</code> routine to remove the <code>changePowerStateTo</code> method's power request.
+    @param ordinal The number of the desired power state in the power state array.
+    @result A return code that can be ignored by the caller. */
+
+    IOReturn changePowerStateToPriv( unsigned long ordinal );
+
+/*! @function powerOverrideOnPriv
+    @abstract Allows a driver to ignore its children's power management requests and only use changePowerStateToPriv to define its own power state.
+    @discussion Power management normally keeps a device at the highest state required by its requests via @link changePowerStateTo changePowerStateTo@/link, @link changePowerStateToPriv changePowerStateToPriv@/link, and its children. However, a driver may ensure a lower power state than otherwise required by itself and its children using <code>powerOverrideOnPriv</code>. When the override is on, power management keeps the device's power state in the state specified by <code>changePowerStateToPriv</code>. Turning on the override will initiate a power change if the driver's <code>changePowerStateToPriv</code> desired power state is different from the maximum of the <code>changePowerStateTo</code> desired power state and the children's desires.
+    @result A return code that can be ignored by the caller. */
+
+    IOReturn powerOverrideOnPriv( void );
+
+/*! @function powerOverrideOffPriv
+    @abstract Allows a driver to disable a power override.
+    @discussion When a driver has enabled an override via @link powerOverrideOnPriv powerOverrideOnPriv@/link, it can disable it again by calling this method in its superclass. Disabling the override reverts to the default algorithm for determining a device's power state. The superclass will now keep the device at the highest state required by <code>changePowerStateTo</code>, <code>changePowerStateToPriv</code>, and its children. Turning off the override will initiate a power change if the driver's desired power state is different from the maximum of the power managed driver's desire and the children's desires.
+    @result A return code that can be ignored by the caller. */
+
+    IOReturn powerOverrideOffPriv( void );
+
+/*! @function powerChangeDone
+    @abstract Tells a driver when a power state change is complete.
+    @discussion Power management uses this method to inform a driver when a power change is completely done, when all interested parties have acknowledged the @link powerStateDidChangeTo powerStateDidChangeTo@/link call. The default implementation of this method is null; the method is meant to be overridden by subclassed power managed drivers. A driver should use this method to find out if a power change it initiated is complete.
+    @param stateNumber The number of the state in the state array that the device has switched from. */
+
+    virtual void powerChangeDone( unsigned long stateNumber );
+#ifdef XNU_KERNEL_PRIVATE
+    /* Power management internals */
+public:
+    void idleTimerExpired( void );
+    void settleTimerExpired( void );
+    IOReturn synchronizePowerTree( IOOptionBits options = 0, IOService * notifyRoot = 0 );
+    bool assertPMDriverCall( IOPMDriverCallEntry * callEntry, IOOptionBits options = 0, IOPMinformee * inform = 0 );
+    void deassertPMDriverCall( IOPMDriverCallEntry * callEntry );
+    IOReturn changePowerStateWithOverrideTo( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );
+    IOReturn changePowerStateForRootDomain( IOPMPowerStateIndex ordinal );
+    IOReturn setIgnoreIdleTimer( bool ignore );
+    IOReturn quiescePowerTree( void * target, IOPMCompletionAction action, void * param );
+    uint32_t getPowerStateForClient( const OSSymbol * client );
+    static const char * getIOMessageString( uint32_t msg );
+    static void setAdvisoryTickleEnable( bool enable );
+    void reset_watchdog_timer( void );
+    void start_watchdog_timer ( void );
+    bool stop_watchdog_timer ( void );
+    IOReturn registerInterestForNotifer( IONotifier *notify, const OSSymbol * typeOfInterest,
+                  IOServiceInterestHandler handler, void * target, void * ref );
+
+#ifdef __LP64__
+    static IOWorkLoop * getPMworkloop( void );
+#endif
+
+protected:
+    bool tellClientsWithResponse( int messageType );
+    void tellClients( int messageType );
+    void PMDebug( uint32_t event, uintptr_t param1, uintptr_t param2 );
+
+private:
+#ifndef __LP64__
+    void ack_timer_ticked ( void );
+    IOReturn serializedAllowPowerChange2 ( unsigned long );
+    IOReturn serializedCancelPowerChange2 ( unsigned long );
+    IOReturn powerDomainWillChangeTo( IOPMPowerFlags, IOPowerConnection * );
+    IOReturn powerDomainDidChangeTo( IOPMPowerFlags, IOPowerConnection * );
+#endif
+    void PMfree( void );
+    bool tellChangeDown1 ( unsigned long );
+    bool tellChangeDown2 ( unsigned long );
+    IOReturn startPowerChange( IOPMPowerChangeFlags, IOPMPowerStateIndex, IOPMPowerFlags, IOPowerConnection *, IOPMPowerFlags );
+    void setParentInfo ( IOPMPowerFlags, IOPowerConnection *, bool );
+    IOReturn notifyAll ( uint32_t nextMS );
+    bool notifyChild ( IOPowerConnection * child );
+    IOPMPowerStateIndex getPowerStateForDomainFlags( IOPMPowerFlags flags );
+
+    // power change initiated by driver
+    void OurChangeStart( void );
+    void OurSyncStart ( void );
+    void OurChangeTellClientsPowerDown ( void );
+    void OurChangeTellUserPMPolicyPowerDown ( void );
+    void OurChangeTellPriorityClientsPowerDown ( void );
+    void OurChangeTellCapabilityWillChange ( void );
+    void OurChangeNotifyInterestedDriversWillChange ( void );
+    void OurChangeSetPowerState ( void );
+    void OurChangeWaitForPowerSettle ( void );
+    void OurChangeNotifyInterestedDriversDidChange ( void );
+    void OurChangeTellCapabilityDidChange ( void );
+    void OurChangeFinish ( void );
+
+    // downward power change initiated by a power parent
+    IOReturn ParentChangeStart( void );
+    void ParentChangeTellPriorityClientsPowerDown ( void );
+    void ParentChangeTellCapabilityWillChange ( void );
+    void ParentChangeNotifyInterestedDriversWillChange ( void );
+    void ParentChangeSetPowerState ( void );
+    void ParentChangeWaitForPowerSettle ( void );
+    void ParentChangeNotifyInterestedDriversDidChange ( void );
+    void ParentChangeTellCapabilityDidChange ( void );
+    void ParentChangeAcknowledgePowerChange ( void );
+    void ParentChangeRootChangeDown( void );
+
+    void all_done ( void );
+    void start_ack_timer ( void );
+    void stop_ack_timer ( void );
+    void start_ack_timer( UInt32 value, UInt32 scale );
+    void startSettleTimer( void );
+    void start_spindump_timer( const char * delay_type );
+    void stop_spindump_timer( void );
+    bool checkForDone ( void );
+    bool responseValid ( uint32_t x, int pid );
+    void computeDesiredState( unsigned long tempDesire, bool computeOnly );
+    void trackSystemSleepPreventers( IOPMPowerStateIndex, IOPMPowerStateIndex, IOPMPowerChangeFlags );
+    void tellSystemCapabilityChange( uint32_t nextMS );
+    void restartIdleTimer( void );
+
+    static void ack_timer_expired( thread_call_param_t, thread_call_param_t );
+    static void watchdog_timer_expired ( thread_call_param_t arg0, thread_call_param_t arg1 );
+    static void spindump_timer_expired( thread_call_param_t arg0, thread_call_param_t arg1 );
+    static IOReturn actionAckTimerExpired(OSObject *, void *, void *, void *, void * );
+    static IOReturn watchdog_timer_expired ( OSObject *, void *, void *, void *, void * );
+    static IOReturn actionSpinDumpTimerExpired(OSObject *, void *, void *, void *, void * );
+
+    static IOReturn actionDriverCalloutDone(OSObject *, void *, void *, void *, void * );
+    static IOPMRequest * acquirePMRequest( IOService * target, IOOptionBits type, IOPMRequest * active = 0 );
+    static void releasePMRequest( IOPMRequest * request );
+    static void pmDriverCallout( IOService * from );
+    static void pmTellAppWithResponse( OSObject * object, void * context );
+    static void pmTellClientWithResponse( OSObject * object, void * context );
+    static void pmTellCapabilityAppWithResponse ( OSObject * object, void * arg );
+    static void pmTellCapabilityClientWithResponse( OSObject * object, void * arg );
+    static void submitPMRequest( IOPMRequest * request );
+    static void submitPMRequests( IOPMRequest ** request, IOItemCount count );
+    bool ackTimerTick( void );
+    void addPowerChild1( IOPMRequest * request );
+    void addPowerChild2( IOPMRequest * request );
+    void addPowerChild3( IOPMRequest * request );
+    void adjustPowerState( uint32_t clamp = 0 );
+    void handlePMstop( IOPMRequest * request );
+    void handleRegisterPowerDriver( IOPMRequest * request );
+    bool handleAcknowledgePowerChange( IOPMRequest * request );
+    void handlePowerDomainWillChangeTo( IOPMRequest * request );
+    void handlePowerDomainDidChangeTo( IOPMRequest * request );
+    void handleRequestPowerState( IOPMRequest * request );
+    void handlePowerOverrideChanged( IOPMRequest * request );
+    void handleActivityTickle( IOPMRequest * request );
+    void handleInterestChanged( IOPMRequest * request );
+    void handleSynchronizePowerTree( IOPMRequest * request );
+    void executePMRequest( IOPMRequest * request );
+    bool actionPMWorkQueueInvoke( IOPMRequest * request, IOPMWorkQueue * queue );
+    bool actionPMWorkQueueRetire( IOPMRequest * request, IOPMWorkQueue * queue );
+    bool actionPMRequestQueue( IOPMRequest * request, IOPMRequestQueue * queue );
+    bool actionPMReplyQueue( IOPMRequest * request, IOPMRequestQueue * queue );
+    bool actionPMCompletionQueue( IOPMRequest * request, IOPMCompletionQueue * queue );
+    bool notifyInterestedDrivers( void );
+    void notifyInterestedDriversDone( void );
+    bool notifyControllingDriver( void );
+    void notifyControllingDriverDone( void );
+    void driverSetPowerState( void );
+    void driverInformPowerChange( void );
+    bool isPMBlocked( IOPMRequest * request, int count );
+    void notifyChildren( void );
+    void notifyChildrenOrdered( void );
+    void notifyChildrenDelayed( void );
+    void notifyRootDomain( void );
+    void notifyRootDomainDone( void );
+    void cleanClientResponses ( bool logErrors );
+    void updatePowerClient( const OSSymbol * client, uint32_t powerState );
+    void removePowerClient( const OSSymbol * client );
+    IOReturn requestPowerState( const OSSymbol * client, uint32_t state );
+    IOReturn requestDomainPower( IOPMPowerStateIndex ourPowerState, IOOptionBits options = 0 );
+    IOReturn configurePowerStatesReport( IOReportConfigureAction action, void *result );
+    IOReturn updatePowerStatesReport( IOReportConfigureAction action, void *result, void *destination );
+    IOReturn configureSimplePowerReport(IOReportConfigureAction action, void *result );
+    IOReturn updateSimplePowerReport( IOReportConfigureAction action, void *result, void *destination );
+    void waitForPMDriverCall( IOService * target = 0 );
+#endif /* XNU_KERNEL_PRIVATE */
+};
+
+#endif /* ! _IOKIT_IOSERVICE_H */
diff -Nur xnu-3247.1.106/iokit/Kernel/IOCatalogue.cpp xnu-3247.1.106-AnV/iokit/Kernel/IOCatalogue.cpp
--- xnu-3247.1.106/iokit/Kernel/IOCatalogue.cpp	2015-12-06 01:32:07.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/Kernel/IOCatalogue.cpp	2015-12-13 17:08:10.000000000 +0100
@@ -299,8 +299,11 @@
     OSCollectionIterator * iter = NULL;       // must release
     OSOrderedSet         * set = NULL;        // must release
     OSObject             * object = NULL;       // do not release
+    //OSDictionary         * dict = NULL;       // do not release
     OSArray              * persons = NULL;    // do not release
-    
+    OSString             * moduleName;
+    bool                   ret;
+
     persons = OSDynamicCast(OSArray, drivers);
     if (!persons) {
         goto finish;
@@ -323,11 +326,55 @@
 
     IORWLockWrite(lock);
     while ( (object = iter->getNextObject()) ) {
-    
-        // xxx Deleted OSBundleModuleDemand check; will handle in other ways for SL
-
         OSDictionary * personality = OSDynamicCast(OSDictionary, object);
 
+        if (blacklistEnabled) {
+            OSString *modName = OSDynamicCast(OSString, personality->getObject(gIOModuleIdentifierKey));
+            const char *modNameStr = NULL;
+            if (modName)
+                modNameStr = modName->getCStringNoCopy();
+            if (modNameStr) {
+                boolean_t shouldMatch = TRUE;
+                for (uint32_t n = 0; blacklistMods[n].name; n++) {
+                    if (strcmp(blacklistMods[n].name, modNameStr))
+                        continue;
+                    if (!blacklistMods[n].hits++)
+                        printf("warning: skipping personalities in blacklisted kext %s\n",
+                            modNameStr);
+                    shouldMatch = FALSE;
+                }
+                if (!shouldMatch)
+                    continue;
+            }
+        }
+        if (confblacklistEnabled) {
+            OSString *modName = OSDynamicCast(OSString, personality->getObject(gIOModuleIdentifierKey));
+            const char *modNameStr = NULL;
+            if (modName)
+                modNameStr = modName->getCStringNoCopy();
+            if (modNameStr) {
+                boolean_t shouldMatch = TRUE;
+                for (uint32_t n = 0; n < confblacklistCount; n++) {
+                    if (strcmp(confblacklistMods[n].name, modNameStr))
+                        continue;
+                    if (!confblacklistMods[n].hits++)
+                        printf("warning: skipping personalities in user blacklisted kext %s\n", modNameStr);
+                    shouldMatch = FALSE;
+                }
+                if (!shouldMatch)
+                    continue;
+            }
+        }
+        if ((moduleName = OSDynamicCast(OSString, personality->getObject("OSBundleModuleDemand"))))
+        {
+            IORWLockUnlock(lock);
+            ret = OSKext::loadKextWithIdentifier(moduleName->getCStringNoCopy(), false);
+            IORWLockWrite(lock);
+            ret = true;
+        }
+        else
+/*** in else Start ***/
+        {
         SInt count;
 
         if (!personality) {
@@ -371,7 +418,9 @@
 	    }
         }
 
-	set->setObject(personality);        
+	set->setObject(personality);
+    }
+/*** in else END ***/
     }
     // Start device matching.
     if (result && doNubMatching && (set->getCount() > 0)) {
diff -Nur xnu-3247.1.106/iokit/Kernel/IOPlatformExpert.cpp xnu-3247.1.106-AnV/iokit/Kernel/IOPlatformExpert.cpp
--- xnu-3247.1.106/iokit/Kernel/IOPlatformExpert.cpp	2015-12-06 01:32:09.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/Kernel/IOPlatformExpert.cpp	2015-12-13 17:08:10.000000000 +0100
@@ -761,10 +761,16 @@
 
 boolean_t PEGetModelName( char * name, int maxLength )
 {
-    if( gIOPlatform)
-	return( gIOPlatform->getModelName( name, maxLength ));
-    else
-	return( false );
+    OSData *prop;
+
+    /* Eureka: Get the model name directly from property instead of calling getModelName(). */
+    prop = (OSData *) IOService::getPlatform()->getProvider()->getProperty(gIODTModelKey);
+    if (prop) {
+     	strlcpy(name, (const char *) prop->getBytesNoCopy(), maxLength - 1);
+       	return true;
+    }
+    
+    return false;
 }
 
 int PEGetPlatformEpoch(void)
diff -Nur xnu-3247.1.106/iokit/Kernel/IOPlatformExpert.cpp.orig xnu-3247.1.106-AnV/iokit/Kernel/IOPlatformExpert.cpp.orig
--- xnu-3247.1.106/iokit/Kernel/IOPlatformExpert.cpp.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/Kernel/IOPlatformExpert.cpp.orig	2015-12-06 01:32:09.000000000 +0100
@@ -0,0 +1,1621 @@
+/*
+ * Copyright (c) 1998-2014 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+ 
+#include <IOKit/IOCPU.h>
+#include <IOKit/IODeviceTreeSupport.h>
+#include <IOKit/IOKitDebug.h>
+#include <IOKit/IOMapper.h>
+#include <IOKit/IOMessage.h>
+#include <IOKit/IONVRAM.h>
+#include <IOKit/IOPlatformExpert.h>
+#include <IOKit/IORangeAllocator.h>
+#include <IOKit/IOWorkLoop.h>
+#include <IOKit/pwr_mgt/RootDomain.h>
+#include <IOKit/IOKitKeys.h>
+#include <IOKit/IOTimeStamp.h>
+#include <IOKit/IOUserClient.h>
+#include <IOKit/IOKitDiagnosticsUserClient.h>
+
+#include <IOKit/system.h>
+
+#include <libkern/c++/OSContainers.h>
+#include <libkern/crypto/sha1.h>
+#include <libkern/OSAtomic.h>
+
+extern "C" {
+#include <machine/machine_routines.h>
+#include <pexpert/pexpert.h>
+#include <uuid/uuid.h>
+}
+
+void printDictionaryKeys (OSDictionary * inDictionary, char * inMsg);
+static void getCStringForObject(OSObject *inObj, char *outStr, size_t outStrLen);
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+#define super IOService
+
+OSDefineMetaClassAndStructors(IOPlatformExpert, IOService)
+
+OSMetaClassDefineReservedUsed(IOPlatformExpert,  0);
+OSMetaClassDefineReservedUsed(IOPlatformExpert,  1);
+OSMetaClassDefineReservedUsed(IOPlatformExpert,  2);
+OSMetaClassDefineReservedUsed(IOPlatformExpert,  3);
+OSMetaClassDefineReservedUsed(IOPlatformExpert,  4);
+
+OSMetaClassDefineReservedUnused(IOPlatformExpert,  5);
+OSMetaClassDefineReservedUnused(IOPlatformExpert,  6);
+OSMetaClassDefineReservedUnused(IOPlatformExpert,  7);
+OSMetaClassDefineReservedUnused(IOPlatformExpert,  8);
+OSMetaClassDefineReservedUnused(IOPlatformExpert,  9);
+OSMetaClassDefineReservedUnused(IOPlatformExpert, 10);
+OSMetaClassDefineReservedUnused(IOPlatformExpert, 11);
+
+static IOPlatformExpert * gIOPlatform;
+static OSDictionary * gIOInterruptControllers;
+static IOLock * gIOInterruptControllersLock;
+static IODTNVRAM *gIOOptionsEntry;
+
+OSSymbol * gPlatformInterruptControllerName;
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+bool IOPlatformExpert::attach( IOService * provider )
+{
+
+    if( !super::attach( provider ))
+	return( false);
+
+    return( true);
+}
+
+bool IOPlatformExpert::start( IOService * provider )
+{
+    IORangeAllocator *	physicalRanges;
+    OSData *		busFrequency;
+    uint32_t		debugFlags;
+    
+    if (!super::start(provider))
+      return false;
+    
+    // Override the mapper present flag is requested by boot arguments.
+    if (PE_parse_boot_argn("dart", &debugFlags, sizeof (debugFlags)) && (debugFlags == 0))
+      removeProperty(kIOPlatformMapperPresentKey);
+    if (PE_parse_boot_argn("-x", &debugFlags, sizeof (debugFlags)))
+      removeProperty(kIOPlatformMapperPresentKey);
+
+    // Register the presence or lack thereof a system 
+    // PCI address mapper with the IOMapper class
+    IOMapper::setMapperRequired(0 != getProperty(kIOPlatformMapperPresentKey));
+    
+    gIOInterruptControllers = OSDictionary::withCapacity(1);
+    gIOInterruptControllersLock = IOLockAlloc();
+    
+    // Correct the bus frequency in the device tree.
+    busFrequency = OSData::withBytesNoCopy((void *)&gPEClockFrequencyInfo.bus_clock_rate_hz, 4);
+    provider->setProperty("clock-frequency", busFrequency);
+    busFrequency->release();
+    
+    gPlatformInterruptControllerName = (OSSymbol *)OSSymbol::withCStringNoCopy("IOPlatformInterruptController");
+    
+    physicalRanges = IORangeAllocator::withRange(0xffffffff, 1, 16,
+						 IORangeAllocator::kLocking);
+    assert(physicalRanges);
+    setProperty("Platform Memory Ranges", physicalRanges);
+    
+    setPlatform( this );
+    gIOPlatform = this;
+    
+    PMInstantiatePowerDomains();
+    
+    // Parse the serial-number data and publish a user-readable string
+    OSData* mydata = (OSData*) (provider->getProperty("serial-number"));
+    if (mydata != NULL) {
+        OSString *serNoString = createSystemSerialNumberString(mydata);
+        if (serNoString != NULL) {
+            provider->setProperty(kIOPlatformSerialNumberKey, serNoString);
+            serNoString->release();
+        }
+    }
+    
+    return( configure(provider) );
+}
+
+bool IOPlatformExpert::configure( IOService * provider )
+{
+    OSSet *		topLevel;
+    OSDictionary *	dict;
+    IOService * 	nub;
+
+    topLevel = OSDynamicCast( OSSet, getProperty("top-level"));
+
+    if( topLevel) {
+        while( (dict = OSDynamicCast( OSDictionary,
+				topLevel->getAnyObject()))) {
+            dict->retain();
+            topLevel->removeObject( dict );
+            nub = createNub( dict );
+            if( 0 == nub)
+                continue;
+            dict->release();
+            nub->attach( this );
+            nub->registerService();
+        }
+    }
+
+    return( true );
+}
+
+IOService * IOPlatformExpert::createNub( OSDictionary * from )
+{
+    IOService *		nub;
+
+    nub = new IOPlatformDevice;
+    if(nub) {
+	if( !nub->init( from )) {
+	    nub->release();
+	    nub = 0;
+	}
+    }
+    return( nub);
+}
+
+bool IOPlatformExpert::compareNubName( const IOService * nub,
+				OSString * name, OSString ** matched ) const
+{
+    return( nub->IORegistryEntry::compareName( name, matched ));
+}
+
+IOReturn IOPlatformExpert::getNubResources( IOService * nub )
+{
+    return( kIOReturnSuccess );
+}
+
+long IOPlatformExpert::getBootROMType(void)
+{
+  return _peBootROMType;
+}
+
+long IOPlatformExpert::getChipSetType(void)
+{
+  return _peChipSetType;
+}
+
+long IOPlatformExpert::getMachineType(void)
+{
+  return _peMachineType;
+}
+
+void IOPlatformExpert::setBootROMType(long peBootROMType)
+{
+  _peBootROMType = peBootROMType;
+}
+
+void IOPlatformExpert::setChipSetType(long peChipSetType)
+{
+  _peChipSetType = peChipSetType;
+}
+
+void IOPlatformExpert::setMachineType(long peMachineType)
+{
+  _peMachineType = peMachineType;
+}
+
+bool IOPlatformExpert::getMachineName( char * /*name*/, int /*maxLength*/)
+{
+    return( false );
+}
+
+bool IOPlatformExpert::getModelName( char * /*name*/, int /*maxLength*/)
+{
+    return( false );
+}
+
+OSString* IOPlatformExpert::createSystemSerialNumberString(OSData* myProperty)
+{
+    return NULL;
+}
+
+IORangeAllocator * IOPlatformExpert::getPhysicalRangeAllocator(void)
+{
+    return(OSDynamicCast(IORangeAllocator,
+			getProperty("Platform Memory Ranges")));
+}
+
+int (*PE_halt_restart)(unsigned int type) = 0;
+
+int IOPlatformExpert::haltRestart(unsigned int type)
+{
+  if (type == kPEPanicSync) return 0;
+
+  if (type == kPEHangCPU) while (true) {}
+
+  if (type == kPEUPSDelayHaltCPU) {
+    // RestartOnPowerLoss feature was turned on, proceed with shutdown.
+    type = kPEHaltCPU;
+  }
+
+  // On ARM kPEPanicRestartCPU is supported in the drivers
+  if (type == kPEPanicRestartCPU)
+	  type = kPERestartCPU;
+
+  if (PE_halt_restart) return (*PE_halt_restart)(type);
+  else return -1;
+}
+
+void IOPlatformExpert::sleepKernel(void)
+{
+#if 0
+  long cnt;
+  boolean_t intState;
+  
+  intState = ml_set_interrupts_enabled(false);
+  
+  for (cnt = 0; cnt < 10000; cnt++) {
+    IODelay(1000);
+  }
+  
+  ml_set_interrupts_enabled(intState);
+#else
+//  PE_initialize_console(0, kPEDisableScreen);
+  
+  IOCPUSleepKernel();
+  
+//  PE_initialize_console(0, kPEEnableScreen);
+#endif
+}
+
+long IOPlatformExpert::getGMTTimeOfDay(void)
+{
+    return(0);
+}
+
+void IOPlatformExpert::setGMTTimeOfDay(long secs)
+{
+}
+
+
+IOReturn IOPlatformExpert::getConsoleInfo( PE_Video * consoleInfo )
+{
+    return( PE_current_console( consoleInfo));
+}
+
+IOReturn IOPlatformExpert::setConsoleInfo( PE_Video * consoleInfo,
+						unsigned int op)
+{
+    return( PE_initialize_console( consoleInfo, op ));
+}
+
+IOReturn IOPlatformExpert::registerInterruptController(OSSymbol *name, IOInterruptController *interruptController)
+{
+  IOLockLock(gIOInterruptControllersLock);
+  
+  gIOInterruptControllers->setObject(name, interruptController);
+  
+  IOLockWakeup(gIOInterruptControllersLock,
+		gIOInterruptControllers, /* one-thread */ false);
+
+  IOLockUnlock(gIOInterruptControllersLock);
+  
+  return kIOReturnSuccess;
+}
+
+IOReturn IOPlatformExpert::deregisterInterruptController(OSSymbol *name)
+{
+  IOLockLock(gIOInterruptControllersLock);
+  
+  gIOInterruptControllers->removeObject(name);
+  
+  IOLockUnlock(gIOInterruptControllersLock);
+  
+  return kIOReturnSuccess;
+}
+
+IOInterruptController *IOPlatformExpert::lookUpInterruptController(OSSymbol *name)
+{
+  OSObject              *object;
+  
+  IOLockLock(gIOInterruptControllersLock);
+  while (1) {
+    
+    object = gIOInterruptControllers->getObject(name);
+    
+    if (object != 0)
+	break;
+    
+    IOLockSleep(gIOInterruptControllersLock,
+		gIOInterruptControllers, THREAD_UNINT);
+  }
+  
+  IOLockUnlock(gIOInterruptControllersLock);
+  return OSDynamicCast(IOInterruptController, object);
+}
+
+
+void IOPlatformExpert::setCPUInterruptProperties(IOService *service)
+{
+  IOCPUInterruptController *controller;
+  
+  controller = OSDynamicCast(IOCPUInterruptController, waitForService(serviceMatching("IOCPUInterruptController")));
+  if (controller) controller->setCPUInterruptProperties(service);
+}
+
+bool IOPlatformExpert::atInterruptLevel(void)
+{
+  return ml_at_interrupt_context();
+}
+
+bool IOPlatformExpert::platformAdjustService(IOService */*service*/)
+{
+  return true;
+}
+
+void IOPlatformExpert::getUTCTimeOfDay(clock_sec_t * secs, clock_nsec_t * nsecs)
+{
+  *secs = getGMTTimeOfDay();
+  *nsecs = 0;
+}
+
+void IOPlatformExpert::setUTCTimeOfDay(clock_sec_t secs, __unused clock_nsec_t nsecs)
+{
+  setGMTTimeOfDay(secs);
+}
+
+
+//*********************************************************************************
+// PMLog
+//
+//*********************************************************************************
+
+void IOPlatformExpert::
+PMLog(const char *who, unsigned long event,
+      unsigned long param1, unsigned long param2)
+{
+	clock_sec_t nows;
+	clock_usec_t nowus;
+	clock_get_system_microtime(&nows, &nowus);
+	nowus += (nows % 1000) * 1000000;
+
+    kprintf("pm%u %p %.30s %d %lx %lx\n",
+		nowus, OBFUSCATE(current_thread()), who,	// Identity
+		(int) event, (long)OBFUSCATE(param1), (long)OBFUSCATE(param2));			// Args
+}
+
+
+//*********************************************************************************
+// PMInstantiatePowerDomains
+//
+// In this vanilla implementation, a Root Power Domain is instantiated.
+// All other objects which register will be children of this Root.
+// Where this is inappropriate, PMInstantiatePowerDomains is overridden 
+// in a platform-specific subclass.
+//*********************************************************************************
+
+void IOPlatformExpert::PMInstantiatePowerDomains ( void )
+{
+    root = new IOPMrootDomain;
+    root->init();
+    root->attach(this);
+    root->start(this);
+}
+
+
+//*********************************************************************************
+// PMRegisterDevice
+//
+// In this vanilla implementation, all callers are made children of the root power domain.
+// Where this is inappropriate, PMRegisterDevice is overridden in a platform-specific subclass.
+//*********************************************************************************
+
+void IOPlatformExpert::PMRegisterDevice(IOService * theNub, IOService * theDevice)
+{
+    root->addPowerChild ( theDevice );
+}
+
+//*********************************************************************************
+// hasPMFeature
+//
+//*********************************************************************************
+
+bool IOPlatformExpert::hasPMFeature (unsigned long featureMask)
+{
+  return ((_pePMFeatures & featureMask) != 0);
+}
+
+//*********************************************************************************
+// hasPrivPMFeature
+//
+//*********************************************************************************
+
+bool IOPlatformExpert::hasPrivPMFeature (unsigned long privFeatureMask)
+{
+  return ((_pePrivPMFeatures & privFeatureMask) != 0);
+}
+
+//*********************************************************************************
+// numBatteriesSupported
+//
+//*********************************************************************************
+
+int IOPlatformExpert::numBatteriesSupported (void)
+{
+  return (_peNumBatteriesSupported);
+}
+
+//*********************************************************************************
+// CheckSubTree
+//
+// This method is called by the instantiated sublass of the platform expert to
+// determine how a device should be inserted into the Power Domain. The subclass
+// provides an XML power tree description against which a device is matched based
+// on class and provider. If a match is found this routine returns true in addition
+// to flagging the description tree at the appropriate node that a device has been
+// registered for the given service.
+//*********************************************************************************
+
+bool IOPlatformExpert::CheckSubTree (OSArray * inSubTree, IOService * theNub, IOService * theDevice, OSDictionary * theParent)
+{
+  unsigned int    i;
+  unsigned int    numPowerTreeNodes;
+  OSDictionary *  entry;
+  OSDictionary *  matchingDictionary;
+  OSDictionary *  providerDictionary;
+  OSDictionary *  deviceDictionary;
+  OSDictionary *  nubDictionary;
+  OSArray *       children;
+  bool            nodeFound            = false;
+  bool            continueSearch       = false;
+  bool            deviceMatch          = false;
+  bool            providerMatch        = false;
+  bool            multiParentMatch     = false;
+
+  if ( (NULL == theDevice) || (NULL == inSubTree) )
+    return false;
+
+  numPowerTreeNodes = inSubTree->getCount ();
+
+  // iterate through the power tree to find a home for this device
+
+  for ( i = 0; i < numPowerTreeNodes; i++ ) {
+
+    entry =  (OSDictionary *) inSubTree->getObject (i);
+
+    matchingDictionary = (OSDictionary *) entry->getObject ("device");
+    providerDictionary = (OSDictionary *) entry->getObject ("provider");
+
+    deviceMatch = true; // if no matching dictionary, this is not a criteria and so must match
+    if ( matchingDictionary ) {
+      deviceMatch = false;
+      if ( NULL != (deviceDictionary = theDevice->dictionaryWithProperties ())) {
+        deviceMatch = deviceDictionary->isEqualTo ( matchingDictionary, matchingDictionary );
+        deviceDictionary->release ();
+      }
+    }
+
+    providerMatch = true; // we indicate a match if there is no nub or provider
+    if ( theNub && providerDictionary ) {
+      providerMatch = false;
+      if ( NULL != (nubDictionary = theNub->dictionaryWithProperties ()) ) {
+        providerMatch = nubDictionary->isEqualTo ( providerDictionary,  providerDictionary );
+        nubDictionary->release ();
+      }
+    }
+
+    multiParentMatch = true; // again we indicate a match if there is no multi-parent node
+    if (deviceMatch && providerMatch) {
+      if (NULL != multipleParentKeyValue) {
+        OSNumber * aNumber = (OSNumber *) entry->getObject ("multiple-parent");
+        multiParentMatch   = (NULL != aNumber) ? multipleParentKeyValue->isEqualTo (aNumber) : false;
+      }
+    }
+
+    nodeFound = (deviceMatch && providerMatch && multiParentMatch);
+
+    // if the power tree specifies a provider dictionary but theNub is
+    // NULL then we cannot match with this entry.
+
+    if ( theNub == NULL && providerDictionary != NULL )
+      nodeFound = false;
+  
+    // if this node is THE ONE...then register the device
+
+    if ( nodeFound ) {
+      if (RegisterServiceInTree (theDevice, entry, theParent, theNub) ) {
+
+        if ( kIOLogPower & gIOKitDebug)
+          IOLog ("PMRegisterDevice/CheckSubTree - service registered!\n");
+
+	numInstancesRegistered++;
+
+	// determine if we need to search for additional nodes for this item
+	multipleParentKeyValue = (OSNumber *) entry->getObject ("multiple-parent");
+      }
+      else
+	nodeFound = false;
+    }
+
+    continueSearch = ( (false == nodeFound) || (NULL != multipleParentKeyValue) );
+
+    if ( continueSearch && (NULL != (children = (OSArray *) entry->getObject ("children"))) ) {
+      nodeFound = CheckSubTree ( children, theNub, theDevice, entry );
+      continueSearch = ( (false == nodeFound) || (NULL != multipleParentKeyValue) );
+    }
+
+    if ( false == continueSearch )
+      break;
+  }
+
+  return ( nodeFound );
+}
+
+//*********************************************************************************
+// RegisterServiceInTree
+//
+// Register a device at the specified node of our power tree.
+//*********************************************************************************
+
+bool IOPlatformExpert::RegisterServiceInTree (IOService * theService, OSDictionary * theTreeNode, OSDictionary * theTreeParentNode, IOService * theProvider)
+{
+  IOService *    aService;
+  bool           registered = false;
+  OSArray *      children;
+  unsigned int   numChildren;
+  OSDictionary * child;
+
+  // make sure someone is not already registered here
+
+  if ( NULL == theTreeNode->getObject ("service") ) {
+
+    if ( theTreeNode->setObject ("service", OSDynamicCast ( OSObject, theService)) ) {
+
+      // 1. CHILDREN ------------------
+
+      // we registered the node in the tree...now if the node has children
+      // registered we must tell this service to add them.
+
+      if ( NULL != (children = (OSArray *) theTreeNode->getObject ("children")) ) {
+        numChildren = children->getCount ();
+        for ( unsigned int i = 0; i < numChildren; i++ ) {
+          if ( NULL != (child = (OSDictionary *) children->getObject (i)) ) {
+            if ( NULL != (aService = (IOService *) child->getObject ("service")) )
+              theService->addPowerChild (aService);
+          }
+        }
+      }
+
+      // 2. PARENT --------------------
+
+      // also we must notify the parent of this node (if a registered service
+      // exists there) of a new child.
+
+      if ( theTreeParentNode ) {
+        if ( NULL != (aService = (IOService *) theTreeParentNode->getObject ("service")) )
+          if (aService != theProvider)
+            aService->addPowerChild (theService);
+      }
+
+      registered = true;
+    }
+  }
+
+  return registered;
+}
+
+//*********************************************************************************
+// printDictionaryKeys
+//
+// Print the keys for the given dictionary and selected contents.
+//*********************************************************************************
+void printDictionaryKeys (OSDictionary * inDictionary, char * inMsg)
+{
+  OSCollectionIterator * mcoll = OSCollectionIterator::withCollection (inDictionary);
+  OSSymbol * mkey;
+  OSString * ioClass;
+  unsigned int i = 0;
+ 
+  mcoll->reset ();
+
+  mkey = OSDynamicCast (OSSymbol, mcoll->getNextObject ());
+
+  while (mkey) {
+
+    // kprintf ("dictionary key #%d: %s\n", i, mkey->getCStringNoCopy () );
+
+    // if this is the IOClass key, print it's contents
+
+    if ( mkey->isEqualTo ("IOClass") ) {
+      ioClass = (OSString *) inDictionary->getObject ("IOClass");
+      if ( ioClass ) IOLog ("%s IOClass is %s\n", inMsg, ioClass->getCStringNoCopy () );
+    }
+
+    // if this is an IOProviderClass key print it
+
+    if ( mkey->isEqualTo ("IOProviderClass") ) {
+      ioClass = (OSString *) inDictionary->getObject ("IOProviderClass");
+      if ( ioClass ) IOLog ("%s IOProviderClass is %s\n", inMsg, ioClass->getCStringNoCopy () );
+
+    }
+
+    // also print IONameMatch keys
+    if ( mkey->isEqualTo ("IONameMatch") ) {
+      ioClass = (OSString *) inDictionary->getObject ("IONameMatch");
+      if ( ioClass ) IOLog ("%s IONameMatch is %s\n", inMsg, ioClass->getCStringNoCopy () );
+    }
+
+    // also print IONameMatched keys
+
+    if ( mkey->isEqualTo ("IONameMatched") ) {
+      ioClass = (OSString *) inDictionary->getObject ("IONameMatched");
+      if ( ioClass ) IOLog ("%s IONameMatched is %s\n", inMsg, ioClass->getCStringNoCopy () );
+    }
+
+#if 0
+    // print clock-id
+
+    if ( mkey->isEqualTo ("AAPL,clock-id") ) {
+      char * cstr;
+      cstr = getCStringForObject (inDictionary->getObject ("AAPL,clock-id"));
+      if (cstr)
+        kprintf (" ===> AAPL,clock-id is %s\n", cstr );
+    }
+#endif
+
+    // print name
+
+    if ( mkey->isEqualTo ("name") ) {
+      char nameStr[64];
+      nameStr[0] = 0;
+      getCStringForObject(inDictionary->getObject("name"), nameStr,
+		      sizeof(nameStr));
+      if (strlen(nameStr) > 0)
+        IOLog ("%s name is %s\n", inMsg, nameStr);
+    }
+
+    mkey = (OSSymbol *) mcoll->getNextObject ();
+
+    i++;
+  }
+
+  mcoll->release ();
+}
+
+static void
+getCStringForObject(OSObject *inObj, char *outStr, size_t outStrLen)
+{
+   char * buffer;
+   unsigned int    len, i;
+
+   if ( (NULL == inObj) || (NULL == outStr))
+     return;
+
+   char * objString = (char *) (inObj->getMetaClass())->getClassName();
+
+   if ((0 == strncmp(objString, "OSString", sizeof("OSString"))) ||
+		   (0 == strncmp(objString, "OSSymbol", sizeof("OSSymbol"))))
+     strlcpy(outStr, ((OSString *)inObj)->getCStringNoCopy(), outStrLen);
+
+   else if (0 == strncmp(objString, "OSData", sizeof("OSData"))) {
+     len = ((OSData *)inObj)->getLength();
+     buffer = (char *)((OSData *)inObj)->getBytesNoCopy();
+     if (buffer && (len > 0)) {
+       for (i=0; i < len; i++) {
+         outStr[i] = buffer[i];
+       }
+       outStr[len] = 0;
+     }
+   }
+}
+
+/* IOShutdownNotificationsTimedOut
+ * - Called from a timer installed by PEHaltRestart
+ */
+static void IOShutdownNotificationsTimedOut(
+    thread_call_param_t p0, 
+    thread_call_param_t p1)
+{
+    int type = (int)(long)p0;
+
+    /* 30 seconds has elapsed - resume shutdown */
+    if(gIOPlatform) gIOPlatform->haltRestart(type);
+}
+
+
+extern "C" {
+
+/*
+ * Callouts from BSD for machine name & model
+ */ 
+
+boolean_t PEGetMachineName( char * name, int maxLength )
+{
+    if( gIOPlatform)
+	return( gIOPlatform->getMachineName( name, maxLength ));
+    else
+	return( false );
+}
+
+boolean_t PEGetModelName( char * name, int maxLength )
+{
+    if( gIOPlatform)
+	return( gIOPlatform->getModelName( name, maxLength ));
+    else
+	return( false );
+}
+
+int PEGetPlatformEpoch(void)
+{
+    if( gIOPlatform)
+	return( gIOPlatform->getBootROMType());
+    else
+	return( -1 );
+}
+
+int PEHaltRestart(unsigned int type)
+{
+  IOPMrootDomain    *pmRootDomain;
+  AbsoluteTime      deadline;
+  thread_call_t     shutdown_hang;
+  IORegistryEntry   *node;
+  OSData            *data;
+  uint32_t          timeout = 30;
+  
+  if(type == kPEHaltCPU || type == kPERestartCPU || type == kPEUPSDelayHaltCPU)
+  {
+    pmRootDomain = IOService::getPMRootDomain();
+    /* Notify IOKit PM clients of shutdown/restart
+       Clients subscribe to this message with a call to
+       IOService::registerInterest()
+    */
+    
+    /* Spawn a thread that will panic in 30 seconds. 
+       If all goes well the machine will be off by the time
+       the timer expires. If the device wants a different
+       timeout, use that value instead of 30 seconds.
+     */
+#define RESTART_NODE_PATH    "/chosen"
+    node = IORegistryEntry::fromPath( RESTART_NODE_PATH, gIODTPlane );
+    if ( node ) {
+      data = OSDynamicCast( OSData, node->getProperty( "halt-restart-timeout" ) );
+      if ( data && data->getLength() == 4 )
+        timeout = *((uint32_t *) data->getBytesNoCopy());
+    }
+
+    shutdown_hang = thread_call_allocate( &IOShutdownNotificationsTimedOut, 
+                        (thread_call_param_t)(uintptr_t) type);
+    clock_interval_to_deadline( timeout, kSecondScale, &deadline );
+    thread_call_enter1_delayed( shutdown_hang, 0, deadline );
+
+    pmRootDomain->handlePlatformHaltRestart(type); 
+    /* This notification should have few clients who all do 
+       their work synchronously.
+             
+       In this "shutdown notification" context we don't give
+       drivers the option of working asynchronously and responding 
+       later. PM internals make it very hard to wait for asynchronous
+       replies.
+     */
+   }
+   else if(type == kPEPanicRestartCPU || type == kPEPanicSync)
+   {
+    IOCPURunPlatformPanicActions(type);
+   }
+
+  if (gIOPlatform) return gIOPlatform->haltRestart(type);
+  else return -1;
+}
+
+UInt32 PESavePanicInfo(UInt8 *buffer, UInt32 length)
+{
+  if (gIOPlatform != 0) return gIOPlatform->savePanicInfo(buffer, length);
+  else return 0;
+}
+
+
+
+inline static int init_gIOOptionsEntry(void)
+{
+    IORegistryEntry *entry;
+    void *nvram_entry;
+    volatile void **options;
+    int ret = -1;
+
+    if (gIOOptionsEntry) 
+        return 0;
+
+    entry = IORegistryEntry::fromPath( "/options", gIODTPlane );
+    if (!entry)
+        return -1;
+
+    nvram_entry = (void *) OSDynamicCast(IODTNVRAM, entry);
+    if (!nvram_entry) 
+        goto release;
+
+    options = (volatile void **) &gIOOptionsEntry;
+    if (!OSCompareAndSwapPtr(NULL, nvram_entry, options)) {
+        ret = 0;
+        goto release;
+    }
+
+    return 0;
+
+release:
+    entry->release();
+    return ret;
+
+}
+
+/* pass in a NULL value if you just want to figure out the len */
+boolean_t PEReadNVRAMProperty(const char *symbol, void *value,
+                              unsigned int *len)
+{
+    OSObject  *obj;
+    OSData *data;
+    unsigned int vlen;
+
+    if (!symbol || !len)
+        goto err;
+
+    if (init_gIOOptionsEntry() < 0)
+        goto err;
+
+    vlen = *len;
+    *len = 0;
+
+    obj = gIOOptionsEntry->getProperty(symbol);
+    if (!obj)
+        goto err;
+
+    /* convert to data */
+    data = OSDynamicCast(OSData, obj);
+    if (!data) 
+        goto err;
+
+    *len  = data->getLength();
+    vlen  = min(vlen, *len);
+    if (value && vlen)
+        memcpy((void *) value, data->getBytesNoCopy(), vlen);
+
+    return TRUE;
+
+err:
+    return FALSE;
+}
+
+boolean_t
+PEWriteNVRAMBooleanProperty(const char *symbol, boolean_t value)
+{
+	const OSSymbol *sym = NULL;
+	OSBoolean *data = NULL;
+	bool ret = false;
+
+	if (symbol == NULL) {
+		goto exit;
+	}
+
+	if (init_gIOOptionsEntry() < 0) {
+		goto exit;
+	}
+
+	if ((sym = OSSymbol::withCStringNoCopy(symbol)) == NULL) {
+		goto exit;
+	}
+
+	data  = value ? kOSBooleanTrue : kOSBooleanFalse;
+	ret = gIOOptionsEntry->setProperty(sym, data);
+
+	sym->release();
+
+	/* success, force the NVRAM to flush writes */
+	if (ret == true) {
+		gIOOptionsEntry->sync();
+	}
+
+exit:
+	return ret;
+}
+
+boolean_t PEWriteNVRAMProperty(const char *symbol, const void *value, 
+                               const unsigned int len)
+{
+    const OSSymbol *sym;
+    OSData *data;
+    bool ret = false;
+
+    if (!symbol || !value || !len)
+        goto err;
+
+    if (init_gIOOptionsEntry() < 0)
+        goto err;
+
+    sym = OSSymbol::withCStringNoCopy(symbol);
+    if (!sym)
+        goto err;
+
+    data = OSData::withBytes((void *) value, len);
+    if (!data)
+        goto sym_done;
+
+    ret = gIOOptionsEntry->setProperty(sym, data);
+    data->release();
+
+sym_done:
+    sym->release();
+
+    if (ret == true) {
+        gIOOptionsEntry->sync();
+        return TRUE;
+    }
+
+err:
+    return FALSE;
+}
+
+
+boolean_t PERemoveNVRAMProperty(const char *symbol)
+{
+    const OSSymbol *sym;
+
+    if (!symbol)
+        goto err;
+
+    if (init_gIOOptionsEntry() < 0)
+        goto err;
+
+    sym = OSSymbol::withCStringNoCopy(symbol);
+    if (!sym)
+        goto err;
+
+    gIOOptionsEntry->removeProperty(sym);
+
+    sym->release();
+
+    gIOOptionsEntry->sync();
+    return TRUE;
+
+err:
+    return FALSE;
+
+}
+
+long PEGetGMTTimeOfDay(void)
+{
+    clock_sec_t     secs;
+    clock_usec_t    usecs;
+
+    PEGetUTCTimeOfDay(&secs, &usecs);
+    return secs;
+}
+
+void PESetGMTTimeOfDay(long secs)
+{
+    PESetUTCTimeOfDay(secs, 0);
+}
+
+void PEGetUTCTimeOfDay(clock_sec_t * secs, clock_usec_t * usecs)
+{
+    clock_nsec_t    nsecs = 0;
+
+    *secs = 0;
+	if (gIOPlatform)
+        gIOPlatform->getUTCTimeOfDay(secs, &nsecs);
+
+    assert(nsecs < NSEC_PER_SEC);
+    *usecs = nsecs / NSEC_PER_USEC;
+}
+
+void PESetUTCTimeOfDay(clock_sec_t secs, clock_usec_t usecs)
+{
+    assert(usecs < USEC_PER_SEC);
+	if (gIOPlatform)
+        gIOPlatform->setUTCTimeOfDay(secs, usecs * NSEC_PER_USEC);
+}
+
+} /* extern "C" */
+
+void IOPlatformExpert::registerNVRAMController(IONVRAMController * caller)
+{
+    OSData *          data;
+    IORegistryEntry * entry;
+    OSString *        string = 0;
+    uuid_string_t     uuid;
+
+    entry = IORegistryEntry::fromPath( "/efi/platform", gIODTPlane );
+    if ( entry )
+    {
+        data = OSDynamicCast( OSData, entry->getProperty( "system-id" ) );
+        if ( data && data->getLength( ) == 16 )
+        {
+            SHA1_CTX     context;
+            uint8_t      digest[ SHA_DIGEST_LENGTH ];
+            const uuid_t space = { 0x2A, 0x06, 0x19, 0x90, 0xD3, 0x8D, 0x44, 0x40, 0xA1, 0x39, 0xC4, 0x97, 0x70, 0x37, 0x65, 0xAC };
+
+            SHA1Init( &context );
+            SHA1Update( &context, space, sizeof( space ) );
+            SHA1Update( &context, data->getBytesNoCopy( ), data->getLength( ) );
+            SHA1Final( digest, &context );
+
+            digest[ 6 ] = ( digest[ 6 ] & 0x0F ) | 0x50;
+            digest[ 8 ] = ( digest[ 8 ] & 0x3F ) | 0x80;
+
+            uuid_unparse( digest, uuid );
+            string = OSString::withCString( uuid );
+        }
+
+        entry->release( );
+    }
+
+    if ( string == 0 )
+    {
+        entry = IORegistryEntry::fromPath( "/options", gIODTPlane );
+        if ( entry )
+        {
+            data = OSDynamicCast( OSData, entry->getProperty( "platform-uuid" ) );
+            if ( data && data->getLength( ) == sizeof( uuid_t ) )
+            {
+                uuid_unparse( ( uint8_t * ) data->getBytesNoCopy( ), uuid );
+                string = OSString::withCString( uuid );
+            }
+
+            entry->release( );
+        }
+    }
+
+    if ( string )
+    {
+        getProvider( )->setProperty( kIOPlatformUUIDKey, string );
+        publishResource( kIOPlatformUUIDKey, string );
+
+        string->release( );
+    }
+
+    publishResource("IONVRAM");
+}
+
+IOReturn IOPlatformExpert::callPlatformFunction(const OSSymbol *functionName,
+						bool waitForFunction,
+						void *param1, void *param2,
+						void *param3, void *param4)
+{
+  IOService *service, *_resources;
+  
+  if (waitForFunction) {
+    _resources = waitForService(resourceMatching(functionName));
+  } else {
+    _resources = getResourceService();
+  }
+  if (_resources == 0) return kIOReturnUnsupported;
+  
+  service = OSDynamicCast(IOService, _resources->getProperty(functionName));
+  if (service == 0) return kIOReturnUnsupported;
+  
+  return service->callPlatformFunction(functionName, waitForFunction,
+				       param1, param2, param3, param4);
+}
+
+IOByteCount IOPlatformExpert::savePanicInfo(UInt8 *buffer, IOByteCount length)
+{
+  return 0;
+}
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+#undef super
+#define super IOPlatformExpert
+
+OSDefineMetaClassAndAbstractStructors( IODTPlatformExpert, IOPlatformExpert )
+
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  0);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  1);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  2);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  3);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  4);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  5);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  6);
+OSMetaClassDefineReservedUnused(IODTPlatformExpert,  7);
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+IOService * IODTPlatformExpert::probe( IOService * provider,
+			       		SInt32 * score )
+{
+    if( !super::probe( provider, score))
+	return( 0 );
+
+    // check machine types
+    if( !provider->compareNames( getProperty( gIONameMatchKey ) ))
+        return( 0 );
+
+    return( this);
+}
+
+bool IODTPlatformExpert::configure( IOService * provider )
+{
+    if( !super::configure( provider))
+	return( false);
+
+    processTopLevel( provider );
+
+    return( true );
+}
+
+IOService * IODTPlatformExpert::createNub( IORegistryEntry * from )
+{
+    IOService *		nub;
+
+    nub = new IOPlatformDevice;
+    if( nub) {
+	if( !nub->init( from, gIODTPlane )) {
+	    nub->free();
+	    nub = 0;
+	}
+    }
+    return( nub);
+}
+
+bool IODTPlatformExpert::createNubs( IOService * parent, OSIterator * iter )
+{
+    IORegistryEntry *	next;
+    IOService *		nub;
+    bool		ok = true;
+
+    if( iter) {
+	while( (next = (IORegistryEntry *) iter->getNextObject())) {
+
+            if( 0 == (nub = createNub( next )))
+                continue;
+
+            nub->attach( parent );
+            nub->registerService();
+        }
+	iter->release();
+    }
+
+    return( ok );
+}
+
+void IODTPlatformExpert::processTopLevel( IORegistryEntry * rootEntry )
+{
+    OSIterator * 	kids;
+    IORegistryEntry *	next;
+    IORegistryEntry *	cpus;
+    IORegistryEntry *	options;
+
+    // infanticide
+    kids = IODTFindMatchingEntries( rootEntry, 0, deleteList() );
+    if( kids) {
+	while( (next = (IORegistryEntry *)kids->getNextObject())) {
+	    next->detachAll( gIODTPlane);
+	}
+	kids->release();
+    }
+
+    // Publish an IODTNVRAM class on /options.
+    options = rootEntry->childFromPath("options", gIODTPlane);
+    if (options) {
+      dtNVRAM = new IODTNVRAM;
+      if (dtNVRAM) {
+        if (!dtNVRAM->init(options, gIODTPlane)) {
+	  dtNVRAM->release();
+	  dtNVRAM = 0;
+        } else {
+	  dtNVRAM->attach(this);
+	  dtNVRAM->registerService();
+	  options->release();
+	}
+      }
+    }
+
+    // Publish the cpus.
+    cpus = rootEntry->childFromPath( "cpus", gIODTPlane);
+    if ( cpus)
+    {
+      createNubs( this, IODTFindMatchingEntries( cpus, kIODTExclusive, 0));
+      cpus->release();
+    }
+
+    // publish top level, minus excludeList
+    createNubs( this, IODTFindMatchingEntries( rootEntry, kIODTExclusive, excludeList()));
+}
+
+IOReturn IODTPlatformExpert::getNubResources( IOService * nub )
+{
+  if( nub->getDeviceMemory())
+    return( kIOReturnSuccess );
+
+  IODTResolveAddressing( nub, "reg", 0);
+
+  return( kIOReturnSuccess);
+}
+
+bool IODTPlatformExpert::compareNubName( const IOService * nub,
+				OSString * name, OSString ** matched ) const
+{
+    return( IODTCompareNubName( nub, name, matched )
+	  || super::compareNubName( nub, name, matched) );
+}
+
+bool IODTPlatformExpert::getModelName( char * name, int maxLength )
+{
+    OSData *		prop;
+    const char *	str;
+    int			len;
+    char		c;
+    bool		ok = false;
+
+    maxLength--;
+
+    prop = (OSData *) getProvider()->getProperty( gIODTCompatibleKey );
+    if( prop ) {
+	str = (const char *) prop->getBytesNoCopy();
+
+	if( 0 == strncmp( str, "AAPL,", strlen( "AAPL," ) ))
+	    str += strlen( "AAPL," );
+
+	len = 0;
+	while( (c = *str++)) {
+	    if( (c == '/') || (c == ' '))
+		c = '-';
+
+	    name[ len++ ] = c;
+	    if( len >= maxLength)
+		break;
+	}
+
+	name[ len ] = 0;
+	ok = true;
+    }
+    return( ok );
+}
+
+bool IODTPlatformExpert::getMachineName( char * name, int maxLength )
+{
+    OSData *		prop;
+    bool		ok = false;
+
+    maxLength--;
+    prop = (OSData *) getProvider()->getProperty( gIODTModelKey );
+    ok = (0 != prop);
+
+    if( ok )
+	strlcpy( name, (const char *) prop->getBytesNoCopy(), maxLength );
+
+    return( ok );
+}
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+void IODTPlatformExpert::registerNVRAMController( IONVRAMController * nvram )
+{
+  if (dtNVRAM) dtNVRAM->registerNVRAMController(nvram);
+  
+  super::registerNVRAMController(nvram);
+}
+
+int IODTPlatformExpert::haltRestart(unsigned int type)
+{
+  if (dtNVRAM) dtNVRAM->sync();
+  
+  return super::haltRestart(type);
+}
+
+IOReturn IODTPlatformExpert::readXPRAM(IOByteCount offset, UInt8 * buffer,
+				       IOByteCount length)
+{
+  if (dtNVRAM) return dtNVRAM->readXPRAM(offset, buffer, length);
+  else return kIOReturnNotReady;
+}
+
+IOReturn IODTPlatformExpert::writeXPRAM(IOByteCount offset, UInt8 * buffer,
+					IOByteCount length)
+{
+  if (dtNVRAM) return dtNVRAM->writeXPRAM(offset, buffer, length);
+  else return kIOReturnNotReady;
+}
+
+IOReturn IODTPlatformExpert::readNVRAMProperty(
+	IORegistryEntry * entry,
+	const OSSymbol ** name, OSData ** value )
+{
+  if (dtNVRAM) return dtNVRAM->readNVRAMProperty(entry, name, value);
+  else return kIOReturnNotReady;
+}
+
+IOReturn IODTPlatformExpert::writeNVRAMProperty(
+	IORegistryEntry * entry,
+	const OSSymbol * name, OSData * value )
+{
+  if (dtNVRAM) return dtNVRAM->writeNVRAMProperty(entry, name, value);
+  else return kIOReturnNotReady;
+}
+
+OSDictionary *IODTPlatformExpert::getNVRAMPartitions(void)
+{
+  if (dtNVRAM) return dtNVRAM->getNVRAMPartitions();
+  else return 0;
+}
+
+IOReturn IODTPlatformExpert::readNVRAMPartition(const OSSymbol * partitionID,
+						IOByteCount offset, UInt8 * buffer,
+						IOByteCount length)
+{
+  if (dtNVRAM) return dtNVRAM->readNVRAMPartition(partitionID, offset,
+						  buffer, length);
+  else return kIOReturnNotReady;
+}
+
+IOReturn IODTPlatformExpert::writeNVRAMPartition(const OSSymbol * partitionID,
+						 IOByteCount offset, UInt8 * buffer,
+						 IOByteCount length)
+{
+  if (dtNVRAM) return dtNVRAM->writeNVRAMPartition(partitionID, offset,
+						   buffer, length);
+  else return kIOReturnNotReady;
+}
+
+IOByteCount IODTPlatformExpert::savePanicInfo(UInt8 *buffer, IOByteCount length)
+{
+  IOByteCount lengthSaved = 0;
+  
+  if (dtNVRAM) lengthSaved = dtNVRAM->savePanicInfo(buffer, length);
+  
+  if (lengthSaved == 0) lengthSaved = super::savePanicInfo(buffer, length);
+  
+  return lengthSaved;
+}
+
+OSString* IODTPlatformExpert::createSystemSerialNumberString(OSData* myProperty) {
+    UInt8* serialNumber;
+    unsigned int serialNumberSize;
+    unsigned short pos = 0;
+    char* temp;
+    char SerialNo[30];
+    
+    if (myProperty != NULL) {
+        serialNumberSize = myProperty->getLength();
+        serialNumber = (UInt8*)(myProperty->getBytesNoCopy());
+        temp = (char*)serialNumber;
+        if (serialNumberSize > 0) {
+            // check to see if this is a CTO serial number...
+            while (pos < serialNumberSize && temp[pos] != '-') pos++;
+            
+            if (pos < serialNumberSize) { // there was a hyphen, so it's a CTO serial number
+                memcpy(SerialNo, serialNumber + 12, 8);
+                memcpy(&SerialNo[8], serialNumber, 3);
+                SerialNo[11] = '-';
+                memcpy(&SerialNo[12], serialNumber + 3, 8);
+                SerialNo[20] = 0;
+            } else { // just a normal serial number
+                memcpy(SerialNo, serialNumber + 13, 8);
+                memcpy(&SerialNo[8], serialNumber, 3);
+                SerialNo[11] = 0;
+            }
+            return OSString::withCString(SerialNo);
+        }
+    }
+    return NULL;
+}
+
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+#undef super
+#define super IOService
+
+OSDefineMetaClassAndStructors(IOPlatformExpertDevice, IOService)
+
+OSMetaClassDefineReservedUnused(IOPlatformExpertDevice,  0);
+OSMetaClassDefineReservedUnused(IOPlatformExpertDevice,  1);
+OSMetaClassDefineReservedUnused(IOPlatformExpertDevice,  2);
+OSMetaClassDefineReservedUnused(IOPlatformExpertDevice,  3);
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+bool IOPlatformExpertDevice::compareName( OSString * name,
+                                        OSString ** matched ) const
+{
+    return( IODTCompareNubName( this, name, matched ));
+}
+
+bool
+IOPlatformExpertDevice::initWithArgs(
+                            void * dtTop, void * p2, void * p3, void * p4 )
+{
+    IORegistryEntry * 	dt = 0;
+    bool		ok;
+
+    // dtTop may be zero on non- device tree systems
+    if( dtTop && (dt = IODeviceTreeAlloc( dtTop )))
+	ok = super::init( dt, gIODTPlane );
+    else
+	ok = super::init();
+
+    if( !ok)
+	return( false);
+
+    reserved = NULL;
+    workLoop = IOWorkLoop::workLoop();
+    if (!workLoop)
+        return false;
+
+    return( true);
+}
+
+IOWorkLoop *IOPlatformExpertDevice::getWorkLoop() const
+{
+    return workLoop;
+}
+
+IOReturn IOPlatformExpertDevice::setProperties( OSObject * properties )
+{
+    OSDictionary * dictionary;
+    OSObject *     object;
+    IOReturn       status;
+
+    status = super::setProperties( properties );
+    if ( status != kIOReturnUnsupported ) return status;
+
+    status = IOUserClient::clientHasPrivilege( current_task( ), kIOClientPrivilegeAdministrator );
+    if ( status != kIOReturnSuccess ) return status;
+
+    dictionary = OSDynamicCast( OSDictionary, properties );
+    if ( dictionary == 0 ) return kIOReturnBadArgument;
+
+    object = dictionary->getObject( kIOPlatformUUIDKey );
+    if ( object )
+    {
+        IORegistryEntry * entry;
+        OSString *        string;
+        uuid_t            uuid;
+
+        string = ( OSString * ) getProperty( kIOPlatformUUIDKey );
+        if ( string ) return kIOReturnNotPermitted;
+
+        string = OSDynamicCast( OSString, object );
+        if ( string == 0 ) return kIOReturnBadArgument;
+
+        status = uuid_parse( string->getCStringNoCopy( ), uuid );
+        if ( status != 0 ) return kIOReturnBadArgument;
+
+        entry = IORegistryEntry::fromPath( "/options", gIODTPlane );
+        if ( entry )
+        {
+            entry->setProperty( "platform-uuid", uuid, sizeof( uuid_t ) );
+            entry->release( );
+        }
+
+        setProperty( kIOPlatformUUIDKey, string );
+        publishResource( kIOPlatformUUIDKey, string );
+
+        return kIOReturnSuccess;
+    }
+
+    return kIOReturnUnsupported;
+}
+
+IOReturn IOPlatformExpertDevice::newUserClient( task_t owningTask, void * securityID,
+                                    UInt32 type,  OSDictionary * properties,
+                                    IOUserClient ** handler )
+{
+    IOReturn            err = kIOReturnSuccess;
+    IOUserClient *      newConnect = 0;
+    IOUserClient *      theConnect = 0;
+
+    switch (type)
+    {
+        case kIOKitDiagnosticsClientType:
+	    newConnect = IOKitDiagnosticsClient::withTask(owningTask);
+	    if (!newConnect) err = kIOReturnNotPermitted;
+            break;
+        default:
+            err = kIOReturnBadArgument;
+    }
+
+    if (newConnect)
+    {
+        if ((false == newConnect->attach(this))
+                || (false == newConnect->start(this)))
+        {
+            newConnect->detach( this );
+            newConnect->release();
+        }
+        else
+            theConnect = newConnect;
+    }
+
+    *handler = theConnect;
+    return (err);
+}
+
+void IOPlatformExpertDevice::free()
+{
+    if (workLoop)
+        workLoop->release();
+}
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+#undef super
+#define super IOService
+
+OSDefineMetaClassAndStructors(IOPlatformDevice, IOService)
+
+OSMetaClassDefineReservedUnused(IOPlatformDevice,  0);
+OSMetaClassDefineReservedUnused(IOPlatformDevice,  1);
+OSMetaClassDefineReservedUnused(IOPlatformDevice,  2);
+OSMetaClassDefineReservedUnused(IOPlatformDevice,  3);
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+bool IOPlatformDevice::compareName( OSString * name,
+					OSString ** matched ) const
+{
+    return( ((IOPlatformExpert *)getProvider())->
+		compareNubName( this, name, matched ));
+}
+
+IOService * IOPlatformDevice::matchLocation( IOService * /* client */ )
+{
+    return( this );
+}
+
+IOReturn IOPlatformDevice::getResources( void )
+{
+    return( ((IOPlatformExpert *)getProvider())->getNubResources( this ));
+}
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+/*********************************************************************
+* IOPanicPlatform class
+*
+* If no legitimate IOPlatformDevice matches, this one does and panics
+* the kernel with a suitable message.
+*********************************************************************/
+
+class IOPanicPlatform : IOPlatformExpert {
+    OSDeclareDefaultStructors(IOPanicPlatform);
+
+public:
+    bool start(IOService * provider) APPLE_KEXT_OVERRIDE;
+};
+
+
+OSDefineMetaClassAndStructors(IOPanicPlatform, IOPlatformExpert);
+
+
+bool IOPanicPlatform::start(IOService * provider) {
+    const char * platform_name = "(unknown platform name)";
+
+    if (provider) platform_name = provider->getName();
+
+    panic("Unable to find driver for this platform: \"%s\".\n",
+        platform_name);
+
+    return false;
+}
+
diff -Nur xnu-3247.1.106/iokit/Kernel/IOStartIOKit.cpp xnu-3247.1.106-AnV/iokit/Kernel/IOStartIOKit.cpp
--- xnu-3247.1.106/iokit/Kernel/IOStartIOKit.cpp	2015-12-06 01:32:10.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/Kernel/IOStartIOKit.cpp	2015-12-13 17:20:41.000000000 +0100
@@ -56,6 +56,7 @@
 extern void OSlibkernInit (void);
 
 void iokit_post_constructor_init(void);
+void SetBlacklistArg(uint32_t bn, char *name, uint32_t start, uint32_t stop);
 
 #include <kern/clock.h>
 #include <sys/time.h>
@@ -76,12 +77,106 @@
     clock_initialize_calendar();
 }
 
+/* kaitek / qoopz: blacklist of common kexts that are known to be problematic or undesirable
+ * for virtually all non-apple hardware. see notes in StartIOKit(). */
+/* AnV - Added configurable blacklistmods */
+boolean_t blacklistEnabled = TRUE;
+boolean_t confblacklistEnabled = FALSE;
+blacklist_mod_t blacklistMods[] = {
+        { "com.apple.driver.AppleIntelMeromProfile",	0 },
+        { "com.apple.driver.AppleIntelNehalemProfile",	0 },
+        { "com.apple.driver.AppleIntelPenrynProfile",	0 },
+        { "com.apple.driver.AppleIntelYonahProfile",	0 },
+        { "com.apple.driver.AppleIntelCPUPowerManagement",	0 }, // must be added to use in 10.6.1+
+        { "com.apple.iokit.CHUDKernLib", 			0 },
+        { "com.apple.iokit.CHUDProf",			0 },
+        { "com.apple.iokit.CHUDUtils",			0 },
+        /*{ "com.apple.driver.AppleEFIRuntime",	    0 },*/
+        { NULL,						0 }
+};
+blacklist_confmod_t confblacklistMods[16];
+uint32_t confblacklistCount = 0;
+
+/* AnV - This function propagates a custom blacklist argument */
+void SetBlacklistArg(uint32_t bn, char *name, uint32_t start, uint32_t stop)
+{
+    int i = 0;
+    uint32_t current = start;
+
+    while ((i < 256) && (current <= stop))
+    {
+        confblacklistMods[bn].name[i] = name[current];
+
+        ++i;
+        ++current;
+    }
+}
+
 void iokit_post_constructor_init(void)
 {
     IORegistryEntry *		root;
     OSObject *			obj;
+    uint32_t			bootArg;
+    char                confArgs[4120];
+    uint32_t            confStart = 0;
+    uint32_t            confStop = 0;
+    uint32_t            confLen = 0;
+    uint32_t            confCur = 0;
+   	/* kaitek: todo: implement some kind of mechanism whereby the user can specify a
+     * custom list of kexts to be blacklisted. perhaps categories with the current
+     * list designated "default" and additional categories like "gfx", etc. */
+    /* AnV - Added configurable blacklist mods */
 
     IOCPUInitialize();
+
+    if (PE_parse_boot_argn("-noblacklist", &bootArg, sizeof(&bootArg))) {
+        blacklistEnabled = FALSE;
+        printf("warning: disabling kext blacklist\n");
+    }
+
+    if (PE_parse_boot_argn("blockkexts", confArgs, sizeof(confArgs)))
+    {
+        //printf("BLDEBUG: blockkext found, arguments: %s\n", confArgs);
+        confLen = strlen(confArgs);
+        confblacklistCount = 0;
+
+        while (confCur < confLen)
+        {
+            if ((confArgs[confCur] == ',') && (confblacklistCount < 16))
+            {
+                confStop = confCur - 1;
+
+                SetBlacklistArg(confblacklistCount, confArgs, confStart, confStop);
+
+                //printf("BLDEBUG: kext %u set for blocking, name: %s\n", confblacklistCount, confblacklistMods[confblacklistCount].name);
+
+                ++confblacklistCount;
+                confStart = confCur + 1;
+            }
+
+            ++confCur;
+        }
+
+        if (confLen == 0)
+        {
+            confblacklistEnabled = FALSE;
+        } else {
+            if ((confblacklistCount < 16) && (confStart < confLen))
+            {
+                confStop = confLen;
+                SetBlacklistArg(confblacklistCount, confArgs, confStart, confStop);
+
+                //printf("BLDEBUG: kext %u set for blocking, name: %s\n", confblacklistCount, confblacklistMods[confblacklistCount].name);
+
+                ++confblacklistCount;
+            }
+
+            confblacklistEnabled = TRUE;
+        }
+    }
+
+    //printf("BLDEBUG: %u kexts set for blocking\n", confblacklistCount);
+
     root = IORegistryEntry::initialize();
     assert( root );
     IOService::initialize();
diff -Nur xnu-3247.1.106/iokit/KernelConfigTables.cpp xnu-3247.1.106-AnV/iokit/KernelConfigTables.cpp
--- xnu-3247.1.106/iokit/KernelConfigTables.cpp	2015-12-06 01:32:11.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/KernelConfigTables.cpp	2015-12-13 17:08:10.000000000 +0100
@@ -39,6 +39,15 @@
 "     'IOProviderClass' = IOPlatformExpertDevice;"
 "     'IOProbeScore'    = 0:32;"
 "   }"
+#if defined(I386) || defined(X86_64)
+/* added during testign with old RTC, enables old RTC driver. */
+"   ,"
+"   {"
+"       'IOClass'           = AppleIntelClock;"
+"       'IOProviderClass'   = IOPlatformDevice;"
+"       'IONameMatch'       = intel-clock;"
+"   }"
+#endif /* I386 */
 ")";
 
 /* This stuff is no longer used at all but was exported in prior
diff -Nur xnu-3247.1.106/iokit/bsddev/IOKitBSDInit.cpp xnu-3247.1.106-AnV/iokit/bsddev/IOKitBSDInit.cpp
--- xnu-3247.1.106/iokit/bsddev/IOKitBSDInit.cpp	2015-12-06 01:32:03.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/bsddev/IOKitBSDInit.cpp	2015-12-13 17:08:10.000000000 +0100
@@ -658,13 +658,21 @@
     return (NULL);
 }
 
+UUID_DEFINE(default_platform_uuid, 0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77, 0x88, 0x99, 0xaa, 0xbb, 0xcc, 0xdd, 0xee, 0xff);
+
 kern_return_t IOBSDGetPlatformUUID( uuid_t uuid, mach_timespec_t timeout )
 {
     IOService * resources;
     OSString *  string;
 
     resources = IOService::waitForService( IOService::resourceMatching( kIOPlatformUUIDKey ), ( timeout.tv_sec || timeout.tv_nsec ) ? &timeout : 0 );
-    if ( resources == 0 ) return KERN_OPERATION_TIMED_OUT;
+    if ( resources == 0 ) {
+		/* kaitek: if no platform uuid has been published, return a fake one. this cannot be published
+		 * here because configd might set it at some later time. todo: this should not be necessary in
+		 * the event that pseudo efi nvram is implemented. */
+		bcopy(default_platform_uuid, uuid, sizeof (default_platform_uuid));
+		return KERN_SUCCESS;
+    }
 
     string = ( OSString * ) IOService::getPlatform( )->getProvider( )->getProperty( kIOPlatformUUIDKey );
     if ( string == 0 ) return KERN_NOT_SUPPORTED;
diff -Nur xnu-3247.1.106/iokit/bsddev/IOKitBSDInit.cpp.orig xnu-3247.1.106-AnV/iokit/bsddev/IOKitBSDInit.cpp.orig
--- xnu-3247.1.106/iokit/bsddev/IOKitBSDInit.cpp.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/bsddev/IOKitBSDInit.cpp.orig	2015-12-06 01:32:03.000000000 +0100
@@ -0,0 +1,942 @@
+/*
+ * Copyright (c) 1998-2011 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+#include <IOKit/IOBSD.h>
+#include <IOKit/IOLib.h>
+#include <IOKit/IOService.h>
+#include <IOKit/IOCatalogue.h>
+#include <IOKit/IODeviceTreeSupport.h>
+#include <IOKit/IOKitKeys.h>
+#include <IOKit/IOPlatformExpert.h>
+#include <IOKit/IOUserClient.h>
+
+extern "C" {
+
+#include <pexpert/pexpert.h>
+#include <kern/clock.h>
+#include <uuid/uuid.h>
+#include <sys/vnode_internal.h>
+#include <sys/mount.h>
+
+// how long to wait for matching root device, secs
+#if DEBUG
+#define ROOTDEVICETIMEOUT       120
+#else
+#define ROOTDEVICETIMEOUT       60
+#endif
+
+int panic_on_exception_triage = 0;
+
+extern dev_t mdevadd(int devid, uint64_t base, unsigned int size, int phys);
+extern dev_t mdevlookup(int devid);
+extern void mdevremoveall(void);
+extern void di_root_ramfile(IORegistryEntry * entry);
+
+
+#if   DEVELOPMENT
+#define IOPOLLED_COREFILE  	1
+// no sizing
+#define kIOCoreDumpSize		0ULL
+#define kIOCoreDumpFreeSize	0ULL
+#else
+#define IOPOLLED_COREFILE  	0
+#endif
+
+
+#if IOPOLLED_COREFILE
+static bool 
+NewKernelCoreMedia(void * target, void * refCon,
+		   IOService * newService,
+		   IONotifier * notifier);
+#endif /* IOPOLLED_COREFILE */
+
+
+kern_return_t
+IOKitBSDInit( void )
+{
+    IOService::publishResource("IOBSD");
+
+    return( kIOReturnSuccess );
+}
+
+void
+IOServicePublishResource( const char * property, boolean_t value )
+{
+    if ( value)
+        IOService::publishResource( property, kOSBooleanTrue );
+    else
+        IOService::getResourceService()->removeProperty( property );
+}
+
+boolean_t
+IOServiceWaitForMatchingResource( const char * property, uint64_t timeout )
+{
+    OSDictionary *	dict = 0;
+    IOService *         match = 0;
+    boolean_t		found = false;
+    
+    do {
+        
+        dict = IOService::resourceMatching( property );
+        if( !dict)
+            continue;
+        match = IOService::waitForMatchingService( dict, timeout );
+        if ( match)
+            found = true;
+        
+    } while( false );
+    
+    if( dict)
+        dict->release();
+    if( match)
+        match->release();
+    
+    return( found );
+}
+
+boolean_t
+IOCatalogueMatchingDriversPresent( const char * property )
+{
+    OSDictionary *	dict = 0;
+    OSOrderedSet *	set = 0;
+    SInt32		generationCount = 0;
+    boolean_t		found = false;
+    
+    do {
+        
+        dict = OSDictionary::withCapacity(1);
+        if( !dict)
+            continue;
+        dict->setObject( property, kOSBooleanTrue );
+        set = gIOCatalogue->findDrivers( dict, &generationCount );
+        if ( set && (set->getCount() > 0))
+            found = true;
+        
+    } while( false );
+    
+    if( dict)
+        dict->release();
+    if( set)
+        set->release();
+    
+    return( found );
+}
+
+OSDictionary * IOBSDNameMatching( const char * name )
+{
+    OSDictionary *	dict;
+    const OSSymbol *	str = 0;
+
+    do {
+
+	dict = IOService::serviceMatching( gIOServiceKey );
+	if( !dict)
+	    continue;
+        str = OSSymbol::withCString( name );
+	if( !str)
+	    continue;
+        dict->setObject( kIOBSDNameKey, (OSObject *) str );
+        str->release();
+
+        return( dict );
+
+    } while( false );
+
+    if( dict)
+	dict->release();
+    if( str)
+	str->release();
+
+    return( 0 );
+}
+
+OSDictionary * IOUUIDMatching( void )
+{
+    return IOService::resourceMatching( "boot-uuid-media" );
+}
+
+OSDictionary * IONetworkNamePrefixMatching( const char * prefix )
+{
+    OSDictionary *	 matching;
+    OSDictionary *   propDict = 0;
+    const OSSymbol * str      = 0;
+	char networkType[128];
+	
+    do {
+        matching = IOService::serviceMatching( "IONetworkInterface" );
+        if ( matching == 0 )
+            continue;
+
+        propDict = OSDictionary::withCapacity(1);
+        if ( propDict == 0 )
+            continue;
+
+        str = OSSymbol::withCString( prefix );
+        if ( str == 0 )
+            continue;
+
+        propDict->setObject( "IOInterfaceNamePrefix", (OSObject *) str );
+        str->release();
+        str = 0;
+
+		// see if we're contrained to netroot off of specific network type
+		if(PE_parse_boot_argn( "network-type", networkType, 128 ))
+		{
+			str = OSSymbol::withCString( networkType );
+			if(str)
+			{
+				propDict->setObject( "IONetworkRootType", str);
+				str->release();
+				str = 0;
+			}
+		}
+
+        if ( matching->setObject( gIOPropertyMatchKey,
+                                  (OSObject *) propDict ) != true )
+            continue;
+
+        propDict->release();
+        propDict = 0;
+
+        return( matching );
+
+    } while ( false );
+
+    if ( matching ) matching->release();
+    if ( propDict ) propDict->release();
+    if ( str      ) str->release();
+
+    return( 0 );
+}
+
+static bool IORegisterNetworkInterface( IOService * netif )
+{
+    // A network interface is typically named and registered
+    // with BSD after receiving a request from a user space
+    // "namer". However, for cases when the system needs to
+    // root from the network, this registration task must be
+    // done inside the kernel and completed before the root
+    // device is handed to BSD.
+
+    IOService *    stack;
+    OSNumber *     zero    = 0;
+    OSString *     path    = 0;
+    OSDictionary * dict    = 0;
+    char *         pathBuf = 0;
+    int            len;
+    enum { kMaxPathLen = 512 };
+
+    do {
+        stack = IOService::waitForService(
+                IOService::serviceMatching("IONetworkStack") );
+        if ( stack == 0 ) break;
+
+        dict = OSDictionary::withCapacity(3);
+        if ( dict == 0 ) break;
+
+        zero = OSNumber::withNumber((UInt64) 0, 32);
+        if ( zero == 0 ) break;
+
+        pathBuf = (char *) IOMalloc( kMaxPathLen );
+        if ( pathBuf == 0 ) break;
+
+        len = kMaxPathLen;
+        if ( netif->getPath( pathBuf, &len, gIOServicePlane )
+             == false ) break;
+
+        path = OSString::withCStringNoCopy( pathBuf );
+        if ( path == 0 ) break;
+
+        dict->setObject( "IOInterfaceUnit", zero );
+        dict->setObject( kIOPathMatchKey,   path );
+
+        stack->setProperties( dict );
+    }
+    while ( false );
+
+    if ( zero ) zero->release();
+    if ( path ) path->release();
+    if ( dict ) dict->release();
+    if ( pathBuf ) IOFree(pathBuf, kMaxPathLen);
+
+	return ( netif->getProperty( kIOBSDNameKey ) != 0 );
+}
+
+OSDictionary * IOOFPathMatching( const char * path, char * buf, int maxLen )
+{
+    OSDictionary *	matching = NULL;
+    OSString *		str;
+    char *		comp;
+    int			len;
+
+    do {
+
+	len = strlen( kIODeviceTreePlane ":" );
+	maxLen -= len;
+	if( maxLen <= 0)
+	    continue;
+
+	strlcpy( buf, kIODeviceTreePlane ":", len + 1 );
+	comp = buf + len;
+
+	len = strlen( path );
+	maxLen -= len;
+	if( maxLen <= 0)
+	    continue;
+	strlcpy( comp, path, len + 1 );
+
+	matching = OSDictionary::withCapacity( 1 );
+	if( !matching)
+	    continue;
+
+	str = OSString::withCString( buf );
+	if( !str)
+	    continue;
+        matching->setObject( kIOPathMatchKey, str );
+	str->release();
+
+	return( matching );
+
+    } while( false );
+
+    if( matching)
+        matching->release();
+
+    return( 0 );
+}
+
+static int didRam = 0;
+enum { kMaxPathBuf = 512, kMaxBootVar = 128 };
+
+kern_return_t IOFindBSDRoot( char * rootName, unsigned int rootNameSize,
+				dev_t * root, u_int32_t * oflags )
+{
+    mach_timespec_t	t;
+    IOService *		service;
+    IORegistryEntry *	regEntry;
+    OSDictionary *	matching = 0;
+    OSString *		iostr;
+    OSNumber *		off;
+    OSData *		data = 0;
+
+    UInt32		flags = 0;
+    int			mnr, mjr;
+    const char *        mediaProperty = 0;
+    char *		rdBootVar;
+    char *		str;
+    const char *	look = 0;
+    int			len;
+    bool		debugInfoPrintedOnce = false;
+    const char * 	uuidStr = NULL;
+
+    static int		mountAttempts = 0;
+				
+    int xchar, dchar;
+                                    
+
+    if( mountAttempts++)
+	IOSleep( 5 * 1000 );
+
+    str = (char *) IOMalloc( kMaxPathBuf + kMaxBootVar );
+    if( !str)
+	return( kIOReturnNoMemory );
+    rdBootVar = str + kMaxPathBuf;
+
+    if (!PE_parse_boot_argn("rd", rdBootVar, kMaxBootVar )
+     && !PE_parse_boot_argn("rootdev", rdBootVar, kMaxBootVar ))
+	rdBootVar[0] = 0;
+
+    do {
+	if( (regEntry = IORegistryEntry::fromPath( "/chosen", gIODTPlane ))) {
+	    di_root_ramfile(regEntry);
+            data = OSDynamicCast(OSData, regEntry->getProperty( "root-matching" ));
+            if (data) {
+               matching = OSDynamicCast(OSDictionary, OSUnserializeXML((char *)data->getBytesNoCopy()));
+                if (matching) {
+                    continue;
+                }
+            }
+
+	    data = (OSData *) regEntry->getProperty( "boot-uuid" );
+	    if( data) {
+		uuidStr = (const char*)data->getBytesNoCopy();
+		OSString *uuidString = OSString::withCString( uuidStr );
+
+		// match the boot-args boot-uuid processing below
+		if( uuidString) {
+		    IOLog("rooting via boot-uuid from /chosen: %s\n", uuidStr);
+		    IOService::publishResource( "boot-uuid", uuidString );
+		    uuidString->release();
+		    matching = IOUUIDMatching();
+		    mediaProperty = "boot-uuid-media";
+		    regEntry->release();
+		    continue;
+		} else {
+		    uuidStr = NULL;
+		}
+	    }
+	    regEntry->release();
+	}
+    } while( false );
+
+//
+//	See if we have a RAMDisk property in /chosen/memory-map.  If so, make it into a device.
+//	It will become /dev/mdx, where x is 0-f. 
+//
+
+	if(!didRam) {												/* Have we already build this ram disk? */
+		didRam = 1;												/* Remember we did this */
+		if((regEntry = IORegistryEntry::fromPath( "/chosen/memory-map", gIODTPlane ))) {	/* Find the map node */
+			data = (OSData *)regEntry->getProperty("RAMDisk");	/* Find the ram disk, if there */
+			if(data) {											/* We found one */
+				uintptr_t *ramdParms;
+				ramdParms = (uintptr_t *)data->getBytesNoCopy();	/* Point to the ram disk base and size */
+				(void)mdevadd(-1, ml_static_ptovirt(ramdParms[0]) >> 12, ramdParms[1] >> 12, 0);	/* Initialize it and pass back the device number */
+			}
+			regEntry->release();								/* Toss the entry */
+		}
+	}
+	
+//
+//	Now check if we are trying to root on a memory device
+//
+
+	if((rdBootVar[0] == 'm') && (rdBootVar[1] == 'd') && (rdBootVar[3] == 0)) {
+		dchar = xchar = rdBootVar[2];							/* Get the actual device */
+		if((xchar >= '0') && (xchar <= '9')) xchar = xchar - '0';	/* If digit, convert */
+		else {
+			xchar = xchar & ~' ';								/* Fold to upper case */
+			if((xchar >= 'A') && (xchar <= 'F')) {				/* Is this a valid digit? */
+				xchar = (xchar & 0xF) + 9;						/* Convert the hex digit */
+				dchar = dchar | ' ';							/* Fold to lower case */
+			}
+			else xchar = -1;									/* Show bogus */
+		}
+		if(xchar >= 0) {										/* Do we have a valid memory device name? */
+			*root = mdevlookup(xchar);							/* Find the device number */
+			if(*root >= 0) {									/* Did we find one? */
+
+				rootName[0] = 'm';								/* Build root name */
+				rootName[1] = 'd';								/* Build root name */
+				rootName[2] = dchar;							/* Build root name */
+				rootName[3] = 0;								/* Build root name */
+				IOLog("BSD root: %s, major %d, minor %d\n", rootName, major(*root), minor(*root));
+				*oflags = 0;									/* Show that this is not network */
+				goto iofrootx;									/* Join common exit... */
+			}
+			panic("IOFindBSDRoot: specified root memory device, %s, has not been configured\n", rdBootVar);	/* Not there */
+		}
+	}
+
+      if( (!matching) && rdBootVar[0] ) {
+	// by BSD name
+	look = rdBootVar;
+	if( look[0] == '*')
+	    look++;
+    
+	if ( strncmp( look, "en", strlen( "en" )) == 0 ) {
+	    matching = IONetworkNamePrefixMatching( "en" );
+	} else if ( strncmp( look, "uuid", strlen( "uuid" )) == 0 ) {
+            char *uuid;
+            OSString *uuidString;
+
+            uuid = (char *)IOMalloc( kMaxBootVar );
+                  
+            if ( uuid ) {
+                if (!PE_parse_boot_argn( "boot-uuid", uuid, kMaxBootVar )) {
+                    panic( "rd=uuid but no boot-uuid=<value> specified" ); 
+                } 
+                uuidString = OSString::withCString( uuid );
+                if ( uuidString ) {
+                    IOService::publishResource( "boot-uuid", uuidString );
+                    uuidString->release();
+                    IOLog( "\nWaiting for boot volume with UUID %s\n", uuid );
+                    matching = IOUUIDMatching();
+                    mediaProperty = "boot-uuid-media";
+                }
+                IOFree( uuid, kMaxBootVar );
+            }
+	} else {
+	    matching = IOBSDNameMatching( look );
+	}
+    }
+
+    if( !matching) {
+	OSString * astring;
+	// Match any HFS media
+	
+        matching = IOService::serviceMatching( "IOMedia" );
+        astring = OSString::withCStringNoCopy("Apple_HFS");
+        if ( astring ) {
+            matching->setObject("Content", astring);
+            astring->release();
+        }
+    }
+
+    if( gIOKitDebug & kIOWaitQuietBeforeRoot ) {
+    	IOLog( "Waiting for matching to complete\n" );
+    	IOService::getPlatform()->waitQuiet();
+    }
+
+    if( true && matching) {
+        OSSerialize * s = OSSerialize::withCapacity( 5 );
+
+        if( matching->serialize( s )) {
+            IOLog( "Waiting on %s\n", s->text() );
+            s->release();
+        }
+    }
+
+    do {
+        t.tv_sec = ROOTDEVICETIMEOUT;
+        t.tv_nsec = 0;
+	matching->retain();
+        service = IOService::waitForService( matching, &t );
+	if( (!service) || (mountAttempts == 10)) {
+            PE_display_icon( 0, "noroot");
+            IOLog( "Still waiting for root device\n" );
+
+            if( !debugInfoPrintedOnce) {
+                debugInfoPrintedOnce = true;
+                if( gIOKitDebug & kIOLogDTree) {
+                    IOLog("\nDT plane:\n");
+                    IOPrintPlane( gIODTPlane );
+                }
+                if( gIOKitDebug & kIOLogServiceTree) {
+                    IOLog("\nService plane:\n");
+                    IOPrintPlane( gIOServicePlane );
+                }
+                if( gIOKitDebug & kIOLogMemory)
+                    IOPrintMemory();
+            }
+	}
+    } while( !service);
+    matching->release();
+
+    if ( service && mediaProperty ) {
+        service = (IOService *)service->getProperty(mediaProperty);
+    }
+
+    mjr = 0;
+    mnr = 0;
+
+    // If the IOService we matched to is a subclass of IONetworkInterface,
+    // then make sure it has been registered with BSD and has a BSD name
+    // assigned.
+
+    if ( service
+    &&   service->metaCast( "IONetworkInterface" )
+    &&   !IORegisterNetworkInterface( service ) )
+    {
+        service = 0;
+    }
+
+    if( service) {
+
+	len = kMaxPathBuf;
+	service->getPath( str, &len, gIOServicePlane );
+	IOLog( "Got boot device = %s\n", str );
+
+	iostr = (OSString *) service->getProperty( kIOBSDNameKey );
+	if( iostr)
+	    strlcpy( rootName, iostr->getCStringNoCopy(), rootNameSize );
+	off = (OSNumber *) service->getProperty( kIOBSDMajorKey );
+	if( off)
+	    mjr = off->unsigned32BitValue();
+	off = (OSNumber *) service->getProperty( kIOBSDMinorKey );
+	if( off)
+	    mnr = off->unsigned32BitValue();
+
+	if( service->metaCast( "IONetworkInterface" ))
+	    flags |= 1;
+
+    } else {
+
+	IOLog( "Wait for root failed\n" );
+        strlcpy( rootName, "en0", rootNameSize );
+        flags |= 1;
+    }
+
+    IOLog( "BSD root: %s", rootName );
+    if( mjr)
+	IOLog(", major %d, minor %d\n", mjr, mnr );
+    else
+	IOLog("\n");
+
+    *root = makedev( mjr, mnr );
+    *oflags = flags;
+
+    IOFree( str,  kMaxPathBuf + kMaxBootVar );
+
+iofrootx:
+    if( (gIOKitDebug & (kIOLogDTree | kIOLogServiceTree | kIOLogMemory)) && !debugInfoPrintedOnce) {
+
+	IOService::getPlatform()->waitQuiet();
+        if( gIOKitDebug & kIOLogDTree) {
+            IOLog("\nDT plane:\n");
+            IOPrintPlane( gIODTPlane );
+        }
+        if( gIOKitDebug & kIOLogServiceTree) {
+            IOLog("\nService plane:\n");
+            IOPrintPlane( gIOServicePlane );
+        }
+        if( gIOKitDebug & kIOLogMemory)
+            IOPrintMemory();
+    }
+
+    return( kIOReturnSuccess );
+}
+
+bool IORamDiskBSDRoot(void)
+{
+    char rdBootVar[kMaxBootVar];
+    if (PE_parse_boot_argn("rd", rdBootVar, kMaxBootVar )
+     || PE_parse_boot_argn("rootdev", rdBootVar, kMaxBootVar )) {
+        if((rdBootVar[0] == 'm') && (rdBootVar[1] == 'd') && (rdBootVar[3] == 0)) {
+            return true;
+        }
+    }
+    return false;
+}
+
+void IOSecureBSDRoot(const char * rootName)
+{
+}
+
+void *
+IOBSDRegistryEntryForDeviceTree(char * path)
+{
+    return (IORegistryEntry::fromPath(path, gIODTPlane));
+}
+
+void
+IOBSDRegistryEntryRelease(void * entry)
+{
+    IORegistryEntry * regEntry = (IORegistryEntry *)entry;
+
+    if (regEntry)
+	regEntry->release();
+    return;
+}
+
+const void *
+IOBSDRegistryEntryGetData(void * entry, char * property_name, 
+			  int * packet_length)
+{
+    OSData *		data;
+    IORegistryEntry * 	regEntry = (IORegistryEntry *)entry;
+
+    data = (OSData *) regEntry->getProperty(property_name);
+    if (data) {
+	*packet_length = data->getLength();
+        return (data->getBytesNoCopy());
+    }
+    return (NULL);
+}
+
+kern_return_t IOBSDGetPlatformUUID( uuid_t uuid, mach_timespec_t timeout )
+{
+    IOService * resources;
+    OSString *  string;
+
+    resources = IOService::waitForService( IOService::resourceMatching( kIOPlatformUUIDKey ), ( timeout.tv_sec || timeout.tv_nsec ) ? &timeout : 0 );
+    if ( resources == 0 ) return KERN_OPERATION_TIMED_OUT;
+
+    string = ( OSString * ) IOService::getPlatform( )->getProvider( )->getProperty( kIOPlatformUUIDKey );
+    if ( string == 0 ) return KERN_NOT_SUPPORTED;
+
+    uuid_parse( string->getCStringNoCopy( ), uuid );
+
+    return KERN_SUCCESS;
+}
+
+kern_return_t IOBSDGetPlatformSerialNumber( char *serial_number_str, u_int32_t len )
+{
+    OSDictionary * platform_dict;
+    IOService *platform;
+    OSString *  string;
+
+    if (len < 1) {
+	    return 0;
+    }
+    serial_number_str[0] = '\0';
+
+    platform_dict = IOService::serviceMatching( "IOPlatformExpertDevice" );
+    if (platform_dict == NULL) {
+	    return KERN_NOT_SUPPORTED;
+    }
+
+    platform = IOService::waitForService( platform_dict );
+    if (platform) {
+	    string = ( OSString * ) platform->getProperty( kIOPlatformSerialNumberKey );
+	    if ( string == 0 ) {
+		    return KERN_NOT_SUPPORTED;
+	    } else {
+		    strlcpy( serial_number_str, string->getCStringNoCopy( ), len );
+	    }
+    }
+    
+    return KERN_SUCCESS;
+}
+
+void IOBSDIterateMediaWithContent(const char *content_uuid_cstring, int (*func)(const char *bsd_dev_name, const char *uuid_str, void *arg), void *arg)
+{
+    OSDictionary *dictionary;
+    OSString *content_uuid_string;
+
+    dictionary = IOService::serviceMatching( "IOMedia" );
+    if( dictionary ) {
+	content_uuid_string = OSString::withCString( content_uuid_cstring );
+	if( content_uuid_string ) {
+	    IOService *service;
+	    OSIterator *iter;
+
+	    dictionary->setObject( "Content", content_uuid_string );
+	    dictionary->retain();
+
+	    iter = IOService::getMatchingServices(dictionary);
+	    while (iter && (service = (IOService *)iter->getNextObject())) {
+		    if( service ) {
+			    OSString *iostr = (OSString *) service->getProperty( kIOBSDNameKey );
+			    OSString *uuidstr = (OSString *) service->getProperty( "UUID" );
+			    const char *uuid;
+
+			    if( iostr) {
+				    if (uuidstr) {
+					    uuid = uuidstr->getCStringNoCopy();
+				    } else {
+					    uuid = "00000000-0000-0000-0000-000000000000";
+				    }
+
+				    // call the callback
+				    if (func && func(iostr->getCStringNoCopy(), uuid, arg) == 0) {
+					    break;
+				    }
+			    }
+		    }
+	    }
+	    if (iter)
+		    iter->release();
+	    
+	    content_uuid_string->release();
+	}
+	dictionary->release();
+    }
+}
+
+
+int IOBSDIsMediaEjectable( const char *cdev_name )
+{
+    int ret = 0;
+    OSDictionary *dictionary;
+    OSString *dev_name;
+
+    if (strncmp(cdev_name, "/dev/", 5) == 0) {
+	    cdev_name += 5;
+    }
+
+    dictionary = IOService::serviceMatching( "IOMedia" );
+    if( dictionary ) {
+	dev_name = OSString::withCString( cdev_name );
+	if( dev_name ) {
+	    IOService *service;
+	    mach_timespec_t tv = { 5, 0 };    // wait up to "timeout" seconds for the device
+
+	    dictionary->setObject( kIOBSDNameKey, dev_name );
+	    dictionary->retain();
+	    service = IOService::waitForService( dictionary, &tv );
+	    if( service ) {
+		OSBoolean *ejectable = (OSBoolean *) service->getProperty( "Ejectable" );
+
+		if( ejectable ) {
+			ret = (int)ejectable->getValue();
+		}
+
+	    }
+	    dev_name->release();
+	}
+	dictionary->release();
+    }
+
+    return ret;
+}
+
+} /* extern "C" */
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+#include <sys/conf.h>
+#include <sys/vnode.h>
+#include <sys/vnode_internal.h>
+#include <sys/fcntl.h>
+#include <IOKit/IOPolledInterface.h>
+#include <IOKit/IOBufferMemoryDescriptor.h>
+
+IOPolledFileIOVars * gIOPolledCoreFileVars;
+
+#if IOPOLLED_COREFILE
+
+static IOReturn 
+IOOpenPolledCoreFile(const char * filename)
+{
+    IOReturn err;
+    unsigned int debug;
+
+    if (gIOPolledCoreFileVars)                             return (kIOReturnBusy);
+    if (!IOPolledInterface::gMetaClass.getInstanceCount()) return (kIOReturnUnsupported);
+
+    debug = 0;
+    PE_parse_boot_argn("debug", &debug, sizeof (debug));
+    if (DB_DISABLE_LOCAL_CORE & debug)                     return (kIOReturnUnsupported);
+
+    err = IOPolledFileOpen(filename, kIOCoreDumpSize, kIOCoreDumpFreeSize,
+			    NULL, 0,
+			    &gIOPolledCoreFileVars, NULL, NULL, 0);
+    if (kIOReturnSuccess != err)                           return (err);
+
+    err = IOPolledFilePollersSetup(gIOPolledCoreFileVars, kIOPolledPreflightCoreDumpState);
+    if (kIOReturnSuccess != err)
+    {
+	IOPolledFileClose(&gIOPolledCoreFileVars, NULL, NULL, 0, 0, 0);
+    }
+
+    return (err);
+}
+
+static void 
+IOClosePolledCoreFile(void)
+{
+    IOPolledFilePollersClose(gIOPolledCoreFileVars, kIOPolledPostflightState);
+    IOPolledFileClose(&gIOPolledCoreFileVars, NULL, NULL, 0, 0, 0);
+}
+
+static thread_call_t gIOOpenPolledCoreFileTC;
+static IONotifier  * gIOPolledCoreFileNotifier;
+static IONotifier  * gIOPolledCoreFileInterestNotifier;
+
+static IOReturn 
+KernelCoreMediaInterest(void * target, void * refCon,
+			UInt32 messageType, IOService * provider,
+			void * messageArgument, vm_size_t argSize )
+{
+    if (kIOMessageServiceIsTerminated == messageType)
+    {
+	gIOPolledCoreFileInterestNotifier->remove();
+	gIOPolledCoreFileInterestNotifier = 0;
+	IOClosePolledCoreFile();
+    }
+
+    return (kIOReturnSuccess);
+}
+
+static void
+OpenKernelCoreMedia(thread_call_param_t p0, thread_call_param_t p1)
+{
+    IOService * newService;
+    OSString  * string;
+    char        filename[16];
+
+    newService = (IOService *) p1;
+    do
+    {
+	if (gIOPolledCoreFileVars) break;
+	string = OSDynamicCast(OSString, newService->getProperty(kIOBSDNameKey));
+	if (!string) break;
+	snprintf(filename, sizeof(filename), "/dev/%s", string->getCStringNoCopy());
+	if (kIOReturnSuccess != IOOpenPolledCoreFile(filename)) break;
+	gIOPolledCoreFileInterestNotifier = newService->registerInterest(
+				gIOGeneralInterest, &KernelCoreMediaInterest, NULL, 0);
+    }
+    while (false);
+
+    newService->release();
+}
+
+static bool 
+NewKernelCoreMedia(void * target, void * refCon,
+		   IOService * newService,
+		   IONotifier * notifier)
+{
+    do
+    {
+	if (gIOPolledCoreFileVars)    break;
+        if (!gIOOpenPolledCoreFileTC) break;
+        newService = newService->getProvider();
+        if (!newService)              break;
+        newService->retain();
+	thread_call_enter1(gIOOpenPolledCoreFileTC, newService);
+    }
+    while (false);
+
+    return (false);
+}
+
+#endif /* IOPOLLED_COREFILE */
+
+extern "C" void 
+IOBSDMountChange(struct mount * mp, uint32_t op)
+{
+#if IOPOLLED_COREFILE
+
+    OSDictionary * bsdMatching;
+    OSDictionary * mediaMatching;
+    OSString     * string;
+
+    if (!gIOPolledCoreFileNotifier) do
+    {
+	if (!gIOOpenPolledCoreFileTC) gIOOpenPolledCoreFileTC = thread_call_allocate(&OpenKernelCoreMedia, NULL);
+	bsdMatching = IOService::serviceMatching("IOMediaBSDClient");
+	if (!bsdMatching) break;
+	mediaMatching = IOService::serviceMatching("IOMedia");
+	string = OSString::withCStringNoCopy("5361644D-6163-11AA-AA11-00306543ECAC");
+	if (!string || !mediaMatching) break;
+	mediaMatching->setObject("Content", string);
+	string->release();
+	bsdMatching->setObject(gIOParentMatchKey, mediaMatching);
+	mediaMatching->release();
+
+	gIOPolledCoreFileNotifier = IOService::addMatchingNotification(
+						  gIOFirstMatchNotification, bsdMatching, 
+						  &NewKernelCoreMedia, NULL, NULL, -1000);
+    }
+    while (false);
+
+#endif /* IOPOLLED_COREFILE */
+}
+
+/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
+
+extern "C" boolean_t 
+IOTaskHasEntitlement(task_t task, const char * entitlement)
+{
+    OSObject * obj;
+    obj = IOUserClient::copyClientEntitlement(task, entitlement);
+    if (!obj) return (false);
+    obj->release();
+    return (obj != kOSBooleanFalse);
+}
+
diff -Nur xnu-3247.1.106/iokit/conf/files.x86_64 xnu-3247.1.106-AnV/iokit/conf/files.x86_64
--- xnu-3247.1.106/iokit/conf/files.x86_64	2015-12-06 01:32:03.000000000 +0100
+++ xnu-3247.1.106-AnV/iokit/conf/files.x86_64	2015-12-13 17:08:10.000000000 +0100
@@ -11,5 +11,8 @@
 # Power Domains
 iokit/Kernel/IOPMrootDomain.cpp    			    optional iokitcpp
 
+# Real Time Clock hack
+iokit/Drivers/platform/drvAppleIntelClock/IntelClock.cpp    optional iokitcpp
+
 # Key Store helper
 iokit/Kernel/i386/IOKeyStoreHelper.cpp			    standard
diff -Nur xnu-3247.1.106/libkern/c++/OSKext.cpp xnu-3247.1.106-AnV/libkern/c++/OSKext.cpp
--- xnu-3247.1.106/libkern/c++/OSKext.cpp	2015-12-06 01:32:12.000000000 +0100
+++ xnu-3247.1.106-AnV/libkern/c++/OSKext.cpp	2015-12-13 17:24:16.000000000 +0100
@@ -4088,88 +4088,10 @@
  *      NOTE - we cannot use the characters "<=" or "<" because we have code 
  *      that serializes plists and treats '<' as a special character.
  *********************************************************************/
-bool 
+/* AnV - Don't use blacklist exclude list */
+bool
 OSKext::isInExcludeList(void)
 {
-    OSString *      versionString           = NULL;  // do not release
-    char *          versionCString          = NULL;  // do not free
-    size_t          i;
-    boolean_t       wantLessThan = false;
-    boolean_t       wantLessThanEqualTo = false;
-    char            myBuffer[32];
-    
-    if (!sExcludeListByID) {
-        return(false);
-    }
-    /* look up by bundleID in our exclude list and if found get version
-     * string (or strings) that we will not allow to load
-     */
-    versionString = OSDynamicCast(OSString, sExcludeListByID->getObject(bundleID));
-    if (versionString == NULL || versionString->getLength() > (sizeof(myBuffer) - 1)) {
-        return(false);
-    }
-    
-    /* parse version strings */
-    versionCString = (char *) versionString->getCStringNoCopy();
-    
-    /* look for "LT" or "LE" form of version string, must be in first two
-     * positions.
-     */
-    if (*versionCString == 'L' && *(versionCString + 1) == 'T') {
-        wantLessThan = true;
-        versionCString +=2; 
-    }
-    else if (*versionCString == 'L' && *(versionCString + 1) == 'E') {
-        wantLessThanEqualTo = true;
-        versionCString +=2;
-    }
-
-    for (i = 0; *versionCString != 0x00; versionCString++) {
-        /* skip whitespace */
-        if (isWhiteSpace(*versionCString)) {
-            continue;
-        }
-        
-        /* peek ahead for version string separator or null terminator */
-        if (*(versionCString + 1) == ',' || *(versionCString + 1) == 0x00) {
-            
-            /* OK, we have a version string */
-            myBuffer[i++] = *versionCString;
-            myBuffer[i] = 0x00;
-
-            OSKextVersion excludeVers;
-            excludeVers = OSKextParseVersionString(myBuffer);
-                
-            if (wantLessThanEqualTo) {
-                if (version <= excludeVers) {
-                    return(true);
-                }
-            }
-            else if (wantLessThan) {
-                if (version < excludeVers) {
-                    return(true);
-                }
-            }
-            else if ( version == excludeVers )  {
-                return(true);
-            }
-            
-            /* reset for the next (if any) version string */
-            i = 0;
-            wantLessThan = false;
-            wantLessThanEqualTo = false;
-        }
-        else {
-            /* save valid version character */
-            myBuffer[i++] = *versionCString;
-            
-            /* make sure bogus version string doesn't overrun local buffer */
-            if ( i >= sizeof(myBuffer) ) {
-                break;
-            }
-        }
-    }
-    
     return(false);
 } 
 
diff -Nur xnu-3247.1.106/libkern/conf/files xnu-3247.1.106-AnV/libkern/conf/files
--- xnu-3247.1.106/libkern/conf/files	2015-12-06 01:32:13.000000000 +0100
+++ xnu-3247.1.106-AnV/libkern/conf/files	2015-12-13 17:08:10.000000000 +0100
@@ -73,6 +73,7 @@
 libkern/crypto/corecrypto_des.c			optional crypto
 libkern/crypto/corecrypto_aes.c			optional crypto
 libkern/crypto/corecrypto_aesxts.c		optional crypto
+libkern/crypto/corecrypto_blowfish.c		optional crypto
 
 libkern/stack_protector.c       standard
 
diff -Nur xnu-3247.1.106/libkern/conf/files.orig xnu-3247.1.106-AnV/libkern/conf/files.orig
--- xnu-3247.1.106/libkern/conf/files.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/libkern/conf/files.orig	2015-12-06 01:32:13.000000000 +0100
@@ -0,0 +1,96 @@
+# options
+
+OPTIONS/libkerncpp					optional libkerncpp
+OPTIONS/kdebug						optional kdebug
+OPTIONS/gprof						optional gprof
+OPTIONS/config_dtrace					optional config_dtrace
+OPTIONS/hibernation					optional hibernation
+OPTIONS/iotracking					optional iotracking
+OPTIONS/networking					optional networking
+OPTIONS/crypto						optional crypto
+OPTIONS/zlib						optional zlib
+
+# libkern
+
+libkern/gen/OSAtomicOperations.c			standard
+libkern/gen/OSDebug.cpp					standard
+
+libkern/c++/OSMetaClass.cpp				optional libkerncpp
+libkern/c++/OSObject.cpp				optional libkerncpp
+
+libkern/c++/OSArray.cpp					optional libkerncpp
+libkern/c++/OSBoolean.cpp				optional libkerncpp
+libkern/c++/OSCollection.cpp				optional libkerncpp
+libkern/c++/OSCollectionIterator.cpp			optional libkerncpp
+libkern/c++/OSData.cpp					optional libkerncpp
+libkern/c++/OSDictionary.cpp				optional libkerncpp
+libkern/c++/OSIterator.cpp				optional libkerncpp
+libkern/c++/OSKext.cpp					optional libkerncpp
+libkern/c++/OSNumber.cpp				optional libkerncpp
+libkern/c++/OSOrderedSet.cpp				optional libkerncpp
+libkern/c++/OSRuntime.cpp				optional libkerncpp
+libkern/c++/OSRuntimeSupport.c				optional libkerncpp
+libkern/c++/OSSerialize.cpp				optional libkerncpp
+libkern/c++/OSSet.cpp					optional libkerncpp
+libkern/c++/OSString.cpp				optional libkerncpp
+libkern/c++/OSSymbol.cpp				optional libkerncpp
+libkern/c++/OSUnserialize.cpp				optional libkerncpp
+libkern/c++/OSUnserializeXML.cpp			optional libkerncpp
+libkern/c++/OSSerializeBinary.cpp			optional libkerncpp
+
+libkern/OSKextLib.cpp					optional libkerncpp
+libkern/mkext.c						standard
+libkern/OSKextVersion.c					standard
+
+libkern/net/inet_aton.c					standard
+libkern/net/inet_ntoa.c					standard
+libkern/net/inet_ntop.c					standard
+libkern/net/inet_pton.c					standard
+
+libkern/stdio/scanf.c					standard
+
+libkern/uuid/uuid.c					standard
+
+libkern/kernel_mach_header.c                            standard
+
+libkern/zlib/adler32.c                                  optional zlib
+libkern/zlib/compress.c                                 optional zlib
+libkern/zlib/crc32.c                                    optional zlib
+libkern/zlib/deflate.c                                  optional zlib
+#libkern/zlib/gzio.c            not needed for kernel   optional zlib
+libkern/zlib/infback.c                                  optional zlib
+libkern/zlib/inffast.c                                  optional zlib
+libkern/zlib/inflate.c                                  optional zlib
+libkern/zlib/inftrees.c                                 optional zlib
+libkern/zlib/trees.c                                    optional zlib
+libkern/zlib/uncompr.c                                  optional zlib
+libkern/zlib/zutil.c                                    optional zlib
+
+libkern/crypto/register_crypto.c		optional crypto
+libkern/crypto/corecrypto_sha2.c    optional crypto_sha2
+libkern/crypto/corecrypto_sha1.c		optional crypto
+libkern/crypto/corecrypto_md5.c			optional crypto
+libkern/crypto/corecrypto_des.c			optional crypto
+libkern/crypto/corecrypto_aes.c			optional crypto
+libkern/crypto/corecrypto_aesxts.c		optional crypto
+
+libkern/stack_protector.c       standard
+
+libkern/kxld/kxld.c             optional config_kxld
+libkern/kxld/kxld_array.c       optional config_kxld
+libkern/kxld/kxld_copyright.c   optional config_kxld
+libkern/kxld/kxld_demangle.c    optional config_kxld
+libkern/kxld/kxld_dict.c        optional config_kxld
+libkern/kxld/kxld_kext.c        optional config_kxld
+libkern/kxld/kxld_reloc.c       optional config_kxld
+libkern/kxld/kxld_object.c      optional config_kxld
+libkern/kxld/kxld_sect.c        optional config_kxld
+libkern/kxld/kxld_seg.c         optional config_kxld
+libkern/kxld/kxld_srcversion.c  optional config_kxld
+libkern/kxld/kxld_sym.c         optional config_kxld
+libkern/kxld/kxld_symtab.c      optional config_kxld
+libkern/kxld/kxld_util.c        optional config_kxld
+libkern/kxld/kxld_uuid.c        optional config_kxld
+libkern/kxld/kxld_versionmin.c  optional config_kxld
+libkern/kxld/kxld_vtable.c      optional config_kxld
+libkern/kxld/kxld_stubs.c       standard
diff -Nur xnu-3247.1.106/libkern/crypto/corecrypto_blowfish.c xnu-3247.1.106-AnV/libkern/crypto/corecrypto_blowfish.c
--- xnu-3247.1.106/libkern/crypto/corecrypto_blowfish.c	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/libkern/crypto/corecrypto_blowfish.c	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,322 @@
+/*
+ * Copyright (c) 2012 Apple Computer, Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+
+#include <libkern/crypto/crypto_internal.h>
+#include <libkern/crypto/blowfish.h>
+#include <corecrypto/ccmode.h>
+#include <corecrypto/ccblowfish.h>
+#include <kern/debug.h>
+
+blowfish_rval blowfish_encrypt_key(const unsigned char *key, int key_len, blowfish_encrypt_ctx cx[1])
+{
+	const struct ccmode_ecb *ecb = g_crypto_funcs->ccblowfish_ecb_encrypt;
+
+    /* Make sure the context size for the mode fits in the one we have */
+    if(ecb->size>sizeof(blowfish_encrypt_ctx))
+        panic("%s: inconsistent size (should be bigger than %d, is %d) for Blowfish encrypt context", __FUNCTION__, (int)ecb->size, (int)sizeof(blowfish_encrypt_ctx));
+
+	ccecb_init(ecb, cx[0].ctx, key_len, key);
+
+	return blowfish_good;
+}
+
+blowfish_rval blowfish_encrypt_ecb(const unsigned char *in_blk, unsigned int num_blk,
+					 unsigned char *out_blk, blowfish_encrypt_ctx cx[1])
+{
+	const struct ccmode_ecb *ecb = g_crypto_funcs->ccblowfish_ecb_encrypt;
+
+    ccecb_update(ecb, cx[0].ctx, num_blk, in_blk, out_blk);	//Actually ecb encrypt.
+
+	return blowfish_good;
+}
+
+#if defined (__i386__) || defined (__x86_64__) || defined (__arm64__)
+/* This does one block of ECB, using the ECB implementation */
+blowfish_rval blowfish_encrypt(const unsigned char *in_blk, unsigned char *out_blk, blowfish_encrypt_ctx cx[1])
+{
+       return blowfish_encrypt_ecb(in_blk, 1, out_blk, cx);
+}
+#endif
+
+#if defined(OPENSSL_SYS_WIN16) || defined(__LP32__) || defined(OPENSSL_SYS_CRAY) || defined(__ILP64__)
+#define BF_LONG unsigned long
+#else
+#define BF_LONG unsigned int
+#endif
+
+#define n2l(c,l)	(l =((unsigned long)(*((c)++)))<<24, \
+l|=((unsigned long)(*((c)++)))<<16, \
+l|=((unsigned long)(*((c)++)))<< 8, \
+l|=((unsigned long)(*((c)++))))
+
+#define l2n(l,c)	(*((c)++)=(unsigned char)(((l)>>24)&0xff), \
+*((c)++)=(unsigned char)(((l)>>16)&0xff), \
+*((c)++)=(unsigned char)(((l)>> 8)&0xff), \
+*((c)++)=(unsigned char)(((l)    )&0xff))
+
+#define l2nn(l1,l2,l3,l4,c,n)	{ \
+c+=n; \
+switch (n) { \
+case 16: *(--(c))=(unsigned char)(((l4)    )&0xff); \
+case 15: *(--(c))=(unsigned char)(((l4)>> 8)&0xff); \
+case 14: *(--(c))=(unsigned char)(((l4)>>16)&0xff); \
+case 13: *(--(c))=(unsigned char)(((l4)>>24)&0xff); \
+case 12: *(--(c))=(unsigned char)(((l3)    )&0xff); \
+case 11: *(--(c))=(unsigned char)(((l3)>> 8)&0xff); \
+case 10: *(--(c))=(unsigned char)(((l3)>>16)&0xff); \
+case  9: *(--(c))=(unsigned char)(((l3)>>24)&0xff); \
+case  8: *(--(c))=(unsigned char)(((l2)    )&0xff); \
+case  7: *(--(c))=(unsigned char)(((l2)>> 8)&0xff); \
+case  6: *(--(c))=(unsigned char)(((l2)>>16)&0xff); \
+case  5: *(--(c))=(unsigned char)(((l2)>>24)&0xff); \
+case  4: *(--(c))=(unsigned char)(((l1)    )&0xff); \
+case  3: *(--(c))=(unsigned char)(((l1)>> 8)&0xff); \
+case  2: *(--(c))=(unsigned char)(((l1)>>16)&0xff); \
+case  1: *(--(c))=(unsigned char)(((l1)>>24)&0xff); \
+} \
+}
+
+#define n2ln(c,l1,l2,l3,l4,n)	{ \
+c+=n; \
+l1=l2=0; \
+switch (n) { \
+case 16: l4 =((unsigned long)(*(--(c))))    ; \
+case 15: l4|=((unsigned long)(*(--(c))))<< 8; \
+case 14: l4|=((unsigned long)(*(--(c))))<<16; \
+case 13: l4|=((unsigned long)(*(--(c))))<<24; \
+case 12: l3 =((unsigned long)(*(--(c))))    ; \
+case 11: l3|=((unsigned long)(*(--(c))))<< 8; \
+case 10: l3|=((unsigned long)(*(--(c))))<<16; \
+case  9: l3|=((unsigned long)(*(--(c))))<<24; \
+case  8: l2 =((unsigned long)(*(--(c))))    ; \
+case  7: l2|=((unsigned long)(*(--(c))))<< 8; \
+case  6: l2|=((unsigned long)(*(--(c))))<<16; \
+case  5: l2|=((unsigned long)(*(--(c))))<<24; \
+case  4: l1 =((unsigned long)(*(--(c))))    ; \
+case  3: l1|=((unsigned long)(*(--(c))))<< 8; \
+case  2: l1|=((unsigned long)(*(--(c))))<<16; \
+case  1: l1|=((unsigned long)(*(--(c))))<<24; \
+} \
+}
+
+blowfish_rval blowfish_encrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
+                                   unsigned char *out_blk, blowfish_encrypt_ctx cx[1])
+{
+    BF_LONG tin0,tin1,tin2,tin3;
+    BF_LONG tout0,tout1,tout2,tout3;
+    BF_LONG tin[4];
+    BF_LONG tout[4];
+    long l = (long)(num_blk * BLOWFISH_BLOCK_SIZE);
+    unsigned char *ivec = (unsigned char *)in_iv;
+
+    n2l(ivec,tout0);
+    n2l(ivec,tout1);
+    n2l(ivec,tout2);
+    n2l(ivec,tout3);
+    ivec-=BLOWFISH_BLOCK_SIZE;
+    for (l-=BLOWFISH_BLOCK_SIZE; l>=0; l-=BLOWFISH_BLOCK_SIZE)
+    {
+        n2l(in_blk,tin0);
+        n2l(in_blk,tin1);
+        n2l(in_blk,tin2);
+        n2l(in_blk,tin3);
+        tin0^=tout0;
+        tin1^=tout1;
+        tin2^=tout2;
+        tin3^=tout3;
+        tin[0]=tin0;
+        tin[1]=tin1;
+        tin[2]=tin2;
+        tin[3]=tin3;
+        blowfish_encrypt_ecb((const unsigned char *)tin, 1, (unsigned char *)tout, cx);
+        tout0=tout[0];
+        tout1=tout[1];
+        tout2=tout[2];
+        tout3=tout[3];
+        l2n(tout0,out_blk);
+        l2n(tout1,out_blk);
+        l2n(tout2,out_blk);
+        l2n(tout3,out_blk);
+    }
+    if (l != -BLOWFISH_BLOCK_SIZE)
+    {
+        n2ln(in_blk,tin0,tin1,tin2,tin3,l+BLOWFISH_BLOCK_SIZE);
+        tin0^=tout0;
+        tin1^=tout1;
+        tin2^=tout2;
+        tin3^=tout3;
+        tin[0]=tin0;
+        tin[1]=tin1;
+        tin[2]=tin2;
+        tin[3]=tin3;
+        blowfish_encrypt_ecb((const unsigned char *)tin, 1, (unsigned char *)tout, cx);
+        tout0=tout[0];
+        tout1=tout[1];
+        tout2=tout[2];
+        tout3=tout[3];
+        l2n(tout0,out_blk);
+        l2n(tout1,out_blk);
+        l2n(tout2,out_blk);
+        l2n(tout3,out_blk);
+    }
+    l2n(tout0,ivec);
+    l2n(tout1,ivec);
+    l2n(tout2,ivec);
+    l2n(tout3,ivec);
+
+    return blowfish_good;
+}
+
+blowfish_rval blowfish_decrypt_key(const unsigned char *key, int key_len, blowfish_decrypt_ctx cx[1])
+{
+	const struct ccmode_ecb *ecb = g_crypto_funcs->ccblowfish_ecb_decrypt;
+
+    /* Make sure the context size for the mode fits in the one we have */
+    if(ecb->size>sizeof(blowfish_decrypt_ctx))
+        panic("%s: inconsistent size (should be bigger than %d, is %d) for Blowfish decrypt context", __FUNCTION__, (int)ecb->size, (int)sizeof(blowfish_decrypt_ctx));
+
+	ccecb_init(ecb, cx[0].ctx, key_len, key);
+
+	return blowfish_good;
+}
+
+blowfish_rval blowfish_decrypt_ecb(const unsigned char *in_blk, unsigned int num_blk,
+					 	 unsigned char *out_blk, blowfish_decrypt_ctx cx[1])
+{
+	const struct ccmode_ecb *ecb = g_crypto_funcs->ccblowfish_ecb_decrypt;
+
+    ccecb_update(ecb, cx[0].ctx, num_blk, in_blk, out_blk);	//Actually ecb decrypt.
+
+	return blowfish_good;
+}
+
+#if defined (__i386__) || defined (__x86_64__) || defined (__arm64__)
+/* This does one block of ECB, using the ECB implementation */
+blowfish_rval blowfish_decrypt(const unsigned char *in_blk, unsigned char *out_blk, blowfish_decrypt_ctx cx[1])
+{
+	return blowfish_decrypt_ecb(in_blk, 1, out_blk, cx);
+}
+#endif
+
+blowfish_rval blowfish_decrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
+                                   unsigned char *out_blk, blowfish_decrypt_ctx cx[1])
+{
+    BF_LONG tin0,tin1,tin2,tin3;
+    BF_LONG tout0,tout1,tout2,tout3,xor0,xor1,xor2,xor3;
+    BF_LONG tin[4];
+    BF_LONG tout[4];
+    long l = (long)(num_blk * BLOWFISH_BLOCK_SIZE);
+    unsigned char *ivec = (unsigned char *)in_iv;
+    
+    n2l(ivec,xor0);
+    n2l(ivec,xor1);
+    n2l(ivec,xor2);
+    n2l(ivec,xor3);
+    ivec-=BLOWFISH_BLOCK_SIZE;
+    for (l-=BLOWFISH_BLOCK_SIZE; l>=0; l-=BLOWFISH_BLOCK_SIZE)
+    {
+        n2l(in_blk,tin0);
+        n2l(in_blk,tin1);
+        n2l(in_blk,tin2);
+        n2l(in_blk,tin3);
+        tin[0]=tin0;
+        tin[1]=tin1;
+        tin[2]=tin2;
+        tin[3]=tin3;
+        blowfish_decrypt_ecb((const unsigned char *)tin, 1, (unsigned char *)tout, cx);
+        tout0=tout[0]^xor0;
+        tout1=tout[1]^xor1;
+        tout2=tout[2]^xor2;
+        tout3=tout[3]^xor3;
+        l2n(tout0,out_blk);
+        l2n(tout1,out_blk);
+        l2n(tout2,out_blk);
+        l2n(tout3,out_blk);
+        xor0=tin0;
+        xor1=tin1;
+        xor2=tin2;
+        xor3=tin3;
+    }
+    if (l != -16)
+    {
+        n2l(in_blk,tin0);
+        n2l(in_blk,tin1);
+        n2l(in_blk,tin2);
+        n2l(in_blk,tin3);
+        tin[0]=tin0;
+        tin[1]=tin1;
+        tin[2]=tin2;
+        tin[3]=tin3;
+        blowfish_decrypt_ecb((const unsigned char *)tin, 1, (unsigned char *)tout, cx);
+        tout0=tout[0]^xor0;
+        tout1=tout[1]^xor1;
+        tout2=tout[2]^xor2;
+        tout3=tout[3]^xor3;
+        l2nn(tout0,tout1,tout2,tout3,out_blk,l+BLOWFISH_BLOCK_SIZE);
+        xor0=tin0;
+        xor1=tin1;
+        xor2=tin2;
+        xor3=tin3;
+    }
+    l2n(xor0,ivec);
+    l2n(xor1,ivec);
+    l2n(xor2,ivec);
+    l2n(xor3,ivec);
+    
+    return blowfish_good;
+}
+
+blowfish_rval blowfish_encrypt_key128(const unsigned char *key, blowfish_encrypt_ctx cx[1])
+{
+	return blowfish_encrypt_key(key, 16, cx);
+}
+
+blowfish_rval blowfish_decrypt_key128(const unsigned char *key, blowfish_decrypt_ctx cx[1])
+{
+	return blowfish_decrypt_key(key, 16, cx);
+}
+
+blowfish_rval blowfish_encrypt_key256(const unsigned char *key, blowfish_encrypt_ctx cx[1])
+{
+	return blowfish_encrypt_key(key, 32, cx);
+}
+
+blowfish_rval blowfish_decrypt_key256(const unsigned char *key, blowfish_decrypt_ctx cx[1])
+{
+	return blowfish_decrypt_key(key, 32, cx);
+}
+
+blowfish_rval blowfish_encrypt_key512(const unsigned char *key, blowfish_encrypt_ctx cx[1])
+{
+    return blowfish_encrypt_key(key, 64, cx);
+}
+
+blowfish_rval blowfish_decrypt_key512(const unsigned char *key, blowfish_decrypt_ctx cx[1])
+{
+    return blowfish_decrypt_key(key, 64, cx);
+}
diff -Nur xnu-3247.1.106/libkern/libkern/crypto/Makefile xnu-3247.1.106-AnV/libkern/libkern/crypto/Makefile
--- xnu-3247.1.106/libkern/libkern/crypto/Makefile	2015-12-06 01:32:18.000000000 +0100
+++ xnu-3247.1.106-AnV/libkern/libkern/crypto/Makefile	2015-12-13 17:08:10.000000000 +0100
@@ -9,7 +9,7 @@
 
 DATAFILES = md5.h sha1.h
 
-PRIVATE_DATAFILES = register_crypto.h sha2.h des.h aes.h aesxts.h
+PRIVATE_DATAFILES = register_crypto.h sha2.h des.h aes.h aesxts.h blowfish.h
 
 INSTALL_KF_MI_LIST = ${DATAFILES}
 
diff -Nur xnu-3247.1.106/libkern/libkern/crypto/blowfish.h xnu-3247.1.106-AnV/libkern/libkern/crypto/blowfish.h
--- xnu-3247.1.106/libkern/libkern/crypto/blowfish.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/libkern/libkern/crypto/blowfish.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,101 @@
+/*
+ * Copyright (c) 2012 Apple Computer, Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+
+#ifndef _BLOWFISH_H
+#define _BLOWFISH_H
+
+#if defined(__cplusplus)
+extern "C"
+{
+#endif
+
+#include <corecrypto/ccmode.h>
+#include <corecrypto/ccn.h>
+
+#define BLOWFISH_BLOCK_SIZE  16  /* the Blowfish block size in bytes          */
+
+//Unholy HACK: this works because we know the size of the context for every
+//possible corecrypto implementation is less than this.
+#define BLOWFISH_ECB_CTX_MAX_SIZE (ccn_sizeof_size(sizeof(void *)) + ccn_sizeof_size(BLOWFISH_BLOCK_SIZE) + ccn_sizeof_size(8192*4))
+
+typedef struct{
+	ccecb_ctx_decl(BLOWFISH_ECB_CTX_MAX_SIZE, ctx);
+} blowfish_decrypt_ctx;
+
+typedef struct{
+	ccecb_ctx_decl(BLOWFISH_ECB_CTX_MAX_SIZE, ctx);
+} blowfish_encrypt_ctx;
+
+typedef struct
+{
+	blowfish_decrypt_ctx decrypt;
+	blowfish_encrypt_ctx encrypt;
+} blowfish_ctx;
+
+/* for compatibility with old apis*/
+#define blowfish_ret     int
+#define blowfish_good    0
+#define blowfish_error  -1
+#define blowfish_rval    blowfish_ret
+
+/* Key lengths in the range 16 <= key_len <= 32 are given in bytes, */
+/* those in the range 128 <= key_len <= 256 are given in bits       */
+
+blowfish_rval blowfish_encrypt_key(const unsigned char *key, int key_len, blowfish_encrypt_ctx cx[1]);
+blowfish_rval blowfish_encrypt_key128(const unsigned char *key, blowfish_encrypt_ctx cx[1]);
+blowfish_rval blowfish_encrypt_key256(const unsigned char *key, blowfish_encrypt_ctx cx[1]);
+blowfish_rval blowfish_encrypt_key512(const unsigned char *key, blowfish_encrypt_ctx cx[1]);
+
+#if defined (__i386__) || defined (__x86_64__) || defined (__arm64__)
+blowfish_rval blowfish_encrypt(const unsigned char *in, unsigned char *out, blowfish_encrypt_ctx cx[1]);
+#endif
+
+blowfish_rval blowfish_encrypt_ecb(const unsigned char *in_blk, unsigned int num_blk,
+					 unsigned char *out_blk, blowfish_encrypt_ctx cx[1]);
+blowfish_rval blowfish_encrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
+                     unsigned char *out_blk, blowfish_encrypt_ctx cx[1]);
+
+blowfish_rval blowfish_decrypt_key(const unsigned char *key, int key_len, blowfish_decrypt_ctx cx[1]);
+blowfish_rval blowfish_decrypt_key128(const unsigned char *key, blowfish_decrypt_ctx cx[1]);
+blowfish_rval blowfish_decrypt_key256(const unsigned char *key, blowfish_decrypt_ctx cx[1]);
+blowfish_rval blowfish_decrypt_key512(const unsigned char *key, blowfish_decrypt_ctx cx[1]);
+
+#if defined (__i386__) || defined (__x86_64__) || defined (__arm64__)
+blowfish_rval blowfish_decrypt(const unsigned char *in, unsigned char *out, blowfish_decrypt_ctx cx[1]);
+#endif
+
+blowfish_rval blowfish_decrypt_ecb(const unsigned char *in_blk, unsigned int num_blk,
+					 unsigned char *out_blk, blowfish_decrypt_ctx cx[1]);
+blowfish_rval blowfish_decrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
+                     unsigned char *out_blk, blowfish_decrypt_ctx cx[1]);
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff -Nur xnu-3247.1.106/makedefs/MakeInc.cmd xnu-3247.1.106-AnV/makedefs/MakeInc.cmd
--- xnu-3247.1.106/makedefs/MakeInc.cmd	2015-12-06 01:32:29.000000000 +0100
+++ xnu-3247.1.106-AnV/makedefs/MakeInc.cmd	2015-12-13 17:08:10.000000000 +0100
@@ -34,7 +34,7 @@
 	XCRUN = /usr/bin/xcrun
 endif
 
-SDKROOT ?= macosx.internal
+SDKROOT ?= /
 HOST_SDKROOT ?= macosx
 
 # SDKROOT may be passed as a shorthand like "iphoneos.internal". We
diff -Nur xnu-3247.1.106/makedefs/MakeInc.cmd.orig xnu-3247.1.106-AnV/makedefs/MakeInc.cmd.orig
--- xnu-3247.1.106/makedefs/MakeInc.cmd.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/makedefs/MakeInc.cmd.orig	2015-12-06 01:32:29.000000000 +0100
@@ -0,0 +1,294 @@
+# -*- mode: makefile;-*-
+#
+# Copyright (C) 1999-2012 Apple Inc. All rights reserved.
+#
+# MakeInc.cmd contains command paths for use during
+# the build, as well as make fragments and text
+# strings that may be evaluated as utility functions.
+#
+
+#
+# Commands for the build environment
+#
+##
+# Verbosity
+##
+ifeq ($(RC_XBS),YES)
+VERBOSE = YES
+else
+VERBOSE = NO
+endif
+ifeq ($(VERBOSE),YES)
+_v =
+_vstdout =
+else
+_v = @
+_vstdout = > /dev/null
+endif
+
+VERBOSE_GENERATED_MAKE_FRAGMENTS = NO
+
+ifeq ($(VERBOSE),YES)
+	XCRUN = /usr/bin/xcrun -verbose
+else
+	XCRUN = /usr/bin/xcrun
+endif
+
+SDKROOT ?= macosx.internal
+HOST_SDKROOT ?= macosx
+
+# SDKROOT may be passed as a shorthand like "iphoneos.internal". We
+# must resolve these to a full path and override SDKROOT.
+
+ifeq ($(SDKROOT_RESOLVED),)
+export SDKROOT_RESOLVED := $(shell $(XCRUN) -sdk $(SDKROOT) -show-sdk-path)
+ifeq ($(strip $(SDKROOT)_$(SDKROOT_RESOLVED)),/_)
+export SDKROOT_RESOLVED := /
+endif
+endif
+override SDKROOT = $(SDKROOT_RESOLVED)
+
+ifeq ($(HOST_SDKROOT_RESOLVED),)
+export HOST_SDKROOT_RESOLVED := $(shell $(XCRUN) -sdk $(HOST_SDKROOT) -show-sdk-path)
+endif
+override HOST_SDKROOT = $(HOST_SDKROOT_RESOLVED)
+
+ifeq ($(PLATFORM),)
+	export PLATFORMPATH := $(shell $(XCRUN) -sdk $(SDKROOT) -show-sdk-platform-path)
+	export PLATFORM := $(shell echo $(PLATFORMPATH) | sed 's,^.*/\([^/]*\)\.platform$$,\1,')
+	ifeq ($(PLATFORM),)
+		export PLATFORM := MacOSX
+	else ifeq ($(shell echo $(PLATFORM) | tr A-Z a-z),watchos)
+		export PLATFORM := WatchOS
+	endif
+endif
+
+ifeq ($(SDKVERSION),)
+     export SDKVERSION := $(shell $(XCRUN) -sdk $(SDKROOT) -show-sdk-version)
+endif
+
+# CC/CXX get defined by make(1) by default, so we can't check them
+# against the empty string to see if they haven't been set
+ifeq ($(origin CC),default)
+	export CC := $(shell $(XCRUN) -sdk $(SDKROOT) -find clang)
+endif
+ifeq ($(origin CXX),default)
+	export CXX := $(shell $(XCRUN) -sdk $(SDKROOT) -find clang++)
+endif
+ifeq ($(MIG),)
+	export MIG := $(shell $(XCRUN) -sdk $(SDKROOT) -find mig)
+endif
+ifeq ($(MIGCOM),)
+	export MIGCOM := $(shell $(XCRUN) -sdk $(SDKROOT) -find migcom)
+endif
+ifeq ($(MIGCC),)
+	export MIGCC := $(CC)
+endif
+ifeq ($(STRIP),)
+	export STRIP := $(shell $(XCRUN) -sdk $(SDKROOT) -find strip)
+endif
+ifeq ($(LIPO),)
+	export LIPO := $(shell $(XCRUN) -sdk $(SDKROOT) -find lipo)
+endif
+ifeq ($(LIBTOOL),)
+	export LIBTOOL := $(shell $(XCRUN) -sdk $(SDKROOT) -find libtool)
+endif
+ifeq ($(NM),)
+	export NM := $(shell $(XCRUN) -sdk $(SDKROOT) -find nm)
+endif
+ifeq ($(UNIFDEF),)
+	export UNIFDEF := $(shell $(XCRUN) -sdk $(SDKROOT) -find unifdef)
+endif
+ifeq ($(DSYMUTIL),)
+	export DSYMUTIL := $(shell $(XCRUN) -sdk $(SDKROOT) -find dsymutil)
+endif
+ifeq ($(CTFCONVERT),)
+	export CTFCONVERT := $(shell $(XCRUN) -sdk $(SDKROOT) -find ctfconvert)
+endif
+ifeq ($(CTFMERGE),)
+	export CTFMERGE :=  $(shell $(XCRUN) -sdk $(SDKROOT) -find ctfmerge)
+endif
+ifeq ($(CTFINSERT),)
+	export CTFINSERT := $(shell $(XCRUN) -sdk $(SDKROOT) -find ctf_insert)
+endif
+ifeq ($(NMEDIT),)
+	export NMEDIT := $(shell $(XCRUN) -sdk $(SDKROOT) -find nmedit)
+endif
+
+#
+# Platform options
+#
+SUPPORTED_EMBEDDED_PLATFORMS := iPhoneOS iPhoneOSNano tvOS AppleTVOS WatchOS
+SUPPORTED_SIMULATOR_PLATFORMS := iPhoneSimulator iPhoneNanoSimulator tvSimulator AppleTVSimulator WatchSimulator
+SUPPORTED_PLATFORMS := MacOSX $(SUPPORTED_SIMULATOR_PLATFORMS) $(SUPPORTED_EMBEDDED_PLATFORMS)
+
+# Platform-specific tools
+ifneq ($(filter $(SUPPORTED_EMBEDDED_PLATFORMS),$(PLATFORM)),)
+ifeq ($(EMBEDDED_DEVICE_MAP),)
+	export EMBEDDED_DEVICE_MAP := $(shell $(XCRUN) -sdk $(SDKROOT) -find embedded_device_map)
+endif
+EDM_DBPATH = $(PLATFORMPATH)/usr/local/standalone/firmware/device_map.db
+endif
+
+# Scripts or tools we build ourselves
+#
+# setsegname - Rename segments in a Mach-O object file
+# kextsymboltool - Create kext pseudo-kext Mach-O kexts binaries
+# decomment - Strip out comments to detect whether a file is comments-only
+# installfile - Atomically copy files, esp. when multiple architectures
+#               are trying to install the same target header
+# replacecontents - Write contents to a file and update modtime *only* if
+#               contents differ
+#
+SEG_HACK = $(OBJROOT)/SETUP/setsegname/setsegname
+KEXT_CREATE_SYMBOL_SET = $(OBJROOT)/SETUP/kextsymboltool/kextsymboltool
+DECOMMENT = $(OBJROOT)/SETUP/decomment/decomment
+NEWVERS = $(SRCROOT)/config/newvers.pl
+INSTALL = $(OBJROOT)/SETUP/installfile/installfile
+REPLACECONTENTS = $(OBJROOT)/SETUP/replacecontents/replacecontents
+JSONCOMPILATIONDB = $(OBJROOT)/SETUP/json_compilation_db/json_compilation_db
+
+# Standard BSD tools
+RM = /bin/rm -f
+RMDIR = /bin/rmdir
+CP = /bin/cp
+MV = /bin/mv
+LN = /bin/ln -fs
+CAT = /bin/cat
+MKDIR = /bin/mkdir -p
+CHMOD = /bin/chmod
+FIND = /usr/bin/find
+XARGS = /usr/bin/xargs
+PAX = /bin/pax
+BASENAME = /usr/bin/basename
+DIRNAME = /usr/bin/dirname
+TR = /usr/bin/tr
+TOUCH = /usr/bin/touch
+SLEEP = /bin/sleep
+AWK = /usr/bin/awk
+SED = /usr/bin/sed
+ECHO = /bin/echo
+PLUTIL = /usr/bin/plutil
+
+#
+# Command to generate host binaries. Intentionally not
+# $(CC), which controls the target compiler
+#
+ifeq ($(HOST_OS_VERSION),)
+	export HOST_OS_VERSION	:= $(shell sw_vers -productVersion)
+endif
+ifeq ($(HOST_CC),)
+	export HOST_CC		:= $(shell $(XCRUN) -sdk $(HOST_SDKROOT) -find clang)
+endif
+ifeq ($(HOST_FLEX),)
+	export HOST_FLEX	:= $(shell $(XCRUN) -sdk $(HOST_SDKROOT) -find flex)
+endif
+ifeq ($(HOST_BISON),)
+	export HOST_BISON	:= $(shell $(XCRUN) -sdk $(HOST_SDKROOT) -find bison)
+endif
+ifeq ($(HOST_GM4),)
+	export HOST_GM4		:= $(shell $(XCRUN) -sdk $(HOST_SDKROOT) -find gm4)
+endif
+ifeq ($(HOST_CODESIGN),)
+	export HOST_CODESIGN	:= /usr/bin/codesign
+endif
+ifeq ($(HOST_CODESIGN_ALLOCATE),)
+	export HOST_CODESIGN_ALLOCATE	:= $(shell $(XCRUN) -sdk $(HOST_SDKROOT) -find codesign_allocate)
+endif
+
+#
+# The following variables are functions invoked with "call", and thus
+# behave similarly to externally compiled commands
+#
+
+# $(1) is an expanded kernel config from a TARGET_CONFIGS_UC tuple
+# $(2) is an expanded arch config from a TARGET_CONFIGS_UC tuple
+# $(3) is an expanded machine config from a TARGET_CONFIGS_UC tuple
+_function_create_build_configs_join = $(strip $(1))^$(strip $(2))^$(strip $(3))
+
+# $(1) is an un-expanded kernel config from a TARGET_CONFIGS_UC tuple
+# $(2) is an un-expanded arch config from a TARGET_CONFIGS_UC tuple
+# $(3) is an un-expanded machine config from a TARGET_CONFIGS_UC tuple
+_function_create_build_configs_do_expand =          $(call _function_create_build_configs_join, \
+							   $(if $(filter DEFAULT,$(1)), \
+							   	$(DEFAULT_KERNEL_CONFIG), \
+								$(1) \
+							    ), \
+							   $(if $(filter DEFAULT,$(2)), \
+							   	$(DEFAULT_ARCH_CONFIG), \
+								$(2) \
+							    ), \
+							   $(if $(filter DEFAULT,$(3)), \
+							   	$(if $(filter DEFAULT,$(2)), \
+							   	     $(DEFAULT_$(DEFAULT_ARCH_CONFIG)_MACHINE_CONFIG), \
+								     $(DEFAULT_$(strip $(2))_MACHINE_CONFIG) \
+							    	), \
+								$(3) \
+							    ) \
+						     )
+
+# $(1) is an un-expanded TARGET_CONFIGS_UC list, which must be consumed
+#      3 elements at a time
+function_create_build_configs = $(sort \
+					$(strip \
+				       	 	 $(call _function_create_build_configs_do_expand, \
+						 	$(word 1,$(1)), \
+						 	$(word 2,$(1)), \
+						 	$(word 3,$(1)), \
+						  ) \
+						 $(if $(word 4,$(1)), \
+						      $(call function_create_build_configs, \
+							     $(wordlist 4,$(words $(1)),$(1)) \
+						       ), \
+						       \
+						  ) \
+					  ) \
+				   )
+
+# $(1) is a fully-expanded kernel config
+# $(2) is a fully-expanded arch config
+# $(3) is a fully-expanded machine config. "NONE" is not represented in the objdir path
+function_convert_target_config_uc_to_objdir = $(if $(filter NONE,$(3)),$(strip $(1))_$(strip $(2)),$(strip $(1))_$(strip $(2))_$(strip $(3)))
+
+# $(1) is a fully-expanded build config (like "RELEASE^X86_64^NONE")
+function_convert_build_config_to_objdir = $(call function_convert_target_config_uc_to_objdir, \
+					  	 $(word 1,$(subst ^, ,$(1))), \
+						 $(word 2,$(subst ^, ,$(1))), \
+						 $(word 3,$(subst ^, ,$(1))) \
+					   )
+
+# $(1) is a fully-expanded build config (like "RELEASE^X86_64^NONE")
+function_extract_kernel_config_from_build_config  = $(word 1,$(subst ^, ,$(1)))
+function_extract_arch_config_from_build_config    = $(word 2,$(subst ^, ,$(1)))
+function_extract_machine_config_from_build_config = $(word 3,$(subst ^, ,$(1)))
+
+# $(1) is an input word
+# $(2) is a list of colon-separate potential substitutions like "FOO:BAR BAZ:QUX"
+# $(3) is a fallback if no substitutions were made
+function_substitute_word_with_replacement = $(strip $(if $(2),								\
+							 $(if $(filter $(word 1,$(subst :, ,$(word 1,$(2)))),$(1)),	\
+							      $(word 2,$(subst :, ,$(word 1,$(2)))),		\
+							      $(call function_substitute_word_with_replacement,$(1),$(wordlist 2,$(words $(2)),$(2)),$(3))), \
+							 $(3)								\
+						     )									\
+					     )
+
+# You can't assign a variable to an empty space without these
+# shenanigans
+empty :=
+space := $(empty) $(empty)
+
+# Arithmetic
+# $(1) is the number to increment
+NUM32 = x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x 
+increment = $(words x $(wordlist 1,$(1),$(NUM32)))
+decrement = $(words $(wordlist 2,$(1),$(NUM32)))
+
+# Create a sequence from 1 to $(1)
+# F(N) = if N > 0: return F(N-1) + "N" else: return ""
+sequence = $(if $(wordlist 1,$(1),$(NUM32)),$(call sequence,$(call decrement,$(1))) $(1),)
+
+# Reverse a list of words in $(1)
+reverse = $(if $(word 2,$(1)),$(call reverse,$(wordlist 2,$(words $(1)),$(1)))) $(word 1,$(1))
+
+# vim: set ft=make:
diff -Nur xnu-3247.1.106/makedefs/MakeInc.def xnu-3247.1.106-AnV/makedefs/MakeInc.def
--- xnu-3247.1.106/makedefs/MakeInc.def	2015-12-06 01:32:29.000000000 +0100
+++ xnu-3247.1.106-AnV/makedefs/MakeInc.def	2015-12-13 17:08:10.000000000 +0100
@@ -91,12 +91,12 @@
 # Compiler warning flags
 #
 
-CWARNFLAGS_STD = \
-	-Wall -Werror -Wno-format-y2k -Wextra -Wstrict-prototypes \
-	-Wmissing-prototypes -Wpointer-arith -Wreturn-type -Wcast-qual \
-	-Wwrite-strings -Wswitch -Wshadow -Wcast-align -Wchar-subscripts \
-	-Winline -Wnested-externs -Wredundant-decls -Wextra-tokens \
-	-Wunreachable-code
+# CWARNFLAGS_STD = \
+#	-Wall -Werror -Wno-format-y2k -Wextra -Wstrict-prototypes \
+#	-Wmissing-prototypes -Wpointer-arith -Wreturn-type -Wcast-qual \
+#	-Wwrite-strings -Wswitch -Wshadow -Wcast-align -Wchar-subscripts \
+#	-Winline -Wnested-externs -Wredundant-decls -Wextra-tokens \
+#	-Wunreachable-code
 
 # Can be overridden in Makefile.template or Makefile.$arch
 export CWARNFLAGS ?= $(CWARNFLAGS_STD)
@@ -105,11 +105,11 @@
 $(1)_CWARNFLAGS_ADD += $2
 endef
 
-CXXWARNFLAGS_STD = \
-	-Wall -Werror -Wno-format-y2k -Wextra -Wpointer-arith -Wreturn-type \
-	-Wcast-qual -Wwrite-strings -Wswitch -Wcast-align -Wchar-subscripts \
-	-Wredundant-decls -Wextra-tokens \
-	-Wunreachable-code
+# CXXWARNFLAGS_STD = \
+#	-Wall -Werror -Wno-format-y2k -Wextra -Wpointer-arith -Wreturn-type \
+#	-Wcast-qual -Wwrite-strings -Wswitch -Wcast-align -Wchar-subscripts \
+#	-Wredundant-decls -Wextra-tokens \
+#	-Wunreachable-code
 
 # overloaded-virtual warnings are non-fatal (9000888)
 CXXWARNFLAGS_STD += -Wno-error=overloaded-virtual
@@ -165,17 +165,17 @@
 CFLAGS_X86_64H = $(CFLAGS_X86_64)
 
 
-CFLAGS_RELEASEX86_64 = -O2
-CFLAGS_DEVELOPMENTX86_64 = -O2
+CFLAGS_RELEASEX86_64 = -O2 -march=k8 -mno-ssse3 -mno-sse3
+CFLAGS_DEVELOPMENTX86_64 = -O2 -march=k8 -mno-ssse3 -mno-sse3
 # No space optimization for the DEBUG kernel for the benefit of gdb:
-CFLAGS_DEBUGX86_64 = -O0
-CFLAGS_PROFILEX86_64 = -O2
+CFLAGS_DEBUGX86_64 = -O0 -march=k8 -mno-ssse3 -mno-sse3
+CFLAGS_PROFILEX86_64 = -O2 -march=k8 -mno-ssse3 -mno-sse3
 
-CFLAGS_RELEASEX86_64H = -O2
-CFLAGS_DEVELOPMENTX86_64H = -O2
+CFLAGS_RELEASEX86_64H = -O2 -march=k8 -mno-ssse3 -mno-sse3
+CFLAGS_DEVELOPMENTX86_64H = -O2 -march=k8 -mno-ssse3 -mno-sse3
 # No space optimization for the DEBUG kernel for the benefit of gdb:
-CFLAGS_DEBUGX86_64H = -O0
-CFLAGS_PROFILEX86_64H = -O2
+CFLAGS_DEBUGX86_64H = -O0 -march=k8 -mno-ssse3 -mno-sse3
+CFLAGS_PROFILEX86_64H = -O2 -march=k8 -mno-ssse3 -mno-sse3
 
 CFLAGS_RELEASEARM = -O2
 CFLAGS_DEVELOPMENTARM = -O2
diff -Nur xnu-3247.1.106/makedefs/MakeInc.def.orig xnu-3247.1.106-AnV/makedefs/MakeInc.def.orig
--- xnu-3247.1.106/makedefs/MakeInc.def.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/makedefs/MakeInc.def.orig	2015-12-06 01:32:29.000000000 +0100
@@ -0,0 +1,592 @@
+# -*- mode: makefile;-*-
+#
+# Copyright (C) 1999-2014 Apple Inc. All rights reserved.
+#
+# MakeInc.def contains global definitions for building,
+# linking, and installing files.
+#
+
+#
+# Architecture Configuration options
+#
+SUPPORTED_ARCH_CONFIGS := X86_64 X86_64H
+
+#
+# Kernel Configuration options  
+#
+SUPPORTED_KERNEL_CONFIGS = RELEASE DEVELOPMENT DEBUG PROFILE
+
+#
+# Machine Configuration options  
+#
+
+SUPPORTED_X86_64_MACHINE_CONFIGS = NONE
+SUPPORTED_X86_64H_MACHINE_CONFIGS = NONE
+
+
+
+#
+# Setup up *_LC variables during recursive invocations
+#
+
+ifndef CURRENT_ARCH_CONFIG_LC
+	export CURRENT_ARCH_CONFIG_LC 	:= $(shell printf "%s" "$(CURRENT_ARCH_CONFIG)" | $(TR) A-Z a-z)
+endif
+
+ifndef CURRENT_KERNEL_CONFIG_LC
+	export CURRENT_KERNEL_CONFIG_LC := $(shell printf "%s" "$(CURRENT_KERNEL_CONFIG)" | $(TR) A-Z a-z)
+endif
+
+ifndef CURRENT_MACHINE_CONFIG_LC
+	export CURRENT_MACHINE_CONFIG_LC := $(shell printf "%s" "$(CURRENT_MACHINE_CONFIG)" | $(TR) A-Z a-z)
+endif
+
+
+#
+# Component List
+#
+COMPONENT_LIST 	= osfmk bsd libkern iokit pexpert libsa security
+COMPONENT 	= $(if $(word 2,$(subst /, ,$(RELATIVE_SOURCE_PATH))),$(word 2,$(subst /, ,$(RELATIVE_SOURCE_PATH))),$(firstword $(subst /, ,$(RELATIVE_SOURCE_PATH))))
+COMPONENT_IMPORT_LIST = $(filter-out $(COMPONENT),$(COMPONENT_LIST)) 
+
+
+#
+# Deployment target flag
+#
+ifeq ($(PLATFORM),MacOSX)
+    DEPLOYMENT_TARGET_FLAGS = -mmacosx-version-min=$(SDKVERSION)
+else ifeq ($(PLATFORM),WatchOS)
+    DEPLOYMENT_TARGET_FLAGS = -mwatchos-version-min=$(SDKVERSION)
+else ifeq ($(PLATFORM),tvOS)
+    DEPLOYMENT_TARGET_FLAGS = -mtvos-version-min=$(SDKVERSION)
+else ifeq ($(PLATFORM),AppleTVOS)
+    DEPLOYMENT_TARGET_FLAGS = -mtvos-version-min=$(SDKVERSION)
+else ifneq ($(filter $(SUPPORTED_EMBEDDED_PLATFORMS),$(PLATFORM)),)
+    DEPLOYMENT_TARGET_FLAGS = -miphoneos-version-min=$(SDKVERSION)
+else ifneq ($(filter $(SUPPORTED_SIMULATOR_PLATFORMS),$(PLATFORM)),)
+    DEPLOYMENT_TARGET_FLAGS =
+else
+    DEPLOYMENT_TARGET_FLAGS =
+endif
+
+DEPLOYMENT_TARGET_DEFINES = -DPLATFORM_$(PLATFORM)
+
+
+#
+# Standard defines list
+#
+DEFINES = -DAPPLE -DKERNEL -DKERNEL_PRIVATE -DXNU_KERNEL_PRIVATE \
+	-DPRIVATE -D__MACHO__=1 -Dvolatile=__volatile $(CONFIG_DEFINES) \
+	$(SEED_DEFINES)
+
+#
+# Compiler command
+#
+KCC  = $(CC)
+KC++ = $(CXX)
+
+GENASSYM_KCC = $(CC)
+
+#
+# Compiler warning flags
+#
+
+CWARNFLAGS_STD = \
+	-Wall -Werror -Wno-format-y2k -Wextra -Wstrict-prototypes \
+	-Wmissing-prototypes -Wpointer-arith -Wreturn-type -Wcast-qual \
+	-Wwrite-strings -Wswitch -Wshadow -Wcast-align -Wchar-subscripts \
+	-Winline -Wnested-externs -Wredundant-decls -Wextra-tokens \
+	-Wunreachable-code
+
+# Can be overridden in Makefile.template or Makefile.$arch
+export CWARNFLAGS ?= $(CWARNFLAGS_STD)
+
+define add_perfile_cflags
+$(1)_CWARNFLAGS_ADD += $2
+endef
+
+CXXWARNFLAGS_STD = \
+	-Wall -Werror -Wno-format-y2k -Wextra -Wpointer-arith -Wreturn-type \
+	-Wcast-qual -Wwrite-strings -Wswitch -Wcast-align -Wchar-subscripts \
+	-Wredundant-decls -Wextra-tokens \
+	-Wunreachable-code
+
+# overloaded-virtual warnings are non-fatal (9000888)
+CXXWARNFLAGS_STD += -Wno-error=overloaded-virtual
+
+# Can be overridden in Makefile.template or Makefile.$arch
+export CXXWARNFLAGS ?= $(CXXWARNFLAGS_STD)
+
+define add_perfile_cxxflags
+$(1)_CXXWARNFLAGS_ADD += $2
+endef
+
+#
+# Default ARCH_FLAGS, for use with compiler/linker/assembler/mig drivers
+
+ARCH_FLAGS_X86_64	  = -arch x86_64
+ARCH_FLAGS_X86_64H	  = -arch x86_64h
+
+
+#
+# Default CFLAGS
+#
+ifdef RC_CFLAGS
+OTHER_CFLAGS	= $(subst $(addprefix -arch ,$(RC_ARCHS)),,$(RC_CFLAGS))
+endif
+
+#
+# Debug info
+#
+DSYMINFODIR	= Contents
+DSYMKGMACROSDIR	= Contents/Resources
+DSYMLLDBMACROSDIR = Contents/Resources/Python
+DSYMDWARFDIR	= Contents/Resources/DWARF
+
+DEBUG_CFLAGS := -gdwarf-2
+BUILD_DSYM := 1
+
+#
+# We must not use -fno-keep-inline-functions, or it will remove the dtrace
+# probes from the kernel.
+#
+CFLAGS_GEN = $(DEBUG_CFLAGS) -nostdinc \
+	-fno-builtin -fno-common \
+	-fsigned-bitfields $(OTHER_CFLAGS)
+
+CFLAGS_RELEASE 	= 
+CFLAGS_DEVELOPMENT 	=
+CFLAGS_DEBUG 	= 
+CFLAGS_PROFILE 	=  -pg
+
+CFLAGS_X86_64	= -Dx86_64 -DX86_64 -D__X86_64__ -DLP64 \
+				-DPAGE_SIZE_FIXED -mkernel -msoft-float 
+
+CFLAGS_X86_64H = $(CFLAGS_X86_64)
+
+
+CFLAGS_RELEASEX86_64 = -O2
+CFLAGS_DEVELOPMENTX86_64 = -O2
+# No space optimization for the DEBUG kernel for the benefit of gdb:
+CFLAGS_DEBUGX86_64 = -O0
+CFLAGS_PROFILEX86_64 = -O2
+
+CFLAGS_RELEASEX86_64H = -O2
+CFLAGS_DEVELOPMENTX86_64H = -O2
+# No space optimization for the DEBUG kernel for the benefit of gdb:
+CFLAGS_DEBUGX86_64H = -O0
+CFLAGS_PROFILEX86_64H = -O2
+
+CFLAGS_RELEASEARM = -O2
+CFLAGS_DEVELOPMENTARM = -O2
+CFLAGS_DEBUGARM = -O0
+CFLAGS_PROFILEARM = -O2
+
+
+CFLAGS 	= $(CFLAGS_GEN) \
+		  $($(addsuffix $(CURRENT_MACHINE_CONFIG),MACHINE_FLAGS_$(CURRENT_ARCH_CONFIG)_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),ARCH_FLAGS_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),CFLAGS_)) \
+		  $($(addsuffix $(CURRENT_KERNEL_CONFIG),CFLAGS_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG), $(addsuffix $(CURRENT_KERNEL_CONFIG),CFLAGS_))) \
+		  $(DEPLOYMENT_TARGET_FLAGS) \
+		  $(DEPLOYMENT_TARGET_DEFINES) \
+		  $(DEFINES)
+
+#
+# Default C++ flags
+#
+
+OTHER_CXXFLAGS	=
+
+CXXFLAGS_GEN  = -fapple-kext $(OTHER_CXXFLAGS)
+
+# For the moment, do not use gnu++11
+#CXXFLAGS_ARM = -std=gnu++11
+
+
+CXXFLAGS      = $(CXXFLAGS_GEN) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),CXXFLAGS_)) \
+		  $($(addsuffix $(CURRENT_KERNEL_CONFIG),CXXFLAGS_))
+
+
+#
+# Assembler command
+#
+AS	= $(CC)
+S_KCC	= $(CC)
+
+#
+# Default SFLAGS
+#
+SFLAGS_GEN = -D__ASSEMBLER__ -DASSEMBLER $(OTHER_CFLAGS)
+
+SFLAGS_RELEASE 	= 
+SFLAGS_DEVELOPMENT 	= 
+SFLAGS_DEBUG 	= 
+SFLAGS_PROFILE 	= 
+
+SFLAGS_X86_64 	= $(CFLAGS_X86_64)
+SFLAGS_X86_64H 	= $(CFLAGS_X86_64H)
+
+SFLAGS 	= $(SFLAGS_GEN) \
+		  $($(addsuffix $(CURRENT_MACHINE_CONFIG),MACHINE_FLAGS_$(CURRENT_ARCH_CONFIG)_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),ARCH_FLAGS_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),SFLAGS_)) \
+		  $($(addsuffix $(CURRENT_KERNEL_CONFIG),SFLAGS_)) \
+		  $(DEPLOYMENT_TARGET_FLAGS) \
+		  $(DEPLOYMENT_TARGET_DEFINES) \
+		  $(DEFINES)
+
+#
+# Linker command
+#
+LD	= $(KC++) -nostdlib
+
+#
+# Default LDFLAGS
+#
+# Availability of DWARF allows DTrace CTF (compressed type format) to be constructed.
+# ctf_insert creates the CTF section.  It needs reserved padding in the
+# headers for the load command segment and the CTF section structures.
+#
+LDFLAGS_KERNEL_GEN = \
+	-nostdlib \
+	-fapple-kext \
+	-Wl,-e,__start \
+	-Wl,-sectalign,__TEXT,__text,0x1000 \
+	-Wl,-sectalign,__DATA,__common,0x1000 \
+	-Wl,-sectalign,__DATA,__bss,0x1000 \
+	-Wl,-sectcreate,__PRELINK_TEXT,__text,/dev/null \
+	-Wl,-sectcreate,__PRELINK_STATE,__kernel,/dev/null \
+        -Wl,-sectcreate,__PRELINK_STATE,__kexts,/dev/null \
+	-Wl,-sectcreate,__PRELINK_INFO,__info,/dev/null \
+	-Wl,-new_linker \
+	-Wl,-pagezero_size,0x0 \
+	-Wl,-version_load_command \
+	-Wl,-function_starts \
+	-Wl,-headerpad,152
+
+LDFLAGS_KERNEL_RELEASE 	=
+LDFLAGS_KERNEL_DEVELOPMENT 	=
+LDFLAGS_KERNEL_DEBUG 	= 
+LDFLAGS_KERNEL_PROFILE 	= 
+
+# KASLR static slide config:
+ifndef SLIDE
+SLIDE=0x00
+endif
+KERNEL_MIN_ADDRESS      = 0xffffff8000000000
+KERNEL_BASE_OFFSET      = 0x100000
+KERNEL_STATIC_SLIDE     = $(shell printf "0x%016x" \
+			  $$[ $(SLIDE) << 21 ])
+KERNEL_STATIC_BASE      = $(shell printf "0x%016x" \
+			  $$[ $(KERNEL_MIN_ADDRESS) + $(KERNEL_BASE_OFFSET) ])
+KERNEL_HIB_SECTION_BASE = $(shell printf "0x%016x" \
+			  $$[ $(KERNEL_STATIC_BASE) + $(KERNEL_STATIC_SLIDE) ])
+KERNEL_TEXT_BASE        = $(shell printf "0x%016x" \
+			  $$[ $(KERNEL_HIB_SECTION_BASE) + 0x100000 ])
+
+LDFLAGS_KERNEL_RELEASEX86_64 = \
+	-Wl,-pie \
+	-Wl,-segaddr,__HIB,$(KERNEL_HIB_SECTION_BASE) \
+	-Wl,-image_base,$(KERNEL_TEXT_BASE) \
+	-Wl,-seg_page_size,__TEXT,0x200000 \
+	-Wl,-sectalign,__DATA,__const,0x1000 \
+	-Wl,-sectalign,__DATA,__sysctl_set,0x1000 \
+	-Wl,-sectalign,__HIB,__bootPT,0x1000 \
+	-Wl,-sectalign,__HIB,__desc,0x1000 \
+	-Wl,-sectalign,__HIB,__data,0x1000 \
+	-Wl,-sectalign,__HIB,__text,0x1000 \
+	-Wl,-sectalign,__HIB,__const,0x1000 \
+	-Wl,-sectalign,__HIB,__bss,0x1000 \
+	-Wl,-sectalign,__HIB,__common,0x1000 \
+	-Wl,-sectalign,__HIB,__llvm_prf_cnts,0x1000 \
+	-Wl,-sectalign,__HIB,__llvm_prf_names,0x1000 \
+	-Wl,-sectalign,__HIB,__llvm_prf_data,0x1000 \
+	-Wl,-sectalign,__HIB,__textcoal_nt,0x1000 \
+	$(LDFLAGS_NOSTRIP_FLAG)
+
+# Define KERNEL_BASE_OFFSET so known at compile time:
+CFLAGS_X86_64 += -DKERNEL_BASE_OFFSET=$(KERNEL_BASE_OFFSET)
+CFLAGS_X86_64H += -DKERNEL_BASE_OFFSET=$(KERNEL_BASE_OFFSET)
+
+LDFLAGS_KERNEL_DEBUGX86_64 = $(LDFLAGS_KERNEL_RELEASEX86_64)
+LDFLAGS_KERNEL_DEVELOPMENTX86_64 = $(LDFLAGS_KERNEL_RELEASEX86_64)
+LDFLAGS_KERNEL_PROFILEX86_64 = $(LDFLAGS_KERNEL_RELEASEX86_64)
+
+LDFLAGS_KERNEL_RELEASEX86_64H = $(LDFLAGS_KERNEL_RELEASEX86_64)
+LDFLAGS_KERNEL_DEBUGX86_64H = $(LDFLAGS_KERNEL_RELEASEX86_64H)
+LDFLAGS_KERNEL_DEVELOPMENTX86_64H = $(LDFLAGS_KERNEL_RELEASEX86_64H)
+LDFLAGS_KERNEL_PROFILEX86_64H = $(LDFLAGS_KERNEL_RELEASEX86_64H)
+
+
+LDFLAGS_KERNEL	= $(LDFLAGS_KERNEL_GEN) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),ARCH_FLAGS_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG),LDFLAGS_KERNEL_)) \
+		  $($(addsuffix $(CURRENT_KERNEL_CONFIG),LDFLAGS_KERNEL_)) \
+		  $($(addsuffix $(CURRENT_ARCH_CONFIG), $(addsuffix $(CURRENT_KERNEL_CONFIG),LDFLAGS_KERNEL_))) \
+		  $(DEPLOYMENT_TARGET_FLAGS)
+
+#
+# Default runtime libraries to be linked with the kernel
+#
+LD_KERNEL_LIBS	= -lcc_kext
+
+#
+# DTrace support
+#
+ifeq ($(CURRENT_KERNEL_CONFIG),RELEASE)
+ifneq ($(filter ARM%,$(CURRENT_ARCH_CONFIG)),)
+DO_CTFCONVERT = 0
+DO_CTFMERGE   = 0
+DO_CTFMACHO   = 0
+else
+DO_CTFCONVERT = $(SUPPORTS_CTFCONVERT)
+DO_CTFMERGE   = 1
+DO_CTFMACHO   = $(NEEDS_CTF_MACHOS)
+endif
+else
+DO_CTFCONVERT = $(SUPPORTS_CTFCONVERT)
+DO_CTFMERGE   = 1
+DO_CTFMACHO   = $(NEEDS_CTF_MACHOS)
+endif
+
+#
+# Default INCFLAGS
+#
+INCFLAGS_IMPORT 	= $(patsubst %, -I$(OBJROOT)/EXPORT_HDRS/%, $(COMPONENT_IMPORT_LIST))
+INCFLAGS_EXTERN 	= -I$(SRCROOT)/EXTERNAL_HEADERS
+INCFLAGS_GEN	= -I$(SRCROOT)/$(COMPONENT) -I$(OBJROOT)/EXPORT_HDRS/$(COMPONENT)
+INCFLAGS_LOCAL	= -I.
+
+INCFLAGS 	= $(INCFLAGS_LOCAL) $(INCFLAGS_GEN) $(INCFLAGS_IMPORT) $(INCFLAGS_EXTERN) $(INCFLAGS_MAKEFILE)
+
+#
+# Default MIGFLAGS
+#
+MIGFLAGS	= $(DEFINES) $(INCFLAGS) -novouchers $($(addsuffix $(CURRENT_ARCH_CONFIG),CFLAGS_)) $($(addsuffix $(CURRENT_ARCH_CONFIG),ARCH_FLAGS_)) \
+		$(DEPLOYMENT_TARGET_FLAGS)
+
+
+# Support for LLVM Profile Guided Optimization (PGO)
+
+ifeq ($(BUILD_PROFILE),1)
+CFLAGS_GEN += -fprofile-instr-generate -DPROFILE
+CXXFLAGS_GEN += -fprofile-instr-generate -DPROFILE
+endif
+
+ifdef USE_PROFILE
+CFLAGS_GEN += -fprofile-instr-use=$(USE_PROFILE)
+CXXFLAGS_GEN += -fprofile-instr-use=$(USE_PROFILE)
+LDFLAGS_KERNEL_GEN += -fprofile-instr-use=$(USE_PROFILE)
+
+CFLAGS_GEN += -Wno-error=profile-instr-out-of-date
+endif
+
+#
+# Support for LLVM Link Time Optimization (LTO)
+#
+# LTO can be explicitly enabled or disabled with BUILD_LTO=1|0
+# and defaults to enabled except for DEBUG kernels
+#
+# CFLAGS_NOLTO_FLAG is needed on a per-file basis (for files
+# that deal poorly with LTO, or files that must be machine
+# code *.o files for xnu to build (i.e, setsegname runs on
+# them).
+#
+# LDFLAGS_NOSTRIP_FLAG is used to support configurations that
+# do not utilize an export list.  For these configs to build,
+# we need to prevent the LTO logic from dead stripping them.
+
+LTO_ENABLED_RELEASE = 1
+LTO_ENABLED_DEVELOPMENT = 1
+LTO_ENABLED_DEBUG = 0
+
+ifneq ($(BUILD_LTO),)
+USE_LTO = $(BUILD_LTO)
+else
+USE_LTO = $(LTO_ENABLED_$(CURRENT_KERNEL_CONFIG))
+endif
+
+SUPPORTS_CTFCONVERT	= 0
+ifeq ($(USE_LTO),1)
+CFLAGS_GEN	+= -flto
+CXXFLAGS_GEN	+= -flto
+LDFLAGS_KERNEL_GEN	+= -Wl,-mllvm,-inline-threshold=125 -Wl,-object_path_lto,$(TARGET)/lto.o # -Wl,-mllvm -Wl,-disable-fp-elim 
+LDFLAGS_NOSTRIP_FLAG = -rdynamic
+CFLAGS_NOLTO_FLAG = -fno-lto
+NEEDS_CTF_MACHOS	= 1
+else
+LDFLAGS_NOSTRIP_FLAG =
+CFLAGS_NOLTO_FLAG =
+ifneq ($(CTFCONVERT),)
+SUPPORTS_CTFCONVERT	= 1
+endif
+NEEDS_CTF_MACHOS	= 0
+endif
+
+ifeq ($(BUILD_JSON_COMPILATION_DATABASE),1)
+BUILD_DSYM	:= 0
+DO_CTFCONVERT	:= 0
+DO_CTFMERGE	:= 0
+DO_CTFMACHO	:= 0
+KCC		= $(JSONCOMPILATIONDB) $(OBJPATH)/compile_commands.json $(PWD) $< $(CC)
+KC++		= $(JSONCOMPILATIONDB) $(OBJPATH)/compile_commands.json $(PWD) $< $(CXX)
+S_KCC		= $(JSONCOMPILATIONDB) $(OBJPATH)/compile_commands.json $(PWD) $< $(CC)
+STRIP		= true
+endif
+
+#
+# Default VPATH
+#
+export VPATH = .:$(SOURCE)
+
+#
+# Macros that control installation of kernel and its header files
+#
+# install flags for header files
+# 
+INSTALL_FLAGS = -c -S -m 0444
+DATA_INSTALL_FLAGS = -c -S -m 0644
+EXEC_INSTALL_FLAGS = -c -S -m 0755
+
+#
+# Header file destinations
+#
+FRAMEDIR = /System/Library/Frameworks
+
+SINCVERS = B
+SINCFRAME = $(FRAMEDIR)/System.framework
+SINCDIR = $(SINCFRAME)/Versions/$(SINCVERS)/Headers
+SPINCDIR = $(SINCFRAME)/Versions/$(SINCVERS)/PrivateHeaders
+SRESDIR = $(SINCFRAME)/Versions/$(SINCVERS)/Resources
+
+ifndef INCDIR
+    INCDIR = /usr/include
+endif
+ifndef LCLDIR
+    LCLDIR = $(SPINCDIR)
+endif
+
+KINCVERS = A
+KINCFRAME = $(FRAMEDIR)/Kernel.framework
+KINCDIR = $(KINCFRAME)/Versions/$(KINCVERS)/Headers
+KPINCDIR = $(KINCFRAME)/Versions/$(KINCVERS)/PrivateHeaders
+KRESDIR = $(KINCFRAME)/Versions/$(KINCVERS)/Resources
+
+XNU_PRIVATE_UNIFDEF = -UMACH_KERNEL_PRIVATE -UBSD_KERNEL_PRIVATE -UIOKIT_KERNEL_PRIVATE -ULIBKERN_KERNEL_PRIVATE -ULIBSA_KERNEL_PRIVATE -UPEXPERT_KERNEL_PRIVATE -UXNU_KERNEL_PRIVATE
+
+
+PLATFORM_UNIFDEF = $(foreach x,$(SUPPORTED_PLATFORMS),$(if $(filter $(PLATFORM),$(x)),-DPLATFORM_$(x) $(foreach token,$(PLATFORM_UNIFDEF_BLACKLIST_TOKENS_$(x)),-U$(token)),-UPLATFORM_$(x)))
+
+SPINCFRAME_UNIFDEF = $(PLATFORM_UNIFDEF) $(XNU_PRIVATE_UNIFDEF) $(SEED_DEFINES) -UKERNEL_PRIVATE -UKERNEL -DPRIVATE -U_OPEN_SOURCE_ -U__OPEN_SOURCE__
+SINCFRAME_UNIFDEF  = $(PLATFORM_UNIFDEF) $(XNU_PRIVATE_UNIFDEF) $(SEED_DEFINES) -UKERNEL_PRIVATE -UKERNEL -UPRIVATE -D_OPEN_SOURCE_ -D__OPEN_SOURCE__
+KPINCFRAME_UNIFDEF = $(PLATFORM_UNIFDEF) $(XNU_PRIVATE_UNIFDEF) $(SEED_DEFINES) -DKERNEL_PRIVATE -DPRIVATE -DKERNEL -U_OPEN_SOURCE_ -U__OPEN_SOURCE__
+KINCFRAME_UNIFDEF  = $(PLATFORM_UNIFDEF) $(XNU_PRIVATE_UNIFDEF) $(SEED_DEFINES) -UKERNEL_PRIVATE -UPRIVATE -DKERNEL -D_OPEN_SOURCE_ -D__OPEN_SOURCE__
+
+#
+# Compononent Header file destinations
+#
+EXPDIR = EXPORT_HDRS/$(COMPONENT)
+
+#
+# Strip Flags
+#
+STRIP_FLAGS_RELEASE	= -S -x 
+STRIP_FLAGS_DEVELOPMENT	= -S 
+STRIP_FLAGS_DEBUG	= -S 
+STRIP_FLAGS_PROFILE	= -S -x
+
+STRIP_FLAGS 	= $($(addsuffix $(CURRENT_KERNEL_CONFIG),STRIP_FLAGS_)) 
+
+#
+# dsymutil flags
+#
+DSYMUTIL_FLAGS_GEN	= --minimize
+
+DSYMUTIL_FLAGS_X86_64	= --arch=x86_64
+DSYMUTIL_FLAGS_X86_64H	= --arch=x86_64h
+
+DSYMUTIL_FLAGS = $(DSYMUTIL_FLAGS_GEN) \
+	$($(addsuffix $(CURRENT_ARCH_CONFIG),DSYMUTIL_FLAGS_))
+
+#
+# Man Page destination
+#
+MANDIR = /usr/share/man
+
+#
+# DEBUG alias location
+#
+DEVELOPER_EXTRAS_DIR = /AppleInternal/CoreOS/xnu_debug
+
+#
+# mach_kernel install location
+#
+INSTALL_KERNEL_DIR = /
+
+#
+# new OS X install location
+#
+SYSTEM_LIBRARY_KERNELS_DIR = /System/Library/Kernels
+
+#
+# File names in DSTROOT
+#
+
+ifeq ($(PLATFORM),MacOSX)
+KERNEL_FILE_NAME_PREFIX = kernel
+else
+KERNEL_FILE_NAME_PREFIX = mach
+endif
+
+ifeq ($(CURRENT_MACHINE_CONFIG),NONE)
+ifeq ($(CURRENT_KERNEL_CONFIG),RELEASE)
+KERNEL_FILE_NAME = $(KERNEL_FILE_NAME_PREFIX)
+KERNEL_LLDBBOOTSTRAP_NAME = $(KERNEL_FILE_NAME_PREFIX).py
+else
+KERNEL_FILE_NAME = $(KERNEL_FILE_NAME_PREFIX).$(CURRENT_KERNEL_CONFIG_LC)
+KERNEL_LLDBBOOTSTRAP_NAME = $(KERNEL_FILE_NAME_PREFIX).py
+endif
+else
+KERNEL_FILE_NAME = $(KERNEL_FILE_NAME_PREFIX).$(CURRENT_KERNEL_CONFIG_LC).$(CURRENT_MACHINE_CONFIG_LC)
+KERNEL_LLDBBOOTSTRAP_NAME = $(KERNEL_FILE_NAME_PREFIX)_$(CURRENT_KERNEL_CONFIG_LC).py
+endif
+
+#
+# System.kext pseudo-kext install location
+#
+INSTALL_EXTENSIONS_DIR = /System/Library/Extensions
+
+#
+# KDK location
+#
+INSTALL_KERNEL_SYM_DIR = /System/Library/Extensions/KDK
+
+#
+# Misc. Etc.
+#
+INSTALL_SHARE_MISC_DIR = /usr/share/misc
+INSTALL_DTRACE_SCRIPTS_DIR = /usr/lib/dtrace
+
+#
+# Overrides for XBS build aliases
+#
+ifeq ($(RC_ProjectName),xnu_debug)
+INSTALL_KERNEL_DIR := $(DEVELOPER_EXTRAS_DIR)
+INSTALL_KERNEL_SYM_DIR := $(DEVELOPER_EXTRAS_DIR)
+INSTALL_KERNEL_SYM_TO_KDK = 1
+INSTALL_XNU_DEBUG_FILES = 1
+else ifneq ($(filter $(SUPPORTED_EMBEDDED_PLATFORMS),$(PLATFORM)),)
+INSTALL_KERNEL_SYM_TO_KDK = 1
+USE_BINARY_PLIST = 1
+else ifneq ($(filter $(SUPPORTED_SIMULATOR_PLATFORMS),$(PLATFORM)),)
+USE_BINARY_PLIST = 1
+else ifeq ($(PLATFORM),MacOSX)
+INSTALL_KERNEL_DIR := $(SYSTEM_LIBRARY_KERNELS_DIR)
+INSTALL_KERNEL_SYM_DIR := $(SYSTEM_LIBRARY_KERNELS_DIR)
+INSTALL_KERNEL_SYM_TO_KDK = $(if $(filter YES,$(DWARF_DSYM_FILE_SHOULD_ACCOMPANY_PRODUCT)),1,0)
+endif
+
+# vim: set ft=make:
diff -Nur xnu-3247.1.106/osfmk/OPEMU/opemu.c xnu-3247.1.106-AnV/osfmk/OPEMU/opemu.c
--- xnu-3247.1.106/osfmk/OPEMU/opemu.c	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/OPEMU/opemu.c	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1563 @@
+/*   ** SINETEK **
+ * This is called the Opcode Emulator: it traps invalid opcode exceptions
+ *   and modifies the state of the running thread accordingly.
+ * There are two entry points, one for user space exceptions, and another for
+ *   exceptions coming from kernel space.
+ *
+ * STATUS
+ *  . SSE3 is implemented.
+ *  . SSSE3 is implemented.
+ *  . SYSENTER is implemented.
+ *  . SYSEXIT is implemented.
+ *  . RDMSR is implemented.
+ *  . Vector register save and restore is implemented.
+ *
+ * This is a new version of AnV Software based on the AMD SSEPlus project
+ * It runs much more reliable and much faster
+ */
+
+#define __SSEPLUS_NATIVE_SSE2_H__ 1
+#define __SSEPLUS_EMULATION_SSE2_H__ 1
+#define __SSEPLUS_ARITHMETIC_SSE2_H__ 1
+
+#include <stdint.h>
+#include "opemu.h"
+#include "opemu_math.h"
+
+#include <SSEPlus/SSEPlus_Base.h>
+#include <SSEPlus/SSEPlus_REF.h>
+#include <SSEPlus/SSEPlus_SSSE3.h>
+
+#ifndef TESTCASE
+#include <kern/sched_prim.h>
+
+//#define EMULATION_FAILED -1
+
+// forward declaration for syscall handlers of mach/bsd (32+64 bit);
+extern void mach_call_munger(x86_saved_state_t *state);
+extern void unix_syscall(x86_saved_state_t *);
+extern void mach_call_munger64(x86_saved_state_t *state);
+extern void unix_syscall64(x86_saved_state_t *);
+// forward declaration of panic handler for kernel traps;
+extern void panic_trap(x86_saved_state64_t *regs);
+
+// AnV - Implemented i386 version
+#ifdef __x86_64__
+unsigned char opemu_ktrap(x86_saved_state_t *state)
+{
+    x86_saved_state64_t *saved_state = saved_state64(state);
+    uint8_t *code_buffer = (uint8_t *)saved_state->isf.rip;
+    unsigned int bytes_skip = 0;
+    int longmode;
+    longmode = is_saved_state64(state);
+
+    //Enable SSSE3 Soft Emulation
+    bytes_skip = ssse3_run(code_buffer, state, longmode, 1);
+
+    //Enable SSE3 Soft Emulation
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run(code_buffer, state, longmode, 1);
+    }
+
+    if(!bytes_skip)
+    {
+        /* since this is ring0, it could be an invalid MSR read.
+         * Instead of crashing the whole machine, report on it and keep running. */
+        if((code_buffer[0]==0x0f) && (code_buffer[1]==0x32))
+        {
+            printf("[MSR] unknown location 0x%016llx\r\n", saved_state->rcx);
+            // best we can do is return 0;
+            saved_state->rdx = saved_state->rax = 0;
+            bytes_skip = 2;
+        }
+    }
+
+    saved_state->isf.rip += bytes_skip;
+
+    if(!bytes_skip)
+    {
+        uint8_t code0 = code_buffer[0];
+        uint8_t code1 = code_buffer[1];
+        uint8_t code2 = code_buffer[2];
+        uint8_t code3 = code_buffer[3];
+        uint8_t code4 = code_buffer[4];
+        uint8_t code5 = code_buffer[5];
+        uint8_t code6 = code_buffer[6];
+        uint8_t code7 = code_buffer[7];
+        uint8_t code8 = code_buffer[8];
+        uint8_t code9 = code_buffer[9];
+
+        printf("invalid kernel opcode (64-bit): %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n", code0, code1, code2, code3, code4, code5, code6, code7, code8, code9);
+        /* Fall through to trap */
+        return 0;
+    }
+
+    return 1;
+}
+#else
+unsigned char opemu_ktrap(x86_saved_state_t *state)
+{
+    x86_saved_state32_t *saved_state = saved_state32(state);
+    uint64_t op = saved_state->eip;
+    uint8_t *code_buffer = (uint8_t*)op ;
+    unsigned int bytes_skip = 0;
+    int longmode;
+    longmode = is_saved_state32(state);
+
+    //Enable SSSE3 Soft Emulation
+    bytes_skip = ssse3_run(code_buffer, state, longmode, 1);
+
+    //Enable SSE3 Soft Emulation
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run(code_buffer, state, longmode, 1);
+    }
+
+    if(!bytes_skip)
+    {
+        /* since this is ring0, it could be an invalid MSR read.
+         * Instead of crashing the whole machine, report on it and keep running. */
+        if(code_buffer[0]==0x0f && code_buffer[1]==0x32)
+        {
+            printf("[MSR] unknown location 0x%016llx\r\n", saved_state->ecx);
+            // best we can do is return 0;
+            saved_state->edx = saved_state->eax = 0;
+            bytes_skip = 2;
+        }
+    }
+
+    saved_state->eip += bytes_skip;
+
+    if(!bytes_skip)
+    {
+        uint8_t code0 = code_buffer[0];
+        uint8_t code1 = code_buffer[1];
+        uint8_t code2 = code_buffer[2];
+        uint8_t code3 = code_buffer[3];
+        uint8_t code4 = code_buffer[4];
+        uint8_t code5 = code_buffer[5];
+        uint8_t code6 = code_buffer[6];
+        uint8_t code7 = code_buffer[7];
+        uint8_t code8 = code_buffer[8];
+        uint8_t code9 = code_buffer[9];
+
+        printf("invalid kernel opcode (32-bit): %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n", code0, code1, code2, code3, code4, code5, code6, code7, code8, code9);
+        /* Fall through to trap */
+        return 0;
+    }
+
+    return 1;
+}
+#endif
+
+void opemu_utrap(x86_saved_state_t *state)
+{
+    int longmode;
+    unsigned int bytes_skip = 0;
+    vm_offset_t addr;
+
+    if ((longmode = is_saved_state64(state)))
+    {
+        x86_saved_state64_t *saved_state = saved_state64(state);
+        uint8_t *code_buffer = (uint8_t*)saved_state->isf.rip;
+        addr = saved_state->isf.rip;
+        uint16_t opcode;
+        opcode = *(uint16_t *) addr;
+        x86_saved_state64_t *regs;
+        regs = saved_state64(state);
+
+		//sysenter
+        if (opcode == 0x340f)
+        {
+            regs->isf.rip = regs->rdx;
+            regs->isf.rsp = regs->rcx;
+
+            if((signed int)regs->rax < 0) {
+                //printf("mach call 64\n");
+                mach_call_munger64(state);
+            } else {
+                //printf("unix call 64\n");
+                unix_syscall64(state);
+            }
+            return;
+        }
+
+		//sysexit
+        if (opcode == 0x350f)
+        {
+            regs->isf.rip = regs->rdx;
+            regs->isf.rsp = regs->rcx;
+            thread_exception_return();
+            /*if (kernel_trap)
+            {
+                addr = regs->rcx;
+                return 0x7FFF;
+            }
+            else
+            {
+                thread_exception_return();
+            }*/
+            return;
+        }
+
+        //Enable SSSE3 Soft Emulation
+        bytes_skip = ssse3_run(code_buffer, state, longmode, 0);
+
+        //Enable SSE3 Soft Emulation
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run(code_buffer, state, longmode, 0);
+        }
+
+        regs->isf.rip += bytes_skip;
+
+        if(!bytes_skip)
+        {
+            uint8_t code0 = code_buffer[0];
+            uint8_t code1 = code_buffer[1];
+            uint8_t code2 = code_buffer[2];
+            uint8_t code3 = code_buffer[3];
+            uint8_t code4 = code_buffer[4];
+            uint8_t code5 = code_buffer[5];
+            uint8_t code6 = code_buffer[6];
+            uint8_t code7 = code_buffer[7];
+            uint8_t code8 = code_buffer[8];
+            uint8_t code9 = code_buffer[9];
+
+            printf("invalid user opcode (64-bit): %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n", code0, code1, code2, code3, code4, code5, code6, code7, code8, code9);
+            /* Fall through to trap */
+            return;
+        }
+    }
+    else
+    {
+        x86_saved_state32_t *saved_state = saved_state32(state);
+        uint64_t op = saved_state->eip;
+        uint8_t *code_buffer = (uint8_t*)op;
+        addr = saved_state->eip;
+        uint16_t opcode;
+        opcode = *(uint16_t *) addr;
+        x86_saved_state32_t *regs;
+        regs = saved_state32(state);
+
+        //sysenter
+        if (opcode == 0x340f)
+        {
+            regs->eip = regs->edx;
+            regs->uesp = regs->ecx;
+
+            if((signed int)regs->eax < 0) {
+                mach_call_munger(state);
+            } else {
+                unix_syscall(state);
+            }
+            return;
+        }
+
+        //sysexit
+        if (opcode == 0x350f)
+        {
+            regs->eip = regs->edx;
+            regs->uesp = regs->ecx;
+            thread_exception_return();
+            return;
+        }
+
+        //Enable SSSE3 Soft Emulation
+        bytes_skip = ssse3_run(code_buffer, state, longmode, 0);
+
+        //Enable SSE3 Soft Emulation
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run(code_buffer, state, longmode, 0);
+        }
+
+        regs->eip += bytes_skip;
+        
+        if(!bytes_skip)
+        {
+            uint8_t code0 = code_buffer[0];
+            uint8_t code1 = code_buffer[1];
+            uint8_t code2 = code_buffer[2];
+            uint8_t code3 = code_buffer[3];
+            uint8_t code4 = code_buffer[4];
+            uint8_t code5 = code_buffer[5];
+            uint8_t code6 = code_buffer[6];
+            uint8_t code7 = code_buffer[7];
+            uint8_t code8 = code_buffer[8];
+            uint8_t code9 = code_buffer[9];
+
+            printf("invalid user opcode (32-bit): %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n", code0, code1, code2, code3, code4, code5, code6, code7, code8, code9);
+            /* Fall through to trap */
+            return;
+        }
+    }
+    thread_exception_return();
+    /*** NOTREACHED ***/
+    //EMULATION_FAILED;
+}
+
+/*
+void print_bytes(uint8_t *from, int size)
+{
+    int i;
+    for(i = 0; i < size; ++i)
+    {
+        printf("%02x ", from[i]);
+    }
+    printf("\n");
+}
+*/
+
+/** Runs the sse3 emulator. returns the number of bytes consumed.
+ **/
+int sse3_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap)
+{
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    ssp_m128 xmmsrc, xmmdst, xmmres;
+    int src_higher = 0, dst_higher = 0;
+    int modbyte = 0; //Calculate byte 0 - modrm long
+    int fisttp = 0;
+    int rex = 0; //REX Mode
+    int hsreg = 0; //High Source Register Only
+
+    // SSE3 Type 1
+    if(*bytep == 0x66 && bytep[1] == 0x0f)
+    {
+        bytep += 2;
+        ins_size += 2;
+        modbyte += 4;
+
+        uint8_t *modrm = &bytep[1];
+        ins_size += 1;
+        int consumed = operands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, rex, hsreg, modbyte, fisttp);
+        ins_size += consumed;
+
+        switch (*bytep)
+        {
+            case 0x7C: //haddpd(&xmmsrc,&xmmdst,&xmmres); break;
+                xmmres.d = ssp_hadd_pd_REF(xmmdst.d, xmmsrc.d); break;
+            case 0x7D: //hsubpd(&xmmsrc,&xmmdst,&xmmres); break;
+                xmmres.d = ssp_hsub_pd_REF(xmmdst.d, xmmsrc.d); break;
+            case 0xD0: //addsubpd(&xmmsrc,&xmmdst,&xmmres); break;
+                xmmres.d = ssp_addsub_pd_REF(xmmdst.d, xmmsrc.d); break;
+            default: return 0;
+        }
+
+        storeresult128(*modrm, dst_higher, xmmres);
+
+        return ins_size;
+    }
+
+    // SSE3 Type 2
+    else if(*bytep == 0xF2 && bytep[1] == 0x0f)
+    {
+        bytep += 2;
+        ins_size += 2;
+        modbyte += 4;
+
+        uint8_t *modrm = &bytep[1];
+        ins_size += 1;
+        int consumed = operands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, rex, hsreg, modbyte, fisttp);
+        ins_size += consumed;
+
+        switch (*bytep)
+        {
+            case 0x12: //movddup(&xmmsrc,&xmmres); break;
+                xmmres.d = ssp_movedup_pd_REF(xmmsrc.d); break;
+            case 0x7C: //haddps(&xmmsrc,&xmmdst,&xmmres); break;
+                xmmres.f = ssp_hadd_ps_REF(xmmdst.f, xmmsrc.f); break;
+            case 0x7D: //hsubps(&xmmsrc,&xmmdst,&xmmres); break;
+                xmmres.f = ssp_hsub_ps_REF(xmmdst.f, xmmsrc.f); break;
+            case 0xD0: //addsubps(&xmmsrc,&xmmdst,&xmmres); break;
+                xmmres.f = ssp_addsub_ps_REF(xmmdst.f, xmmsrc.f); break;
+            case 0xF0: //lddqu(&xmmsrc,&xmmres); break;
+                xmmres.i = ssp_lddqu_si128_REF(&xmmsrc.i); break;
+            default: return 0;
+        }
+
+        storeresult128(*modrm, dst_higher, xmmres);
+
+        return ins_size;
+    }
+
+    // SSE3 Type 3
+    else if(*bytep == 0xF3 && bytep[1] == 0x0f)
+    {
+        bytep += 2;
+        ins_size += 2;
+        modbyte += 4;
+
+        uint8_t *modrm = &bytep[1];
+        ins_size += 1;
+        int consumed = operands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, rex, hsreg, modbyte, fisttp);
+        ins_size += consumed;
+
+        switch (*bytep)
+        {
+            case 0x12: //movsldup(&xmmsrc,&xmmres); break;
+                xmmres.f = ssp_moveldup_ps_REF(xmmsrc.f); break;
+            case 0x16: //movshdup(&xmmsrc,&xmmres); break;
+                xmmres.f = ssp_movehdup_ps_REF(xmmsrc.f); break;
+            default: return 0;
+        }
+
+        storeresult128(*modrm, dst_higher, xmmres);
+
+        return ins_size;
+    }
+
+    //SSE3 FISTTP
+    //else if ((*bytep == 0x66 && bytep[1] == 0xDB)||(*bytep == 0x66 && bytep[1] == 0xDD)||(*bytep == 0x66 && bytep[1] == 0xDF))
+    else if ((*bytep == 0xDB)||(*bytep == 0xDD)||(*bytep == 0xDF))
+    {
+        //bytep++;
+        //ins_size += 2;
+        //modbyte += 3;
+
+        ins_size++;
+        modbyte += 2;
+
+        switch (*bytep)
+        {
+            case 0xDB: //fild 0xDB
+                fisttp = 1;
+                break;
+            case 0xDD: //fld 0xDD
+                fisttp = 2;
+                break;
+            case 0xDF: //fild 0xDF
+                fisttp = 3;
+                break;
+        }
+
+        uint8_t *modrm = &bytep[1];
+        int consumed = operands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, rex, hsreg, modbyte, fisttp);
+        ins_size += consumed;
+
+        return ins_size;
+    }
+
+    //SSE3 monitor/mwait
+    else if((*bytep == 0x0F) && (bytep[1] == 0x01))
+    {
+
+        bytep += 2;
+        ins_size += 3;
+
+        switch(*bytep)
+        {
+            
+            case 0xC8: break; //monitor: 0x0f,0x01,0xc8
+            case 0xC9: break; //mwait: 0x0f,0x01,0xc9
+            default: return 0;
+        }
+
+        return ins_size;
+    }
+
+    return ins_size;
+}
+
+/** Runs the ssse3 emulator. returns the number of bytes consumed.
+ **/
+int ssse3_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap)
+{
+    // pointer to the current byte we're working on
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    int is_128 = 0, src_higher = 0, dst_higher = 0;
+    int modbyte = 0; //Calculate byte 0 - modrm long
+    int fisttp = 0;
+    int rex = 0; //REX Mode
+    int hsreg = 0; //High Source Register Only
+
+    ssp_m128 xmmsrc, xmmdst, xmmres;
+    ssp_m64 mmsrc,mmdst, mmres;
+
+    /* We can get a few prefixes, in any order:
+     * 66 throws into 128-bit xmm mode.
+     */
+    if(*bytep == 0x66)
+    {
+        is_128 = 1;
+        bytep++;
+        ins_size++;
+        modbyte++;
+    }
+
+    /* REX Prefixes 40-4F Use REX Mode.
+     * Use higher registers.
+     * xmm8-15 or R8-R15.
+     */
+    if((*bytep & 0xF0) == 0x40)
+    {
+        rex = 1;
+        if(*bytep & 1) src_higher = 1;
+        if(*bytep & 4) dst_higher = 1;
+
+        /*** High Source Register Only ***/
+        if((*bytep == 0x41)||(*bytep == 0x43)||(*bytep == 0x49)||(*bytep == 0x4B))
+            hsreg = 1;
+
+        bytep++;
+        ins_size++;
+        modbyte++;
+    }
+
+    if(*bytep != 0x0f) return 0;
+
+    bytep++;
+    ins_size++;
+    modbyte++;
+
+    /* Two SSSE3 instruction prefixes. */
+    if(((*bytep == 0x38) && (bytep[1] != 0x0f)) || ((*bytep == 0x3a) && (bytep[1] == 0x0f)))
+    {
+        uint8_t opcode = bytep[1];
+        uint8_t *modrm = &bytep[2];
+        uint8_t imm;
+/*
+        uint8_t mod = *modrm >> 6;
+        uint8_t num_src = *modrm & 0x7;
+        uint8_t *sib = &bytep[3];
+        uint8_t base = *sib & 0x7;
+
+        if (mod == 0)
+        {
+            if (num_src == 4)
+                if(base == 5) imm = *((uint8_t*)&bytep[8]); //modrm offset + 6
+                else imm = *((uint8_t*)&bytep[4]); //modrm offset + 2
+            else if (num_src == 5) imm = *((uint8_t*)&bytep[7]); //modrm offset + 5
+            else imm = *((uint8_t*)&bytep[3]); //modrm offset + 1
+        }
+
+        if (mod == 1)
+        {
+            if(num_src == 4) imm = *((uint8_t*)&bytep[5]); //modrm offset + 3
+            else imm = *((uint8_t*)&bytep[4]); //modrm offset + 2
+        }
+
+        if (mod == 2)
+        {
+            if(num_src == 4) imm = *((uint8_t*)&bytep[8]); //modrm offset + 6
+            else imm = *((uint8_t*)&bytep[7]); //modrm offset + 5
+        }
+
+        if (mod == 3) imm = *((uint8_t*)&bytep[3]); //modrm offset + 1
+*/
+        ins_size += 2; // not counting modRM byte or anything after.
+        if((*bytep == 0x3a) && (bytep[1] == 0x0f)) modbyte += 4;
+        else modbyte += 3;
+
+        if(is_128)
+        {
+            int consumed = operands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, rex, hsreg, modbyte, fisttp);
+            imm = *((uint8_t*)&bytep[2 + consumed]);
+            ins_size += consumed;
+
+            switch(opcode)
+            {
+                case 0x00: //pshufb128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_shuffle_epi8_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x01: //phaddw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_hadd_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x02: //phaddd128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_hadd_epi32_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x03: //phaddsw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_hadds_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x04: //pmaddubsw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_maddubs_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x05: //phsubw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_hsub_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x06: //phsubd128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_hsub_epi32_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x07: //phsubsw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_hsubs_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x08: //psignb128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_sign_epi8_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x09: //psignw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_sign_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x0A: //psignd128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_sign_epi32_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x0B: //pmulhrsw128(&xmmsrc,&xmmdst,&xmmres); break;
+                    xmmres.i = ssp_mulhrs_epi16_SSSE3 (xmmdst.i, xmmsrc.i); break;
+                case 0x0F: //palignr128(&xmmsrc,&xmmdst,&xmmres,imm); ins_size++; break;
+                    xmmres.i = ssp_alignr_epi8_SSSE3 (xmmdst.i, xmmsrc.i, imm); ins_size++; break;
+                case 0x1C: //pabsb128(&xmmsrc,&xmmres); break;
+                    xmmres.i = ssp_abs_epi8_SSSE3 (xmmsrc.i); break;
+                case 0x1D: //pabsw128(&xmmsrc,&xmmres); break;
+                    xmmres.i = ssp_abs_epi16_SSSE3 (xmmsrc.i); break;
+                case 0x1E: //pabsd128(&xmmsrc,&xmmres); break;
+                    xmmres.i = ssp_abs_epi32_SSSE3 (xmmsrc.i); break;
+                default: return 0;
+            }
+
+            storeresult128(*modrm, dst_higher, xmmres);
+        }
+        else
+        {
+            int consumed = operands(modrm, src_higher, dst_higher, &mmsrc, &mmdst, longmode, state, kernel_trap, 0, rex, hsreg, modbyte, fisttp);
+            imm = *((uint8_t*)&bytep[2 + consumed]);
+            ins_size += consumed;
+
+            switch(opcode)
+            {
+                case 0x00: //pshufb64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_shuffle_pi8_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x01: //phaddw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_hadd_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x02: //phaddd64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_hadd_pi32_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x03: //phaddsw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_hadds_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x04: //pmaddubsw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_maddubs_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x05: //phsubw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_hsub_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x06: //phsubd64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_hsub_pi32_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x07: //phsubsw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_hsubs_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x08: //psignb64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_sign_pi8_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x09: //psignw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_sign_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x0A: //psignd64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_sign_pi32_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x0B: //pmulhrsw64(&mmsrc,&mmdst,&mmres); break;
+                    mmres.m64 = ssp_mulhrs_pi16_SSSE3 (mmdst.m64, mmsrc.m64); break;
+                case 0x0F: //palignr64(&mmsrc,&mmdst,&mmres, imm); ins_size++; break;
+                    mmres.m64 = ssp_alignr_pi8_SSSE3 (mmdst.m64, mmsrc.m64, imm); ins_size++; break;
+                case 0x1C: //pabsb64(&mmsrc,&mmres); break;
+                    mmres.m64 = ssp_abs_pi8_SSSE3 (mmsrc.m64); break;
+                case 0x1D: //pabsw64(&mmsrc,&mmres); break;
+                    mmres.m64 = ssp_abs_pi16_SSSE3 (mmsrc.m64); break;
+                case 0x1E: //pabsd64(&mmsrc,&mmres); break;
+                    mmres.m64 = ssp_abs_pi32_SSSE3 (mmsrc.m64); break;
+                default: return 0;
+            }
+
+            storeresult64(*modrm, dst_higher, mmres);
+        }
+    }
+    else
+    {
+        // opcode wasn't handled here
+        return 0;
+    }
+
+    return ins_size;
+}
+
+/* Fetch SSEX operands (except immediate values, which are fetched elsewhere).
+ * We depend on memory copies, which differs depending on whether we're in kernel space
+ * or not. For this reason, need to pass in a lot of information, including the state of
+ * registers.
+ *
+ * The return value is the number of bytes used, including the ModRM byte,
+ * and displacement values, as well as SIB if used.
+ */
+int operands(uint8_t *ModRM, unsigned int hsrc, unsigned int hdst, void *src, void *dst, unsigned int longmode, x86_saved_state_t *saved_state, int kernel_trap, int size_128, int rex, int hsreg, int modbyte, int fisttp)
+{
+    /*** ModRM Is First Addressing Modes ***/
+    /*** SIB Is Second Addressing Modes ***/
+    unsigned int num_src = *ModRM & 0x7; // R/M (register or memory) 
+    unsigned int num_dst = (*ModRM >> 3) & 0x7; // digit/xmm register (xmm/mm)
+    unsigned int mod = *ModRM >> 6; // Mod
+    int consumed = 1; //modrm + 1 byte
+    uint8_t bytelong = 0x00;
+
+    if(hsrc) num_src += 8;
+    if(hdst) num_dst += 8;
+
+    if(size_128) getxmm((ssp_m128*)dst, num_dst);
+    else getmm((ssp_m64*)dst, num_dst);
+
+    if(mod == 3) //mod field = 11b
+    {
+        if(size_128) getxmm((ssp_m128*)src, num_src);
+        else getmm((ssp_m64*)src, num_src);
+    }
+
+    // Implemented 64-bit fetch
+    else if ((longmode = is_saved_state64(saved_state)))
+    {
+        uint64_t address;
+        // DST is always an XMM register. decode for SRC.
+        uint64_t reg_sel[8];
+        x86_saved_state64_t *r64 = saved_state64(saved_state);
+
+        if (hsrc)
+        {
+            reg_sel[0] = r64->r8;
+            reg_sel[1] = r64->r9;
+            reg_sel[2] = r64->r10;
+            reg_sel[3] = r64->r11;
+            reg_sel[4] = r64->r12;
+            reg_sel[5] = r64->r13;
+            reg_sel[6] = r64->r14;
+            reg_sel[7] = r64->r15;
+        }
+        else
+        {
+            reg_sel[0] = r64->rax;
+            reg_sel[1] = r64->rcx;
+            reg_sel[2] = r64->rdx;
+            reg_sel[3] = r64->rbx;
+            reg_sel[4] = r64->isf.rsp;
+            reg_sel[5] = r64->rbp;
+            reg_sel[6] = r64->rsi;
+            reg_sel[7] = r64->rdi;
+        }
+
+        /*** DEBUG ***/
+        //if(hdst) printf("opemu debug: high Register ssse\n"); // use xmm8-xmm15 register
+
+        /*** REX Prefixes 40-4F Use REX Mode ***/
+        if (rex)
+        {
+            /*** R/M = RSP/R12 USE SIB Addressing Modes ***/
+            if (num_src == 4)
+            {
+                uint8_t *sib = (uint8_t *)&ModRM[1]; //Second Addressing Modes
+                uint8_t scale = *sib >> 6; //SIB Scale field
+                uint8_t base = *sib & 0x7; //SIB Base
+                uint8_t index = (*sib >> 3) & 0x7; //SIB Index
+                uint8_t factor; //Scaling factor
+                if (scale == 0) factor = 1;
+                else if (scale == 1) factor = 2;
+                else if (scale == 2) factor = 4;
+                else if (scale == 3) factor = 8;
+
+                if (mod == 0) //mod field = 00b
+                {
+                    if(base == 5) //Base Register = RBP
+                    {
+                        if(index == 4) //Base Register = RBP & Index Register = RSP
+                        {
+                            if (hsreg)
+                            {
+                                //ModRM                 0  1  2  3  4  5
+                                //byte   0  1  2  3  4  5  6  7  8  9  10
+                                //OPCPDE 66 43 0F 38 01 04 65 04 03 02 01
+                                //INS    phaddw xmm0, xmmword [ds:0x1020304+r12*2]
+                                //PTR = Disp32 + (Index*Scale)
+                                bytelong = modbyte + 5;
+                                address = (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2])) + (reg_sel[index] * factor);
+                                consumed += 5;
+                            }
+                            else
+                            {
+                                //ModRM                 0  1  2  3  4  5
+                                //byte   0  1  2  3  4  5  6  7  8  9  10
+                                //OPCPDE 66 40 0F 38 01 04 65 04 03 02 01
+                                //INS    phaddw xmm0, xmmword [ds:0x1020304]
+                                //PTR = Disp32
+                                bytelong = modbyte + 5;
+                                address = r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]);
+                                consumed += 5;
+                            }
+                        }
+                        else //base 5 & Not Index 4
+                        {
+                            //ModRM                 0  1  2  3  4  5
+                            //byte   0  1  2  3  4  5  6  7  8  9  10
+                            //OPCPDE 66 43 0F 38 01 04 45 04 03 02 01
+                            //INS    phaddw xmm0, xmmword [ds:0x1020304+r8*2]
+                            //PTR = Disp32 + (Index*Scale)
+                            bytelong = modbyte + 5;
+                            address = (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2])) + (reg_sel[index] * factor);
+                            consumed += 5;
+                        }
+                    }
+                    else //Not Base 5
+                    {
+                        if (index == 4) // Index Register = RSP
+                        {
+                            if (hsreg)
+                            {
+                                //ModRM                 0  1
+                                //byte   0  1  2  3  4  5  6
+                                //OPCPDE 66 43 0F 38 01 04 64
+                                //INS    phaddw xmm0, xmmword [ds:r12+r12*2]
+                                //PTR = Base + (Index*Scale)
+                                address = reg_sel[base] + (reg_sel[index] * factor);
+                                consumed++;
+                            }
+                            else
+                            {
+                                //ModRM                 0  1
+                                //byte   0  1  2  3  4  5  6
+                                //OPCPDE 66 40 0F 38 01 04 63
+                                //INS    phaddw xmm0, xmmword [ds:rbx]
+                                //PTR = Base
+                                address = reg_sel[base];
+                                consumed++;
+                            }
+                        }
+                        else //SIB General Mode
+                        {
+                            //ModRM                 0  1
+                            //byte   0  1  2  3  4  5  6
+                            //OPCPDE 66 43 0F 38 01 04 44
+                            //INS    phaddw xmm0, xmmword [ds:r12+r8*2]
+                            //PTR = Base + (Index*Scale)
+                            address = reg_sel[base] + (reg_sel[index] * factor);
+                            consumed++;
+                        }
+                    }
+                }
+
+                else if (mod == 1) //mod field = 01b
+                {
+                    if (index == 4) // Index Register = RSP
+                    {
+                        if (hsreg)
+                        {
+                            //ModRM                 0  1  2
+                            //byte   0  1  2  3  4  5  6  7
+                            //OPCPDE 66 43 0F 38 01 44 65 04
+                            //INS    phaddw     xmm0, xmmword [ds:r13+r12*2+0x4]
+                            //PTR = Base + (Index*Scale) + Disp8
+                            address = reg_sel[base] + (reg_sel[index] * factor) + *((int8_t*)&ModRM[2]);
+                            consumed+= 2;
+                        }
+                        else
+                        {
+                            //ModRM                 0  1  2
+                            //byte   0  1  2  3  4  5  6  7
+                            //OPCPDE 66 40 0F 38 01 44 65 04
+                            //INS    phaddw xmm0, xmmword [ss:rbp+0x4]
+                            //PTR = Base + Disp8
+                            address = reg_sel[base] + *((int8_t*)&ModRM[2]);
+                            consumed+= 2;
+                        }
+                    }
+                    else //SIB General Mode
+                    {
+                        //ModRM                 0  1  2
+                        //byte   0  1  2  3  4  5  6  7
+                        //OPCPDE 66 40 0F 38 01 44 44 04
+                        //INS    phaddw xmm0, xmmword [ss:rsp+rax*2+0x4]
+                        //PTR = Base + (Index*Scale) + Disp32
+                        address = reg_sel[base] + (reg_sel[index] * factor) + *((int8_t*)&ModRM[2]);
+                        consumed+= 2;
+                    }
+                }
+
+                else if (mod == 2) //mod field = 10b
+                {
+                    if (index == 4) // Index Register = RSP
+                    {
+                        if (hsreg)
+                        {
+                            //ModRM                 0  1  2  3  4  5
+                            //byte   0  1  2  3  4  5  6  7  8  9  10
+                            //OPCPDE 66 43 0F 38 01 84 64 04 03 02 01
+                            //INS    phaddw xmm0, xmmword [ds:r12+r12*2+0x1020304]
+                            //PTR = Base + (Index*Scale) + Disp32
+                            bytelong = modbyte + 5;
+                            address = reg_sel[base] + (reg_sel[index] * factor) + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]));
+                            consumed += 5;
+                        }
+                        else
+                        {
+                            //ModRM                 0  1  2  3  4  5
+                            //byte   0  1  2  3  4  5  6  7  8  9  10
+                            //OPCPDE 66 40 0F 38 01 84 64 04 03 02 01
+                            //INS    phaddw xmm0, xmmword [ss:rsp+0x1020304]
+                            //PTR = Base + Disp32
+                            bytelong = modbyte + 5;
+                            address = reg_sel[base] + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]));
+                            consumed += 5;
+                        }
+                    }
+                    else //SIB General Mode
+                    {
+                        //ModRM                 0  1  2  3  4  5
+                        //byte   0  1  2  3  4  5  6  7  8  9  10
+                        //OPCPDE 66 43 0F 38 01 84 44 04 03 02 01
+                        //INS    phaddw xmm0, xmmword [ds:r12+r8*2+0x1020304]
+                        //PTR = Base + (Index*Scale) + Disp32
+                        bytelong = modbyte + 5;
+                        address = reg_sel[base] + (reg_sel[index] * factor) + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]));
+                        consumed += 5;
+                    }
+                }
+            }
+            /*** R/M = RBP in mod 0 Use Disp32 Offset ***/
+            else if (num_src == 5)
+            {
+                if (mod == 0)
+                {
+                    //ModRM                 0  1  2  3  4
+                    //byte   0  1  2  3  4  5  6  7  8  9
+                    //OPCPDE 66 43 0F 38 01 05 04 03 02 01
+                    //INS    phaddw xmm0, xmmword [ds:0x102112e]
+                    //PTR = Disp32
+                    bytelong = modbyte + 4;
+                    address = r64->isf.rip + bytelong + *((int32_t*)&ModRM[1]);
+                    consumed += 4;
+                }
+            }
+
+            /*** General Mode ***/
+            else
+            {
+                if (mod == 0) //mod field = 00b
+                {
+                    //ModRM                 0
+                    //BYTE   0  1  2  3  4  5
+                    //OPCPDE 66 43 0F 38 01 03
+                    //INS    phaddw xmm0, xmmword [ds:r11]
+                    //PTR = R/M
+                    address = reg_sel[num_src];
+                }
+                else if (mod == 1) //mod field = 01b
+                {
+                    //ModRM                 0  1
+                    //byte   0  1  2  3  4  5  6
+                    //OPCPDE 66 43 0F 38 01 43 01
+                    //INS    phaddw xmm0, xmmword [ds:r11+0x1]
+                    //PTR = R/M + Disp8
+                    address = reg_sel[num_src] + *((int8_t*)&ModRM[1]);
+                    consumed++;
+                }
+                else if (mod == 2) //mod field = 10b
+                {
+                    //ModRM                 0  1  2  3  4
+                    //byte   0  1  2  3  4  5  6  7  8  9
+                    //OPCPDE 66 43 0F 38 01 83 04 03 02 01
+                    //INS    phaddw xmm0, xmmword [ds:r11+0x1020304]
+                    //PTR = R/M + Disp32
+                    bytelong = modbyte + 4;
+                    address = reg_sel[num_src] + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[1]));
+                    consumed += 4;
+                }
+            }
+
+        } //REX Mode END
+        else
+        {
+            /*** R/M = RSP USE SIB Addressing Modes ***/
+            if (num_src == 4)
+            {
+                uint8_t *sib = (uint8_t *)&ModRM[1]; //Second Addressing Modes
+                uint8_t scale = *sib >> 6; //SIB Scale field
+                uint8_t base = *sib & 0x7; //SIB Base
+                uint8_t index = (*sib >> 3) & 0x7; //SIB Index
+                uint8_t factor; //Scaling factor
+                if (scale == 0) factor = 1;
+                else if (scale == 1) factor = 2;
+                else if (scale == 2) factor = 4;
+                else if (scale == 3) factor = 8;
+
+                if (mod == 0) //mod field = 00b
+                {
+                    if(base == 5) //Base Register = RBP
+                    {
+                        if(index == 4) //Base Register = RBP & Index Register = RSP
+                        {
+                            //ModRM              0  1  2  3  4  5
+                            //byte   0  1  2  3  4  5  6  7  8  9
+                            //OPCPDE 66 0F 38 01 04 25 04 03 02 01
+                            //INS    phaddw xmm0, xmmword [ds:0x1020304]
+                            //PTR = Disp32
+                            bytelong = modbyte + 5;
+                            address = r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]);
+                            consumed += 5;
+                        }
+                        else //base 5 & Not Index 4
+                        {
+                            //ModRM              0  1  2  3  4  5
+                            //byte   0  1  2  3  4  5  6  7  8  9
+                            //OPCPDE 66 0F 38 01 04 05 04 03 02 01
+                            //INS    phaddw xmm0, xmmword [ds:0x1020304+rax]
+                            //PTR = Disp32 + (Index*Scale)
+                            bytelong = modbyte + 5;
+                            address = (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2])) + (reg_sel[index] * factor);
+                            consumed += 5;
+                        }
+                    }
+                    else //Not Base 5
+                    {
+                        if (index == 4) // Index Register = RSP
+                        {
+                            //ModRM              0  1
+                            //byte   0  1  2  3  4  5
+                            //OPCPDE 66 0F 38 01 04 61
+                            //INS    phaddw xmm0, xmmword [ds:rcx]
+                            //PTR = Base
+                            address = reg_sel[base];
+                            consumed++;
+                        }
+                        else //SIB General Mode
+                        {
+                            //ModRM              0  1
+                            //byte   0  1  2  3  4  5
+                            //OPCPDE 66 0F 38 01 04 44
+                            //INS    phaddw xmm0, xmmword ptr [rsp+rax*2]
+                            //PTR = Base + (Index*Scale)
+                            address = reg_sel[base] + (reg_sel[index] * factor);
+                            consumed++;
+                        }
+                    }
+                }
+
+                else if (mod == 1) //mod field = 01b
+                {
+                    if (index == 4) // Index Register = RSP
+                    {
+                        //ModRM              0  1  2
+                        //BYTE   0  1  2  3  4  5  6
+                        //OPCPDE 66 0F 38 01 44 27 80
+                        //INS    phaddw xmm0, xmmword ptr [rdi-80h]
+                        //PTR = Base + Disp8
+                        address = reg_sel[base] + *((int8_t*)&ModRM[2]);
+                        consumed+= 2;
+                    }
+                    else //SIB General Mode
+                    {
+                        //ModRM              0  1  2
+                        //BYTE   0  1  2  3  4  5  6
+                        //OPCPDE 66 0F 38 01 44 44 80
+                        //INS    phaddw xmm0, xmmword ptr [rsp+rax*2-80h]
+                        //PTR = Base + (Index*Scale) + Disp8
+                        address = reg_sel[base] + (reg_sel[index] * factor) + *((int8_t*)&ModRM[2]);
+                        consumed+= 2;
+                    }
+                }
+
+                else if (mod == 2) //mod field = 10b
+                {
+                    if (index == 4) // Index Register = RSP
+                    {
+                        //ModRM              0  1  2  3  4  5
+                        //BYTE   0  1  2  3  4  5  6  7  8  9
+                        //OPCPDE 66 0F 38 01 84 20 04 03 02 01
+                        //INS    phaddw xmm0, xmmword ptr [rax+1020304h]
+                        //PTR = Base + Disp32
+                        bytelong = modbyte + 5;
+                        address = reg_sel[base] + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]));
+                        consumed += 5;
+                    }
+                    else //SIB General Mode
+                    {
+                        //ModRM              0  1  2  3  4  5
+                        //BYTE   0  1  2  3  4  5  6  7  8  9
+                        //OPCPDE 66 0F 38 01 84 44 04 03 02 01
+                        //INS    phaddw xmm0, xmmword ptr [rsp+rax*2+1020304h]
+                        //PTR = Base + (Index*Scale) + Disp32
+                        bytelong = modbyte + 5;
+                        address = reg_sel[base] + (reg_sel[index] * factor) + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[2]));
+                        consumed += 5;
+                    }
+                }
+            }
+            /*** R/M = RBP in mod 0 Use Disp32 Offset ***/
+            else if (num_src == 5)
+            {
+                if (mod == 0) //mod field = 00b
+                {
+                    //ModRM              0  1  2  3  4
+                    //BYTE   0  1  2  3  4  5  6  7  8
+                    //OPCPDE 66 0F 38 01 05 01 02 03 04
+                    //INS    phaddw xmm0, xmmword ptr [cs:04030201h]
+                    //PTR = Disp32
+                    bytelong = modbyte + 4;
+                    address = r64->isf.rip + bytelong + *((int32_t*)&ModRM[1]);
+                    consumed += 4;
+                    /***DEBUG***/
+                    //int32_t ds = *((int32_t*)&ModRM[1]);
+                    //printf("DEBUG-MSG: byte=%x, RIP=%llx ,DS=%d, PTR = %llx\n", bytelong , r64->isf.rip, ds, address);
+                    /***DEBUG***/
+                }
+            }
+
+            /*** General Mode ***/
+            else
+            {
+                if (mod == 0) //mod field = 00b
+                {
+                    //ModRM              0
+                    //BYTE   0  1  2  3  4
+                    //OPCPDE 66 0F 38 01 03
+                    //INS    phaddw xmm0, xmmword [ds:rbx]
+                    //PTR = R/M
+                    address = reg_sel[num_src];
+                }
+                else if (mod == 1) //mod field = 01b
+                {
+                    //ModRM              0  1
+                    //BYTE   0  1  2  3  4  5
+                    //OPCPDE 66 0F 38 01 43 01
+                    //INS    phaddw xmm0, xmmword [ds:rbx+0x1]
+                    //PTR = R/M + Disp8
+                    address = reg_sel[num_src] + *((int8_t*)&ModRM[1]);
+                    consumed++;
+                }
+                else if (mod == 2) //mod field = 10b
+                {
+                    //ModRM              0  1  2  3  4
+                    //BYTE   0  1  2  3  4  5  6  7  8
+                    //OPCPDE 66 0F 38 01 83 01 02 03 04
+                    //INS    phaddw xmm0, xmmword [ds:rbx+0x4030201]
+                    //PTR = R/M + Disp32
+                    bytelong = modbyte + 4;
+                    address = reg_sel[num_src] + (r64->isf.rip + bytelong + *((int32_t*)&ModRM[1]));
+                    consumed += 4;
+                }
+            }
+        }
+
+        if (fisttp == 1) //fild 0x66 0xDB
+        {
+            *(int *)address = fisttpl((double *)address);
+        }
+        else if (fisttp == 2) //fld 0x66 0xDD
+        {
+            *(long long *)address = fisttpq((long double *)address);
+        }
+        else if (fisttp == 3) //fild 0x66 0xDF
+        {
+            *(short *)address = fisttps((float *)address);
+        }
+        else //fisttp = 0
+        {
+            // address is good now, do read and store operands.
+            if(kernel_trap)
+            {
+                if(size_128) ((ssp_m128*)src)->ui = *((__uint128_t*)address);
+                else ((ssp_m64*)src)->u64 = *((uint64_t*)address);
+            }
+            else
+            {
+                //printf("XNU: PTR = %llx, RIP=%llx, RSP=%llx\n", address, r64->isf.rip, reg_sel[4]);
+                if(size_128) copyin(address, (char*)& ((ssp_m128*)src)->ui, 16);
+                else copyin(address, (char*)& ((ssp_m64*)src)->u64, 8);
+            }
+        }
+    }
+    // AnV - Implemented 32-bit fetch
+    else
+    {
+        uint32_t address;
+        // DST is always an XMM register. decode for SRC.
+        uint32_t reg_sel[8];
+        x86_saved_state32_t* r32 = saved_state32(saved_state);
+
+        reg_sel[0] = r32->eax;
+        reg_sel[1] = r32->ecx;
+        reg_sel[2] = r32->edx;
+        reg_sel[3] = r32->ebx;
+        reg_sel[4] = r32->uesp;
+        reg_sel[5] = r32->ebp;
+        reg_sel[6] = r32->esi;
+        reg_sel[7] = r32->edi;
+
+        if (hsrc) printf("opemu error: high r/m Reg ssse\n"); // FIXME
+
+        /*** R/M = ESP USE SIB Addressing Modes ***/
+        if (num_src == 4)
+        {
+            uint8_t *sib = (uint8_t *)&ModRM[1]; //Second Addressing Modes
+            uint8_t scale = *sib >> 6; //SIB Scale field
+            uint8_t base = *sib & 0x7; //SIB Base
+            uint8_t index = (*sib >> 3) & 0x7; //SIB Index
+            uint8_t factor; //Scaling factor
+            if (scale == 0) factor = 1;
+            else if (scale == 1) factor = 2;
+            else if (scale == 2) factor = 4;
+            else if (scale == 3) factor = 8;
+
+            if (mod == 0) //mod field = 00b
+            {
+                if(base == 5) // Base Register = EBP
+                {
+                    if(index == 4) // Index Register = ESP & Base Register = EBP
+                    {
+                        //ModRM           0  1  2  3  4  5
+                        //byte   0  1  2  3  4  5  6  7  8
+                        //OPCPDE 0F 38 01 04 25 04 03 02 01
+                        //INS    phaddw mm0, qword [ds:0x1020304]
+                        //PTR = DISP32 OFFSET
+                        address = *((uint32_t*)&ModRM[2]);
+                        consumed += 5;
+                    }
+                    else //base 5 & Not Index 4 
+                    {
+                        //ModRM           0  1  2  3  4  5
+                        //byte   0  1  2  3  4  5  6  7  8
+                        //OPCPDE 0F 38 01 04 05 04 03 02 01
+                        //INS    phaddw mm0, qword [ds:0x1020304+rax]
+                        //PTR = (DISP32 OFFSET) + (INDEX REG * SCALE)
+                        address = *((uint32_t*)&ModRM[2]) + (reg_sel[index] * factor);
+                        consumed += 5;
+                    }
+                }
+                else //Not Base 5
+                {
+                    if (index == 4) // Index Register = ESP
+                    {
+                        //ModRM           0  1
+                        //byte   0  1  2  3  4
+                        //OPCPDE 0F 38 01 04 61
+                        //INS    phaddw mm0, qword [ds:rcx]
+                        address = reg_sel[base];
+                        consumed++;
+                    }
+                    else //SIB General Mode
+                    {
+                        //ModRM           0  1
+                        //byte   0  1  2  3  4
+                        //OPCPDE 0F 38 01 04 44
+                        //INS    phaddw mm0, qword ptr [rsp+rax*2]
+                        address = reg_sel[base] + (reg_sel[index] * factor);
+                        consumed++;
+                    }
+                }
+            }
+            else if (mod == 1) //mod field = 01b
+            {
+                if (index == 4) // Index Register = ESP
+                {
+                    //ModRM           0  1  2
+                    //BYTE   0  1  2  3  4  5
+                    //OPCPDE 0F 38 01 44 27 80
+                    //INS    phaddw mm0, qword ptr [rdi-80h]
+                    //PTR = BASE REG + (DISP8 OFFSET)
+                    address = reg_sel[base] + *((int8_t*)&ModRM[2]);
+                    consumed+= 2;
+                }
+                else //SIB General Mode
+                {
+                    //ModRM           0  1  2
+                    //BYTE   0  1  2  3  4  5
+                    //OPCPDE 0F 38 01 44 44 80
+                    //INS    phaddw mm0, qword ptr [rsp+rax*2-80h]
+                    //PTR = BASE REG + (INDEX REG * SCALE) + (DISP8 OFFSET)
+                    address = reg_sel[base] + (reg_sel[index] * factor) + *((int8_t*)&ModRM[2]);
+                    consumed+= 2;
+                }
+            }
+            else if (mod == 2) //mod field = 10b
+            {
+                if (index == 4) // Index Register = ESP
+                {
+                    //ModRM           0  1  2  3  4  5
+                    //BYTE   0  1  2  3  4  5  6  7  8
+                    //OPCPDE 0F 38 01 84 20 04 03 02 01
+                    //INS    phaddw mm0, qword ptr [rax+1020304h]
+                    //PTR = BASE REG + (DISP32 OFFSET)
+                    address = reg_sel[base] + *((uint32_t*)&ModRM[2]);
+                    consumed += 5;
+                }
+                else //SIB General Mode
+                {
+                    //ModRM           0  1  2  3  4  5
+                    //BYTE   0  1  2  3  4  5  6  7  8
+                    //OPCPDE 0F 38 01 84 44 04 03 02 01
+                    //INS    phaddw mm0, qword ptr [rsp+rax*2+1020304h]
+                    //PTR = BASE REG + (INDEX REG * SCALE) + (DISP32 OFFSET)
+                    address = reg_sel[base] + (reg_sel[index] * factor) + *((uint32_t*)&ModRM[2]);
+                    consumed += 5;
+                }
+            }
+        }
+        /*** R/M = EBP in mod 0 Use Disp32 Offset ***/
+        else if (num_src == 5)
+        {
+            if (mod == 0) //mod field = 00b
+            {
+                //ModRM           0  1  2  3  4
+                //BYTE   0  1  2  3  4  5  6  7
+                //OPCPDE 0F 38 01 05 01 02 03 04
+                //INS    phaddw mm0, qword ptr [cs:04030201h]
+                //PTR = DISP32 OFFSET
+                address = *((uint32_t*)&ModRM[1]);
+                consumed += 4;
+            }
+        }
+        /*** General Mode ***/
+        else
+        {
+            if (mod == 0) //mod field = 00b
+            {
+                //ModRM           0
+                //BYTE   0  1  2  3
+                //OPCPDE 0F 38 01 03
+                //INS    phaddw mm0, qword [ds:rbx]
+                address = reg_sel[num_src];
+            }
+            else if (mod == 1) //mod field = 01b
+            {
+                //ModRM           0  1
+                //BYTE   0  1  2  3  4
+                //OPCPDE 0F 38 01 43 01
+                //INS    phaddw mm0, qword [ds:rbx+0x1]
+                //PTR = R/M REG + (DISP8 OFFSET)
+                address = reg_sel[num_src] + *((int8_t*)&ModRM[1]);
+                consumed++;
+            }
+            else if (mod == 2) //mod field = 10b
+            {
+                //ModRM           0  1  2  3  4
+                //BYTE   0  1  2  3  4  5  6  7
+                //OPCPDE 0F 38 01 83 01 02 03 04
+                //INS    phaddw mm0, qword [ds:rbx+0x4030201]
+                //PTR = R/M REG + (DISP32 OFFSET)
+                address = reg_sel[num_src] + *((uint32_t*)&ModRM[1]);
+                consumed += 4;
+            }
+        }
+
+        // address is good now, do read and store operands.
+        uint64_t addr = address;
+
+        if (fisttp == 1) //fild 0x66 0xDB
+        {
+            *(int *)addr = fisttpl((double *)addr);
+        }
+        else if (fisttp == 2) //fld 0x66 0xDD
+        {
+            *(long long *)addr = fisttpq((long double *)addr);
+        }
+        else if (fisttp == 3) //fild 0x66 0xDF
+        {
+            *(short *)addr = fisttps((float *)addr);
+        }
+        else //fisttp = 0
+        {
+            if(kernel_trap)
+            {
+                if(size_128) ((ssp_m128*)src)->ui = *((__uint128_t*)addr);
+                else ((ssp_m64*)src)->u64 = *((uint64_t*)addr);
+            }
+            else
+            {
+                //printf("xnu: da = %llx, rsp=%llx,  rip=%llx\n", address, reg_sel[4], r32->eip);
+                if(size_128) copyin(addr, (char*) &((ssp_m128*)src)->ui, 16);
+                else copyin(addr, (char*) &((ssp_m64*)src)->u64, 8);
+            }
+        }
+    }
+    //AnV - Implemented 32-bit fetch END
+    return consumed;
+}
+
+void storeresult128(uint8_t ModRM, unsigned int hdst, ssp_m128 res)
+{
+    unsigned int num_dst = (ModRM >> 3) & 0x7;
+    if(hdst) num_dst += 8;
+    movxmm(&res, num_dst);
+}
+void storeresult64(uint8_t ModRM, unsigned int __unused hdst, ssp_m64 res)
+{
+    unsigned int num_dst = (ModRM >> 3) & 0x7;
+    movmm(&res, num_dst);
+}
+
+#endif /* TESTCASE */
+
+/* get value from the xmm register i */
+void getxmm(ssp_m128 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movdqu %%xmm0, %0" : "=m" (*v->s8));
+            break;
+        case 1:
+            asm __volatile__ ("movdqu %%xmm1, %0" : "=m" (*v->s8));
+            break;
+        case 2:
+            asm __volatile__ ("movdqu %%xmm2, %0" : "=m" (*v->s8));
+            break;
+        case 3:
+            asm __volatile__ ("movdqu %%xmm3, %0" : "=m" (*v->s8));
+            break;
+        case 4:
+            asm __volatile__ ("movdqu %%xmm4, %0" : "=m" (*v->s8));
+            break;
+        case 5:
+            asm __volatile__ ("movdqu %%xmm5, %0" : "=m" (*v->s8));
+            break;
+        case 6:
+            asm __volatile__ ("movdqu %%xmm6, %0" : "=m" (*v->s8));
+            break;
+        case 7:
+            asm __volatile__ ("movdqu %%xmm7, %0" : "=m" (*v->s8));
+            break;
+#ifdef __x86_64__
+        case 8:
+            asm __volatile__ ("movdqu %%xmm8, %0" : "=m" (*v->s8));
+            break;
+        case 9:
+            asm __volatile__ ("movdqu %%xmm9, %0" : "=m" (*v->s8));
+            break;
+        case 10:
+            asm __volatile__ ("movdqu %%xmm10, %0" : "=m" (*v->s8));
+            break;
+        case 11:
+            asm __volatile__ ("movdqu %%xmm11, %0" : "=m" (*v->s8));
+            break;
+        case 12:
+            asm __volatile__ ("movdqu %%xmm12, %0" : "=m" (*v->s8));
+            break;
+        case 13:
+            asm __volatile__ ("movdqu %%xmm13, %0" : "=m" (*v->s8));
+            break;
+        case 14:
+            asm __volatile__ ("movdqu %%xmm14, %0" : "=m" (*v->s8));
+            break;
+        case 15:
+            asm __volatile__ ("movdqu %%xmm15, %0" : "=m" (*v->s8));
+            break;
+#endif
+    }
+}
+
+/* get value from the mm register i  */
+void getmm(ssp_m64 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movq %%mm0, %0" : "=m" (*v->s8));
+            break;
+        case 1:
+            asm __volatile__ ("movq %%mm1, %0" : "=m" (*v->s8));
+            break;
+        case 2:
+            asm __volatile__ ("movq %%mm2, %0" : "=m" (*v->s8));
+            break;
+        case 3:
+            asm __volatile__ ("movq %%mm3, %0" : "=m" (*v->s8));
+            break;
+        case 4:
+            asm __volatile__ ("movq %%mm4, %0" : "=m" (*v->s8));
+            break;
+        case 5:
+            asm __volatile__ ("movq %%mm5, %0" : "=m" (*v->s8));
+            break;
+        case 6:
+            asm __volatile__ ("movq %%mm6, %0" : "=m" (*v->s8));
+            break;
+        case 7:
+            asm __volatile__ ("movq %%mm7, %0" : "=m" (*v->s8));
+            break;
+    }
+}
+
+/* move value over to xmm register i */
+void movxmm(ssp_m128 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movdqu %0, %%xmm0" :: "m" (*v->s8) );
+            break;
+        case 1:
+            asm __volatile__ ("movdqu %0, %%xmm1" :: "m" (*v->s8) );
+            break;
+        case 2:
+            asm __volatile__ ("movdqu %0, %%xmm2" :: "m" (*v->s8) );
+            break;
+        case 3:
+            asm __volatile__ ("movdqu %0, %%xmm3" :: "m" (*v->s8) );
+            break;
+        case 4:
+            asm __volatile__ ("movdqu %0, %%xmm4" :: "m" (*v->s8) );
+            break;
+        case 5:
+            asm __volatile__ ("movdqu %0, %%xmm5" :: "m" (*v->s8) );
+            break;
+        case 6:
+            asm __volatile__ ("movdqu %0, %%xmm6" :: "m" (*v->s8) );
+            break;
+        case 7:
+            asm __volatile__ ("movdqu %0, %%xmm7" :: "m" (*v->s8) );
+            break;
+#ifdef __x86_64__
+        case 8:
+            asm __volatile__ ("movdqu %0, %%xmm8" :: "m" (*v->s8) );
+            break;
+        case 9:
+            asm __volatile__ ("movdqu %0, %%xmm9" :: "m" (*v->s8) );
+            break;
+        case 10:
+            asm __volatile__ ("movdqu %0, %%xmm10" :: "m" (*v->s8) );
+            break;
+        case 11:
+            asm __volatile__ ("movdqu %0, %%xmm11" :: "m" (*v->s8) );
+            break;
+        case 12:
+            asm __volatile__ ("movdqu %0, %%xmm12" :: "m" (*v->s8) );
+            break;
+        case 13:
+            asm __volatile__ ("movdqu %0, %%xmm13" :: "m" (*v->s8) );
+            break;
+        case 14:
+            asm __volatile__ ("movdqu %0, %%xmm14" :: "m" (*v->s8) );
+            break;
+        case 15:
+            asm __volatile__ ("movdqu %0, %%xmm15" :: "m" (*v->s8) );
+            break;
+#endif
+    }
+}
+
+/* move value over to mm register i */
+void movmm(ssp_m64 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movq %0, %%mm0" :: "m" (*v->s8) );
+            break;
+        case 1:
+            asm __volatile__ ("movq %0, %%mm1" :: "m" (*v->s8) );
+            break;
+        case 2:
+            asm __volatile__ ("movq %0, %%mm2" :: "m" (*v->s8) );
+            break;
+        case 3:
+            asm __volatile__ ("movq %0, %%mm3" :: "m" (*v->s8) );
+            break;
+        case 4:
+            asm __volatile__ ("movq %0, %%mm4" :: "m" (*v->s8) );
+            break;
+        case 5:
+            asm __volatile__ ("movq %0, %%mm5" :: "m" (*v->s8) );
+            break;
+        case 6:
+            asm __volatile__ ("movq %0, %%mm6" :: "m" (*v->s8) );
+            break;
+        case 7:
+            asm __volatile__ ("movq %0, %%mm7" :: "m" (*v->s8) );
+            break;
+    }
+}
+
+short fisttps(float *res)
+{
+    float value = opemu_truncf(*res);
+    __asm__ ("fistps %0" : : "m" (value));
+    *res = value;
+    return (short)(res);
+}
+
+int fisttpl(double *res)
+{
+    double value = opemu_trunc(*res);
+    __asm__ ("fistpl %0" : : "m" (value));
+    *res = value;
+    return (int)res;
+}
+
+long long fisttpq(long double *res)
+{
+    long double value = *res;
+    __asm__ ("fistpq %0" : : "m" (value));
+    *res = value;
+    return (long long)res;
+}
diff -Nur xnu-3247.1.106/osfmk/OPEMU/opemu.h xnu-3247.1.106-AnV/osfmk/OPEMU/opemu.h
--- xnu-3247.1.106/osfmk/OPEMU/opemu.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/OPEMU/opemu.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,39 @@
+#ifndef OPEMU_H
+#define OPEMU_H
+#include <stdint.h>
+
+#ifndef TESTCASE
+#include <mach/thread_status.h>
+#endif
+
+#define __SSEPLUS_CPUID_H__ 1
+#define __SSEPLUS_EMULATION_COMPS_SSE3_H__ 1
+
+#include <SSEPlus/SSEPlus_base.h>
+#include <SSEPlus/SSEPlus_REF.h>
+#include <SSEPlus/SSEPlus_SSE2.h>
+
+#ifndef TESTCASE
+/** XNU TRAP HANDLERS **/
+unsigned char opemu_ktrap(x86_saved_state_t *state);
+void opemu_utrap(x86_saved_state_t *state);
+int ssse3_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int);
+int sse3_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int );
+int operands(uint8_t *ModRM, unsigned int hsrc, unsigned int hdst, void *src, void *dst,
+				  unsigned int longmode, x86_saved_state_t *saved_state, int kernel_trap, int size_128, int rex, int hsreg, int modbyte, int fisttp);
+void storeresult128(uint8_t ModRM, unsigned int hdst, ssp_m128 res);
+void storeresult64(uint8_t ModRM, unsigned int hdst, ssp_m64 res);
+#endif
+
+void print_bytes(uint8_t *from, int size);
+
+void getxmm(ssp_m128 *v, unsigned int i);
+void getmm(ssp_m64 *v, unsigned int i);
+void movxmm(ssp_m128 *v, unsigned int i);
+void movmm(ssp_m64 *v, unsigned int i);
+
+short fisttps(float *res);
+int fisttpl(double *res);
+long long fisttpq(long double *res);
+
+#endif
\ No newline at end of file
diff -Nur xnu-3247.1.106/osfmk/OPEMU/opemu_math.c xnu-3247.1.106-AnV/osfmk/OPEMU/opemu_math.c
--- xnu-3247.1.106/osfmk/OPEMU/opemu_math.c	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/OPEMU/opemu_math.c	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,138 @@
+#include <stdint.h>
+#include "opemu_math.h"
+
+/* 32-bit */
+float opemu_truncf(float x)
+{
+	float ret = x;
+	int32_t signbit=0;
+	int32_t w=0;
+	int32_t exponent_less_127=0;
+
+	GET_FLOAT_WORD(w,ret);
+
+	/* Extract sign bit. */
+	signbit = w & 0x80000000;
+
+	/* Extract exponent field. */
+	exponent_less_127 = ((w & 0x7f800000) >> 23) - 127;
+
+	if (exponent_less_127 < 23)
+	{
+		if (exponent_less_127 < 0)
+		{
+			/* -1 < x < 1, so result is +0 or -0. */
+			SET_FLOAT_WORD(ret, signbit);
+		}
+		else
+		{
+			SET_FLOAT_WORD(ret, signbit | (w & ~(0x007fffff >> exponent_less_127)));
+		}
+	}
+	else
+	{
+		if (exponent_less_127 == 255)
+			/* x is NaN or infinite. */
+			return ret + ret;
+			/* All bits in the fraction field are relevant. */
+	}
+	return ret;
+}
+
+/* 64-bit */
+double opemu_trunc(double x)
+{
+	double ret = x;
+	int signbit=0;
+	/* Most significant word, least significant word. */
+	int msw=0;
+	unsigned int lsw=0;
+	int exponent_less_1023=0;
+
+	EXTRACT_WORDS(msw, lsw, ret);
+
+	/* Extract sign bit. */
+	signbit = msw & 0x80000000;
+
+	/* Extract exponent field. */
+	exponent_less_1023 = ((msw & 0x7ff00000) >> 20) - 1023;
+
+	if (exponent_less_1023 < 20)
+	{
+		/* All significant digits are in msw. */
+		if (exponent_less_1023 < 0)
+		{
+			/* -1 < x < 1, so result is +0 or -0. */
+			INSERT_WORDS(ret, signbit, 0);
+		}
+		else
+		{
+			/* All relevant fraction bits are in msw, so lsw of the result is 0. */
+			INSERT_WORDS(ret, signbit | (msw & ~(0x000fffff >> exponent_less_1023)), 0);
+		}
+	}
+	else if (exponent_less_1023 > 51)
+	{
+		if (exponent_less_1023 == 1024)
+		{
+			/* x is infinite, or not a number, so trigger an exception. */
+			return ret + ret;
+		}
+		/* All bits in the fraction fields of the msw and lsw are needed in the result. */
+	}
+	else
+	{
+		/* All fraction bits in msw are relevant.  Truncate irrelevant bits from lsw. */
+		INSERT_WORDS(ret, msw, lsw & ~(0xffffffffu >> (exponent_less_1023 - 20)));
+	}
+	return ret;
+}
+
+/* 128-bit */
+long double opemu_truncl(long double x)
+{
+	long double ret = x;
+	uint64_t signbit;
+	/* Most significant word, least significant word. */
+	uint64_t msw=0;
+	uint64_t lsw=0;
+	int32_t exponent_less_16383=0;
+
+	GET_LDOUBLE_WORDS(msw, lsw, ret);
+
+	/* Extract sign bit. */
+	signbit = msw & 0x8000000000000000ULL;
+
+	/* Extract exponent field. */
+	exponent_less_16383 = ((msw & 0x7fff000000000000ULL) >> 48) - 16383;
+
+	if (exponent_less_16383 < 48)
+	{
+		/* All significant digits are in msw. */
+		if (exponent_less_16383 < 0)
+		{
+			/* -1 < x < 1, so result is +0 or -0. */
+			SET_LDOUBLE_WORDS(ret, signbit, 0);
+		}
+		else
+		{
+			/* All relevant fraction bits are in msw, so lsw of the result is 0. */
+			SET_LDOUBLE_WORDS(ret, msw & ~(0x0000ffffffffffffLL >> exponent_less_16383), 0);
+		}
+	}
+	else if (exponent_less_16383 > 111)
+	{
+		if (exponent_less_16383 == 16384)
+		{
+			/* x is infinite, or not a number, so trigger an exception. */
+			return ret + ret;
+		}
+		/* All bits in the fraction fields of the msw and lsw are needed in the result. */
+	}
+	else
+	{
+		/* All fraction bits in msw are relevant.  Truncate irrelevant bits from lsw. */
+		SET_LDOUBLE_WORDS(ret, msw, lsw & ~(0xffffffffffffffffULL >> (exponent_less_16383 - 48)));
+	}
+	return ret;
+}
\ No newline at end of file
diff -Nur xnu-3247.1.106/osfmk/OPEMU/opemu_math.h xnu-3247.1.106-AnV/osfmk/OPEMU/opemu_math.h
--- xnu-3247.1.106/osfmk/OPEMU/opemu_math.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/OPEMU/opemu_math.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,80 @@
+/* Needed unions */
+typedef union
+{
+	float value;
+	uint32_t word;
+} ieee_float_shape_type;
+
+typedef union
+{
+	double value;
+	struct
+	{
+		uint32_t lsw;
+		uint32_t msw;
+	} parts;
+} ieee_double_shape_type;
+
+typedef union
+{
+	long double value;
+	struct
+	{
+		uint64_t lsw;
+		uint64_t msw;
+	} parts;
+} ieee_long_double_shape_type;
+
+/* 32-bit (float) */
+#define GET_FLOAT_WORD(i,d) \
+  do { \
+        ieee_float_shape_type gf_u; \
+        gf_u.value = (d); \
+        (i) = gf_u.word; \
+      } while (0)
+
+#define SET_FLOAT_WORD(d,i) \
+  do { \
+        ieee_float_shape_type sf_u; \
+        sf_u.word = (i); \
+        (d) = sf_u.value; \
+      } while (0)
+
+/* 64-bit (double) */
+#define EXTRACT_WORDS(ix0,ix1,d) \
+  do { \
+        ieee_double_shape_type ew_u; \
+        ew_u.value = (d); \
+        (ix0) = ew_u.parts.msw; \
+        (ix1) = ew_u.parts.lsw; \
+      } while (0)
+
+#define INSERT_WORDS(d,ix0,ix1) \
+  do { \
+        ieee_double_shape_type iw_u; \
+        iw_u.parts.msw = (ix0); \
+        iw_u.parts.lsw = (ix1); \
+        (d) = iw_u.value; \
+      } while (0)
+
+/* 128-bit (long double) */
+#define GET_LDOUBLE_WORDS(ix0,ix1,d) \
+  do { \
+        ieee_long_double_shape_type qw_u; \
+        qw_u.value = (d); \
+        (ix0) = qw_u.parts.msw; \
+        (ix1) = qw_u.parts.lsw; \
+      } while (0)
+
+#define SET_LDOUBLE_WORDS(d,ix0,ix1) \
+  do { \
+        ieee_long_double_shape_type qw_u; \
+        qw_u.parts.msw = (ix0); \
+        qw_u.parts.lsw = (ix1); \
+        (d) = qw_u.value; \
+      } while (0)
+
+
+float opemu_truncf(float x);
+double opemu_trunc(double x);
+long double opemu_truncl(long double x);
\ No newline at end of file
diff -Nur xnu-3247.1.106/osfmk/conf/files.x86_64 xnu-3247.1.106-AnV/osfmk/conf/files.x86_64
--- xnu-3247.1.106/osfmk/conf/files.x86_64	2015-12-06 01:32:31.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/conf/files.x86_64	2015-12-13 17:08:10.000000000 +0100
@@ -127,3 +127,6 @@
 osfmk/x86_64/idt64.s		standard
 
 osfmk/i386/panic_hooks.c	standard
+
+osfmk/OPEMU/opemu.c		standard
+osfmk/OPEMU/opemu_math.c		standard
diff -Nur xnu-3247.1.106/osfmk/conf/files.x86_64.orig xnu-3247.1.106-AnV/osfmk/conf/files.x86_64.orig
--- xnu-3247.1.106/osfmk/conf/files.x86_64.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/conf/files.x86_64.orig	2015-12-06 01:32:31.000000000 +0100
@@ -0,0 +1,129 @@
+OPTIONS/fb				optional fb
+
+OPTIONS/debug			optional debug
+
+OPTIONS/gprof		optional gprof
+
+osfmk/vm/vm_apple_protect.c	 standard
+
+#osfmk/x86_64/hi_res_clock_map.c 	optional hi_res_clock
+
+osfmk/x86_64/pmap.c		standard
+osfmk/i386/pmap_x86_common.c	standard
+osfmk/i386/pmap_common.c	standard
+osfmk/x86_64/pmap_pcid.c	standard
+
+osfmk/i386/pal_routines.c	optional pal_i386
+osfmk/x86_64/pal_routines_asm.s	optional pal_i386
+
+osfmk/i386/bsd_i386.c		optional mach_bsd
+osfmk/i386/bsd_i386_native.c	optional mach_bsd
+osfmk/i386/machdep_call.c	optional mach_bsd
+
+# Order is important here for __HIB section
+osfmk/x86_64/boot_pt.c		standard
+osfmk/i386/mp_desc.c		standard
+osfmk/i386/gdt.c		standard
+osfmk/x86_64/start.s		standard
+
+osfmk/x86_64/bcopy.s		standard
+osfmk/x86_64/bzero.s		standard
+osfmk/x86_64/WKdmDecompress_new.s	standard
+osfmk/x86_64/WKdmCompress_new.s		standard
+osfmk/x86_64/WKdmData_new.s		standard
+osfmk/i386/cpu.c		standard
+osfmk/i386/cpuid.c		standard
+osfmk/i386/cpu_threads.c	standard
+osfmk/i386/cpu_topology.c	standard
+osfmk/i386/i386_timer.c		standard
+osfmk/i386/fpu.c		standard
+osfmk/i386/i386_lock.s		standard
+osfmk/i386/i386_init.c		standard
+osfmk/i386/i386_vm_init.c	standard
+osfmk/i386/io_map.c		standard
+osfmk/i386/ktss.c		standard
+osfmk/i386/ldt.c		standard
+osfmk/x86_64/loose_ends.c	standard
+osfmk/x86_64/copyio.c		standard
+osfmk/i386/locks_i386.c	standard
+osfmk/x86_64/locore.s	standard
+osfmk/x86_64/lowmem_vectors.c	standard
+osfmk/x86_64/cswitch.s	standard
+osfmk/i386/machine_routines.c		standard
+osfmk/x86_64/machine_routines_asm.s	standard
+osfmk/i386/machine_check.c	optional config_mca
+osfmk/i386/machine_task.c		standard
+osfmk/x86_64/mcount.s		optional profile
+#osfmk/x86_64/ntoh.s		standard
+osfmk/i386/pcb.c		standard
+osfmk/i386/pcb_native.c		standard
+osfmk/i386/phys.c		standard
+osfmk/i386/rtclock.c		standard
+osfmk/i386/rtclock_native.c	standard
+osfmk/i386/trap.c		standard
+osfmk/i386/trap_native.c	standard
+osfmk/i386/user_ldt.c		standard
+osfmk/i386/Diagnostics.c	standard
+osfmk/i386/pmCPU.c		standard
+osfmk/i386/tsc.c		standard
+
+osfmk/i386/commpage/commpage.c	standard
+osfmk/i386/commpage/commpage_asm.s	standard
+osfmk/i386/commpage/fifo_queues.s	standard
+
+osfmk/i386/AT386/conf.c		standard
+osfmk/i386/AT386/model_dep.c	standard
+
+osfmk/i386/lapic.c		standard
+osfmk/i386/lapic_native.c	standard
+osfmk/i386/mp.c			standard
+osfmk/i386/mp_native.c		standard
+
+osfmk/i386/acpi.c		standard
+
+osfmk/i386/mtrr.c		optional    config_mtrr
+
+osfmk/console/i386/serial_console.c  optional  serial_console
+
+osfmk/console/video_console.c        optional  video_console
+osfmk/console/i386/video_scroll.c    optional  video_console
+
+#osfmk/profiling/x86_64/profile-md.c	optional gprof
+#osfmk/profiling/x86_64/profile-asm.s	optional gprof
+#osfmk/profiling/profile-kgmon.c		optional gprof
+#osfmk/profiling/profile-mk.c		optional gprof
+
+osfmk/kdp/ml/x86_64/kdp_machdep.c	optional	mach_kdp
+osfmk/kdp/ml/x86_64/kdp_vm.c		optional	mach_kdp
+osfmk/kdp/ml/i386/kdp_x86_common.c	optional	mach_kdp
+
+osfmk/i386/hibernate_i386.c		optional hibernation
+osfmk/i386/hibernate_restore.c		optional hibernation
+
+osfmk/chud/i386/chud_osfmk_callback_i386.c	standard
+osfmk/chud/i386/chud_cpu_i386.c			standard
+osfmk/chud/i386/chud_thread_i386.c		standard
+
+osfmk/i386/ucode.c				standard
+
+osfmk/i386/vmx/vmx_cpu.c			optional config_vmx
+osfmk/i386/vmx/vmx_shims.c			optional config_vmx
+
+osfmk/kern/hv_support.c				optional hypervisor
+
+# DUMMIES TO FORCE GENERATION OF .h FILES
+#osfmk/OPTIONS/ln		optional ln
+#osfmk/OPTIONS/eisa		optional eisa
+#osfmk/OPTIONS/himem		optional himem
+#osfmk/OPTIONS/ec		optional ec
+#osfmk/OPTIONS/hi_res_clock	optional hi_res_clock
+
+# Kernel performance monitoring
+osfmk/kperf/x86_64/kperf_mp.c   optional kperf
+osfmk/kperf/x86_64/kperf_meminfo.c  optional kperf
+osfmk/x86_64/kpc_x86.c              optional kpc
+
+osfmk/i386/startup64.c		standard
+osfmk/x86_64/idt64.s		standard
+
+osfmk/i386/panic_hooks.c	standard
diff -Nur xnu-3247.1.106/osfmk/i386/AT386/model_dep.c xnu-3247.1.106-AnV/osfmk/i386/AT386/model_dep.c
--- xnu-3247.1.106/osfmk/i386/AT386/model_dep.c	2015-12-06 01:32:34.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/AT386/model_dep.c	2015-12-13 17:08:10.000000000 +0100
@@ -124,6 +124,8 @@
 
 #include <mach/branch_predicates.h>
 
+#include <chud/chud_xnu.h>
+
 #if	DEBUG
 #define DPRINTF(x...)	kprintf(x)
 #else
@@ -808,9 +810,18 @@
 void
 halt_all_cpus(boolean_t reboot)
 {
+	/* ovof / paulicat: Disable all cores on shutdown to prevent the system hanging */
+	uint32_t ncpus, i;
+	ncpus = chudxnu_logical_cpu_count();
+	for (i = 0; i < ncpus; i++)
+		chudxnu_enable_cpu(i, FALSE);
+
 	if (reboot) {
 		printf("MACH Reboot\n");
 		PEHaltRestart( kPERestartCPU );
+		asm volatile ("movb $0xfe, %al\n"
+			"outb %al, $0x64\n"
+			"hlt\n");
 	} else {
 		printf("CPU halted\n");
 		PEHaltRestart( kPEHaltCPU );
diff -Nur xnu-3247.1.106/osfmk/i386/AT386/model_dep.c.orig xnu-3247.1.106-AnV/osfmk/i386/AT386/model_dep.c.orig
--- xnu-3247.1.106/osfmk/i386/AT386/model_dep.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/AT386/model_dep.c.orig	2015-12-06 01:32:34.000000000 +0100
@@ -0,0 +1,1478 @@
+/*
+ * Copyright (c) 2000-2012 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+/* 
+ * Mach Operating System
+ * Copyright (c) 1991,1990,1989, 1988 Carnegie Mellon University
+ * All Rights Reserved.
+ * 
+ * Permission to use, copy, modify and distribute this software and its
+ * documentation is hereby granted, provided that both the copyright
+ * notice and this permission notice appear in all copies of the
+ * software, derivative works or modified versions, and any portions
+ * thereof, and that both notices appear in supporting documentation.
+ * 
+ * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
+ * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR
+ * ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
+ * 
+ * Carnegie Mellon requests users of this software to return to
+ * 
+ *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU
+ *  School of Computer Science
+ *  Carnegie Mellon University
+ *  Pittsburgh PA 15213-3890
+ * 
+ * any improvements or extensions that they make and grant Carnegie Mellon
+ * the rights to redistribute these changes.
+ */
+
+/*
+ */
+
+/*
+ *	File:	model_dep.c
+ *	Author:	Avadis Tevanian, Jr., Michael Wayne Young
+ *
+ *	Copyright (C) 1986, Avadis Tevanian, Jr., Michael Wayne Young
+ *
+ *	Basic initialization for I386 - ISA bus machines.
+ */
+
+
+#include <mach/i386/vm_param.h>
+
+#include <string.h>
+#include <mach/vm_param.h>
+#include <mach/vm_prot.h>
+#include <mach/machine.h>
+#include <mach/time_value.h>
+#include <sys/kdebug.h>
+#include <kern/spl.h>
+#include <kern/assert.h>
+#include <kern/debug.h>
+#include <kern/misc_protos.h>
+#include <kern/startup.h>
+#include <kern/clock.h>
+#include <kern/cpu_data.h>
+#include <kern/machine.h>
+#include <i386/postcode.h>
+#include <i386/mp_desc.h>
+#include <i386/misc_protos.h>
+#include <i386/thread.h>
+#include <i386/trap.h>
+#include <i386/machine_routines.h>
+#include <i386/mp.h>		/* mp_rendezvous_break_lock */
+#include <i386/cpuid.h>
+#include <i386/fpu.h>
+#include <i386/machine_cpu.h>
+#include <i386/pmap.h>
+#if CONFIG_MTRR
+#include <i386/mtrr.h>
+#endif
+#include <i386/ucode.h>
+#include <i386/pmCPU.h>
+#include <i386/panic_hooks.h>
+
+#include <architecture/i386/pio.h> /* inb() */
+#include <pexpert/i386/boot.h>
+
+#include <kdp/kdp_dyld.h>
+#include <vm/pmap.h>
+#include <vm/vm_map.h>
+#include <vm/vm_kern.h>
+
+#include <IOKit/IOPlatformExpert.h>
+#include <IOKit/IOHibernatePrivate.h>
+
+#include <pexpert/i386/efi.h>
+
+#include <kern/thread.h>
+#include <kern/sched.h>
+#include <mach-o/loader.h>
+#include <mach-o/nlist.h>
+
+#include <libkern/kernel_mach_header.h>
+#include <libkern/OSKextLibPrivate.h>
+
+#include <mach/branch_predicates.h>
+
+#if	DEBUG
+#define DPRINTF(x...)	kprintf(x)
+#else
+#define DPRINTF(x...)
+#endif
+
+static void machine_conf(void);
+void panic_print_symbol_name(vm_address_t search);
+
+extern boolean_t init_task_died;
+extern const char	version[];
+extern char 	osversion[];
+extern int		max_unsafe_quanta;
+extern int		max_poll_quanta;
+extern unsigned int	panic_is_inited;
+
+extern int	proc_pid(void *p);
+
+/* Definitions for frame pointers */
+#define FP_ALIGNMENT_MASK      ((uint32_t)(0x3))
+#define FP_LR_OFFSET           ((uint32_t)4)
+#define FP_LR_OFFSET64         ((uint32_t)8)
+#define FP_MAX_NUM_TO_EVALUATE (50)
+
+int db_run_mode;
+
+volatile int pbtcpu = -1;
+hw_lock_data_t pbtlock;		/* backtrace print lock */
+uint32_t pbtcnt = 0;
+
+volatile int panic_double_fault_cpu = -1;
+
+#define PRINT_ARGS_FROM_STACK_FRAME	0
+
+typedef struct _cframe_t {
+    struct _cframe_t	*prev;
+    uintptr_t		caller;
+#if PRINT_ARGS_FROM_STACK_FRAME
+    unsigned		args[0];
+#endif
+} cframe_t;
+
+static unsigned panic_io_port;
+static unsigned	commit_paniclog_to_nvram;
+
+unsigned int debug_boot_arg;
+
+/*
+ * Backtrace a single frame.
+ */
+void
+print_one_backtrace(pmap_t pmap, vm_offset_t topfp, const char *cur_marker,
+	boolean_t is_64_bit, boolean_t nvram_format) 
+{
+	int		    i = 0;
+	addr64_t	lr;
+	addr64_t	fp;
+	addr64_t	fp_for_ppn;
+	ppnum_t		ppn;
+	boolean_t	dump_kernel_stack;
+
+	fp = topfp;
+	fp_for_ppn = 0;
+	ppn = (ppnum_t)NULL;
+
+	if (fp >= VM_MIN_KERNEL_ADDRESS)
+		dump_kernel_stack = TRUE;
+	else
+		dump_kernel_stack = FALSE;
+
+	do {
+		if ((fp == 0) || ((fp & FP_ALIGNMENT_MASK) != 0))
+			break;
+		if (dump_kernel_stack && ((fp < VM_MIN_KERNEL_ADDRESS) || (fp > VM_MAX_KERNEL_ADDRESS)))
+			break;
+		if ((!dump_kernel_stack) && (fp >=VM_MIN_KERNEL_ADDRESS))
+			break;
+			
+        /* Check to see if current address will result in a different
+           ppn than previously computed (to avoid recomputation) via
+           (addr) ^ fp_for_ppn) >> PAGE_SHIFT) */
+
+		if ((((fp + FP_LR_OFFSET) ^ fp_for_ppn) >> PAGE_SHIFT) != 0x0U) {
+			ppn = pmap_find_phys(pmap, fp + FP_LR_OFFSET);
+			fp_for_ppn = fp + (is_64_bit ? FP_LR_OFFSET64 : FP_LR_OFFSET);
+		}
+		if (ppn != (ppnum_t)NULL) {
+			if (is_64_bit) {
+				lr = ml_phys_read_double_64(((((vm_offset_t)ppn) << PAGE_SHIFT)) | ((fp + FP_LR_OFFSET64) & PAGE_MASK));
+			} else {
+				lr = ml_phys_read_word(((((vm_offset_t)ppn) << PAGE_SHIFT)) | ((fp + FP_LR_OFFSET) & PAGE_MASK));
+			}
+		} else {
+			if (is_64_bit) {
+				kdb_printf("%s\t  Could not read LR from frame at 0x%016llx\n", cur_marker, fp + FP_LR_OFFSET64);
+			} else {
+				kdb_printf("%s\t  Could not read LR from frame at 0x%08x\n", cur_marker, (uint32_t)(fp + FP_LR_OFFSET));
+			}
+			break;
+		}
+		if (((fp ^ fp_for_ppn) >> PAGE_SHIFT) != 0x0U) {
+			ppn = pmap_find_phys(pmap, fp);
+			fp_for_ppn = fp;
+		}
+		if (ppn != (ppnum_t)NULL) {
+			if (is_64_bit) {
+				fp = ml_phys_read_double_64(((((vm_offset_t)ppn) << PAGE_SHIFT)) | (fp & PAGE_MASK));
+			} else {
+				fp = ml_phys_read_word(((((vm_offset_t)ppn) << PAGE_SHIFT)) | (fp & PAGE_MASK));
+			}
+		} else {
+			if (is_64_bit) {
+				kdb_printf("%s\t  Could not read FP from frame at 0x%016llx\n", cur_marker, fp);
+			} else {
+				kdb_printf("%s\t  Could not read FP from frame at 0x%08x\n", cur_marker, (uint32_t)fp);
+			}
+			break;
+		}
+
+		if (nvram_format) {
+			if (is_64_bit) {
+				kdb_printf("%s\t0x%016llx\n", cur_marker, lr);
+			} else {
+				kdb_printf("%s\t0x%08x\n", cur_marker, (uint32_t)lr);
+			}
+		} else {		
+			if (is_64_bit) {
+				kdb_printf("%s\t  lr: 0x%016llx  fp: 0x%016llx\n", cur_marker, lr, fp);
+			} else {
+				kdb_printf("%s\t  lr: 0x%08x  fp: 0x%08x\n", cur_marker, (uint32_t)lr, (uint32_t)fp);
+			}
+		}
+	} while ((++i < FP_MAX_NUM_TO_EVALUATE) && (fp != topfp));
+}
+void
+machine_startup(void)
+{
+	int	boot_arg;
+
+#if 0
+	if( PE_get_hotkey( kPEControlKey ))
+            halt_in_debugger = halt_in_debugger ? 0 : 1;
+#endif
+
+	if (PE_parse_boot_argn("debug", &debug_boot_arg, sizeof (debug_boot_arg))) {
+		panicDebugging = TRUE;
+		if (debug_boot_arg & DB_HALT) halt_in_debugger=1;
+		if (debug_boot_arg & DB_PRT) disable_debug_output=FALSE; 
+		if (debug_boot_arg & DB_SLOG) systemLogDiags=TRUE; 
+		if (debug_boot_arg & DB_LOG_PI_SCRN) logPanicDataToScreen=TRUE;
+#if KDEBUG_MOJO_TRACE
+		if (debug_boot_arg & DB_PRT_KDEBUG) {
+			kdebug_serial = TRUE;
+			disable_debug_output = FALSE;
+		}
+#endif
+	} else {
+		debug_boot_arg = 0;
+	}
+
+	if (!PE_parse_boot_argn("nvram_paniclog", &commit_paniclog_to_nvram, sizeof (commit_paniclog_to_nvram)))
+		commit_paniclog_to_nvram = 1;
+
+	/*
+	 * Entering the debugger will put the CPUs into a "safe"
+	 * power mode.
+	 */
+	if (PE_parse_boot_argn("pmsafe_debug", &boot_arg, sizeof (boot_arg)))
+	    pmsafe_debug = boot_arg;
+
+#if NOTYET
+	hw_lock_init(&debugger_lock);	/* initialize debugger lock */
+#endif
+	hw_lock_init(&pbtlock);		/* initialize print backtrace lock */
+
+	if (PE_parse_boot_argn("preempt", &boot_arg, sizeof (boot_arg))) {
+		default_preemption_rate = boot_arg;
+	}
+	if (PE_parse_boot_argn("unsafe", &boot_arg, sizeof (boot_arg))) {
+		max_unsafe_quanta = boot_arg;
+	}
+	if (PE_parse_boot_argn("poll", &boot_arg, sizeof (boot_arg))) {
+		max_poll_quanta = boot_arg;
+	}
+	if (PE_parse_boot_argn("yield", &boot_arg, sizeof (boot_arg))) {
+		sched_poll_yield_shift = boot_arg;
+	}
+/* The I/O port to issue a read from, in the event of a panic. Useful for
+ * triggering logic analyzers.
+ */
+	if (PE_parse_boot_argn("panic_io_port", &boot_arg, sizeof (boot_arg))) {
+		/*I/O ports range from 0 through 0xFFFF */
+		panic_io_port = boot_arg & 0xffff;
+	}
+
+	machine_conf();
+
+	panic_hooks_init();
+
+	/*
+	 * Start the system.
+	 */
+	kernel_bootstrap();
+	/*NOTREACHED*/
+}
+
+
+static void
+machine_conf(void)
+{
+	machine_info.memory_size = (typeof(machine_info.memory_size))mem_size;
+}
+
+
+extern void *gPEEFIRuntimeServices;
+extern void *gPEEFISystemTable;
+
+/*-
+ *  COPYRIGHT (C) 1986 Gary S. Brown.  You may use this program, or
+ *  code or tables extracted from it, as desired without restriction.
+ *
+ *  First, the polynomial itself and its table of feedback terms.  The
+ *  polynomial is
+ *  X^32+X^26+X^23+X^22+X^16+X^12+X^11+X^10+X^8+X^7+X^5+X^4+X^2+X^1+X^0
+ *
+ *  Note that we take it "backwards" and put the highest-order term in
+ *  the lowest-order bit.  The X^32 term is "implied"; the LSB is the
+ *  X^31 term, etc.  The X^0 term (usually shown as "+1") results in
+ *  the MSB being 1
+ *
+ *  Note that the usual hardware shift register implementation, which
+ *  is what we're using (we're merely optimizing it by doing eight-bit
+ *  chunks at a time) shifts bits into the lowest-order term.  In our
+ *  implementation, that means shifting towards the right.  Why do we
+ *  do it this way?  Because the calculated CRC must be transmitted in
+ *  order from highest-order term to lowest-order term.  UARTs transmit
+ *  characters in order from LSB to MSB.  By storing the CRC this way
+ *  we hand it to the UART in the order low-byte to high-byte; the UART
+ *  sends each low-bit to hight-bit; and the result is transmission bit
+ *  by bit from highest- to lowest-order term without requiring any bit
+ *  shuffling on our part.  Reception works similarly
+ *
+ *  The feedback terms table consists of 256, 32-bit entries.  Notes
+ *
+ *      The table can be generated at runtime if desired; code to do so
+ *      is shown later.  It might not be obvious, but the feedback
+ *      terms simply represent the results of eight shift/xor opera
+ *      tions for all combinations of data and CRC register values
+ *
+ *      The values must be right-shifted by eight bits by the "updcrc
+ *      logic; the shift must be unsigned (bring in zeroes).  On some
+ *      hardware you could probably optimize the shift in assembler by
+ *      using byte-swap instructions
+ *      polynomial $edb88320
+ *
+ *
+ * CRC32 code derived from work by Gary S. Brown.
+ */
+
+static uint32_t crc32_tab[] = {
+	0x00000000, 0x77073096, 0xee0e612c, 0x990951ba, 0x076dc419, 0x706af48f,
+	0xe963a535, 0x9e6495a3,	0x0edb8832, 0x79dcb8a4, 0xe0d5e91e, 0x97d2d988,
+	0x09b64c2b, 0x7eb17cbd, 0xe7b82d07, 0x90bf1d91, 0x1db71064, 0x6ab020f2,
+	0xf3b97148, 0x84be41de,	0x1adad47d, 0x6ddde4eb, 0xf4d4b551, 0x83d385c7,
+	0x136c9856, 0x646ba8c0, 0xfd62f97a, 0x8a65c9ec,	0x14015c4f, 0x63066cd9,
+	0xfa0f3d63, 0x8d080df5,	0x3b6e20c8, 0x4c69105e, 0xd56041e4, 0xa2677172,
+	0x3c03e4d1, 0x4b04d447, 0xd20d85fd, 0xa50ab56b,	0x35b5a8fa, 0x42b2986c,
+	0xdbbbc9d6, 0xacbcf940,	0x32d86ce3, 0x45df5c75, 0xdcd60dcf, 0xabd13d59,
+	0x26d930ac, 0x51de003a, 0xc8d75180, 0xbfd06116, 0x21b4f4b5, 0x56b3c423,
+	0xcfba9599, 0xb8bda50f, 0x2802b89e, 0x5f058808, 0xc60cd9b2, 0xb10be924,
+	0x2f6f7c87, 0x58684c11, 0xc1611dab, 0xb6662d3d,	0x76dc4190, 0x01db7106,
+	0x98d220bc, 0xefd5102a, 0x71b18589, 0x06b6b51f, 0x9fbfe4a5, 0xe8b8d433,
+	0x7807c9a2, 0x0f00f934, 0x9609a88e, 0xe10e9818, 0x7f6a0dbb, 0x086d3d2d,
+	0x91646c97, 0xe6635c01, 0x6b6b51f4, 0x1c6c6162, 0x856530d8, 0xf262004e,
+	0x6c0695ed, 0x1b01a57b, 0x8208f4c1, 0xf50fc457, 0x65b0d9c6, 0x12b7e950,
+	0x8bbeb8ea, 0xfcb9887c, 0x62dd1ddf, 0x15da2d49, 0x8cd37cf3, 0xfbd44c65,
+	0x4db26158, 0x3ab551ce, 0xa3bc0074, 0xd4bb30e2, 0x4adfa541, 0x3dd895d7,
+	0xa4d1c46d, 0xd3d6f4fb, 0x4369e96a, 0x346ed9fc, 0xad678846, 0xda60b8d0,
+	0x44042d73, 0x33031de5, 0xaa0a4c5f, 0xdd0d7cc9, 0x5005713c, 0x270241aa,
+	0xbe0b1010, 0xc90c2086, 0x5768b525, 0x206f85b3, 0xb966d409, 0xce61e49f,
+	0x5edef90e, 0x29d9c998, 0xb0d09822, 0xc7d7a8b4, 0x59b33d17, 0x2eb40d81,
+	0xb7bd5c3b, 0xc0ba6cad, 0xedb88320, 0x9abfb3b6, 0x03b6e20c, 0x74b1d29a,
+	0xead54739, 0x9dd277af, 0x04db2615, 0x73dc1683, 0xe3630b12, 0x94643b84,
+	0x0d6d6a3e, 0x7a6a5aa8, 0xe40ecf0b, 0x9309ff9d, 0x0a00ae27, 0x7d079eb1,
+	0xf00f9344, 0x8708a3d2, 0x1e01f268, 0x6906c2fe, 0xf762575d, 0x806567cb,
+	0x196c3671, 0x6e6b06e7, 0xfed41b76, 0x89d32be0, 0x10da7a5a, 0x67dd4acc,
+	0xf9b9df6f, 0x8ebeeff9, 0x17b7be43, 0x60b08ed5, 0xd6d6a3e8, 0xa1d1937e,
+	0x38d8c2c4, 0x4fdff252, 0xd1bb67f1, 0xa6bc5767, 0x3fb506dd, 0x48b2364b,
+	0xd80d2bda, 0xaf0a1b4c, 0x36034af6, 0x41047a60, 0xdf60efc3, 0xa867df55,
+	0x316e8eef, 0x4669be79, 0xcb61b38c, 0xbc66831a, 0x256fd2a0, 0x5268e236,
+	0xcc0c7795, 0xbb0b4703, 0x220216b9, 0x5505262f, 0xc5ba3bbe, 0xb2bd0b28,
+	0x2bb45a92, 0x5cb36a04, 0xc2d7ffa7, 0xb5d0cf31, 0x2cd99e8b, 0x5bdeae1d,
+	0x9b64c2b0, 0xec63f226, 0x756aa39c, 0x026d930a, 0x9c0906a9, 0xeb0e363f,
+	0x72076785, 0x05005713, 0x95bf4a82, 0xe2b87a14, 0x7bb12bae, 0x0cb61b38,
+	0x92d28e9b, 0xe5d5be0d, 0x7cdcefb7, 0x0bdbdf21, 0x86d3d2d4, 0xf1d4e242,
+	0x68ddb3f8, 0x1fda836e, 0x81be16cd, 0xf6b9265b, 0x6fb077e1, 0x18b74777,
+	0x88085ae6, 0xff0f6a70, 0x66063bca, 0x11010b5c, 0x8f659eff, 0xf862ae69,
+	0x616bffd3, 0x166ccf45, 0xa00ae278, 0xd70dd2ee, 0x4e048354, 0x3903b3c2,
+	0xa7672661, 0xd06016f7, 0x4969474d, 0x3e6e77db, 0xaed16a4a, 0xd9d65adc,
+	0x40df0b66, 0x37d83bf0, 0xa9bcae53, 0xdebb9ec5, 0x47b2cf7f, 0x30b5ffe9,
+	0xbdbdf21c, 0xcabac28a, 0x53b39330, 0x24b4a3a6, 0xbad03605, 0xcdd70693,
+	0x54de5729, 0x23d967bf, 0xb3667a2e, 0xc4614ab8, 0x5d681b02, 0x2a6f2b94,
+	0xb40bbe37, 0xc30c8ea1, 0x5a05df1b, 0x2d02ef8d
+};
+
+static uint32_t
+crc32(uint32_t crc, const void *buf, size_t size)
+{
+	const uint8_t *p;
+
+	p = buf;
+	crc = crc ^ ~0U;
+
+	while (size--)
+		crc = crc32_tab[(crc ^ *p++) & 0xFF] ^ (crc >> 8);
+
+	return crc ^ ~0U;
+}
+
+static void
+efi_set_tables_64(EFI_SYSTEM_TABLE_64 * system_table)
+{
+    EFI_RUNTIME_SERVICES_64 *runtime;
+    uint32_t hdr_cksum;
+    uint32_t cksum;
+
+    DPRINTF("Processing 64-bit EFI tables at %p\n", system_table);
+    do {
+	DPRINTF("Header:\n");
+	DPRINTF("  Signature:   0x%016llx\n", system_table->Hdr.Signature);
+	DPRINTF("  Revision:    0x%08x\n", system_table->Hdr.Revision);
+	DPRINTF("  HeaderSize:  0x%08x\n", system_table->Hdr.HeaderSize);
+	DPRINTF("  CRC32:       0x%08x\n", system_table->Hdr.CRC32);
+	DPRINTF("RuntimeServices: 0x%016llx\n", system_table->RuntimeServices);
+        if (system_table->Hdr.Signature != EFI_SYSTEM_TABLE_SIGNATURE) {
+	    kprintf("Bad EFI system table signature\n");
+            break;
+        }
+        // Verify signature of the system table
+        hdr_cksum = system_table->Hdr.CRC32;
+        system_table->Hdr.CRC32 = 0;
+        cksum = crc32(0L, system_table, system_table->Hdr.HeaderSize);
+
+        DPRINTF("System table calculated CRC32 = 0x%x, header = 0x%x\n", cksum, hdr_cksum);
+        system_table->Hdr.CRC32 = hdr_cksum;
+        if (cksum != hdr_cksum) {
+            kprintf("Bad EFI system table checksum\n");
+            break;
+        }
+
+        gPEEFISystemTable     = system_table;
+
+        if(system_table->RuntimeServices == 0) {
+            kprintf("No runtime table present\n");
+            break;
+        }
+        DPRINTF("RuntimeServices table at 0x%qx\n", system_table->RuntimeServices);
+        // 64-bit virtual address is OK for 64-bit EFI and 64/32-bit kernel.
+        runtime = (EFI_RUNTIME_SERVICES_64 *) (uintptr_t)system_table->RuntimeServices;
+        DPRINTF("Checking runtime services table %p\n", runtime);
+        if (runtime->Hdr.Signature != EFI_RUNTIME_SERVICES_SIGNATURE) {
+            kprintf("Bad EFI runtime table signature\n");
+            break;
+        }
+
+	// Verify signature of runtime services table
+	hdr_cksum = runtime->Hdr.CRC32;
+	runtime->Hdr.CRC32 = 0;
+	cksum = crc32(0L, runtime, runtime->Hdr.HeaderSize);
+
+	DPRINTF("Runtime table calculated CRC32 = 0x%x, header = 0x%x\n", cksum, hdr_cksum);
+	runtime->Hdr.CRC32 = hdr_cksum;
+	if (cksum != hdr_cksum) {
+	    kprintf("Bad EFI runtime table checksum\n");
+	    break;
+	}
+
+	gPEEFIRuntimeServices = runtime;
+    }
+    while (FALSE);
+}
+
+static void
+efi_set_tables_32(EFI_SYSTEM_TABLE_32 * system_table)
+{
+    EFI_RUNTIME_SERVICES_32 *runtime;
+    uint32_t hdr_cksum;
+    uint32_t cksum;
+
+    DPRINTF("Processing 32-bit EFI tables at %p\n", system_table);
+    do {
+	DPRINTF("Header:\n");
+	DPRINTF("  Signature:   0x%016llx\n", system_table->Hdr.Signature);
+	DPRINTF("  Revision:    0x%08x\n", system_table->Hdr.Revision);
+	DPRINTF("  HeaderSize:  0x%08x\n", system_table->Hdr.HeaderSize);
+	DPRINTF("  CRC32:       0x%08x\n", system_table->Hdr.CRC32);
+	DPRINTF("RuntimeServices: 0x%08x\n", system_table->RuntimeServices);
+        if (system_table->Hdr.Signature != EFI_SYSTEM_TABLE_SIGNATURE) {
+            kprintf("Bad EFI system table signature\n");
+            break;
+        }
+        // Verify signature of the system table
+        hdr_cksum = system_table->Hdr.CRC32;
+        system_table->Hdr.CRC32 = 0;
+        DPRINTF("System table at %p HeaderSize 0x%x\n", system_table, system_table->Hdr.HeaderSize);
+        cksum = crc32(0L, system_table, system_table->Hdr.HeaderSize);
+
+        DPRINTF("System table calculated CRC32 = 0x%x, header = 0x%x\n", cksum, hdr_cksum);
+        system_table->Hdr.CRC32 = hdr_cksum;
+        if (cksum != hdr_cksum) {
+            kprintf("Bad EFI system table checksum\n");
+            break;
+        }
+
+        gPEEFISystemTable     = system_table;
+
+        if(system_table->RuntimeServices == 0) {
+            kprintf("No runtime table present\n");
+            break;
+        }
+        DPRINTF("RuntimeServices table at 0x%x\n", system_table->RuntimeServices);
+        // 32-bit virtual address is OK for 32-bit EFI and 32-bit kernel.
+        // For a 64-bit kernel, booter provides a virtual address mod 4G
+        runtime = (EFI_RUNTIME_SERVICES_32 *)
+			(system_table->RuntimeServices | VM_MIN_KERNEL_ADDRESS);
+	DPRINTF("Runtime table addressed at %p\n", runtime);
+        if (runtime->Hdr.Signature != EFI_RUNTIME_SERVICES_SIGNATURE) {
+            kprintf("Bad EFI runtime table signature\n");
+            break;
+        }
+
+	// Verify signature of runtime services table
+	hdr_cksum = runtime->Hdr.CRC32;
+	runtime->Hdr.CRC32 = 0;
+	cksum = crc32(0L, runtime, runtime->Hdr.HeaderSize);
+
+	DPRINTF("Runtime table calculated CRC32 = 0x%x, header = 0x%x\n", cksum, hdr_cksum);
+	runtime->Hdr.CRC32 = hdr_cksum;
+	if (cksum != hdr_cksum) {
+	    kprintf("Bad EFI runtime table checksum\n");
+	    break;
+	}
+
+	DPRINTF("Runtime functions\n");
+	DPRINTF("  GetTime                  : 0x%x\n", runtime->GetTime);
+	DPRINTF("  SetTime                  : 0x%x\n", runtime->SetTime);
+	DPRINTF("  GetWakeupTime            : 0x%x\n", runtime->GetWakeupTime);
+	DPRINTF("  SetWakeupTime            : 0x%x\n", runtime->SetWakeupTime);
+	DPRINTF("  SetVirtualAddressMap     : 0x%x\n", runtime->SetVirtualAddressMap);
+	DPRINTF("  ConvertPointer           : 0x%x\n", runtime->ConvertPointer);
+	DPRINTF("  GetVariable              : 0x%x\n", runtime->GetVariable);
+	DPRINTF("  GetNextVariableName      : 0x%x\n", runtime->GetNextVariableName);
+	DPRINTF("  SetVariable              : 0x%x\n", runtime->SetVariable);
+	DPRINTF("  GetNextHighMonotonicCount: 0x%x\n", runtime->GetNextHighMonotonicCount);
+	DPRINTF("  ResetSystem              : 0x%x\n", runtime->ResetSystem);
+
+	gPEEFIRuntimeServices = runtime;
+    }
+    while (FALSE);
+}
+
+
+/* Map in EFI runtime areas. */
+static void
+efi_init(void)
+{
+    boot_args *args = (boot_args *)PE_state.bootArgs;
+
+    kprintf("Initializing EFI runtime services\n");
+
+    do
+    {
+	vm_offset_t vm_size, vm_addr;
+	vm_map_offset_t phys_addr;
+	EfiMemoryRange *mptr;
+	unsigned int msize, mcount;
+	unsigned int i;
+
+	msize = args->MemoryMapDescriptorSize;
+	mcount = args->MemoryMapSize / msize;
+
+	DPRINTF("efi_init() kernel base: 0x%x size: 0x%x\n",
+		args->kaddr, args->ksize);
+	DPRINTF("           efiSystemTable physical: 0x%x virtual: %p\n",
+		args->efiSystemTable,
+		(void *) ml_static_ptovirt(args->efiSystemTable));
+	DPRINTF("           efiRuntimeServicesPageStart: 0x%x\n",
+		args->efiRuntimeServicesPageStart);
+	DPRINTF("           efiRuntimeServicesPageCount: 0x%x\n",
+		args->efiRuntimeServicesPageCount);
+	DPRINTF("           efiRuntimeServicesVirtualPageStart: 0x%016llx\n",
+		args->efiRuntimeServicesVirtualPageStart);
+	mptr = (EfiMemoryRange *)ml_static_ptovirt(args->MemoryMap);
+	for (i=0; i < mcount; i++, mptr = (EfiMemoryRange *)(((vm_offset_t)mptr) + msize)) {
+	    if (((mptr->Attribute & EFI_MEMORY_RUNTIME) == EFI_MEMORY_RUNTIME) ) {
+		vm_size = (vm_offset_t)i386_ptob((uint32_t)mptr->NumberOfPages);
+		vm_addr =   (vm_offset_t) mptr->VirtualStart;
+		/* For K64 on EFI32, shadow-map into high KVA */
+		if (vm_addr < VM_MIN_KERNEL_ADDRESS)
+			vm_addr |= VM_MIN_KERNEL_ADDRESS;
+		phys_addr = (vm_map_offset_t) mptr->PhysicalStart;
+		DPRINTF(" Type: %x phys: %p EFIv: %p kv: %p size: %p\n",
+			mptr->Type,
+			(void *) (uintptr_t) phys_addr,
+			(void *) (uintptr_t) mptr->VirtualStart,
+			(void *) vm_addr,
+			(void *) vm_size);
+		pmap_map_bd(vm_addr, phys_addr, phys_addr + round_page(vm_size),
+		     (mptr->Type == kEfiRuntimeServicesCode) ? VM_PROT_READ | VM_PROT_EXECUTE : VM_PROT_READ|VM_PROT_WRITE,
+		     (mptr->Type == EfiMemoryMappedIO)       ? VM_WIMG_IO   : VM_WIMG_USE_DEFAULT);
+	    }
+	}
+
+        if (args->Version != kBootArgsVersion2)
+            panic("Incompatible boot args version %d revision %d\n", args->Version, args->Revision);
+
+	DPRINTF("Boot args version %d revision %d mode %d\n", args->Version, args->Revision, args->efiMode);
+        if (args->efiMode == kBootArgsEfiMode64) {
+            efi_set_tables_64((EFI_SYSTEM_TABLE_64 *) ml_static_ptovirt(args->efiSystemTable));
+        } else {
+            efi_set_tables_32((EFI_SYSTEM_TABLE_32 *) ml_static_ptovirt(args->efiSystemTable));
+        }
+    }
+    while (FALSE);
+
+    return;
+}
+
+/* Remap EFI runtime areas. */
+void
+hibernate_newruntime_map(void * map, vm_size_t map_size, uint32_t system_table_offset)
+{
+    boot_args *args = (boot_args *)PE_state.bootArgs;
+
+    kprintf("Reinitializing EFI runtime services\n");
+
+    do
+    {
+        vm_offset_t vm_size, vm_addr;
+	vm_map_offset_t phys_addr;
+	EfiMemoryRange *mptr;
+	unsigned int msize, mcount;
+	unsigned int i;
+
+	gPEEFISystemTable     = 0;
+	gPEEFIRuntimeServices = 0;
+
+	system_table_offset += ptoa_32(args->efiRuntimeServicesPageStart);
+
+	kprintf("Old system table 0x%x, new 0x%x\n",
+	    (uint32_t)args->efiSystemTable,    system_table_offset);
+
+	args->efiSystemTable    = system_table_offset;
+
+	kprintf("Old map:\n");
+	msize = args->MemoryMapDescriptorSize;
+	mcount = args->MemoryMapSize / msize;
+	mptr = (EfiMemoryRange *)ml_static_ptovirt(args->MemoryMap);
+	for (i=0; i < mcount; i++, mptr = (EfiMemoryRange *)(((vm_offset_t)mptr) + msize)) {
+	    if ((mptr->Attribute & EFI_MEMORY_RUNTIME) == EFI_MEMORY_RUNTIME) {
+
+		vm_size = (vm_offset_t)i386_ptob((uint32_t)mptr->NumberOfPages);
+		vm_addr =   (vm_offset_t) mptr->VirtualStart;
+		/* K64 on EFI32 */
+		if (vm_addr < VM_MIN_KERNEL_ADDRESS)
+			vm_addr |= VM_MIN_KERNEL_ADDRESS;
+		phys_addr = (vm_map_offset_t) mptr->PhysicalStart;
+
+		kprintf("mapping[%u] %qx @ %lx, %llu\n", mptr->Type, phys_addr, (unsigned long)vm_addr, mptr->NumberOfPages);
+	    }
+	}
+
+	pmap_remove(kernel_pmap, i386_ptob(args->efiRuntimeServicesPageStart), 
+				 i386_ptob(args->efiRuntimeServicesPageStart + args->efiRuntimeServicesPageCount));
+
+	kprintf("New map:\n");
+	msize = args->MemoryMapDescriptorSize;
+	mcount = (unsigned int )(map_size / msize);
+	mptr = map;
+	for (i=0; i < mcount; i++, mptr = (EfiMemoryRange *)(((vm_offset_t)mptr) + msize)) {
+	    if ((mptr->Attribute & EFI_MEMORY_RUNTIME) == EFI_MEMORY_RUNTIME) {
+
+		vm_size = (vm_offset_t)i386_ptob((uint32_t)mptr->NumberOfPages);
+		vm_addr =   (vm_offset_t) mptr->VirtualStart;
+		if (vm_addr < VM_MIN_KERNEL_ADDRESS)
+			vm_addr |= VM_MIN_KERNEL_ADDRESS;
+		phys_addr = (vm_map_offset_t) mptr->PhysicalStart;
+
+		kprintf("mapping[%u] %qx @ %lx, %llu\n", mptr->Type, phys_addr, (unsigned long)vm_addr, mptr->NumberOfPages);
+
+		pmap_map(vm_addr, phys_addr, phys_addr + round_page(vm_size),
+			 (mptr->Type == kEfiRuntimeServicesCode) ? VM_PROT_READ | VM_PROT_EXECUTE : VM_PROT_READ|VM_PROT_WRITE,
+			 (mptr->Type == EfiMemoryMappedIO)       ? VM_WIMG_IO   : VM_WIMG_USE_DEFAULT);
+	    }
+	}
+
+        if (args->Version != kBootArgsVersion2)
+            panic("Incompatible boot args version %d revision %d\n", args->Version, args->Revision);
+
+        kprintf("Boot args version %d revision %d mode %d\n", args->Version, args->Revision, args->efiMode);
+        if (args->efiMode == kBootArgsEfiMode64) {
+	    efi_set_tables_64((EFI_SYSTEM_TABLE_64 *) ml_static_ptovirt(args->efiSystemTable));
+        } else {
+	    efi_set_tables_32((EFI_SYSTEM_TABLE_32 *) ml_static_ptovirt(args->efiSystemTable));
+        }
+    }
+    while (FALSE);
+
+    kprintf("Done reinitializing EFI runtime services\n");
+
+    return;
+}
+
+/*
+ * Find devices.  The system is alive.
+ */
+void
+machine_init(void)
+{
+	/* Now with VM up, switch to dynamically allocated cpu data */
+	cpu_data_realloc();
+
+        /* Ensure panic buffer is initialized. */
+        debug_log_init();
+
+	/*
+	 * Display CPU identification
+	 */
+	cpuid_cpu_display("CPU identification");
+	cpuid_feature_display("CPU features");
+	cpuid_extfeature_display("CPU extended features");
+
+        /*
+         * Initialize EFI runtime services.
+         */
+        efi_init();
+
+	smp_init();
+
+	/*
+	 * Set up to use floating point.
+	 */
+	init_fpu();
+
+	/*
+	 * Configure clock devices.
+	 */
+	clock_config();
+
+#if CONFIG_MTRR
+	/*
+	 * Initialize MTRR from boot processor.
+	 */
+	mtrr_init();
+
+	/*
+	 * Set up PAT for boot processor.
+	 */
+	pat_init();
+#endif
+
+	/*
+	 * Free lowmem pages and complete other setup
+	 */
+	pmap_lowmem_finalize();
+}
+
+/*
+ * Halt a cpu.
+ */
+void
+halt_cpu(void)
+{
+	halt_all_cpus(FALSE);
+}
+
+int reset_mem_on_reboot = 1;
+
+/*
+ * Halt the system or reboot.
+ */
+void
+halt_all_cpus(boolean_t reboot)
+{
+	if (reboot) {
+		printf("MACH Reboot\n");
+		PEHaltRestart( kPERestartCPU );
+	} else {
+		printf("CPU halted\n");
+		PEHaltRestart( kPEHaltCPU );
+	}
+	while(1);
+}
+
+ 
+/* Issue an I/O port read if one has been requested - this is an event logic
+ * analyzers can use as a trigger point.
+ */
+
+void
+panic_io_port_read(void) {
+	if (panic_io_port)
+		(void)inb(panic_io_port);
+}
+
+/* For use with the MP rendezvous mechanism
+ */
+
+uint64_t panic_restart_timeout = ~(0ULL);
+
+#define PANIC_RESTART_TIMEOUT (3ULL * NSEC_PER_SEC)
+
+static void
+machine_halt_cpu(void) {
+	uint64_t deadline;
+
+	panic_io_port_read();
+
+	/* Halt here forever if we're not rebooting */
+	if (!PE_reboot_on_panic() && panic_restart_timeout == ~(0ULL)) {
+		pmCPUHalt(PM_HALT_DEBUG);
+		return;
+	}
+
+	if (PE_reboot_on_panic())
+		deadline = mach_absolute_time() + PANIC_RESTART_TIMEOUT;
+	else
+		deadline = mach_absolute_time() + panic_restart_timeout;
+
+	while (mach_absolute_time() < deadline)
+		cpu_pause();
+
+	kprintf("Invoking PE_halt_restart\n");
+	/* Attempt restart via ACPI RESET_REG; at the time of this
+	 * writing, this is routine is chained through AppleSMC->
+	 * AppleACPIPlatform
+	 */
+	if (PE_halt_restart)
+		(*PE_halt_restart)(kPERestartCPU);
+	pmCPUHalt(PM_HALT_DEBUG);
+}
+
+static int pid_from_task(task_t task)
+{
+        int pid = -1;
+
+        if (task->bsd_info)
+                pid = proc_pid(task->bsd_info);
+
+        return pid;
+}
+
+void
+DebuggerWithContext(
+	__unused unsigned int	reason,
+	__unused void 		*ctx,
+	const char		*message)
+{
+	Debugger(message);
+}
+
+void
+Debugger(
+	const char	*message)
+{
+	unsigned long pi_size = 0;
+	void *stackptr;
+	int cn = cpu_number();
+	task_t task = current_task();
+	int	task_pid = pid_from_task(task);
+	boolean_t old_doprnt_hide_pointers = doprnt_hide_pointers;
+
+	hw_atomic_add(&debug_mode, 1);   
+	if (!panic_is_inited) {
+		postcode(PANIC_HLT);
+		asm("hlt");
+	}
+
+	doprnt_hide_pointers = FALSE;
+
+	printf("Debugger called: <%s>\n", message);
+	kprintf("Debugger called: <%s>\n", message);
+
+	/*
+	 * Skip the graphical panic box if no panic string.
+	 * This is the case if we're being called from
+	 *   host_reboot(,HOST_REBOOT_DEBUGGER)
+	 * as a quiet way into the debugger.
+	 */
+
+	if (panicstr) {
+		disable_preemption();
+
+/* Issue an I/O port read if one has been requested - this is an event logic
+ * analyzers can use as a trigger point.
+ */
+		panic_io_port_read();
+
+		/* Obtain current frame pointer */
+		__asm__ volatile("movq %%rbp, %0" : "=m" (stackptr));
+
+		/* Print backtrace - callee is internally synchronized */
+		if (task_pid == 1 && (init_task_died)) {
+			/* Special handling of launchd died panics */
+			print_launchd_info();
+		} else {
+			panic_i386_backtrace(stackptr, ((panic_double_fault_cpu == cn) ? 80: 48), NULL, FALSE, NULL);
+		}
+
+		/* everything should be printed now so copy to NVRAM
+		 */
+
+		if( debug_buf_size > 0) {
+		  /* Optionally sync the panic log, if any, to NVRAM
+		   * This is the default.
+		   */
+		    if (commit_paniclog_to_nvram) {
+			unsigned int bufpos;
+			uintptr_t cr0;
+			
+			debug_putc(0);
+
+			/* Now call the compressor */
+			/* XXX Consider using the WKdm compressor in the
+			 * future, rather than just packing - would need to
+			 * be co-ordinated with crashreporter, which decodes
+			 * this post-restart. The compressor should be
+			 * capable of in-place compression.
+			 */
+			bufpos = packA(debug_buf,
+			    (unsigned int) (debug_buf_ptr - debug_buf), debug_buf_size);
+			/* If compression was successful,
+			 * use the compressed length
+			 */
+			pi_size = bufpos ? bufpos : (unsigned) (debug_buf_ptr - debug_buf);
+
+			/* Save panic log to non-volatile store
+			 * Panic info handler must truncate data that is 
+			 * too long for this platform.
+			 * This call must save data synchronously,
+			 * since we can subsequently halt the system.
+			 */
+
+
+/* The following sequence is a workaround for:
+ * <rdar://problem/5915669> SnowLeopard10A67: AppleEFINVRAM should not invoke
+ * any routines that use floating point (MMX in this case) when saving panic
+ * logs to nvram/flash.
+ */
+			cr0 = get_cr0();
+			clear_ts();
+
+			kprintf("Attempting to commit panic log to NVRAM\n");
+			pi_size = PESavePanicInfo((unsigned char *)debug_buf,
+					(uint32_t)pi_size );
+			set_cr0(cr0);
+
+			/* Uncompress in-place, to permit examination of
+			 * the panic log by debuggers.
+			 */
+
+			if (bufpos) {
+			  unpackA(debug_buf, bufpos);
+			}
+                    }
+                }
+
+		if (!panicDebugging) {
+			unsigned cnum;
+			/* Clear the MP rendezvous function lock, in the event
+			 * that a panic occurred while in that codepath.
+			 */
+			mp_rendezvous_break_lock();
+
+			/* Non-maskably interrupt all other processors
+			 * If a restart timeout is specified, this processor
+			 * will attempt a restart.
+			 */
+			kprintf("Invoking machine_halt_cpu on CPU %d\n", cn);
+			for (cnum = 0; cnum < real_ncpus; cnum++) {
+				if (cnum != (unsigned) cn) {
+					cpu_NMI_interrupt(cnum);
+				}
+			}
+			machine_halt_cpu();
+			/* NOT REACHED */
+		}
+        }
+
+	doprnt_hide_pointers = old_doprnt_hide_pointers;
+	__asm__("int3");
+	hw_atomic_sub(&debug_mode, 1);   
+}
+
+char *
+machine_boot_info(char *buf, __unused vm_size_t size)
+{
+	*buf ='\0';
+	return buf;
+}
+
+/* Routines for address - symbol translation. Not called unless the "keepsyms"
+ * boot-arg is supplied.
+ */
+
+static int
+panic_print_macho_symbol_name(kernel_mach_header_t *mh, vm_address_t search, const char *module_name)
+{
+    kernel_nlist_t	*sym = NULL;
+    struct load_command		*cmd;
+    kernel_segment_command_t	*orig_ts = NULL, *orig_le = NULL;
+    struct symtab_command	*orig_st = NULL;
+    unsigned int			i;
+    char					*strings, *bestsym = NULL;
+    vm_address_t			bestaddr = 0, diff, curdiff;
+
+    /* Assume that if it's loaded and linked into the kernel, it's a valid Mach-O */
+    
+    cmd = (struct load_command *) &mh[1];
+    for (i = 0; i < mh->ncmds; i++) {
+        if (cmd->cmd == LC_SEGMENT_KERNEL) {
+            kernel_segment_command_t *orig_sg = (kernel_segment_command_t *) cmd;
+            
+            if (strncmp(SEG_TEXT, orig_sg->segname,
+				    sizeof(orig_sg->segname)) == 0)
+                orig_ts = orig_sg;
+            else if (strncmp(SEG_LINKEDIT, orig_sg->segname,
+				    sizeof(orig_sg->segname)) == 0)
+                orig_le = orig_sg;
+            else if (strncmp("", orig_sg->segname,
+				    sizeof(orig_sg->segname)) == 0)
+                orig_ts = orig_sg; /* pre-Lion i386 kexts have a single unnamed segment */
+        }
+        else if (cmd->cmd == LC_SYMTAB)
+            orig_st = (struct symtab_command *) cmd;
+        
+        cmd = (struct load_command *) ((uintptr_t) cmd + cmd->cmdsize);
+    }
+    
+    if ((orig_ts == NULL) || (orig_st == NULL) || (orig_le == NULL))
+        return 0;
+    
+    if ((search < orig_ts->vmaddr) ||
+        (search >= orig_ts->vmaddr + orig_ts->vmsize)) {
+        /* search out of range for this mach header */
+        return 0;
+    }
+    
+    sym = (kernel_nlist_t *)(uintptr_t)(orig_le->vmaddr + orig_st->symoff - orig_le->fileoff);
+    strings = (char *)(uintptr_t)(orig_le->vmaddr + orig_st->stroff - orig_le->fileoff);
+    diff = search;
+    
+    for (i = 0; i < orig_st->nsyms; i++) {
+        if (sym[i].n_type & N_STAB) continue;
+
+        if (sym[i].n_value <= search) {
+            curdiff = search - (vm_address_t)sym[i].n_value;
+            if (curdiff < diff) {
+                diff = curdiff;
+                bestaddr = sym[i].n_value;
+                bestsym = strings + sym[i].n_un.n_strx;
+            }
+        }
+    }
+    
+    if (bestsym != NULL) {
+        if (diff != 0) {
+            kdb_printf("%s : %s + 0x%lx", module_name, bestsym, (unsigned long)diff);
+        } else {
+            kdb_printf("%s : %s", module_name, bestsym);
+        }
+        return 1;
+    }
+    return 0;
+}
+
+extern kmod_info_t * kmod; /* the list of modules */
+
+static void
+panic_print_kmod_symbol_name(vm_address_t search)
+{
+    u_int i;
+
+    if (gLoadedKextSummaries == NULL)
+	    return;
+    for (i = 0; i < gLoadedKextSummaries->numSummaries; ++i) {
+        OSKextLoadedKextSummary *summary = gLoadedKextSummaries->summaries + i;
+
+        if ((search >= summary->address) &&
+            (search < (summary->address + summary->size)))
+        {
+            kernel_mach_header_t *header = (kernel_mach_header_t *)(uintptr_t) summary->address;
+            if (panic_print_macho_symbol_name(header, search, summary->name) == 0) {
+                kdb_printf("%s + %llu", summary->name, (unsigned long)search - summary->address);
+            }
+            break;
+        }
+    }
+}
+
+void
+panic_print_symbol_name(vm_address_t search)
+{
+    /* try searching in the kernel */
+    if (panic_print_macho_symbol_name(&_mh_execute_header, search, "mach_kernel") == 0) {
+        /* that failed, now try to search for the right kext */
+        panic_print_kmod_symbol_name(search);
+    }
+}
+
+/* Generate a backtrace, given a frame pointer - this routine
+ * should walk the stack safely. The trace is appended to the panic log
+ * and conditionally, to the console. If the trace contains kernel module
+ * addresses, display the module name, load address and dependencies.
+ */
+
+#define DUMPFRAMES 32
+#define PBT_TIMEOUT_CYCLES (5 * 1000 * 1000 * 1000ULL)
+void
+panic_i386_backtrace(void *_frame, int nframes, const char *msg, boolean_t regdump, x86_saved_state_t *regs)
+{
+	cframe_t	*frame = (cframe_t *)_frame;
+	vm_offset_t raddrs[DUMPFRAMES];
+	vm_offset_t PC = 0;
+	int frame_index;
+	volatile uint32_t *ppbtcnt = &pbtcnt;
+	uint64_t bt_tsc_timeout;
+	boolean_t keepsyms = FALSE;
+	int cn = cpu_number();
+	boolean_t old_doprnt_hide_pointers = doprnt_hide_pointers;
+
+	if(pbtcpu != cn) {
+		hw_atomic_add(&pbtcnt, 1);
+		/* Spin on print backtrace lock, which serializes output
+		 * Continue anyway if a timeout occurs.
+		 */
+		hw_lock_to(&pbtlock, ~0U);
+		pbtcpu = cn;
+	}
+
+	if (__improbable(doprnt_hide_pointers == TRUE)) {
+		/* If we're called directly, the Debugger() function will not be called,
+		 * so we need to reset the value in here. */
+		doprnt_hide_pointers = FALSE;
+	}
+
+	panic_check_hook();
+
+	PE_parse_boot_argn("keepsyms", &keepsyms, sizeof (keepsyms));
+
+	if (msg != NULL) {
+		kdb_printf("%s", msg);
+	}
+
+	if ((regdump == TRUE) && (regs != NULL)) {
+		x86_saved_state64_t	*ss64p = saved_state64(regs);
+		kdb_printf(
+		    "RAX: 0x%016llx, RBX: 0x%016llx, RCX: 0x%016llx, RDX: 0x%016llx\n"
+		    "RSP: 0x%016llx, RBP: 0x%016llx, RSI: 0x%016llx, RDI: 0x%016llx\n"
+		    "R8:  0x%016llx, R9:  0x%016llx, R10: 0x%016llx, R11: 0x%016llx\n"
+		    "R12: 0x%016llx, R13: 0x%016llx, R14: 0x%016llx, R15: 0x%016llx\n"
+		    "RFL: 0x%016llx, RIP: 0x%016llx, CS:  0x%016llx, SS:  0x%016llx\n",
+		    ss64p->rax, ss64p->rbx, ss64p->rcx, ss64p->rdx,
+		    ss64p->isf.rsp, ss64p->rbp, ss64p->rsi, ss64p->rdi,
+		    ss64p->r8,  ss64p->r9,  ss64p->r10, ss64p->r11,
+		    ss64p->r12, ss64p->r13, ss64p->r14, ss64p->r15,
+		    ss64p->isf.rflags, ss64p->isf.rip, ss64p->isf.cs,
+		    ss64p->isf.ss);
+		PC = ss64p->isf.rip;
+	}
+
+	kdb_printf("Backtrace (CPU %d), "
+#if PRINT_ARGS_FROM_STACK_FRAME
+	"Frame : Return Address (4 potential args on stack)\n", cn);
+#else
+	"Frame : Return Address\n", cn);
+#endif
+
+	for (frame_index = 0; frame_index < nframes; frame_index++) {
+		vm_offset_t curframep = (vm_offset_t) frame;
+
+		if (!curframep)
+			break;
+
+		if (curframep & 0x3) {
+			kdb_printf("Unaligned frame\n");
+			goto invalid;
+		}
+
+		if (!kvtophys(curframep) ||
+		    !kvtophys(curframep + sizeof(cframe_t) - 1)) {
+			kdb_printf("No mapping exists for frame pointer\n");
+			goto invalid;
+		}
+
+		kdb_printf("%p : 0x%lx ", frame, frame->caller);
+		if (frame_index < DUMPFRAMES)
+			raddrs[frame_index] = frame->caller;
+
+#if PRINT_ARGS_FROM_STACK_FRAME
+		if (kvtophys((vm_offset_t)&(frame->args[3])))
+			kdb_printf("(0x%x 0x%x 0x%x 0x%x) ",
+			    frame->args[0], frame->args[1],
+			    frame->args[2], frame->args[3]);
+#endif
+
+		/* Display address-symbol translation only if the "keepsyms"
+		 * boot-arg is suppplied, since we unload LINKEDIT otherwise.
+		 * This routine is potentially unsafe; also, function
+		 * boundary identification is unreliable after a strip -x.
+		 */
+		if (keepsyms)
+			panic_print_symbol_name((vm_address_t)frame->caller);
+		
+		kdb_printf("\n");
+
+		frame = frame->prev;
+	}
+
+	if (frame_index >= nframes)
+		kdb_printf("\tBacktrace continues...\n");
+
+	goto out;
+
+invalid:
+	kdb_printf("Backtrace terminated-invalid frame pointer %p\n",frame);
+out:
+
+	/* Identify kernel modules in the backtrace and display their
+	 * load addresses and dependencies. This routine should walk
+	 * the kmod list safely.
+	 */
+	if (frame_index)
+		kmod_panic_dump((vm_offset_t *)&raddrs[0], frame_index);
+
+	if (PC != 0)
+		kmod_panic_dump(&PC, 1);
+
+	panic_display_system_configuration();
+
+	doprnt_hide_pointers = old_doprnt_hide_pointers;
+
+	/* Release print backtrace lock, to permit other callers in the
+	 * event of panics on multiple processors.
+	 */
+	hw_lock_unlock(&pbtlock);
+	hw_atomic_sub(&pbtcnt, 1);
+	/* Wait for other processors to complete output
+	 * Timeout and continue after PBT_TIMEOUT_CYCLES.
+	 */
+	bt_tsc_timeout = rdtsc64() + PBT_TIMEOUT_CYCLES;
+	while(*ppbtcnt && (rdtsc64() < bt_tsc_timeout));
+}
+
+static boolean_t
+debug_copyin(pmap_t p, uint64_t uaddr, void *dest, size_t size)
+{
+        size_t rem = size;
+        char *kvaddr = dest;
+
+        while (rem) {
+                ppnum_t upn = pmap_find_phys(p, uaddr);
+                uint64_t phys_src = ptoa_64(upn) | (uaddr & PAGE_MASK);
+                uint64_t phys_dest = kvtophys((vm_offset_t)kvaddr);
+                uint64_t src_rem = PAGE_SIZE - (phys_src & PAGE_MASK);
+                uint64_t dst_rem = PAGE_SIZE - (phys_dest & PAGE_MASK);
+                size_t cur_size = (uint32_t) MIN(src_rem, dst_rem);
+                cur_size = MIN(cur_size, rem);
+
+                if (upn && pmap_valid_page(upn) && phys_dest) {
+                        bcopy_phys(phys_src, phys_dest, cur_size);
+                }
+                else
+                        break;
+                uaddr += cur_size;
+                kvaddr += cur_size;
+                rem -= cur_size;
+        }
+        return (rem == 0);
+}
+
+void
+print_threads_registers(thread_t thread)
+{
+	x86_saved_state_t *savestate;
+	
+	savestate = get_user_regs(thread);
+	kdb_printf(
+		"\nRAX: 0x%016llx, RBX: 0x%016llx, RCX: 0x%016llx, RDX: 0x%016llx\n"
+	    "RSP: 0x%016llx, RBP: 0x%016llx, RSI: 0x%016llx, RDI: 0x%016llx\n"
+	    "R8:  0x%016llx, R9:  0x%016llx, R10: 0x%016llx, R11: 0x%016llx\n"
+		"R12: 0x%016llx, R13: 0x%016llx, R14: 0x%016llx, R15: 0x%016llx\n"
+		"RFL: 0x%016llx, RIP: 0x%016llx, CS:  0x%016llx, SS:  0x%016llx\n\n",
+		savestate->ss_64.rax, savestate->ss_64.rbx, savestate->ss_64.rcx, savestate->ss_64.rdx,
+		savestate->ss_64.isf.rsp, savestate->ss_64.rbp, savestate->ss_64.rsi, savestate->ss_64.rdi,
+		savestate->ss_64.r8, savestate->ss_64.r9,  savestate->ss_64.r10, savestate->ss_64.r11,
+		savestate->ss_64.r12, savestate->ss_64.r13, savestate->ss_64.r14, savestate->ss_64.r15,
+		savestate->ss_64.isf.rflags, savestate->ss_64.isf.rip, savestate->ss_64.isf.cs,
+		savestate->ss_64.isf.ss);
+}
+
+void
+print_tasks_user_threads(task_t task)
+{
+	thread_t		thread = current_thread();
+	x86_saved_state_t *savestate;
+	pmap_t			pmap = 0;
+	uint64_t		rbp;
+	const char		*cur_marker = 0;
+	int             j;
+	
+	for (j = 0, thread = (thread_t) queue_first(&task->threads); j < task->thread_count;
+			++j, thread = (thread_t) queue_next(&thread->task_threads)) {
+
+		kdb_printf("Thread %d: %p\n", j, thread);
+		pmap = get_task_pmap(task);
+		savestate = get_user_regs(thread);
+		rbp = savestate->ss_64.rbp;
+		print_one_backtrace(pmap, (vm_offset_t)rbp, cur_marker, TRUE, TRUE);
+		kdb_printf("\n");
+		}
+}
+
+void
+print_thread_num_that_crashed(task_t task)
+{
+	thread_t		c_thread = current_thread();
+	thread_t		thread;
+	int             j;
+	
+	for (j = 0, thread = (thread_t) queue_first(&task->threads); j < task->thread_count;
+			++j, thread = (thread_t) queue_next(&thread->task_threads)) {
+
+		if (c_thread == thread) {
+			kdb_printf("\nThread %d crashed\n", j);
+			break;
+		}
+	}
+}
+
+#define PANICLOG_UUID_BUF_SIZE 256
+
+void print_uuid_info(task_t task)
+{
+	uint32_t		uuid_info_count = 0;
+	mach_vm_address_t	uuid_info_addr = 0;
+	boolean_t		have_map = (task->map != NULL) &&	(ml_validate_nofault((vm_offset_t)(task->map), sizeof(struct _vm_map)));
+	boolean_t		have_pmap = have_map && (task->map->pmap != NULL) && (ml_validate_nofault((vm_offset_t)(task->map->pmap), sizeof(struct pmap)));
+	int				task_pid = pid_from_task(task);
+	char			uuidbuf[PANICLOG_UUID_BUF_SIZE] = {0};
+	char			*uuidbufptr = uuidbuf;
+	uint32_t		k;
+
+	if (have_pmap && task->active && task_pid > 0) {
+		/* Read dyld_all_image_infos struct from task memory to get UUID array count & location */
+		struct user64_dyld_all_image_infos task_image_infos;
+		if (debug_copyin(task->map->pmap, task->all_image_info_addr,
+			&task_image_infos, sizeof(struct user64_dyld_all_image_infos))) {
+			uuid_info_count = (uint32_t)task_image_infos.uuidArrayCount;
+			uuid_info_addr = task_image_infos.uuidArray;
+		}
+
+		/* If we get a NULL uuid_info_addr (which can happen when we catch dyld
+		 * in the middle of updating this data structure), we zero the
+		 * uuid_info_count so that we won't even try to save load info for this task
+		 */
+		if (!uuid_info_addr) {
+			uuid_info_count = 0;
+		}
+	}
+
+	if (task_pid > 0 && uuid_info_count > 0) {
+		uint32_t uuid_info_size = sizeof(struct user64_dyld_uuid_info);
+		uint32_t uuid_array_size = uuid_info_count * uuid_info_size;
+		uint32_t uuid_copy_size = 0;
+		uint32_t uuid_image_count = 0;
+		char *current_uuid_buffer = NULL;
+		/* Copy in the UUID info array. It may be nonresident, in which case just fix up nloadinfos to 0 */
+		
+		kdb_printf("\nuuid info:\n");
+		while (uuid_array_size) {
+			if (uuid_array_size <= PANICLOG_UUID_BUF_SIZE) {
+				uuid_copy_size = uuid_array_size;
+				uuid_image_count = uuid_array_size/uuid_info_size;
+			} else {
+				uuid_image_count = PANICLOG_UUID_BUF_SIZE/uuid_info_size;
+				uuid_copy_size = uuid_image_count * uuid_info_size;
+			}
+			if (have_pmap && !debug_copyin(task->map->pmap, uuid_info_addr, uuidbufptr,
+				uuid_copy_size)) {
+				kdb_printf("Error!! Failed to copy UUID info for task %p pid %d\n", task, task_pid);
+				uuid_image_count = 0;
+				break;
+			}
+
+			if (uuid_image_count > 0) {
+				current_uuid_buffer = uuidbufptr;
+				for (k = 0; k < uuid_image_count; k++) {
+					kdb_printf(" %#llx", *(uint64_t *)current_uuid_buffer);
+					current_uuid_buffer += sizeof(uint64_t);
+					uint8_t *uuid = (uint8_t *)current_uuid_buffer;
+					kdb_printf("\tuuid = <%02x%02x%02x%02x-%02x%02x-%02x%02x-%02x%02x-%02x%02x%02x%02x%02x%02x>\n",
+					uuid[0], uuid[1], uuid[2], uuid[3], uuid[4], uuid[5], uuid[6], uuid[7], uuid[8],
+					uuid[9], uuid[10], uuid[11], uuid[12], uuid[13], uuid[14], uuid[15]);
+					current_uuid_buffer += 16;
+				}
+				bzero(&uuidbuf, sizeof(uuidbuf));
+			}
+			uuid_info_addr += uuid_copy_size;
+			uuid_array_size -= uuid_copy_size;
+		}
+	}
+}
+
+void print_launchd_info(void)
+{
+	task_t		task = current_task();
+	thread_t	thread = current_thread();
+	volatile	uint32_t *ppbtcnt = &pbtcnt;
+	uint64_t	bt_tsc_timeout;
+	int		cn = cpu_number();
+
+	if(pbtcpu != cn) {
+		hw_atomic_add(&pbtcnt, 1);
+		/* Spin on print backtrace lock, which serializes output
+		 * Continue anyway if a timeout occurs.
+		 */
+		hw_lock_to(&pbtlock, ~0U);
+		pbtcpu = cn;
+	}
+	
+	print_uuid_info(task);
+	print_thread_num_that_crashed(task);
+	print_threads_registers(thread);
+	print_tasks_user_threads(task);
+	kdb_printf("Mac OS version: %s\n", (osversion[0] != 0) ? osversion : "Not yet set");
+	kdb_printf("Kernel version: %s\n", version);
+	panic_display_kernel_uuid();
+	panic_display_model_name();
+	
+	/* Release print backtrace lock, to permit other callers in the
+	 * event of panics on multiple processors.
+	 */
+	hw_lock_unlock(&pbtlock);
+	hw_atomic_sub(&pbtcnt, 1);
+	/* Wait for other processors to complete output
+	 * Timeout and continue after PBT_TIMEOUT_CYCLES.
+	 */
+	bt_tsc_timeout = rdtsc64() + PBT_TIMEOUT_CYCLES;
+	while(*ppbtcnt && (rdtsc64() < bt_tsc_timeout));
+
+}
diff -Nur xnu-3247.1.106/osfmk/i386/commpage/commpage.c xnu-3247.1.106-AnV/osfmk/i386/commpage/commpage.c
--- xnu-3247.1.106/osfmk/i386/commpage/commpage.c	2015-12-06 01:32:35.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/commpage/commpage.c	2015-12-13 17:38:41.000000000 +0100
@@ -124,7 +124,7 @@
 	kern_return_t	kr;
 
 	if (submap == NULL)
-		panic("commpage submap is null");
+		printf("commpage submap is null");
 
 	if ((kr = vm_map(kernel_map,
 			 &kernel_addr,
@@ -137,14 +137,14 @@
 			 VM_PROT_ALL,
 			 VM_PROT_ALL,
 			 VM_INHERIT_NONE)))
-		panic("cannot allocate commpage %d", kr);
+		printf("cannot allocate commpage %d", kr);
 
 	if ((kr = vm_map_wire(kernel_map,
 			      kernel_addr,
 			      kernel_addr+area_used,
 			      VM_PROT_DEFAULT|VM_PROT_MEMORY_TAG_MAKE(VM_KERN_MEMORY_OSFMK),
 			      FALSE)))
-		panic("cannot wire commpage: %d", kr);
+		printf("cannot wire commpage: %d", kr);
 
 	/* 
 	 * Now that the object is created and wired into the kernel map, mark it so that no delay
@@ -155,7 +155,7 @@
 	 * JMM - What we really need is a way to create it like this in the first place.
 	 */
 	if (!(kr = vm_map_lookup_entry( kernel_map, vm_map_trunc_page(kernel_addr, VM_MAP_PAGE_MASK(kernel_map)), &entry) || entry->is_sub_map))
-		panic("cannot find commpage entry %d", kr);
+		printf("cannot find commpage entry %d", kr);
 	VME_OBJECT(entry)->copy_strategy = MEMORY_OBJECT_COPY_NONE;
 
 	if ((kr = mach_make_memory_entry( kernel_map,		// target map
@@ -164,7 +164,7 @@
 				    uperm,	// protections as specified
 				    &handle,		// this is the object handle we get
 				    NULL )))		// parent_entry (what is this?)
-		panic("cannot make entry for commpage %d", kr);
+		printf("cannot make entry for commpage %d\n", kr);
 
 	if ((kr = vm_map_64(	submap,				// target map (shared submap)
 			&zero,				// address (map into 1st page in submap)
@@ -177,7 +177,7 @@
 			uperm,   // cur_protection (R-only in user map)
 			uperm,   // max_protection
 		        VM_INHERIT_SHARE )))             // inheritance
-		panic("cannot map commpage %d", kr);
+		printf("cannot map commpage %d\n", kr);
 
 	ipc_port_release(handle);
 	/* Make the kernel mapping non-executable. This cannot be done
@@ -210,7 +210,7 @@
 	cpus = ml_get_max_cpus();                   // NB: this call can block
 
 	if (cpus == 0)
-		panic("commpage cpus==0");
+		printf("commpage cpus==0");
 	if (cpus > 0xFF)
 		cpus = 0xFF;
 
@@ -269,6 +269,7 @@
 		default:
 			break;
 	}
+
 	cpus = commpage_cpus();			// how many CPUs do we have
 
 	bits |= (cpus << kNumCPUsShift);
@@ -306,7 +307,13 @@
 	setif(bits, kHasADX,     cpuid_features() &
 					CPUID_LEAF7_FEATURE_ADX);
 	
-	uint64_t misc_enable = rdmsr64(MSR_IA32_MISC_ENABLE);
+	uint64_t misc_enable = 0;
+
+	if (IsIntelCPU())
+	{
+		misc_enable = rdmsr64(MSR_IA32_MISC_ENABLE);
+	}
+
 	setif(bits, kHasENFSTRG, (misc_enable & 1ULL) &&
 				 (cpuid_leaf7_features() &
 					CPUID_LEAF7_FEATURE_ERMS));
@@ -358,7 +365,7 @@
     void	*dest = commpage_addr_of(address);
     
     if (address < next)
-       panic("commpage overlap at address 0x%p, 0x%x < 0x%x", dest, address, next);
+       printf("commpage overlap at address 0x%p, 0x%x < 0x%x\n", dest, address, next);
     
     bcopy(source,dest,length);
     
@@ -434,7 +441,7 @@
 	commpage_stuff(_COMM_PAGE_CPUFAMILY, &cfamily, 4);
 
 	if (next > _COMM_PAGE_END)
-		panic("commpage overflow: next = 0x%08x, commPagePtr = 0x%p", next, commPagePtr);
+		printf("compare overflow: next = 0x%08x, commPagePtr = 0x%p", next, commPagePtr);
 
 }
 
@@ -536,7 +543,7 @@
 	}
 
 	if (next > _COMM_PAGE_TEXT_END) 
-		panic("commpage text overflow: next=0x%08x, commPagePtr=%p", next, commPagePtr); 
+		printf("commpage text overflow: next=0x%08x, commPagePtr=%p\n", next, commPagePtr);
 
 }
 
@@ -561,11 +568,11 @@
 		return;
 		
 	if ( generation != p32->nt_generation )
-		panic("nanotime trouble 1");	/* possibly not serialized */
+		printf("nanotime trouble 1\n");	/* possibly not serialized */
 	if ( ns_base < p32->nt_ns_base )
-		panic("nanotime trouble 2");
+		printf("nanotime trouble 2\n");
 	if ((shift != 0) && ((_cpu_capabilities & kSlow)==0) )
-		panic("nanotime trouble 3");
+		printf("nanotime trouble 3\n");
 		
 	next_gen = ++generation;
 	if (next_gen == 0)
diff -Nur xnu-3247.1.106/osfmk/i386/cpu_topology.c xnu-3247.1.106-AnV/osfmk/i386/cpu_topology.c
--- xnu-3247.1.106/osfmk/i386/cpu_topology.c	2015-12-06 01:32:35.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/cpu_topology.c	2015-12-13 17:08:10.000000000 +0100
@@ -29,6 +29,7 @@
 #include <mach/machine.h>
 #include <mach/processor.h>
 #include <kern/kalloc.h>
+#include <i386/cpuid.h>
 #include <i386/cpu_affinity.h>
 #include <i386/cpu_topology.h>
 #include <i386/cpu_threads.h>
@@ -145,8 +146,15 @@
 		x86_cpu_cache_t		*LLC_cachep;
 		x86_affinity_set_t	*aset;
 
-		LLC_cachep = lcpup->caches[topoParms.LLCDepth];
-		assert(LLC_cachep->type == CPU_CACHE_TYPE_UNIF);
+        if (IsIntelCPU())
+        {
+            LLC_cachep = lcpup->caches[topoParms.LLCDepth];
+        } else {
+            LLC_cachep = lcpup->caches[topoParms.LLCDepth+1];
+        }
+
+		//assert(LLC_cachep->type == CPU_CACHE_TYPE_UNIF);
+
 		aset = find_cache_affinity(LLC_cachep); 
 		if (aset == NULL) {
 			aset = (x86_affinity_set_t *) kalloc(sizeof(*aset));
diff -Nur xnu-3247.1.106/osfmk/i386/cpuid.c xnu-3247.1.106-AnV/osfmk/i386/cpuid.c
--- xnu-3247.1.106/osfmk/i386/cpuid.c	2015-12-06 01:32:35.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/cpuid.c	2015-12-13 17:26:17.000000000 +0100
@@ -30,8 +30,10 @@
  */
 #include <vm/vm_page.h>
 #include <pexpert/pexpert.h>
+#include <kern/voodoo_assert.h>
 
 #include <i386/cpuid.h>
+#include "cpuid_legacy.h"
 
 static	boolean_t	cpuid_dbg
 #if DEBUG
@@ -53,6 +55,99 @@
 #define bitmask32(h,l)		((bit32(h)|(bit32(h)-1)) & ~(bit32(l)-1))
 #define bitfield32(x,h,l)	((((x) & bitmask32(h,l)) >> l))
 
+//#define bit(n)		(1U << (n))
+//#define bitmask(h,l)		((bit(h)|(bit(h)-1)) & ~(bit(l)-1))
+//#define bitfield(x,h,l)	((((x) & bitmask(h,l)) >> l))
+
+boolean_t ForceAmdCpu = FALSE;
+
+/* For AMD CPU's */
+/* CPUID			eax			ebx			ecx			edx
+ * 0x00000000		0x00000005	0x68747541	0x444D4163	0x69746E65
+ * 
+ * ebx=HEX:41757468 ASCII=Auth
+ * ecx=HEX:63414D44 ASCII=cAMD
+ * edx=HEX:656E7469 ASCII=enti
+ * ebx+edx+ecx ASCII=AuthenticAMD
+ */
+
+boolean_t IsAmdCPU(void) {
+	if (ForceAmdCpu)
+		return TRUE;
+
+	uint32_t ourcpuid[4];
+	do_cpuid(0, ourcpuid);
+	if (ourcpuid[ebx] == 0x68747541 &&
+		ourcpuid[ecx] == 0x444D4163 &&
+		ourcpuid[edx] == 0x69746E65)
+		return TRUE;
+
+	return FALSE;
+};
+
+/* For Intel CPU's */
+/* CPUID			eax			ebx			ecx			edx
+ * 0x00000000		0x00000005	0x756E6547	0x6C65746E	0x49656E69
+ * 
+ * ebx=HEX:47656E75 ASCII=Genu
+ * ecx=HEX:6E74656C ASCII=ntel
+ * edx=HEX:696E6549 ASCII=ineI
+ * ebx+edx+ecx ASCII=GenuineIntel
+ */
+
+boolean_t IsIntelCPU(void) {
+	uint32_t ourcpuid[4];
+	do_cpuid(0, ourcpuid);
+	if (ourcpuid[ebx] == 0x756E6547 &&
+		ourcpuid[ecx] == 0x6C65746E &&
+		ourcpuid[edx] == 0x49656E69)
+		return TRUE;
+
+	if (!IsAmdCPU())
+		return TRUE;
+
+	return FALSE;
+}
+
+uint32_t
+extractBitField(uint32_t inField, uint32_t width, uint32_t offset)
+{
+    uint32_t bitMask;
+    uint32_t outField;
+
+    if ((offset+width) == 32)
+    {
+        bitMask = (0xFFFFFFFF<<offset);
+    }
+    else
+    {
+        bitMask = (0xFFFFFFFF<<offset) ^ (0xFFFFFFFF<<(offset+width));
+
+    }
+
+    outField = (inField & bitMask) >> offset;
+    return outField;
+}
+
+uint32_t
+getBitFieldWidth(uint32_t number)
+{
+    uint32_t fieldWidth;
+
+    number--;
+    if (number == 0)
+    {
+        return 0;
+    }
+    
+    __asm__ volatile ( "bsr %%eax, %%ecx\n\t"
+                      : "=c" (fieldWidth)
+                      : "a"(number));
+    
+    
+    return fieldWidth+1;  /* bsr returns the position, we want the width */
+}
+
 /*
  * Leaf 2 cache descriptor encodings.
  */
@@ -246,6 +341,8 @@
 static void
 cpuid_set_cache_info( i386_cpu_info_t * info_p )
 {
+    if(IsIntelCPU())
+    {
 	uint32_t	cpuid_result[4];
 	uint32_t	reg[4];
 	uint32_t	index;
@@ -395,22 +492,22 @@
 	 * If deterministic cache parameters are not available, use
 	 * something else
 	 */
-	if (info_p->cpuid_cores_per_package == 0) {
+	if (info_p->cpuid_cores_per_package == 0)
+	{
 		info_p->cpuid_cores_per_package = 1;
 
-		/* cpuid define in 1024 quantities */
-		info_p->cache_size[L2U] = info_p->cpuid_cache_size * 1024;
-		info_p->cache_sharing[L2U] = 1;
-		info_p->cache_partitions[L2U] = 1;
-
-		linesizes[L2U] = info_p->cpuid_cache_linesize;
-
-		DBG(" cache_size[L2U]      : %d\n",
-		    info_p->cache_size[L2U]);
-		DBG(" cache_sharing[L2U]   : 1\n");
-		DBG(" cache_partitions[L2U]: 1\n");
-		DBG(" linesizes[L2U]       : %d\n",
-		    info_p->cpuid_cache_linesize);
+             info_p->cache_size[L2U] = info_p->cpuid_cache_size * 1024;
+             info_p->cache_sharing[L2U] = 1;
+             info_p->cache_partitions[L2U] = 1;
+             
+             linesizes[L2U] = info_p->cpuid_cache_linesize;
+             
+             DBG(" cache_size[L2U]      : %d\n",
+                 info_p->cache_size[L2U]);
+             DBG(" cache_sharing[L2U]   : 1\n");
+             DBG(" cache_partitions[L2U]: 1\n");
+             DBG(" linesizes[L2U]       : %d\n",
+                 info_p->cpuid_cache_linesize);
 	}
 	
 	/*
@@ -470,6 +567,297 @@
 		}
 	}
 	DBG("\n");
+    }
+}
+
+static uint32_t amdGetAssociativity(uint32_t flag)
+{
+	uint32_t asso = 0;
+
+    switch ( flag )
+    {
+        case 0: asso = 0; break;
+        case 1: asso = 1; break;
+        case 2: asso = 2; break;
+        case 4: asso = 4; break;
+        case 6: asso = 8; break;
+        case 8: asso = 16; break;
+        case 10: asso = 32; break;
+        case 11: asso = 48; break;
+        case 12: asso = 64; break;
+        case 13: asso = 96; break;
+        case 14: asso = 128; break;
+        case 15: asso = 0; break;
+        default: break;
+    }
+    return asso;
+}
+
+/************************************************
+ ********* AMD L/1/2/3 Cache Calculate **********
+ ************************************************/
+static
+void
+get_amd_cache_info(i386_cpu_info_t *info_p)
+{
+    uint32_t	reg[4] = {0, 0, 0, 0};
+    uint32_t    cpuid_result[4];
+    uint32_t	cache_level;
+    uint32_t	cache_partitions;
+    uint32_t	cache_sharing;
+    uint32_t	cache_linesize;
+    uint32_t	cache_associativity;
+    uint32_t	cache_size;
+    uint32_t	cache_byte;
+    uint32_t	cache_sets;
+    uint32_t	colors;
+    uint32_t	cache_type;
+    cache_type_t    type = Lnone;
+    uint32_t	linesizes[LCACHE_MAX];
+    bzero( linesizes, sizeof(linesizes) );
+    uint32_t asso = 0;
+
+    uint32_t    cores;
+    boolean_t	cpuid_deterministic_supported = FALSE;
+
+    cpuid_fn(0x80000008, reg);
+    cores = bitfield32(reg[ecx],7,0)+1;
+    info_p->cpuid_cores_per_package = cores;
+
+    //cpuid_fn(0x80000005, reg);
+    //uint32_t L1DLinesPerTag = bitfield32(reg[ecx], 11, 8);
+    //uint32_t L1ILinesPerTag = bitfield32(reg[edx], 11, 8);
+
+    cpuid_fn(0x80000006, reg);
+    //uint32_t L2ULinesPerTag = bitfield32(reg[ecx], 11, 8);
+    uint32_t L3ULinesPerTag = bitfield32(reg[edx], 11, 8);
+
+    int i=0;
+
+    if (info_p->cpuid_family < 21)
+    {
+
+    	for ( i = 1; i<5 ; i++)
+    	{
+    		switch (i)
+    		{
+    			case 1:
+    			{
+    				type = 1 == 1 ? L1D : Lnone;
+    				cpuid_fn(0x80000005, reg);
+    				cache_byte = bitfield32(reg[ecx],31,24);
+    				cache_linesize = bitfield32(reg[ecx],7,0);
+    				cache_associativity = bitfield32(reg[ecx],23,16);
+    				cache_partitions = bitfield32(reg[ecx], 11, 8);
+
+    				cache_sharing = 1;
+    				cache_size = cache_byte * 1024;
+    				cache_sets = cache_size / (cache_associativity * cache_linesize);
+    				info_p->cache_size[L1D] = cache_size;
+    				info_p->cache_sharing[L1D] = cache_sharing;
+    				info_p->cache_partitions[L1D] = cache_partitions;
+    				linesizes[L1D] = cache_linesize;
+
+    				colors = ( cache_linesize * cache_sets ) >> 12;
+    				if ( colors > vm_cache_geometry_colors )
+    					vm_cache_geometry_colors = colors ;
+    			}
+    			break;
+
+    			case 2:
+    			{
+    				type = 2 == 2 ? L1I : Lnone;
+    				cpuid_fn(0x80000005, reg);
+    				cache_byte = bitfield32(reg[edx],31,24);
+    				cache_linesize = bitfield32(reg[edx],7,0);
+    				cache_associativity = bitfield32(reg[edx],23,16);
+    				cache_partitions = bitfield32(reg[edx], 11, 8);
+
+    				cache_sharing = 1;
+    				cache_size = cache_byte * 1024;
+    				cache_sets = cache_size / (cache_associativity * cache_linesize);
+    				info_p->cache_size[L1I] = cache_size;
+    				info_p->cache_sharing[L1I] = cache_sharing;
+    				info_p->cache_partitions[L1I] = cache_partitions;
+    				linesizes[L1I] = cache_linesize;
+
+    				colors = ( cache_linesize * cache_sets ) >> 12;
+    				if ( colors > vm_cache_geometry_colors )
+    					vm_cache_geometry_colors = colors ;
+    			}
+    			break;
+
+    			case 3:
+    			{
+    				type = 3 == 3 ? L2U : Lnone;
+    				cpuid_fn(0x80000006, reg);
+    				cache_byte = bitfield32(reg[ecx],31,16);
+    				cache_linesize = bitfield32(reg[ecx],7,0);
+    				asso = bitfield32(reg[ecx],15,12);
+    				cache_associativity = amdGetAssociativity(asso);
+    				cache_partitions = bitfield32(reg[ecx], 11, 8);
+
+    				cache_size = cache_byte * 1024;
+    				cache_sets = cache_size / (cache_associativity * cache_linesize);
+    				info_p->cache_size[L2U] = cache_size;
+    				info_p->cache_sharing[L2U] = cores;
+    				info_p->cache_partitions[L2U] = cache_partitions;
+    				linesizes[L2U] = cache_linesize;
+
+    				info_p->cpuid_cache_L2_associativity = cache_associativity;
+
+    				colors = ( cache_linesize * cache_sets ) >> 12;
+    				if ( colors > vm_cache_geometry_colors )
+    					vm_cache_geometry_colors = colors ;
+    			}
+    			break;
+
+    			case 4:
+    			{
+    				if (L3ULinesPerTag)
+    				{
+    					type = 3 == 3 ? L3U : Lnone;
+    					cpuid_fn(0x80000006, reg);
+    					cache_byte = bitfield32(reg[edx],31,18);
+    					cache_linesize = bitfield32(reg[edx],7,0);
+    					asso = bitfield32(reg[edx],15,12);
+    					cache_associativity = amdGetAssociativity(asso);
+    					cache_partitions = bitfield32(reg[edx], 11, 8);
+
+						cache_size = cache_byte * 1024 * 128;
+						cache_sets = cache_size / (cache_associativity * cache_linesize);
+						info_p->cache_size[L3U] = cache_size;
+						info_p->cache_sharing[L3U] = cores;
+						info_p->cache_partitions[L3U] = cache_partitions;
+						linesizes[L3U] = cache_linesize;
+
+    					colors = ( cache_linesize * cache_sets ) >> 12;
+    					if ( colors > vm_cache_geometry_colors )
+    						vm_cache_geometry_colors = colors ;
+    				}
+    			}
+    			break;
+
+    			case Lnone:
+    			default:
+    				return;
+    		}
+    	} 
+    } //10h-14h END
+	else //15h-16h
+	{
+    	cpuid_fn(0x8000001D, cpuid_result);
+    	if (cpuid_result[eax] >= 4)
+    		cpuid_deterministic_supported = TRUE;
+
+    	for (i = 0; cpuid_deterministic_supported ; i++)
+    	{
+    		reg[eax] = 0x8000001D;
+    		reg[ecx] = i;
+    		cpuid(reg);
+
+    		DBG("cpuid(0x8000001D) i=%d eax=0x%x\n", i, reg[eax]);
+    		cache_type = bitfield32(reg[eax], 4, 0);
+    		if (cache_type == 0) break;
+    		cache_level = bitfield32(reg[eax],  7,  5);
+    		cache_sharing = bitfield32(reg[eax], 25, 14) + 1;
+    		cache_linesize = bitfield32(reg[ebx], 11,  0) + 1;
+    		cache_partitions = bitfield32(reg[ebx], 21, 12) + 1;
+    		cache_associativity	= bitfield32(reg[ebx], 31, 22) + 1;
+    		cache_sets = bitfield32(reg[ecx], 31,  0) + 1;
+
+    		switch (cache_level)
+    		{
+    			case 1:
+    				type = cache_type == 1 ? L1D :
+    				cache_type == 2 ? L1I : Lnone;
+    				break;
+    			case 2:
+    				type = cache_type == 3 ? L2U : Lnone;
+    				break;
+    			case 3:
+    				type = cache_type == 3 ? L3U : Lnone;
+    				break;
+    			default:
+    				type = Lnone;
+    		}
+
+    		if (type != Lnone)
+    		{
+    			cache_size = cache_linesize * cache_sets * cache_associativity * cache_partitions;
+
+    			info_p->cache_size[type] = cache_size;
+    			info_p->cache_sharing[type] = cache_sharing;
+    			info_p->cache_partitions[type] = cache_partitions;
+    			linesizes[type] = cache_linesize;
+
+    			if (type == L2U)
+    				info_p->cpuid_cache_L2_associativity = cache_associativity;
+
+    			colors = ( cache_linesize * cache_sets ) >> 12;
+    			if ( colors > vm_cache_geometry_colors )
+    				vm_cache_geometry_colors = colors;
+    		}
+    	}
+    } //15h-16h END
+
+	if (info_p->cpuid_cores_per_package == 0)
+	{
+		info_p->cpuid_cores_per_package = 1;
+
+             info_p->cache_size[L2U] = info_p->cpuid_cache_size * 1024;
+             info_p->cache_sharing[L2U] = 1;
+             info_p->cache_partitions[L2U] = 1;
+             
+             linesizes[L2U] = info_p->cpuid_cache_linesize;
+             
+             DBG(" cache_size[L2U]      : %d\n",
+                 info_p->cache_size[L2U]);
+             DBG(" cache_sharing[L2U]   : 1\n");
+             DBG(" cache_partitions[L2U]: 1\n");
+             DBG(" linesizes[L2U]       : %d\n",
+                 info_p->cpuid_cache_linesize);
+	}
+
+	/*
+	 * What linesize to publish?  We use the L2 linesize if any,
+	 * else the L1D.
+	 */
+	if ( linesizes[L2U] )
+		info_p->cache_linesize = linesizes[L2U];
+	else if (linesizes[L1D])
+		info_p->cache_linesize = linesizes[L1D];
+	else panic("no linesize");
+	DBG(" cache_linesize    : %d\n", info_p->cache_linesize);
+
+    cpuid_fn(0x80000005, reg);
+    // uint64_t L1DTlb2and4MAssoc = (uint32_t)bitfield(eax, 31, 24);
+    uint32_t L1DTlb2and4MSize  = (uint32_t)bitfield32(reg[eax], 23, 16);
+    // uint64_t L1ITlb2and4MAssoc = (uint32_t)bitfield(eax, 15, 8);
+    uint32_t L1ITlb2and4MSize  = (uint32_t)bitfield32(reg[eax], 7, 0);
+    // uint64_t L1DTlb4KAssoc = (uint32_t)bitfield(ebx, 31, 24);
+    uint32_t L1DTlb4KSize = (uint32_t)bitfield32(reg[ebx], 23, 16);
+    // uint64_t L1ITlb4KAssoc = (uint32_t)bitfield(ebx, 15, 8);
+    uint32_t L1ITlb4KSize = (uint32_t)bitfield32(reg[ebx], 7, 0);
+
+    cpuid_fn(0x80000006, reg);
+    //  uint64_t L2DTlb2and4MAssoc = (uint32_t)bitfield(eax, 31, 28);
+    uint32_t L2DTlb2and4MSize = (uint32_t)bitfield32(reg[eax], 27, 16);
+    //  uint64_t L2ITlb2and4MAssoc = (uint32_t)bitfield(eax, 15, 12);
+    uint32_t L2ITlb2and4MSize = (uint32_t)bitfield32(reg[eax], 11, 0);
+    // uint64_t L2DTlb4KAssoc = (uint32_t)bitfield(ebx, 31, 28);
+    uint32_t L2DTlb4KSize = (uint32_t)bitfield32(reg[ebx], 27, 16);
+    // uint64_t L2ITlb4KAssoc = (uint32_t)bitfield(ebx, 15, 12);
+    uint32_t L2ITlb4KSize = (uint32_t)bitfield32(reg[ebx], 11, 0);
+
+    info_p->cpuid_tlb[0][0][0] =  L1ITlb4KSize;
+    info_p->cpuid_tlb[1][0][0] =  L1DTlb4KSize;
+    info_p->cpuid_tlb[0][0][1] =  L2ITlb4KSize;
+    info_p->cpuid_tlb[1][0][1] =  L2DTlb4KSize;
+    info_p->cpuid_tlb[0][1][0] =  L1ITlb2and4MSize;
+    info_p->cpuid_tlb[1][1][0] =  L1DTlb2and4MSize;
+    info_p->cpuid_tlb[0][1][1] =  L2ITlb2and4MSize;
+    info_p->cpuid_tlb[1][1][1] =  L2DTlb2and4MSize;
 }
 
 static void
@@ -477,12 +865,28 @@
 {
 	uint32_t	reg[4];
         char            str[128], *p;
+    unsigned char dummyvar;
 
 	DBG("cpuid_set_generic_info(%p)\n", info_p);
 
 	/* do cpuid 0 to get vendor */
-	cpuid_fn(0, reg);
-	info_p->cpuid_max_basic = reg[eax];
+    static char Genu[5] = "Genu";
+    static char ineI[5] = "ineI";
+    static char ntel[5] = "ntel";
+
+    /* Made by Bronzovka, adapted by AnV */
+    if (PE_parse_boot_argn("-emulateintel", &dummyvar, sizeof(dummyvar)))
+    {
+        cpuid_fn(0, reg);
+        info_p->cpuid_max_basic = reg[eax];
+        bcopy(Genu, (char *)&reg[ebx], 4); /* ug */
+        bcopy(ntel, (char *)&reg[ecx], 4);
+        bcopy(ineI, (char *)&reg[edx], 4);
+    } else {
+        cpuid_fn(0, reg);
+        info_p->cpuid_max_basic = reg[eax];
+    }
+
 	bcopy((char *)&reg[ebx], &info_p->cpuid_vendor[0], 4); /* ug */
 	bcopy((char *)&reg[ecx], &info_p->cpuid_vendor[8], 4);
 	bcopy((char *)&reg[edx], &info_p->cpuid_vendor[4], 4);
@@ -522,7 +926,8 @@
 	}
     
 	/* Get cache and addressing info. */
-	if (info_p->cpuid_max_ext >= 0x80000006) {
+	if (info_p->cpuid_max_ext >= 0x80000006)
+	{
 		uint32_t assoc;
 		cpuid_fn(0x80000006, reg);
 		info_p->cpuid_cache_linesize   = bitfield32(reg[ecx], 7, 0);
@@ -534,14 +939,23 @@
 		 * Overwritten by associativity as determined via CPUID.4
 		 * if available.
 		 */
+		if (IsIntelCPU())
+		{
 		if (assoc == 6)
 			assoc = 8;
 		else if (assoc == 8)
 			assoc = 16;
 		else if (assoc == 0xF)
 			assoc = 0xFFFF;
+		}
+		else
+		{
+			assoc = amdGetAssociativity(assoc);
+		}
+
 		info_p->cpuid_cache_L2_associativity = assoc;
 		info_p->cpuid_cache_size       = bitfield32(reg[ecx],31,16);
+
 		cpuid_fn(0x80000008, reg);
 		info_p->cpuid_address_bits_physical =
 						 bitfield32(reg[eax], 7, 0);
@@ -554,10 +968,17 @@
 	 * and bracket this with the approved procedure for reading the
 	 * the microcode version number a.k.a. signature a.k.a. BIOS ID
 	 */
-	wrmsr64(MSR_IA32_BIOS_SIGN_ID, 0);
-	cpuid_fn(1, reg);
-	info_p->cpuid_microcode_version =
+    if (IsIntelCPU())
+    {
+        wrmsr64(MSR_IA32_BIOS_SIGN_ID, 0);
+        cpuid_fn(1, reg);
+        info_p->cpuid_microcode_version =
 		(uint32_t) (rdmsr64(MSR_IA32_BIOS_SIGN_ID) >> 32);
+    } else {
+        cpuid_fn(1, reg);
+        info_p->cpuid_microcode_version = 21;
+    }
+
 	info_p->cpuid_signature = reg[eax];
 	info_p->cpuid_stepping  = bitfield32(reg[eax],  3,  0);
 	info_p->cpuid_model     = bitfield32(reg[eax],  7,  4);
@@ -569,7 +990,22 @@
 	info_p->cpuid_features  = quad(reg[ecx], reg[edx]);
 
 	/* Get "processor flag"; necessary for microcode update matching */
-	info_p->cpuid_processor_flag = (rdmsr64(MSR_IA32_PLATFORM_ID)>> 50) & 0x7;
+    if (IsIntelCPU())
+    {
+        info_p->cpuid_processor_flag = (rdmsr64(MSR_IA32_PLATFORM_ID)>> 50) & 0x7;
+
+        /* AnV - Fix for Pentium G series CPU */
+        if (!PE_parse_boot_argn("-nopentiumgfix", &dummyvar, sizeof(dummyvar)))
+        {
+            if (info_p->cpuid_signature == 0x0306C3)
+            {
+                info_p->cpuid_model = 0xE;
+                info_p->cpuid_extmodel = 0x1;
+            }
+        }
+    } else {
+        info_p->cpuid_processor_flag = 1;
+    }
 
 	/* Fold extensions into family/model */
 	if (info_p->cpuid_family == 0x0f)
@@ -585,8 +1021,15 @@
 
 	if (info_p->cpuid_max_ext >= 0x80000001) {
 		cpuid_fn(0x80000001, reg);
-		info_p->cpuid_extfeatures =
+        if (IsIntelCPU())
+        {
+            info_p->cpuid_extfeatures =
 				quad(reg[ecx], reg[edx]);
+        } else {
+            /* Sinetek: AMD doesn't like the XD bit. */
+            info_p->cpuid_extfeatures =
+                quad(reg[ecx], reg[edx]) & ~CPUID_EXTFEATURE_XD;
+        }
 	}
 
 	DBG(" max_basic           : %d\n", info_p->cpuid_max_basic);
@@ -608,7 +1051,7 @@
 
 	/* Fold in the Invariant TSC feature bit, if present */
 	if (info_p->cpuid_max_ext >= 0x80000007) {
-		cpuid_fn(0x80000007, reg);  
+		cpuid_fn(0x80000007, reg);
 		info_p->cpuid_extfeatures |=
 				reg[edx] & (uint32_t)CPUID_EXTFEATURE_TSCI;
 		DBG(" extfeatures         : 0x%016llx\n",
@@ -725,20 +1168,27 @@
 		/*
 		 * Leaf7 Features:
 		 */
-		cpuid_fn(0x7, reg);
-		info_p->cpuid_leaf7_features = quad(reg[ecx], reg[ebx]);
+		if (IsIntelCPU())
+ 		{
+			cpuid_fn(0x7, reg);
+			info_p->cpuid_leaf7_features = quad(reg[ecx], reg[ebx]);
+		}
 
 		DBG(" Feature Leaf7:\n");
 		DBG("  EBX           : 0x%x\n", reg[ebx]);
 		DBG("  ECX           : 0x%x\n", reg[ecx]);
+	} else {
+		info_p->cpuid_leaf7_features = 0;
 	}
 }
 
 static uint32_t
 cpuid_set_cpufamily(i386_cpu_info_t *info_p)
 {
-	uint32_t cpufamily = CPUFAMILY_UNKNOWN;
+	uint32_t cpufamily = CPUFAMILY_INTEL_MEROM;
 
+    if (IsIntelCPU())
+    {
 	switch (info_p->cpuid_family) {
 	case 6:
 		switch (info_p->cpuid_model) {
@@ -779,12 +1229,51 @@
 			break;
 		}
 		break;
-	}
+
+    case 15:
+        switch (info_p->cpuid_model) {
+        case 2:
+            cpufamily = CPU_FAMILY_PENTIUM_4_M2;
+            break;
+        case 4:
+            cpufamily = CPU_FAMILY_PENTIUM_4;
+            break;
+        }
+        break;
+	}
+    } else {
+        // Bronzovka : here should hw.cpufamily = 0x78ea4fbc
+        cpufamily = CPUFAMILY_INTEL_PENRYN;
+    }
 
 	info_p->cpuid_cpufamily = cpufamily;
 	DBG("cpuid_set_cpufamily(%p) returning 0x%x\n", info_p, cpufamily);
 	return cpufamily;
 }
+
+/* AnV: AMD TLB Fix */
+/*
+void
+FixAMDTLB(void)
+{
+    uint64_t value = 0;
+
+    // re-enable TLB caching if BIOS disabled it
+    // MSR_K8_HWCR mod
+    value = rdmsr64(0xC0010015);
+    value &= ~(1UL << 3);
+    wrmsr64(0xC0010015, value);
+
+    // MSR_C0011023 mod
+    value = rdmsr64(0xC0011023);
+    value &= ~(1UL << 1);
+    wrmsr64(0xC0011023, value);
+}
+*/
+/* AnV - Brand string data taken from SMBIOS of MacPro3,1 and iMac10,1 */
+#define CPUID_BRAND_XEON "Intel(R) Xeon(R) CPU           E"
+#define CPUID_BRAND_CORE2 "Intel(R) Core(TM)2 Duo CPU     E7600  @ 3.06GHz"
+
 /*
  * Must be invoked either when executing single threaded, or with
  * independent synchronization.
@@ -793,19 +1282,16 @@
 cpuid_set_info(void)
 {
 	i386_cpu_info_t		*info_p = &cpuid_cpu_info;
-	boolean_t		enable_x86_64h = TRUE;
+	//boolean_t		enable_x86_64h = FALSE;
+	boolean_t		disable_x86_64h = TRUE;
+	uint32_t dummyVar;
 
 	cpuid_set_generic_info(info_p);
-
-	/* verify we are running on a supported CPU */
-	if ((strncmp(CPUID_VID_INTEL, info_p->cpuid_vendor,
-		     min(strlen(CPUID_STRING_UNKNOWN) + 1,
-			 sizeof(info_p->cpuid_vendor)))) ||
-	   (cpuid_set_cpufamily(info_p) == CPUFAMILY_UNKNOWN))
-		panic("Unsupported CPU");
+	cpuid_set_cpufamily(info_p);
 
 	info_p->cpuid_cpu_type = CPU_TYPE_X86;
-
+	info_p->cpuid_cpu_subtype = CPU_SUBTYPE_X86_ARCH1;
+/*
 	if (!PE_parse_boot_argn("-enable_x86_64h", &enable_x86_64h, sizeof(enable_x86_64h))) {
 		boolean_t		disable_x86_64h = FALSE;
 
@@ -813,7 +1299,6 @@
 			enable_x86_64h = FALSE;
 		}
 	}
-
 	if (enable_x86_64h &&
 	    ((info_p->cpuid_features & CPUID_X86_64_H_FEATURE_SUBSET) == CPUID_X86_64_H_FEATURE_SUBSET) &&
 	    ((info_p->cpuid_extfeatures & CPUID_X86_64_H_EXTFEATURE_SUBSET) == CPUID_X86_64_H_EXTFEATURE_SUBSET) &&
@@ -822,9 +1307,66 @@
 	} else {
 		info_p->cpuid_cpu_subtype = CPU_SUBTYPE_X86_ARCH1;
 	}
+*/
+
+	if (IsIntelCPU())
+		cpuid_set_cache_info(info_p);
 
-	/* Must be invoked after set_generic_info */
-	cpuid_set_cache_info(info_p);
+	else
+	{
+		/* if (PE_parse_boot_argn("-amdtlbfix", &dummyVar, sizeof(dummyVar)))
+		{
+			FixAMDTLB();
+		}*/
+
+		get_amd_cache_info(&cpuid_cpu_info);
+ 
+		if (PE_parse_boot_argn("-emulateintel", &dummyVar, sizeof(dummyVar)))
+		{
+			if (info_p->cpuid_cores_per_package >= 3)
+			{
+				/* Xeon for 3 cores or more... */
+				info_p->cpuid_signature = 0x00010676; /* Xeon Signature */
+				/* Set Xeon brand string */
+				strlcpy(info_p->cpuid_brand_string, CPUID_BRAND_XEON, sizeof(CPUID_BRAND_XEON));
+			} else {
+				/* Core 2 Duo for 2 cores or less... */
+				info_p->cpuid_signature = 0x0001067A; /* Core 2 Duo Signature */
+				/* Set Core 2 Duo brand string */
+				strlcpy(info_p->cpuid_brand_string, CPUID_BRAND_CORE2, sizeof(CPUID_BRAND_CORE2));
+			}
+
+			info_p->cpuid_stepping  = bitfield32(info_p->cpuid_signature,  3,  0);
+			/*info_p->cpuid_model     = bitfield32(info_p->cpuid_signature,  7,  4);
+			info_p->cpuid_family    = bitfield32(info_p->cpuid_signature, 11,  8);*/
+			info_p->cpuid_type      = bitfield32(info_p->cpuid_signature, 13, 12);
+			info_p->cpuid_extmodel  = bitfield32(info_p->cpuid_signature, 19, 16);
+			info_p->cpuid_extfamily = bitfield32(info_p->cpuid_signature, 27, 20);
+			info_p->cpuid_brand     = 0;
+ 
+			/* Correct family and model due to Intel norms... */
+			/*if (info_p->cpuid_family == 0x0f)
+				info_p->cpuid_family += info_p->cpuid_extfamily;
+			if (info_p->cpuid_family == 0x0f || info_p->cpuid_family == 0x06)
+				info_p->cpuid_model += (info_p->cpuid_extmodel << 4);*/
+		}
+	}
+ 
+	/* AnV - Fix cpuid features bit for SSE3 / SSSE3 and CPU family to PENRYN for Intel too (Pentium 4 fix) - Thanks to Sinetek for the suggestion... */
+	// Bro: or other cpufamily
+	//if (!(info_p->cpuid_features & CPUID_FEATURE_SSE3) || !(info_p->cpuid_features & CPUID_FEATURE_SSSE3))
+	//{
+		//info_p->cpuid_features |= CPUID_FEATURE_SSE3;
+		//info_p->cpuid_features |= CPUID_FEATURE_SSSE3;
+		/* Needed for CPU's without SSSE3 like 64-bit Pentium 4 */
+		//info_p->cpuid_cpufamily = CPUFAMILY_INTEL_PENRYN;
+
+		/* Graphics fix */
+		/*if (IsAmdCPU())
+		{
+			info_p->cpuid_features &= ~CPUID_FEATURE_SSE3;
+		} */
+	//}
 
 	/*
 	 * Find the number of enabled cores and threads
@@ -994,19 +1536,54 @@
 char *
 cpuid_get_feature_names(uint64_t features, char *buf, unsigned buf_len)
 {
-	return cpuid_get_names(feature_map, features, buf, buf_len); 
+	//return cpuid_get_names(feature_map, features, buf, buf_len);
+    size_t	len = 0;
+	char	*p = buf;
+	int	i;
+
+	for (i = 0; feature_map[i].mask != 0; i++) {
+		if ((features & feature_map[i].mask) == 0)
+			continue;
+		if (len && ((size_t)(p - buf) < (buf_len - 1)))
+			*p++ = ' ';
+
+		len = min(strlen(feature_map[i].name), (size_t) ((buf_len-1) - (p-buf)));
+		if (len == 0)
+			break;
+		bcopy(feature_map[i].name, p, len);
+		p += len;
+	}
+	*p = '\0';
+	return buf;
 }
 
 char *
 cpuid_get_extfeature_names(uint64_t extfeatures, char *buf, unsigned buf_len)
 {
-	return cpuid_get_names(extfeature_map, extfeatures, buf, buf_len); 
+	//return cpuid_get_names(extfeature_map, extfeatures, buf, buf_len);
+    size_t	len = 0;
+	char	*p = buf;
+	int	i;
+
+	for (i = 0; extfeature_map[i].mask != 0; i++) {
+		if ((extfeatures & extfeature_map[i].mask) == 0)
+			continue;
+		if (len && ((size_t) (p - buf) < (buf_len - 1)))
+			*p++ = ' ';
+		len = min(strlen(extfeature_map[i].name), (size_t) ((buf_len-1)-(p-buf)));
+		if (len == 0)
+			break;
+		bcopy(extfeature_map[i].name, p, len);
+		p += len;
+	}
+	*p = '\0';
+	return buf;
 }
 
 char *
 cpuid_get_leaf7_feature_names(uint64_t features, char *buf, unsigned buf_len)
 {
-	return cpuid_get_names(leaf7_feature_map, features, buf, buf_len); 
+	return cpuid_get_names(leaf7_feature_map, features, buf, buf_len);
 }
 
 void
@@ -1023,7 +1600,7 @@
 	kprintf("\n");
 	if (cpuid_features() & CPUID_FEATURE_HTT) {
 #define s_if_plural(n)	((n > 1) ? "s" : "")
-		kprintf("  HTT: %d core%s per package;"
+		printf("  HTT: %d core%s per package;"
 			     " %d logical cpu%s per package\n",
 			cpuid_cpu_infop->cpuid_cores_per_package,
 			s_if_plural(cpuid_cpu_infop->cpuid_cores_per_package),
@@ -1061,6 +1638,11 @@
 uint32_t
 cpuid_cpufamily(void)
 {
+    if (IsAmdCPU())
+    {
+        return CPUFAMILY_INTEL_PENRYN;
+    }
+
 	return cpuid_info()->cpuid_cpufamily;
 }
 
diff -Nur xnu-3247.1.106/osfmk/i386/cpuid.h xnu-3247.1.106-AnV/osfmk/i386/cpuid.h
--- xnu-3247.1.106/osfmk/i386/cpuid.h	2015-12-06 01:32:35.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/cpuid.h	2015-12-13 17:08:10.000000000 +0100
@@ -210,12 +210,23 @@
 #define CPUID_VMM_FAMILY_VMWARE		0x1
 #define CPUID_VMM_FAMILY_PARALLELS	0x2
 
+#define K10_FAMILY 0x10U
+#define K15_FAMILY 0x15U
+#define K8_FAMILY  0xFU
+
+#define CPU_MODEL_PENTIUM_4_M2 0x2
+#define CPU_MODEL_PENTIUM_4 0x4
+
+#define CPUID_MODEL_PHENOM_II_K10    0x6
+#define CPUID_MODEL_AMD_Dual_Core_Processor_E_350_1 0x1
+
 #ifndef ASSEMBLER
 #include <stdint.h>
 #include <mach/mach_types.h>
 #include <kern/kern_types.h>
 #include <mach/machine.h>
 
+#define CPUSIG_PENTIUMG  0x0306C3
 
 typedef enum { eax, ebx, ecx, edx } cpuid_register_t;
 static inline void
@@ -426,11 +437,27 @@
 extern uint32_t		cpuid_vmm_family(void);
 #endif
 
+/* AnV : AMD TLB fix function */
+extern void     FixAMDTLB(void);
+extern boolean_t	IsAmdCPU(void);
+extern boolean_t	IsIntelCPU(void);
+extern uint32_t extractBitField(uint32_t inField, uint32_t width, uint32_t offset);
+extern uint32_t getBitFieldWidth(uint32_t number);
+
 #ifdef __cplusplus
 }
 #endif
 
 #endif /* ASSEMBLER */
 
+#define CPU_FAMILY_PENTIUM_M	(0x6)
+#define CPU_FAMILY_PENTIUM_4	(0xF)
+#define CPU_FAMILY_PENTIUM_4_M2 (0xF)
+#define CPU_FAMILY_AMD_PHENOM	(0x10)
+#define CPU_FAMILY_AMD_SHANGHAI	(0x11)
+#define CPU_FAMILY_I5		(0x1E)
+#define CPU_FAMILY_I9		(0x2C)
+#define CPU_FAMILY_SANDY	(0x2A)
+
 #endif /* __APPLE_API_PRIVATE */
 #endif /* _MACHINE_CPUID_H_ */
diff -Nur xnu-3247.1.106/osfmk/i386/cpuid.h.orig xnu-3247.1.106-AnV/osfmk/i386/cpuid.h.orig
--- xnu-3247.1.106/osfmk/i386/cpuid.h.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/cpuid.h.orig	2015-12-06 01:32:35.000000000 +0100
@@ -0,0 +1,436 @@
+/*
+ * Copyright (c) 2000-2006 Apple Computer, Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+
+/*
+ * x86 CPU identification
+ *
+ */
+
+#ifndef _MACHINE_CPUID_H_
+#define _MACHINE_CPUID_H_
+
+#include <sys/appleapiopts.h>
+
+#ifdef __APPLE_API_PRIVATE
+
+#define	CPUID_VID_INTEL		"GenuineIntel"
+#define	CPUID_VID_AMD		"AuthenticAMD"
+
+#define CPUID_VMM_ID_VMWARE		"VMwareVMware"
+#define CPUID_VMM_ID_PARALLELS	"Parallels\0\0\0"
+
+#define CPUID_STRING_UNKNOWN    "Unknown CPU Typ"
+
+#define _Bit(n)			(1ULL << n)
+#define _HBit(n)		(1ULL << ((n)+32))
+
+/*
+ * The CPUID_FEATURE_XXX values define 64-bit values
+ * returned in %ecx:%edx to a CPUID request with %eax of 1: 
+ */
+#define CPUID_FEATURE_FPU       _Bit(0)   /* Floating point unit on-chip */
+#define CPUID_FEATURE_VME       _Bit(1)   /* Virtual Mode Extension */
+#define CPUID_FEATURE_DE        _Bit(2)   /* Debugging Extension */
+#define CPUID_FEATURE_PSE       _Bit(3)   /* Page Size Extension */
+#define CPUID_FEATURE_TSC       _Bit(4)   /* Time Stamp Counter */
+#define CPUID_FEATURE_MSR       _Bit(5)   /* Model Specific Registers */
+#define CPUID_FEATURE_PAE       _Bit(6)   /* Physical Address Extension */
+#define CPUID_FEATURE_MCE       _Bit(7)   /* Machine Check Exception */
+#define CPUID_FEATURE_CX8       _Bit(8)   /* CMPXCHG8B */
+#define CPUID_FEATURE_APIC      _Bit(9)   /* On-chip APIC */
+#define CPUID_FEATURE_SEP       _Bit(11)  /* Fast System Call */
+#define CPUID_FEATURE_MTRR      _Bit(12)  /* Memory Type Range Register */
+#define CPUID_FEATURE_PGE       _Bit(13)  /* Page Global Enable */
+#define CPUID_FEATURE_MCA       _Bit(14)  /* Machine Check Architecture */
+#define CPUID_FEATURE_CMOV      _Bit(15)  /* Conditional Move Instruction */
+#define CPUID_FEATURE_PAT       _Bit(16)  /* Page Attribute Table */
+#define CPUID_FEATURE_PSE36     _Bit(17)  /* 36-bit Page Size Extension */
+#define CPUID_FEATURE_PSN       _Bit(18)  /* Processor Serial Number */
+#define CPUID_FEATURE_CLFSH     _Bit(19)  /* CLFLUSH Instruction supported */
+#define CPUID_FEATURE_DS        _Bit(21)  /* Debug Store */
+#define CPUID_FEATURE_ACPI      _Bit(22)  /* Thermal monitor and Clock Ctrl */
+#define CPUID_FEATURE_MMX       _Bit(23)  /* MMX supported */
+#define CPUID_FEATURE_FXSR      _Bit(24)  /* Fast floating pt save/restore */
+#define CPUID_FEATURE_SSE       _Bit(25)  /* Streaming SIMD extensions */
+#define CPUID_FEATURE_SSE2      _Bit(26)  /* Streaming SIMD extensions 2 */
+#define CPUID_FEATURE_SS        _Bit(27)  /* Self-Snoop */
+#define CPUID_FEATURE_HTT       _Bit(28)  /* Hyper-Threading Technology */
+#define CPUID_FEATURE_TM        _Bit(29)  /* Thermal Monitor (TM1) */
+#define CPUID_FEATURE_PBE       _Bit(31)  /* Pend Break Enable */
+ 
+#define CPUID_FEATURE_SSE3      _HBit(0)  /* Streaming SIMD extensions 3 */
+#define CPUID_FEATURE_PCLMULQDQ _HBit(1)  /* PCLMULQDQ instruction */
+#define CPUID_FEATURE_DTES64    _HBit(2)  /* 64-bit DS layout */
+#define CPUID_FEATURE_MONITOR   _HBit(3)  /* Monitor/mwait */
+#define CPUID_FEATURE_DSCPL     _HBit(4)  /* Debug Store CPL */
+#define CPUID_FEATURE_VMX       _HBit(5)  /* VMX */
+#define CPUID_FEATURE_SMX       _HBit(6)  /* SMX */
+#define CPUID_FEATURE_EST       _HBit(7)  /* Enhanced SpeedsTep (GV3) */
+#define CPUID_FEATURE_TM2       _HBit(8)  /* Thermal Monitor 2 */
+#define CPUID_FEATURE_SSSE3     _HBit(9)  /* Supplemental SSE3 instructions */
+#define CPUID_FEATURE_CID       _HBit(10) /* L1 Context ID */
+#define CPUID_FEATURE_SEGLIM64  _HBit(11) /* 64-bit segment limit checking */
+#define CPUID_FEATURE_FMA       _HBit(12) /* Fused-Multiply-Add support */
+#define CPUID_FEATURE_CX16      _HBit(13) /* CmpXchg16b instruction */
+#define CPUID_FEATURE_xTPR      _HBit(14) /* Send Task PRiority msgs */
+#define CPUID_FEATURE_PDCM      _HBit(15) /* Perf/Debug Capability MSR */
+
+#define CPUID_FEATURE_PCID      _HBit(17) /* ASID-PCID support */
+#define CPUID_FEATURE_DCA       _HBit(18) /* Direct Cache Access */
+#define CPUID_FEATURE_SSE4_1    _HBit(19) /* Streaming SIMD extensions 4.1 */
+#define CPUID_FEATURE_SSE4_2    _HBit(20) /* Streaming SIMD extensions 4.2 */
+#define CPUID_FEATURE_x2APIC    _HBit(21) /* Extended APIC Mode */
+#define CPUID_FEATURE_MOVBE     _HBit(22) /* MOVBE instruction */
+#define CPUID_FEATURE_POPCNT    _HBit(23) /* POPCNT instruction */
+#define CPUID_FEATURE_TSCTMR    _HBit(24) /* TSC deadline timer */
+#define CPUID_FEATURE_AES       _HBit(25) /* AES instructions */
+#define CPUID_FEATURE_XSAVE     _HBit(26) /* XSAVE instructions */
+#define CPUID_FEATURE_OSXSAVE   _HBit(27) /* XGETBV/XSETBV instructions */
+#define CPUID_FEATURE_AVX1_0	_HBit(28) /* AVX 1.0 instructions */
+#define CPUID_FEATURE_F16C	_HBit(29) /* Float16 convert instructions */
+#define CPUID_FEATURE_RDRAND	_HBit(30) /* RDRAND instruction */
+#define CPUID_FEATURE_VMM       _HBit(31) /* VMM (Hypervisor) present */
+
+/*
+ * Leaf 7, subleaf 0 additional features.
+ * Bits returned in %ebx:%ecx to a CPUID request with {%eax,%ecx} of (0x7,0x0}:
+ */
+#define CPUID_LEAF7_FEATURE_RDWRFSGS _Bit(0)	/* FS/GS base read/write */
+#define CPUID_LEAF7_FEATURE_TSCOFF   _Bit(1)	/* TSC thread offset */
+#define CPUID_LEAF7_FEATURE_BMI1     _Bit(3)	/* Bit Manipulation Instrs, set 1 */
+#define CPUID_LEAF7_FEATURE_HLE      _Bit(4)	/* Hardware Lock Elision*/
+#define CPUID_LEAF7_FEATURE_AVX2     _Bit(5)	/* AVX2 Instructions */
+#define CPUID_LEAF7_FEATURE_SMEP     _Bit(7)	/* Supervisor Mode Execute Protect */
+#define CPUID_LEAF7_FEATURE_BMI2     _Bit(8)	/* Bit Manipulation Instrs, set 2 */
+#define CPUID_LEAF7_FEATURE_ERMS     _Bit(9)	/* Enhanced Rep Movsb/Stosb */
+#define CPUID_LEAF7_FEATURE_INVPCID  _Bit(10)	/* INVPCID intruction, TDB */
+#define CPUID_LEAF7_FEATURE_RTM      _Bit(11)	/* RTM */
+#define CPUID_LEAF7_FEATURE_RDSEED   _Bit(18)	/* RDSEED Instruction */
+#define CPUID_LEAF7_FEATURE_ADX      _Bit(19)	/* ADX Instructions */
+#define CPUID_LEAF7_FEATURE_SMAP     _Bit(20)	/* Supervisor Mode Access Protect */
+
+/*
+ * The CPUID_EXTFEATURE_XXX values define 64-bit values
+ * returned in %ecx:%edx to a CPUID request with %eax of 0x80000001: 
+ */
+#define CPUID_EXTFEATURE_SYSCALL   _Bit(11)	/* SYSCALL/sysret */
+#define CPUID_EXTFEATURE_XD	   _Bit(20)	/* eXecute Disable */
+
+#define CPUID_EXTFEATURE_1GBPAGE   _Bit(26)	/* 1GB pages */
+#define CPUID_EXTFEATURE_RDTSCP	   _Bit(27)	/* RDTSCP */
+#define CPUID_EXTFEATURE_EM64T	   _Bit(29)	/* Extended Mem 64 Technology */
+
+#define CPUID_EXTFEATURE_LAHF	   _HBit(0)	/* LAFH/SAHF instructions */
+#define CPUID_EXTFEATURE_LZCNT     _HBit(5)	/* LZCNT instruction */
+#define CPUID_EXTFEATURE_PREFETCHW _HBit(8)	/* PREFETCHW instruction */
+
+/*
+ * The CPUID_EXTFEATURE_XXX values define 64-bit values
+ * returned in %ecx:%edx to a CPUID request with %eax of 0x80000007: 
+ */
+#define CPUID_EXTFEATURE_TSCI      _Bit(8)	/* TSC Invariant */
+
+/*
+ * CPUID_X86_64_H_FEATURE_SUBSET and CPUID_X86_64_H_LEAF7_FEATURE_SUBSET
+ * indicate the bitmask of features that must be present before the system
+ * is eligible to run the "x86_64h" "Haswell feature subset" slice.
+ */
+#define CPUID_X86_64_H_FEATURE_SUBSET ( CPUID_FEATURE_FMA    | \
+                                        CPUID_FEATURE_SSE4_2 | \
+                                        CPUID_FEATURE_MOVBE  | \
+                                        CPUID_FEATURE_POPCNT | \
+                                        CPUID_FEATURE_AVX1_0   \
+                                      )
+
+#define CPUID_X86_64_H_EXTFEATURE_SUBSET ( CPUID_EXTFEATURE_LZCNT \
+                                         )
+
+#define CPUID_X86_64_H_LEAF7_FEATURE_SUBSET ( CPUID_LEAF7_FEATURE_BMI1 | \
+                                              CPUID_LEAF7_FEATURE_AVX2 | \
+                                              CPUID_LEAF7_FEATURE_BMI2   \
+                                            )
+
+#define	CPUID_CACHE_SIZE	16	/* Number of descriptor values */
+
+#define CPUID_MWAIT_EXTENSION	_Bit(0)	/* enumeration of WMAIT extensions */
+#define CPUID_MWAIT_BREAK	_Bit(1)	/* interrupts are break events	   */
+
+#define CPUID_MODEL_YONAH		0x0E
+#define CPUID_MODEL_MEROM		0x0F
+#define CPUID_MODEL_PENRYN		0x17
+#define CPUID_MODEL_NEHALEM		0x1A
+#define CPUID_MODEL_FIELDS		0x1E	/* Lynnfield, Clarksfield */
+#define CPUID_MODEL_DALES		0x1F	/* Havendale, Auburndale */
+#define CPUID_MODEL_NEHALEM_EX		0x2E
+#define CPUID_MODEL_DALES_32NM		0x25	/* Clarkdale, Arrandale */
+#define CPUID_MODEL_WESTMERE		0x2C	/* Gulftown, Westmere-EP/-WS */
+#define CPUID_MODEL_WESTMERE_EX		0x2F
+#define CPUID_MODEL_SANDYBRIDGE		0x2A
+#define CPUID_MODEL_JAKETOWN		0x2D
+#define CPUID_MODEL_IVYBRIDGE		0x3A
+#define CPUID_MODEL_IVYBRIDGE_EP	0x3E
+#define CPUID_MODEL_CRYSTALWELL		0x46
+#define CPUID_MODEL_HASWELL		0x3C
+#define CPUID_MODEL_HASWELL_EP		0x3F
+#define CPUID_MODEL_HASWELL_ULT		0x45
+#define CPUID_MODEL_BROADWELL		0x3D
+#define CPUID_MODEL_BROADWELL_ULX	0x3D
+#define CPUID_MODEL_BROADWELL_ULT	0x3D
+#define CPUID_MODEL_BRYSTALWELL		0x47
+
+#define CPUID_VMM_FAMILY_UNKNOWN	0x0
+#define CPUID_VMM_FAMILY_VMWARE		0x1
+#define CPUID_VMM_FAMILY_PARALLELS	0x2
+
+#ifndef ASSEMBLER
+#include <stdint.h>
+#include <mach/mach_types.h>
+#include <kern/kern_types.h>
+#include <mach/machine.h>
+
+
+typedef enum { eax, ebx, ecx, edx } cpuid_register_t;
+static inline void
+cpuid(uint32_t *data)
+{
+	__asm__ volatile ("cpuid"
+		: "=a" (data[eax]),
+		  "=b" (data[ebx]),
+		  "=c" (data[ecx]),
+		  "=d" (data[edx])
+		: "a"  (data[eax]),
+		  "b"  (data[ebx]),
+		  "c"  (data[ecx]),
+		  "d"  (data[edx]));
+}
+
+static inline void
+do_cpuid(uint32_t selector, uint32_t *data)
+{
+	__asm__ volatile ("cpuid"
+		: "=a" (data[0]),
+		  "=b" (data[1]),
+		  "=c" (data[2]),
+		  "=d" (data[3])
+		: "a"(selector),
+		  "b" (0),
+		  "c" (0),
+		  "d" (0));
+}
+
+/*
+ * Cache ID descriptor structure, used to parse CPUID leaf 2.
+ * Note: not used in kernel.
+ */
+typedef enum { Lnone, L1I, L1D, L2U, L3U, LCACHE_MAX } cache_type_t ; 
+typedef struct {
+	unsigned char	value;          /* Descriptor value */
+	cache_type_t 	type;           /* Cache type */
+	unsigned int 	size;           /* Cache size */
+	unsigned int 	linesize;       /* Cache line size */
+#ifdef KERNEL
+	const char	*description;   /* Cache description */
+#endif /* KERNEL */
+} cpuid_cache_desc_t;  
+
+#ifdef KERNEL
+#define CACHE_DESC(value,type,size,linesize,text) \
+	{ value, type, size, linesize, text }
+#else
+#define CACHE_DESC(value,type,size,linesize,text) \
+	{ value, type, size, linesize }
+#endif /* KERNEL */
+
+/* Monitor/mwait Leaf: */
+typedef struct {
+	uint32_t	linesize_min;
+	uint32_t	linesize_max;
+	uint32_t	extensions;
+	uint32_t	sub_Cstates;
+} cpuid_mwait_leaf_t;
+
+/* Thermal and Power Management Leaf: */
+typedef struct {
+	boolean_t	sensor;
+	boolean_t	dynamic_acceleration;
+	boolean_t	invariant_APIC_timer;
+	boolean_t	core_power_limits;
+	boolean_t	fine_grain_clock_mod;
+	boolean_t	package_thermal_intr;
+	uint32_t	thresholds;
+	boolean_t	ACNT_MCNT;
+	boolean_t	hardware_feedback;
+	boolean_t	energy_policy;
+} cpuid_thermal_leaf_t;
+
+
+/* XSAVE Feature Leaf: */
+typedef struct {
+	uint32_t	extended_state[4];	/* eax .. edx */
+} cpuid_xsave_leaf_t;
+
+
+/* Architectural Performance Monitoring Leaf: */
+typedef struct {
+	uint8_t		version;
+	uint8_t		number;
+	uint8_t		width;
+	uint8_t		events_number;
+	uint32_t	events;
+	uint8_t		fixed_number;
+	uint8_t		fixed_width;
+} cpuid_arch_perf_leaf_t;
+
+/* Physical CPU info - this is exported out of the kernel (kexts), so be wary of changes */
+typedef struct {
+	char		cpuid_vendor[16];
+	char		cpuid_brand_string[48];
+	const char	*cpuid_model_string;
+
+	cpu_type_t	cpuid_type;	/* this is *not* a cpu_type_t in our <mach/machine.h> */
+	uint8_t		cpuid_family;
+	uint8_t		cpuid_model;
+	uint8_t		cpuid_extmodel;
+	uint8_t		cpuid_extfamily;
+	uint8_t		cpuid_stepping;
+	uint64_t	cpuid_features;
+	uint64_t	cpuid_extfeatures;
+	uint32_t	cpuid_signature;
+	uint8_t   	cpuid_brand; 
+	uint8_t		cpuid_processor_flag;
+	
+	uint32_t	cache_size[LCACHE_MAX];
+	uint32_t	cache_linesize;
+
+	uint8_t		cache_info[64];    /* list of cache descriptors */
+
+	uint32_t	cpuid_cores_per_package;
+	uint32_t	cpuid_logical_per_package;
+	uint32_t	cache_sharing[LCACHE_MAX];
+	uint32_t	cache_partitions[LCACHE_MAX];
+
+	cpu_type_t	cpuid_cpu_type;			/* <mach/machine.h> */
+	cpu_subtype_t	cpuid_cpu_subtype;		/* <mach/machine.h> */	
+
+	/* Per-vendor info */
+	cpuid_mwait_leaf_t	cpuid_mwait_leaf;	
+#define cpuid_mwait_linesize_max	cpuid_mwait_leaf.linesize_max
+#define cpuid_mwait_linesize_min	cpuid_mwait_leaf.linesize_min
+#define cpuid_mwait_extensions		cpuid_mwait_leaf.extensions
+#define cpuid_mwait_sub_Cstates		cpuid_mwait_leaf.sub_Cstates
+	cpuid_thermal_leaf_t	cpuid_thermal_leaf;
+	cpuid_arch_perf_leaf_t	cpuid_arch_perf_leaf;
+	uint32_t	unused[4];			/* cpuid_xsave_leaf */
+
+	/* Cache details: */
+	uint32_t	cpuid_cache_linesize;
+	uint32_t	cpuid_cache_L2_associativity;
+	uint32_t	cpuid_cache_size;
+
+	/* Virtual and physical address aize: */
+	uint32_t	cpuid_address_bits_physical;
+	uint32_t	cpuid_address_bits_virtual;
+
+	uint32_t	cpuid_microcode_version;
+
+	/* Numbers of tlbs per processor [i|d, small|large, level0|level1] */
+	uint32_t	cpuid_tlb[2][2][2];
+			#define	TLB_INST	0
+			#define	TLB_DATA	1
+			#define	TLB_SMALL	0
+			#define	TLB_LARGE	1
+	uint32_t	cpuid_stlb;
+
+	uint32_t	core_count;
+	uint32_t	thread_count;
+
+	/* Max leaf ids available from CPUID */
+	uint32_t	cpuid_max_basic;
+	uint32_t	cpuid_max_ext;
+
+	/* Family-specific info links */
+	uint32_t		cpuid_cpufamily;
+	cpuid_mwait_leaf_t	*cpuid_mwait_leafp;	
+	cpuid_thermal_leaf_t	*cpuid_thermal_leafp;
+	cpuid_arch_perf_leaf_t	*cpuid_arch_perf_leafp;
+	cpuid_xsave_leaf_t	*cpuid_xsave_leafp;
+	uint64_t		cpuid_leaf7_features;
+	cpuid_xsave_leaf_t	cpuid_xsave_leaf[2];
+} i386_cpu_info_t;
+
+#ifdef MACH_KERNEL_PRIVATE
+typedef struct {
+	char		cpuid_vmm_vendor[16];
+	uint32_t	cpuid_vmm_family;
+	uint32_t	cpuid_vmm_bus_frequency;
+	uint32_t	cpuid_vmm_tsc_frequency;
+} i386_vmm_info_t;
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*
+ * External declarations
+ */
+extern cpu_type_t	cpuid_cputype(void);
+extern cpu_subtype_t	cpuid_cpusubtype(void);
+extern void		cpuid_cpu_display(const char *);
+extern void		cpuid_feature_display(const char *);
+extern void		cpuid_extfeature_display(const char *);
+extern char *		cpuid_get_feature_names(uint64_t, char *, unsigned);
+extern char *		cpuid_get_extfeature_names(uint64_t, char *, unsigned);
+extern char *		cpuid_get_leaf7_feature_names(uint64_t, char *, unsigned);
+
+extern uint64_t		cpuid_features(void);
+extern uint64_t		cpuid_extfeatures(void);
+extern uint64_t		cpuid_leaf7_features(void);
+extern uint32_t		cpuid_family(void);
+extern uint32_t		cpuid_cpufamily(void);
+	
+extern i386_cpu_info_t	*cpuid_info(void);
+extern void		cpuid_set_info(void);
+
+#ifdef MACH_KERNEL_PRIVATE
+extern boolean_t	cpuid_vmm_present(void);
+extern i386_vmm_info_t	*cpuid_vmm_info(void);
+extern uint32_t		cpuid_vmm_family(void);
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* ASSEMBLER */
+
+#endif /* __APPLE_API_PRIVATE */
+#endif /* _MACHINE_CPUID_H_ */
diff -Nur xnu-3247.1.106/osfmk/i386/cpuid_legacy.h xnu-3247.1.106-AnV/osfmk/i386/cpuid_legacy.h
--- xnu-3247.1.106/osfmk/i386/cpuid_legacy.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/cpuid_legacy.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,78 @@
+/*
+ *  cpuid_legacy.h
+ *  xnu
+ *
+ *  Created by mercurysquad on 21/9/08.
+ *
+ */
+
+/* This file is added to avoid polluting stock cpuid.h */
+
+#ifndef _CPUID_LEGACY_H_
+#define _CPUID_LEGACY_H_
+
+/* Declarations for non deterministic cache info */
+typedef struct {
+	uint32_t	encoding;
+	cache_type_t	type;
+	uint32_t	totalsize;
+	uint32_t	associativity;
+	uint32_t	linesize;
+	uint32_t	partitions;
+} intel_nd_cache_info;
+
+//#define KB (1024)
+#define	MB (1024*KB)
+
+/* For encoding information, refer to IA32 instruction set reference A-M, CPUID instruction */
+/* Wonder who at Intel came up with this mess */
+const intel_nd_cache_info nonDet_CacheInfo[43] = {
+/*	byte,	type,	size,	assoc,	linesize */
+{	0x06,	L1I,	8*KB,	4,	32,	1	},
+{	0x08,	L1I,	16*KB,	4,	32,	1	},
+{	0x0A,	L1D,	8*KB,	2,	32,	1	},
+{	0x0C,	L1D,	16*KB,	4,	32,	1	},
+{	0x0E,	L1D,	24*KB,	6,	64,	1	},
+{	0x22,	L3U,	512*KB,	4,	64,	2	},
+{	0x23,	L3U,	1*MB,	8,	64,	2	},
+{	0x25,	L3U,	2*MB,	8,	64,	2	},
+{	0x29,	L3U,	4*MB,	8,	64,	2	},
+{	0x2C,	L1D,	32*KB,	8,	64,	2	},
+{	0x30,	L1I,	32*KB,	8,	64,	1	},
+{	0x41,	L2U,	128*KB,	4,	32,	1	},
+{	0x42,	L2U,	256*KB,	4,	32,	1	},
+{	0x43,	L2U,	512*KB,	4,	32,	1	},
+{	0x44,	L2U,	1*MB,	4,	32,	1	},
+{	0x45,	L2U,	2*MB,	4,	32,	1	},
+{	0x46,	L3U,	4*MB,	4,	64,	1	},
+{	0x47,	L3U,	8*MB,	8,	64,	1	},
+{	0x48,	L2U,	3*MB,	12,	64,	1	},
+{	0x49,	L2U,	4*MB,	16,	64,	1	}, // for Xeons family Fh model 6h it's L3U but we dont care
+{	0x4A,	L3U,	6*MB,	12,	64,	1	},
+{	0x4B,	L3U,	8*MB,	16,	64,	1	},
+{	0x4C,	L3U,	12*MB,	12,	64,	1	},
+{	0x4D,	L3U,	16*MB,	16,	64,	1	},
+{	0x4E,	L2U,	6*MB,	24,	64,	1	},
+{	0x60,	L1D,	16*KB,	8,	64,	1	},
+{	0x66,	L1D,	8*KB,	4,	64,	1	},
+{	0x67,	L1D,	16*KB,	4,	64,	1	},
+{	0x68,	L1D,	32*KB,	4,	64,	1	},
+{	0x78,	L2U,	1*MB,	4,	64,	1	},
+{	0x79,	L2U,	128*KB,	8,	64,	2	},
+{	0x7A,	L2U,	256*KB,	8,	64,	2	},
+{	0x7B,	L2U,	512*KB,	8,	64,	2	},
+{	0x7C,	L2U,	1*MB,	8,	64,	2	},
+{	0x7D,	L2U,	2*MB,	8,	64,	1	},
+{	0x7F,	L2U,	512*KB,	2,	64,	1	},
+{	0x80,	L2U,	512*KB,	8,	64,	1	},
+{	0x82,	L2U,	256*KB,	8,	32,	1	},
+{	0x83,	L2U,	512*KB,	8,	32,	1	},
+{	0x84,	L2U,	1*MB,	8,	32,	1	},
+{	0x85,	L2U,	2*MB,	8,	32,	1	},
+{	0x86,	L2U,	512*KB,	4,	64,	1	},
+{	0x87,	L2U,	1*MB,	8,	64,	1	}
+};
+
+#define CPUID_MODEL_GULFTOWN 44
+
+#endif // _CPUID_LEGACY_H_
diff -Nur xnu-3247.1.106/osfmk/i386/i386_init.c xnu-3247.1.106-AnV/osfmk/i386/i386_init.c
--- xnu-3247.1.106/osfmk/i386/i386_init.c	2015-12-06 01:32:36.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/i386_init.c	2015-12-13 17:08:10.000000000 +0100
@@ -528,6 +528,7 @@
 do_init_slave(boolean_t fast_restart)
 {
 	void	*init_param	= FULL_SLAVE_INIT;
+    int boot_arg = 0;
 
 	postcode(I386_INIT_SLAVE);
 
@@ -564,7 +565,8 @@
 
 #if CONFIG_VMX
 	/* resume VT operation */
-	vmx_resume();
+    if (cpuid_extfeatures() & CPUID_FEATURE_VMX)
+        vmx_resume();
 #endif
 
 #if CONFIG_MTRR
@@ -572,7 +574,8 @@
 	    pat_init();
 #endif
 
-	cpu_thread_init();	/* not strictly necessary */
+    if (!PE_parse_boot_argn("-nokthreads", &boot_arg, sizeof(boot_arg)))
+        cpu_thread_init();	/* not strictly necessary */
 
 	cpu_init();	/* Sets cpu_running which starter cpu waits for */
  	slave_main(init_param);
diff -Nur xnu-3247.1.106/osfmk/i386/i386_init.c.orig xnu-3247.1.106-AnV/osfmk/i386/i386_init.c.orig
--- xnu-3247.1.106/osfmk/i386/i386_init.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/i386_init.c.orig	2015-12-06 01:32:36.000000000 +0100
@@ -0,0 +1,607 @@
+/*
+ * Copyright (c) 2003-2012 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+/* 
+ * Mach Operating System
+ * Copyright (c) 1991,1990,1989, 1988 Carnegie Mellon University
+ * All Rights Reserved.
+ * 
+ * Permission to use, copy, modify and distribute this software and its
+ * documentation is hereby granted, provided that both the copyright
+ * notice and this permission notice appear in all copies of the
+ * software, derivative works or modified versions, and any portions
+ * thereof, and that both notices appear in supporting documentation.
+ * 
+ * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
+ * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR
+ * ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
+ * 
+ * Carnegie Mellon requests users of this software to return to
+ * 
+ *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU
+ *  School of Computer Science
+ *  Carnegie Mellon University
+ *  Pittsburgh PA 15213-3890
+ * 
+ * any improvements or extensions that they make and grant Carnegie Mellon
+ * the rights to redistribute these changes.
+ */
+
+
+#include <mach/i386/vm_param.h>
+
+#include <string.h>
+#include <mach/vm_param.h>
+#include <mach/vm_prot.h>
+#include <mach/machine.h>
+#include <mach/time_value.h>
+#include <kern/spl.h>
+#include <kern/assert.h>
+#include <kern/debug.h>
+#include <kern/misc_protos.h>
+#include <kern/startup.h>
+#include <kern/clock.h>
+#include <kern/pms.h>
+#include <kern/xpr.h>
+#include <kern/cpu_data.h>
+#include <kern/processor.h>
+#include <sys/kdebug.h>
+#include <console/serial_protos.h>
+#include <vm/vm_page.h>
+#include <vm/pmap.h>
+#include <vm/vm_kern.h>
+#include <machine/pal_routines.h>
+#include <i386/fpu.h>
+#include <i386/pmap.h>
+#include <i386/misc_protos.h>
+#include <i386/cpu_threads.h>
+#include <i386/cpuid.h>
+#include <i386/lapic.h>
+#include <i386/mp.h>
+#include <i386/mp_desc.h>
+#if CONFIG_MTRR
+#include <i386/mtrr.h>
+#endif
+#include <i386/machine_routines.h>
+#if CONFIG_MCA
+#include <i386/machine_check.h>
+#endif
+#include <i386/ucode.h>
+#include <i386/postcode.h>
+#include <i386/Diagnostics.h>
+#include <i386/pmCPU.h>
+#include <i386/tsc.h>
+#include <i386/locks.h> /* LcksOpts */
+#if DEBUG
+#include <machine/pal_routines.h>
+#endif
+#if DEBUG
+#define DBG(x...)       kprintf(x)
+#else
+#define DBG(x...)
+#endif
+
+int			debug_task;
+
+static boot_args	*kernelBootArgs;
+
+extern int		disableConsoleOutput;
+extern const char	version[];
+extern const char	version_variant[];
+extern int		nx_enabled;
+
+uint64_t		physmap_base, physmap_max;
+
+pd_entry_t		*KPTphys;
+pd_entry_t		*IdlePTD;
+pdpt_entry_t		*IdlePDPT;
+pml4_entry_t		*IdlePML4;
+
+char *physfree;
+
+/*
+ * Note: ALLOCPAGES() can only be used safely within Idle_PTs_init()
+ * due to the mutation of physfree.
+ */
+static void *
+ALLOCPAGES(int npages)
+{
+	uintptr_t tmp = (uintptr_t)physfree;
+	bzero(physfree, npages * PAGE_SIZE);
+	physfree += npages * PAGE_SIZE;
+	tmp += VM_MIN_KERNEL_ADDRESS & ~LOW_4GB_MASK;
+	return (void *)tmp;
+}
+
+static void
+fillkpt(pt_entry_t *base, int prot, uintptr_t src, int index, int count)
+{
+	int i;
+	for (i=0; i<count; i++) {
+		base[index] = src | prot | INTEL_PTE_VALID;
+		src += PAGE_SIZE;
+		index++;
+	}
+}
+
+extern pmap_paddr_t first_avail;
+
+int break_kprintf = 0;
+
+uint64_t
+x86_64_pre_sleep(void)
+{
+	IdlePML4[0] = IdlePML4[KERNEL_PML4_INDEX];
+	uint64_t oldcr3 = get_cr3_raw();
+	set_cr3_raw((uint32_t) (uintptr_t)ID_MAP_VTOP(IdlePML4));
+	return oldcr3;
+}
+
+void
+x86_64_post_sleep(uint64_t new_cr3)
+{
+	IdlePML4[0] = 0;
+	set_cr3_raw((uint32_t) new_cr3);
+}
+
+
+
+
+// Set up the physical mapping - NPHYSMAP GB of memory mapped at a high address
+// NPHYSMAP is determined by the maximum supported RAM size plus 4GB to account
+// the PCI hole (which is less 4GB but not more).
+
+/* Compile-time guard: NPHYSMAP is capped to 256GiB, accounting for
+ * randomisation
+ */
+extern int maxphymapsupported[NPHYSMAP <= (PTE_PER_PAGE/2) ? 1 : -1];
+
+static void
+physmap_init(void)
+{
+	pt_entry_t *physmapL3 = ALLOCPAGES(1);
+	struct {
+		pt_entry_t entries[PTE_PER_PAGE];
+	} * physmapL2 = ALLOCPAGES(NPHYSMAP);
+
+	uint64_t i;
+	uint8_t phys_random_L3 = early_random() & 0xFF;
+
+	/* We assume NX support. Mark all levels of the PHYSMAP NX
+	 * to avoid granting executability via a single bit flip.
+	 */
+#if DEVELOPMENT || DEBUG
+	uint32_t reg[4];
+	do_cpuid(0x80000000, reg);
+	if (reg[eax] >= 0x80000001) {
+		do_cpuid(0x80000001, reg);
+		assert(reg[edx] & CPUID_EXTFEATURE_XD);
+	}
+#endif /* DEVELOPMENT || DEBUG */
+
+	for(i = 0; i < NPHYSMAP; i++) {
+		physmapL3[i + phys_random_L3] =
+				((uintptr_t)ID_MAP_VTOP(&physmapL2[i]))
+				| INTEL_PTE_VALID
+				| INTEL_PTE_NX
+				| INTEL_PTE_WRITE;
+
+		uint64_t j;
+		for(j = 0; j < PTE_PER_PAGE; j++) {
+			physmapL2[i].entries[j] =
+			    ((i * PTE_PER_PAGE + j) << PDSHIFT)
+							| INTEL_PTE_PS
+							| INTEL_PTE_VALID
+							| INTEL_PTE_NX
+							| INTEL_PTE_WRITE;
+		}
+	}
+
+	IdlePML4[KERNEL_PHYSMAP_PML4_INDEX] =
+					((uintptr_t)ID_MAP_VTOP(physmapL3))
+					| INTEL_PTE_VALID
+					| INTEL_PTE_NX
+					| INTEL_PTE_WRITE;
+
+	physmap_base = KVADDR(KERNEL_PHYSMAP_PML4_INDEX, phys_random_L3, 0, 0);
+	physmap_max = physmap_base + NPHYSMAP * GB;
+	DBG("Physical address map base: 0x%qx\n", physmap_base);
+	DBG("Physical map idlepml4[%d]: 0x%llx\n",
+		KERNEL_PHYSMAP_PML4_INDEX, IdlePML4[KERNEL_PHYSMAP_PML4_INDEX]);
+}
+
+static void
+descriptor_alias_init()
+{
+	vm_offset_t	master_gdt_phys;
+	vm_offset_t	master_gdt_alias_phys;
+	vm_offset_t	master_idt_phys;
+	vm_offset_t	master_idt_alias_phys;
+
+	assert(((vm_offset_t)master_gdt & PAGE_MASK) == 0);
+	assert(((vm_offset_t)master_idt64 & PAGE_MASK) == 0);
+
+	master_gdt_phys       = (vm_offset_t) ID_MAP_VTOP(master_gdt);
+	master_idt_phys       = (vm_offset_t) ID_MAP_VTOP(master_idt64);
+	master_gdt_alias_phys = (vm_offset_t) ID_MAP_VTOP(MASTER_GDT_ALIAS);
+	master_idt_alias_phys = (vm_offset_t) ID_MAP_VTOP(MASTER_IDT_ALIAS);
+	
+	DBG("master_gdt_phys:       %p\n", (void *) master_gdt_phys);
+	DBG("master_idt_phys:       %p\n", (void *) master_idt_phys);
+	DBG("master_gdt_alias_phys: %p\n", (void *) master_gdt_alias_phys);
+	DBG("master_idt_alias_phys: %p\n", (void *) master_idt_alias_phys);
+
+	KPTphys[atop_kernel(master_gdt_alias_phys)] = master_gdt_phys |
+		INTEL_PTE_VALID | INTEL_PTE_NX | INTEL_PTE_WRITE;
+	KPTphys[atop_kernel(master_idt_alias_phys)] = master_idt_phys |
+		INTEL_PTE_VALID | INTEL_PTE_NX;	/* read-only */
+}
+
+static void
+Idle_PTs_init(void)
+{
+	/* Allocate the "idle" kernel page tables: */
+	KPTphys  = ALLOCPAGES(NKPT);		/* level 1 */
+	IdlePTD  = ALLOCPAGES(NPGPTD);		/* level 2 */
+	IdlePDPT = ALLOCPAGES(1);		/* level 3 */
+	IdlePML4 = ALLOCPAGES(1);		/* level 4 */
+
+	// Fill the lowest level with everything up to physfree
+	fillkpt(KPTphys,
+		INTEL_PTE_WRITE, 0, 0, (int)(((uintptr_t)physfree) >> PAGE_SHIFT));
+
+	/* IdlePTD */
+	fillkpt(IdlePTD,
+		INTEL_PTE_WRITE, (uintptr_t)ID_MAP_VTOP(KPTphys), 0, NKPT);
+
+	// IdlePDPT entries
+	fillkpt(IdlePDPT,
+		INTEL_PTE_WRITE, (uintptr_t)ID_MAP_VTOP(IdlePTD), 0, NPGPTD);
+
+	// IdlePML4 single entry for kernel space.
+	fillkpt(IdlePML4 + KERNEL_PML4_INDEX,
+		INTEL_PTE_WRITE, (uintptr_t)ID_MAP_VTOP(IdlePDPT), 0, 1);
+	
+	postcode(VSTART_PHYSMAP_INIT);
+
+	physmap_init();
+
+	postcode(VSTART_DESC_ALIAS_INIT);
+
+	descriptor_alias_init();
+
+	postcode(VSTART_SET_CR3);
+
+	// Switch to the page tables..
+	set_cr3_raw((uintptr_t)ID_MAP_VTOP(IdlePML4));
+
+}
+
+
+/*
+ * vstart() is called in the natural mode (64bit for K64, 32 for K32)
+ * on a set of bootstrap pagetables which use large, 2MB pages to map 
+ * all of physical memory in both. See idle_pt.c for details.
+ *
+ * In K64 this identity mapping is mirrored the top and bottom 512GB 
+ * slots of PML4.
+ *
+ * The bootstrap processor called with argument boot_args_start pointing to
+ * the boot-args block. The kernel's (4K page) page tables are allocated and
+ * initialized before switching to these.
+ *
+ * Non-bootstrap processors are called with argument boot_args_start NULL.
+ * These processors switch immediately to the existing kernel page tables.
+ */
+void
+vstart(vm_offset_t boot_args_start)
+{
+	boolean_t	is_boot_cpu = !(boot_args_start == 0);
+	int		cpu;
+	uint32_t	lphysfree;
+
+	postcode(VSTART_ENTRY);
+
+	if (is_boot_cpu) {
+		/*
+		 * Get startup parameters.
+		 */
+		kernelBootArgs = (boot_args *)boot_args_start;
+		lphysfree = kernelBootArgs->kaddr + kernelBootArgs->ksize;
+		physfree = (void *)(uintptr_t)((lphysfree + PAGE_SIZE - 1) &~ (PAGE_SIZE - 1));
+
+#if DEVELOPMENT || DEBUG
+		pal_serial_init();
+#endif
+		DBG("revision      0x%x\n", kernelBootArgs->Revision);
+		DBG("version       0x%x\n", kernelBootArgs->Version);
+		DBG("command line  %s\n", kernelBootArgs->CommandLine);
+		DBG("memory map    0x%x\n", kernelBootArgs->MemoryMap);
+		DBG("memory map sz 0x%x\n", kernelBootArgs->MemoryMapSize);
+		DBG("kaddr         0x%x\n", kernelBootArgs->kaddr);
+		DBG("ksize         0x%x\n", kernelBootArgs->ksize);
+		DBG("physfree      %p\n", physfree);
+		DBG("bootargs: %p, &ksize: %p &kaddr: %p\n",
+			kernelBootArgs, 
+			&kernelBootArgs->ksize,
+			&kernelBootArgs->kaddr);
+		DBG("SMBIOS mem sz 0x%llx\n", kernelBootArgs->PhysicalMemorySize);
+
+		/*
+		 * Setup boot args given the physical start address.
+		 * Note: PE_init_platform needs to be called before Idle_PTs_init
+		 * because access to the DeviceTree is required to read the
+		 * random seed before generating a random physical map slide.
+		 */
+		kernelBootArgs = (boot_args *)
+		    ml_static_ptovirt(boot_args_start);
+		DBG("i386_init(0x%lx) kernelBootArgs=%p\n",
+		    (unsigned long)boot_args_start, kernelBootArgs);
+		PE_init_platform(FALSE, kernelBootArgs);
+		postcode(PE_INIT_PLATFORM_D);
+
+		Idle_PTs_init();
+		postcode(VSTART_IDLE_PTS_INIT);
+
+		first_avail = (vm_offset_t)ID_MAP_VTOP(physfree);
+
+		cpu = 0;
+		cpu_data_alloc(TRUE);
+	} else {
+		/* Switch to kernel's page tables (from the Boot PTs) */
+		set_cr3_raw((uintptr_t)ID_MAP_VTOP(IdlePML4));
+		/* Find our logical cpu number */
+		cpu = lapic_to_cpu[(LAPIC_READ(ID)>>LAPIC_ID_SHIFT) & LAPIC_ID_MASK];
+		DBG("CPU: %d, GSBASE initial value: 0x%llx\n", cpu, rdmsr64(MSR_IA32_GS_BASE));
+	}
+
+	postcode(VSTART_CPU_DESC_INIT);
+	if(is_boot_cpu)
+		cpu_desc_init64(cpu_datap(cpu));
+	cpu_desc_load64(cpu_datap(cpu));
+	postcode(VSTART_CPU_MODE_INIT);
+	if (is_boot_cpu)
+		cpu_mode_init(current_cpu_datap()); /* cpu_mode_init() will be
+						     * invoked on the APs
+						     * via i386_init_slave()
+						     */
+	postcode(VSTART_EXIT);
+	x86_init_wrapper(is_boot_cpu ? (uintptr_t) i386_init
+				     : (uintptr_t) i386_init_slave,
+			 cpu_datap(cpu)->cpu_int_stack_top);
+}
+
+void
+pstate_trace(void)
+{
+}
+
+/*
+ *	Cpu initialization.  Running virtual, but without MACH VM
+ *	set up.
+ */
+void
+i386_init(void)
+{
+	unsigned int	maxmem;
+	uint64_t	maxmemtouse;
+	unsigned int	cpus = 0;
+	boolean_t	fidn;
+	boolean_t	IA32e = TRUE;
+	char		namep[16];
+
+	postcode(I386_INIT_ENTRY);
+
+	pal_i386_init();
+	tsc_init();
+	rtclock_early_init();	/* mach_absolute_time() now functionsl */
+
+	kernel_debug_string_simple("i386_init");
+	pstate_trace();
+
+#if CONFIG_MCA
+	/* Initialize machine-check handling */
+	mca_cpu_init();
+#endif
+
+	master_cpu = 0;
+	cpu_init();
+
+	postcode(CPU_INIT_D);
+
+	printf_init();			/* Init this in case we need debugger */
+	panic_init();			/* Init this in case we need debugger */
+
+	/* setup debugging output if one has been chosen */
+	kernel_debug_string_simple("PE_init_kprintf");
+	PE_init_kprintf(FALSE);
+
+	if(PE_parse_boot_argn("-show_pointers", &namep, sizeof (namep)))
+		doprnt_hide_pointers = FALSE;
+
+	kernel_debug_string_simple("kernel_early_bootstrap");
+	kernel_early_bootstrap();
+
+	if (!PE_parse_boot_argn("diag", &dgWork.dgFlags, sizeof (dgWork.dgFlags)))
+		dgWork.dgFlags = 0;
+
+	serialmode = 0;
+	if(PE_parse_boot_argn("serial", &serialmode, sizeof (serialmode))) {
+		/* We want a serial keyboard and/or console */
+		kprintf("Serial mode specified: %08X\n", serialmode);
+	}
+	if(serialmode & 1) {
+		(void)switch_to_serial_console();
+		disableConsoleOutput = FALSE;	/* Allow printfs to happen */
+	}
+
+	/* setup console output */
+	kernel_debug_string_simple("PE_init_printf");
+	PE_init_printf(FALSE);
+
+	kprintf("version_variant = %s\n", version_variant);
+	kprintf("version         = %s\n", version);
+	
+	if (!PE_parse_boot_argn("maxmem", &maxmem, sizeof (maxmem)))
+		maxmemtouse = 0;
+	else
+	        maxmemtouse = ((uint64_t)maxmem) * MB;
+
+	if (PE_parse_boot_argn("cpus", &cpus, sizeof (cpus))) {
+		if ((0 < cpus) && (cpus < max_ncpus))
+                        max_ncpus = cpus;
+	}
+
+	/*
+	 * debug support for > 4G systems
+	 */
+	PE_parse_boot_argn("himemory_mode", &vm_himemory_mode, sizeof (vm_himemory_mode));
+	if (vm_himemory_mode != 0)
+		kprintf("himemory_mode: %d\n", vm_himemory_mode);
+
+	if (!PE_parse_boot_argn("immediate_NMI", &fidn, sizeof (fidn)))
+		force_immediate_debugger_NMI = FALSE;
+	else
+		force_immediate_debugger_NMI = fidn;
+
+#if DEBUG
+	nanoseconds_to_absolutetime(URGENCY_NOTIFICATION_ASSERT_NS, &urgency_notification_assert_abstime_threshold);
+#endif
+	PE_parse_boot_argn("urgency_notification_abstime",
+	    &urgency_notification_assert_abstime_threshold,
+	    sizeof(urgency_notification_assert_abstime_threshold));
+
+	if (!(cpuid_extfeatures() & CPUID_EXTFEATURE_XD))
+		nx_enabled = 0;
+
+	/*   
+	 * VM initialization, after this we're using page tables...
+	 * Thn maximum number of cpus must be set beforehand.
+	 */
+	kernel_debug_string_simple("i386_vm_init");
+	i386_vm_init(maxmemtouse, IA32e, kernelBootArgs);
+
+	/* create the console for verbose or pretty mode */
+	/* Note: doing this prior to tsc_init() allows for graceful panic! */
+	PE_init_platform(TRUE, kernelBootArgs);
+	PE_create_console();
+
+	kernel_debug_string_simple("power_management_init");
+	power_management_init();
+	processor_bootstrap();
+	thread_bootstrap();
+
+	pstate_trace();
+	kernel_debug_string_simple("machine_startup");
+	machine_startup();
+	pstate_trace();
+}
+
+static void
+do_init_slave(boolean_t fast_restart)
+{
+	void	*init_param	= FULL_SLAVE_INIT;
+
+	postcode(I386_INIT_SLAVE);
+
+	if (!fast_restart) {
+		/* Ensure that caching and write-through are enabled */
+		set_cr0(get_cr0() & ~(CR0_NW|CR0_CD));
+  
+		DBG("i386_init_slave() CPU%d: phys (%d) active.\n",
+		    get_cpu_number(), get_cpu_phys_number());
+  
+		assert(!ml_get_interrupts_enabled());
+  
+		cpu_mode_init(current_cpu_datap());
+		pmap_cpu_init();
+  
+#if CONFIG_MCA
+		mca_cpu_init();
+#endif
+  
+		LAPIC_INIT();
+		lapic_configure();
+		LAPIC_DUMP();
+		LAPIC_CPU_MAP_DUMP();
+  
+		init_fpu();
+  
+#if CONFIG_MTRR
+		mtrr_update_cpu();
+#endif
+		/* update CPU microcode */
+		ucode_update_wake();
+	} else
+	    init_param = FAST_SLAVE_INIT;
+
+#if CONFIG_VMX
+	/* resume VT operation */
+	vmx_resume();
+#endif
+
+#if CONFIG_MTRR
+	if (!fast_restart)
+	    pat_init();
+#endif
+
+	cpu_thread_init();	/* not strictly necessary */
+
+	cpu_init();	/* Sets cpu_running which starter cpu waits for */
+ 	slave_main(init_param);
+  
+ 	panic("do_init_slave() returned from slave_main()");
+}
+
+/*
+ * i386_init_slave() is called from pstart.
+ * We're in the cpu's interrupt stack with interrupts disabled.
+ * At this point we are in legacy mode. We need to switch on IA32e
+ * if the mode is set to 64-bits.
+ */
+void
+i386_init_slave(void)
+{
+    	do_init_slave(FALSE);
+}
+
+/*
+ * i386_init_slave_fast() is called from pmCPUHalt.
+ * We're running on the idle thread and need to fix up
+ * some accounting and get it so that the scheduler sees this
+ * CPU again.
+ */
+void
+i386_init_slave_fast(void)
+{
+    	do_init_slave(TRUE);
+}
+
+
diff -Nur xnu-3247.1.106/osfmk/i386/lapic_native.c xnu-3247.1.106-AnV/osfmk/i386/lapic_native.c
--- xnu-3247.1.106/osfmk/i386/lapic_native.c	2015-12-06 01:32:36.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/lapic_native.c	2015-12-13 17:08:10.000000000 +0100
@@ -81,7 +81,7 @@
 static uint64_t lapic_error_time_threshold = 0;
 static unsigned lapic_master_error_count = 0;
 static unsigned lapic_error_count_threshold = 5;
-static boolean_t lapic_dont_panic = FALSE;
+static boolean_t lapic_dont_panic = TRUE;
 
 #ifdef MP_DEBUG
 void
@@ -272,8 +272,10 @@
 	LAPIC_INIT();
 
 	kprintf("ID: 0x%x LDR: 0x%x\n", LAPIC_READ(ID), LAPIC_READ(LDR));
-	if ((LAPIC_READ(VERSION)&LAPIC_VERSION_MASK) < 0x14) {
-		panic("Local APIC version 0x%x, 0x14 or more expected\n",
+	/* Changed by bronzovka from < 0x14 to fix panic on some AMDs.
+	   R:A:W:X86 made panic to printf to avoid potential - might be unnecessary. */
+	if ((LAPIC_READ(VERSION)&LAPIC_VERSION_MASK) < 0x10) {
+		printf("Local APIC version 0x%x, 0x10 or more expected\n",
 			(LAPIC_READ(VERSION)&LAPIC_VERSION_MASK));
 	}
 
@@ -503,8 +505,8 @@
 
 	if (lapic_error_time_threshold == 0 && cpu_number() == 0) {
 		nanoseconds_to_absolutetime(NSEC_PER_SEC >> 2, &lapic_error_time_threshold);
-		if (!PE_parse_boot_argn("lapic_dont_panic", &lapic_dont_panic, sizeof(lapic_dont_panic))) {
-			lapic_dont_panic = FALSE;
+		if (!PE_parse_boot_argn("lapic_do_panic", &lapic_dont_panic, sizeof(lapic_dont_panic))) {
+			lapic_dont_panic = TRUE;
 		}
 	}
 
@@ -520,6 +522,11 @@
 		value |= LAPIC_LVT_DM_EXTINT;
 		LAPIC_WRITE(LVT_LINT0, value);
 	}
+    else
+		LAPIC_WRITE(LVT_LINT0, LAPIC_LVT_DM_EXTINT | LAPIC_LVT_MASKED);
+    
+	/* NMI: ummasked, off course */
+	LAPIC_WRITE(LVT_LINT1, LAPIC_LVT_DM_NMI);
 
 	/* Timer: unmasked, one-shot */
 	LAPIC_WRITE(LVT_TIMER, LAPIC_VECTOR(TIMER));
@@ -765,12 +772,16 @@
 		esr = lapic_esr_read();
 		lapic_dump();
 
-		if ((debug_boot_arg && (lapic_dont_panic == FALSE)) ||
-			cpu_number() != master_cpu) {
+        /* apocolipse LAPIC fix */
+        if (debug_boot_arg && (lapic_dont_panic == FALSE))
+        {
 			panic("Local APIC error, ESR: %d\n", esr);
+        } else {
+			if (cpu_number() != master_cpu)
+				printf("Local APIC error, ESR: %d\n", esr);
 		}
 
-		if (cpu_number() == master_cpu) {
+		if (TRUE) {
 			uint64_t abstime = mach_absolute_time();
 			if ((abstime - lapic_last_master_error) < lapic_error_time_threshold) {
 				if (lapic_master_error_count++ > lapic_error_count_threshold) {
@@ -783,7 +794,7 @@
 				lapic_last_master_error = abstime;
 				lapic_master_error_count = 0;
 			}
-			printf("Local APIC error on master CPU, ESR: %d, error count this run: %d\n", esr, lapic_master_error_count);
+			printf("Local APIC error on CPU%d, ESR: %d, error count this run: %d\n", cpu_number(), esr, lapic_master_error_count);
 		}
 
 		_lapic_end_of_interrupt();
diff -Nur xnu-3247.1.106/osfmk/i386/lapic_native.c.orig xnu-3247.1.106-AnV/osfmk/i386/lapic_native.c.orig
--- xnu-3247.1.106/osfmk/i386/lapic_native.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/lapic_native.c.orig	2015-12-06 01:32:36.000000000 +0100
@@ -0,0 +1,970 @@
+/*
+ * Copyright (c) 2008-2011 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+
+#include <mach/mach_types.h>
+#include <mach/kern_return.h>
+
+#include <kern/kern_types.h>
+#include <kern/cpu_number.h>
+#include <kern/cpu_data.h>
+#include <kern/assert.h>
+#include <kern/machine.h>
+#include <kern/debug.h>
+
+#include <vm/vm_map.h>
+#include <vm/vm_kern.h>
+
+#include <i386/lapic.h>
+#include <i386/cpuid.h>
+#include <i386/proc_reg.h>
+#include <i386/machine_cpu.h>
+#include <i386/misc_protos.h>
+#include <i386/mp.h>
+#include <i386/postcode.h>
+#include <i386/cpu_threads.h>
+#include <i386/machine_routines.h>
+#include <i386/tsc.h>
+#if CONFIG_MCA
+#include <i386/machine_check.h>
+#endif
+
+#include <sys/kdebug.h>
+
+#if	MP_DEBUG
+#define PAUSE		delay(1000000)
+#define DBG(x...)	kprintf(x)
+#else
+#define DBG(x...)
+#define PAUSE
+#endif	/* MP_DEBUG */
+
+lapic_ops_table_t	*lapic_ops;	/* Lapic operations switch */
+
+static vm_map_offset_t	lapic_pbase;	/* Physical base memory-mapped regs */
+static vm_offset_t	lapic_vbase;	/* Virtual base memory-mapped regs */
+
+static i386_intr_func_t	lapic_intr_func[LAPIC_FUNC_TABLE_SIZE];
+
+/* TRUE if local APIC was enabled by the OS not by the BIOS */
+static boolean_t lapic_os_enabled = FALSE;
+
+static boolean_t lapic_errors_masked = FALSE;
+static uint64_t lapic_last_master_error = 0;
+static uint64_t lapic_error_time_threshold = 0;
+static unsigned lapic_master_error_count = 0;
+static unsigned lapic_error_count_threshold = 5;
+static boolean_t lapic_dont_panic = FALSE;
+
+#ifdef MP_DEBUG
+void
+lapic_cpu_map_dump(void)
+{
+	int	i;
+
+	for (i = 0; i < MAX_CPUS; i++) {
+		if (cpu_to_lapic[i] == -1)
+			continue;
+		kprintf("cpu_to_lapic[%d]: %d\n",
+			i, cpu_to_lapic[i]);
+	}
+	for (i = 0; i < MAX_LAPICIDS; i++) {
+		if (lapic_to_cpu[i] == -1)
+			continue;
+		kprintf("lapic_to_cpu[%d]: %d\n",
+			i, lapic_to_cpu[i]);
+	}
+}
+#endif /* MP_DEBUG */
+
+static void
+legacy_init(void)
+{
+	int		result;
+	vm_map_entry_t	entry;
+	vm_map_offset_t lapic_vbase64;
+	/* Establish a map to the local apic */
+
+	if (lapic_vbase == 0) {
+		lapic_vbase64 = (vm_offset_t)vm_map_min(kernel_map);
+		result = vm_map_find_space(kernel_map,
+					   &lapic_vbase64,
+					   round_page(LAPIC_SIZE), 0,
+					   VM_MAKE_TAG(VM_KERN_MEMORY_IOKIT), &entry);
+		/* Convert 64-bit vm_map_offset_t to "pointer sized" vm_offset_t
+		 */
+		lapic_vbase = (vm_offset_t) lapic_vbase64;
+		if (result != KERN_SUCCESS) {
+			panic("legacy_init: vm_map_find_entry FAILED (err=%d)", result);
+		}
+		vm_map_unlock(kernel_map);
+
+		/*
+		 * Map in the local APIC non-cacheable, as recommended by Intel
+		 * in section 8.4.1 of the "System Programming Guide".
+		 * In fact, this is redundant because EFI will have assigned an
+		 * MTRR physical range containing the local APIC's MMIO space as
+		 * UC and this will override the default PAT setting.
+		 */
+		pmap_enter(pmap_kernel(),
+				lapic_vbase,
+				(ppnum_t) i386_btop(lapic_pbase),
+				VM_PROT_READ|VM_PROT_WRITE,
+				VM_PROT_NONE,
+				VM_WIMG_IO,
+				TRUE);
+	}
+
+	/*
+	 * Set flat delivery model, logical processor id
+	 * This should already be the default set.
+	 */
+	LAPIC_WRITE(DFR, LAPIC_DFR_FLAT);
+	LAPIC_WRITE(LDR, (get_cpu_number()) << LAPIC_LDR_SHIFT);
+}
+
+
+static uint32_t
+legacy_read(lapic_register_t reg)
+{
+	return  *LAPIC_MMIO(reg);
+}
+
+static void
+legacy_write(lapic_register_t reg, uint32_t value)
+{
+	*LAPIC_MMIO(reg) = value;
+}
+
+static uint64_t
+legacy_read_icr(void)
+{
+	return (((uint64_t)*LAPIC_MMIO(ICRD)) << 32) | ((uint64_t)*LAPIC_MMIO(ICR));
+}
+
+static void
+legacy_write_icr(uint32_t dst, uint32_t cmd)
+{
+	*LAPIC_MMIO(ICRD) = dst << LAPIC_ICRD_DEST_SHIFT;
+	*LAPIC_MMIO(ICR) = cmd;
+}
+
+static lapic_ops_table_t legacy_ops = {
+	legacy_init,
+	legacy_read,
+	legacy_write,
+	legacy_read_icr,
+	legacy_write_icr
+};
+
+static	boolean_t is_x2apic = FALSE;
+
+static void
+x2apic_init(void)
+{
+	uint32_t	lo;
+	uint32_t	hi;
+
+	rdmsr(MSR_IA32_APIC_BASE, lo, hi);
+	if ((lo & MSR_IA32_APIC_BASE_EXTENDED) == 0)  {
+		lo |= MSR_IA32_APIC_BASE_EXTENDED;
+		wrmsr(MSR_IA32_APIC_BASE, lo, hi);
+		kprintf("x2APIC mode enabled\n");
+	}
+}
+
+static uint32_t
+x2apic_read(lapic_register_t reg)
+{
+	uint32_t	lo;
+	uint32_t	hi;
+
+	rdmsr(LAPIC_MSR(reg), lo, hi);
+	return lo;
+}
+
+static void
+x2apic_write(lapic_register_t reg, uint32_t value)
+{
+	wrmsr(LAPIC_MSR(reg), value, 0);
+}
+
+static uint64_t
+x2apic_read_icr(void)
+{
+	return rdmsr64(LAPIC_MSR(ICR));;
+}
+
+static void
+x2apic_write_icr(uint32_t dst, uint32_t cmd)
+{
+	  wrmsr(LAPIC_MSR(ICR), cmd, dst);
+}
+
+static lapic_ops_table_t x2apic_ops = {
+	x2apic_init,
+	x2apic_read,
+	x2apic_write,
+	x2apic_read_icr,
+	x2apic_write_icr
+};
+
+void
+lapic_init(void)
+{
+	uint32_t	lo;
+	uint32_t	hi;
+	boolean_t	is_boot_processor;
+	boolean_t	is_lapic_enabled;
+
+	/* Examine the local APIC state */
+	rdmsr(MSR_IA32_APIC_BASE, lo, hi);
+	is_boot_processor = (lo & MSR_IA32_APIC_BASE_BSP) != 0;
+	is_lapic_enabled  = (lo & MSR_IA32_APIC_BASE_ENABLE) != 0;
+	is_x2apic         = (lo & MSR_IA32_APIC_BASE_EXTENDED) != 0;
+	lapic_pbase = (lo &  MSR_IA32_APIC_BASE_BASE);
+	kprintf("MSR_IA32_APIC_BASE 0x%llx %s %s mode %s\n", lapic_pbase,
+		is_lapic_enabled ? "enabled" : "disabled",
+		is_x2apic ? "extended" : "legacy",
+		is_boot_processor ? "BSP" : "AP");
+	if (!is_boot_processor || !is_lapic_enabled)
+		panic("Unexpected local APIC state\n");
+
+	/*
+	 * If x2APIC is available and not already enabled, enable it.
+	 * Unless overriden by boot-arg.
+	 */
+	if (!is_x2apic && (cpuid_features() & CPUID_FEATURE_x2APIC)) {
+		PE_parse_boot_argn("-x2apic", &is_x2apic, sizeof(is_x2apic));
+		kprintf("x2APIC supported %s be enabled\n",
+			is_x2apic ? "and will" : "but will not");
+	}
+
+	lapic_ops = is_x2apic ? &x2apic_ops : &legacy_ops;
+
+	LAPIC_INIT();
+
+	kprintf("ID: 0x%x LDR: 0x%x\n", LAPIC_READ(ID), LAPIC_READ(LDR));
+	if ((LAPIC_READ(VERSION)&LAPIC_VERSION_MASK) < 0x14) {
+		panic("Local APIC version 0x%x, 0x14 or more expected\n",
+			(LAPIC_READ(VERSION)&LAPIC_VERSION_MASK));
+	}
+
+	/* Set up the lapic_id <-> cpu_number map and add this boot processor */
+	lapic_cpu_map_init();
+	lapic_cpu_map((LAPIC_READ(ID)>>LAPIC_ID_SHIFT)&LAPIC_ID_MASK, 0);
+	current_cpu_datap()->cpu_phys_number = cpu_to_lapic[0];
+	kprintf("Boot cpu local APIC id 0x%x\n", cpu_to_lapic[0]);
+}
+
+
+static int
+lapic_esr_read(void)
+{
+	/* write-read register */
+	LAPIC_WRITE(ERROR_STATUS, 0);
+	return LAPIC_READ(ERROR_STATUS);
+}
+
+static void 
+lapic_esr_clear(void)
+{
+	LAPIC_WRITE(ERROR_STATUS, 0);
+	LAPIC_WRITE(ERROR_STATUS, 0);
+}
+
+static const char *DM_str[8] = {
+	"Fixed",
+	"Lowest Priority",
+	"Invalid",
+	"Invalid",
+	"NMI",
+	"Reset",
+	"Invalid",
+	"ExtINT"};
+
+static const char *TMR_str[] = {
+	"OneShot",
+	"Periodic",
+	"TSC-Deadline",
+	"Illegal"
+};
+
+void
+lapic_dump(void)
+{
+	int	i;
+
+#define BOOL(a) ((a)?' ':'!')
+#define VEC(lvt) \
+	LAPIC_READ(lvt)&LAPIC_LVT_VECTOR_MASK
+#define	DS(lvt)	\
+	(LAPIC_READ(lvt)&LAPIC_LVT_DS_PENDING)?" SendPending" : "Idle"
+#define DM(lvt) \
+	DM_str[(LAPIC_READ(lvt)>>LAPIC_LVT_DM_SHIFT)&LAPIC_LVT_DM_MASK]
+#define MASK(lvt) \
+	BOOL(LAPIC_READ(lvt)&LAPIC_LVT_MASKED)
+#define TM(lvt) \
+	(LAPIC_READ(lvt)&LAPIC_LVT_TM_LEVEL)? "Level" : "Edge"
+#define IP(lvt) \
+	(LAPIC_READ(lvt)&LAPIC_LVT_IP_PLRITY_LOW)? "Low " : "High"
+
+	kprintf("LAPIC %d at %p version 0x%x\n", 
+		(LAPIC_READ(ID)>>LAPIC_ID_SHIFT)&LAPIC_ID_MASK,
+		(void *) lapic_vbase,
+		LAPIC_READ(VERSION)&LAPIC_VERSION_MASK);
+	kprintf("Priorities: Task 0x%x  Arbitration 0x%x  Processor 0x%x\n",
+		LAPIC_READ(TPR)&LAPIC_TPR_MASK,
+		LAPIC_READ(APR)&LAPIC_APR_MASK,
+		LAPIC_READ(PPR)&LAPIC_PPR_MASK);
+	kprintf("Destination Format 0x%x Logical Destination 0x%x\n",
+		is_x2apic ? 0 : LAPIC_READ(DFR)>>LAPIC_DFR_SHIFT,
+		LAPIC_READ(LDR)>>LAPIC_LDR_SHIFT);
+	kprintf("%cEnabled %cFocusChecking SV 0x%x\n",
+		BOOL(LAPIC_READ(SVR)&LAPIC_SVR_ENABLE),
+		BOOL(!(LAPIC_READ(SVR)&LAPIC_SVR_FOCUS_OFF)),
+		LAPIC_READ(SVR) & LAPIC_SVR_MASK);
+#if CONFIG_MCA
+	if (mca_is_cmci_present())
+		kprintf("LVT_CMCI:    Vector 0x%02x [%s] %s %cmasked\n",
+			VEC(LVT_CMCI),
+			DM(LVT_CMCI),
+			DS(LVT_CMCI),
+			MASK(LVT_CMCI));
+#endif
+	kprintf("LVT_TIMER:   Vector 0x%02x %s %cmasked %s\n",
+		VEC(LVT_TIMER),
+		DS(LVT_TIMER),
+		MASK(LVT_TIMER),
+		TMR_str[(LAPIC_READ(LVT_TIMER) >> LAPIC_LVT_TMR_SHIFT)
+                                               &  LAPIC_LVT_TMR_MASK]);
+	kprintf("  Initial Count: 0x%08x \n", LAPIC_READ(TIMER_INITIAL_COUNT));
+	kprintf("  Current Count: 0x%08x \n", LAPIC_READ(TIMER_CURRENT_COUNT));
+	kprintf("  Divide Config: 0x%08x \n", LAPIC_READ(TIMER_DIVIDE_CONFIG));
+	kprintf("LVT_PERFCNT: Vector 0x%02x [%s] %s %cmasked\n",
+		VEC(LVT_PERFCNT),
+		DM(LVT_PERFCNT),
+		DS(LVT_PERFCNT),
+		MASK(LVT_PERFCNT));
+	kprintf("LVT_THERMAL: Vector 0x%02x [%s] %s %cmasked\n",
+		VEC(LVT_THERMAL),
+		DM(LVT_THERMAL),
+		DS(LVT_THERMAL),
+		MASK(LVT_THERMAL));
+	kprintf("LVT_LINT0:   Vector 0x%02x [%s][%s][%s] %s %cmasked\n",
+		VEC(LVT_LINT0),
+		DM(LVT_LINT0),
+		TM(LVT_LINT0),
+		IP(LVT_LINT0),
+		DS(LVT_LINT0),
+		MASK(LVT_LINT0));
+	kprintf("LVT_LINT1:   Vector 0x%02x [%s][%s][%s] %s %cmasked\n",
+		VEC(LVT_LINT1),
+		DM(LVT_LINT1),
+		TM(LVT_LINT1),
+		IP(LVT_LINT1),
+		DS(LVT_LINT1),
+		MASK(LVT_LINT1));
+	kprintf("LVT_ERROR:   Vector 0x%02x %s %cmasked\n",
+		VEC(LVT_ERROR),
+		DS(LVT_ERROR),
+		MASK(LVT_ERROR));
+	kprintf("ESR: %08x \n", lapic_esr_read());
+	kprintf("       ");
+	for(i=0xf; i>=0; i--)
+		kprintf("%x%x%x%x",i,i,i,i);
+	kprintf("\n");
+	kprintf("TMR: 0x");
+	for(i=7; i>=0; i--)
+		kprintf("%08x",LAPIC_READ_OFFSET(TMR_BASE, i));
+	kprintf("\n");
+	kprintf("IRR: 0x");
+	for(i=7; i>=0; i--)
+		kprintf("%08x",LAPIC_READ_OFFSET(IRR_BASE, i));
+	kprintf("\n");
+	kprintf("ISR: 0x");
+	for(i=7; i >= 0; i--)
+		kprintf("%08x",LAPIC_READ_OFFSET(ISR_BASE, i));
+	kprintf("\n");
+}
+
+boolean_t
+lapic_probe(void)
+{
+	uint32_t	lo;
+	uint32_t	hi;
+
+	if (cpuid_features() & CPUID_FEATURE_APIC)
+		return TRUE;
+
+	if (cpuid_family() == 6 || cpuid_family() == 15) {
+		/*
+		 * Mobile Pentiums:
+		 * There may be a local APIC which wasn't enabled by BIOS.
+		 * So we try to enable it explicitly.
+		 */
+		rdmsr(MSR_IA32_APIC_BASE, lo, hi);
+		lo &= ~MSR_IA32_APIC_BASE_BASE;
+		lo |= MSR_IA32_APIC_BASE_ENABLE | LAPIC_START;
+		lo |= MSR_IA32_APIC_BASE_ENABLE;
+		wrmsr(MSR_IA32_APIC_BASE, lo, hi);
+
+		/*
+		 * Re-initialize cpu features info and re-check.
+		 */
+		cpuid_set_info();
+		/* We expect this codepath will never be traversed
+		 * due to EFI enabling the APIC. Reducing the APIC
+		 * interrupt base dynamically is not supported.
+		 */
+		if (cpuid_features() & CPUID_FEATURE_APIC) {
+			printf("Local APIC discovered and enabled\n");
+			lapic_os_enabled = TRUE;
+			lapic_interrupt_base = LAPIC_REDUCED_INTERRUPT_BASE;
+			return TRUE;
+		}
+	}
+
+	return FALSE;
+}
+
+void
+lapic_shutdown(void)
+{
+	uint32_t lo;
+	uint32_t hi;
+	uint32_t value;
+
+	/* Shutdown if local APIC was enabled by OS */
+	if (lapic_os_enabled == FALSE)
+		return;
+
+	mp_disable_preemption();
+
+	/* ExtINT: masked */
+	if (get_cpu_number() == master_cpu) {
+		value = LAPIC_READ(LVT_LINT0);
+		value |= LAPIC_LVT_MASKED;
+		LAPIC_WRITE(LVT_LINT0, value);
+	}
+
+	/* Error: masked */
+	LAPIC_WRITE(LVT_ERROR, LAPIC_READ(LVT_ERROR) | LAPIC_LVT_MASKED);
+
+	/* Timer: masked */
+	LAPIC_WRITE(LVT_TIMER, LAPIC_READ(LVT_TIMER) | LAPIC_LVT_MASKED);
+
+	/* Perfmon: masked */
+	LAPIC_WRITE(LVT_PERFCNT, LAPIC_READ(LVT_PERFCNT) | LAPIC_LVT_MASKED);
+
+	/* APIC software disabled */
+	LAPIC_WRITE(SVR, LAPIC_READ(SVR) & ~LAPIC_SVR_ENABLE);
+
+	/* Bypass the APIC completely and update cpu features */
+	rdmsr(MSR_IA32_APIC_BASE, lo, hi);
+	lo &= ~MSR_IA32_APIC_BASE_ENABLE;
+	wrmsr(MSR_IA32_APIC_BASE, lo, hi);
+	cpuid_set_info();
+
+	mp_enable_preemption();
+}
+
+void
+lapic_configure(void)
+{
+	int	value;
+
+	if (lapic_error_time_threshold == 0 && cpu_number() == 0) {
+		nanoseconds_to_absolutetime(NSEC_PER_SEC >> 2, &lapic_error_time_threshold);
+		if (!PE_parse_boot_argn("lapic_dont_panic", &lapic_dont_panic, sizeof(lapic_dont_panic))) {
+			lapic_dont_panic = FALSE;
+		}
+	}
+
+	/* Accept all */
+	LAPIC_WRITE(TPR, 0);
+
+	LAPIC_WRITE(SVR, LAPIC_VECTOR(SPURIOUS) | LAPIC_SVR_ENABLE);
+
+	/* ExtINT */
+	if (get_cpu_number() == master_cpu) {
+		value = LAPIC_READ(LVT_LINT0);
+		value &= ~LAPIC_LVT_MASKED;
+		value |= LAPIC_LVT_DM_EXTINT;
+		LAPIC_WRITE(LVT_LINT0, value);
+	}
+
+	/* Timer: unmasked, one-shot */
+	LAPIC_WRITE(LVT_TIMER, LAPIC_VECTOR(TIMER));
+
+	/* Perfmon: unmasked */
+	LAPIC_WRITE(LVT_PERFCNT, LAPIC_VECTOR(PERFCNT));
+
+	/* Thermal: unmasked */
+	LAPIC_WRITE(LVT_THERMAL, LAPIC_VECTOR(THERMAL));
+
+#if CONFIG_MCA
+	/* CMCI, if available */
+	if (mca_is_cmci_present())
+		LAPIC_WRITE(LVT_CMCI, LAPIC_VECTOR(CMCI));
+#endif
+
+	if (((cpu_number() == master_cpu) && lapic_errors_masked == FALSE) ||
+		(cpu_number() != master_cpu)) {
+		lapic_esr_clear();
+		LAPIC_WRITE(LVT_ERROR, LAPIC_VECTOR(ERROR));
+	}
+}
+
+void
+lapic_set_timer(
+	boolean_t		interrupt_unmasked,
+	lapic_timer_mode_t	mode,
+	lapic_timer_divide_t	divisor,
+	lapic_timer_count_t	initial_count)
+{
+	uint32_t	timer_vector;
+
+	mp_disable_preemption();
+	timer_vector = LAPIC_READ(LVT_TIMER);
+	timer_vector &= ~(LAPIC_LVT_MASKED|LAPIC_LVT_PERIODIC);;
+	timer_vector |= interrupt_unmasked ? 0 : LAPIC_LVT_MASKED;
+	timer_vector |= (mode == periodic) ? LAPIC_LVT_PERIODIC : 0;
+	LAPIC_WRITE(LVT_TIMER, timer_vector);
+	LAPIC_WRITE(TIMER_DIVIDE_CONFIG, divisor);
+	LAPIC_WRITE(TIMER_INITIAL_COUNT, initial_count);
+	mp_enable_preemption();
+}
+
+void
+lapic_config_timer(
+	boolean_t		interrupt_unmasked,
+	lapic_timer_mode_t	mode,
+	lapic_timer_divide_t	divisor)
+{
+	uint32_t	timer_vector;
+
+	mp_disable_preemption();
+	timer_vector = LAPIC_READ(LVT_TIMER);
+	timer_vector &= ~(LAPIC_LVT_MASKED |
+			  LAPIC_LVT_PERIODIC |
+			  LAPIC_LVT_TSC_DEADLINE);
+	timer_vector |= interrupt_unmasked ? 0 : LAPIC_LVT_MASKED;
+	timer_vector |= (mode == periodic) ? LAPIC_LVT_PERIODIC : 0;
+	LAPIC_WRITE(LVT_TIMER, timer_vector);
+	LAPIC_WRITE(TIMER_DIVIDE_CONFIG, divisor);
+	mp_enable_preemption();
+}
+
+/*
+ * Configure TSC-deadline timer mode. The lapic interrupt is always unmasked.
+ */
+void
+lapic_config_tsc_deadline_timer(void)
+{
+	uint32_t	timer_vector;
+
+	DBG("lapic_config_tsc_deadline_timer()\n");
+	mp_disable_preemption();
+	timer_vector = LAPIC_READ(LVT_TIMER);
+	timer_vector &= ~(LAPIC_LVT_MASKED |
+			  LAPIC_LVT_PERIODIC);
+	timer_vector |= LAPIC_LVT_TSC_DEADLINE;
+	LAPIC_WRITE(LVT_TIMER, timer_vector);
+
+	/* Serialize writes per Intel OSWG */
+	do {
+		lapic_set_tsc_deadline_timer(rdtsc64() + (1ULL<<32));
+	} while (lapic_get_tsc_deadline_timer() == 0);
+	lapic_set_tsc_deadline_timer(0);
+
+	mp_enable_preemption();
+	DBG("lapic_config_tsc_deadline_timer() done\n");
+}
+
+void
+lapic_set_timer_fast(
+	lapic_timer_count_t	initial_count)
+{
+	LAPIC_WRITE(LVT_TIMER, LAPIC_READ(LVT_TIMER) & ~LAPIC_LVT_MASKED);
+	LAPIC_WRITE(TIMER_INITIAL_COUNT, initial_count);
+}
+
+void
+lapic_set_tsc_deadline_timer(uint64_t deadline)
+{
+	/* Don't bother disarming: wrmsr64(MSR_IA32_TSC_DEADLINE, 0); */
+	wrmsr64(MSR_IA32_TSC_DEADLINE, deadline);
+}
+
+uint64_t
+lapic_get_tsc_deadline_timer(void)
+{
+	return rdmsr64(MSR_IA32_TSC_DEADLINE);
+}
+
+void
+lapic_get_timer(
+	lapic_timer_mode_t	*mode,
+	lapic_timer_divide_t	*divisor,
+	lapic_timer_count_t	*initial_count,
+	lapic_timer_count_t	*current_count)
+{
+	mp_disable_preemption();
+	if (mode)
+		*mode = (LAPIC_READ(LVT_TIMER) & LAPIC_LVT_PERIODIC) ?
+				periodic : one_shot;
+	if (divisor)
+		*divisor = LAPIC_READ(TIMER_DIVIDE_CONFIG) & LAPIC_TIMER_DIVIDE_MASK;
+	if (initial_count)
+		*initial_count = LAPIC_READ(TIMER_INITIAL_COUNT);
+	if (current_count)
+		*current_count = LAPIC_READ(TIMER_CURRENT_COUNT);
+	mp_enable_preemption();
+} 
+
+static inline void
+_lapic_end_of_interrupt(void)
+{
+	LAPIC_WRITE(EOI, 0);
+}
+
+void
+lapic_end_of_interrupt(void)
+{
+	_lapic_end_of_interrupt();
+}
+
+void lapic_unmask_perfcnt_interrupt(void) {
+	LAPIC_WRITE(LVT_PERFCNT, LAPIC_VECTOR(PERFCNT));
+}
+
+void lapic_set_perfcnt_interrupt_mask(boolean_t mask) {
+	uint32_t m = (mask ? LAPIC_LVT_MASKED : 0);
+	LAPIC_WRITE(LVT_PERFCNT, LAPIC_VECTOR(PERFCNT) | m);
+}
+
+void
+lapic_set_intr_func(int vector, i386_intr_func_t func)
+{
+	if (vector > lapic_interrupt_base)
+		vector -= lapic_interrupt_base;
+
+	switch (vector) {
+	case LAPIC_NMI_INTERRUPT:
+	case LAPIC_INTERPROCESSOR_INTERRUPT:
+	case LAPIC_TIMER_INTERRUPT:
+	case LAPIC_THERMAL_INTERRUPT:
+	case LAPIC_PERFCNT_INTERRUPT:
+	case LAPIC_CMCI_INTERRUPT:
+	case LAPIC_PM_INTERRUPT:
+		lapic_intr_func[vector] = func;
+		break;
+	default:
+		panic("lapic_set_intr_func(%d,%p) invalid vector\n",
+			vector, func);
+	}
+}
+
+void	lapic_set_pmi_func(i386_intr_func_t func) {
+	lapic_set_intr_func(LAPIC_VECTOR(PERFCNT), func);
+}
+
+int
+lapic_interrupt(int interrupt_num, x86_saved_state_t *state)
+{
+	int	retval = 0;
+	int 	esr = -1;
+
+	interrupt_num -= lapic_interrupt_base;
+	if (interrupt_num < 0) {
+		if (interrupt_num == (LAPIC_NMI_INTERRUPT - lapic_interrupt_base) &&
+		    lapic_intr_func[LAPIC_NMI_INTERRUPT] != NULL) {
+			retval = (*lapic_intr_func[LAPIC_NMI_INTERRUPT])(state);
+			return retval;
+		}
+		else
+			return 0;
+	}
+
+	switch(interrupt_num) {
+	case LAPIC_TIMER_INTERRUPT:
+	case LAPIC_THERMAL_INTERRUPT:
+	case LAPIC_INTERPROCESSOR_INTERRUPT:
+	case LAPIC_PM_INTERRUPT:
+		if (lapic_intr_func[interrupt_num] != NULL)
+			(void) (*lapic_intr_func[interrupt_num])(state);
+		_lapic_end_of_interrupt();
+		retval = 1;
+		break;
+	case LAPIC_PERFCNT_INTERRUPT:
+		/* If a function has been registered, invoke it.  Otherwise,
+		 * pass up to IOKit.
+		 */
+		if (lapic_intr_func[interrupt_num] != NULL) {
+			(void) (*lapic_intr_func[interrupt_num])(state);
+			/* Unmask the interrupt since we don't expect legacy users
+			 * to be responsible for it.
+			 */
+			lapic_unmask_perfcnt_interrupt();
+			_lapic_end_of_interrupt();
+			retval = 1;
+		}
+		break;
+	case LAPIC_CMCI_INTERRUPT:
+		if (lapic_intr_func[interrupt_num] != NULL)
+			(void) (*lapic_intr_func[interrupt_num])(state);
+		/* return 0 for plaform expert to handle */
+		break;
+	case LAPIC_ERROR_INTERRUPT:
+		/* We treat error interrupts on APs as fatal.
+		 * The current interrupt steering scheme directs most
+		 * external interrupts to the BSP (HPET interrupts being
+		 * a notable exception); hence, such an error
+		 * on an AP may signify LVT corruption (with "may" being
+		 * the operative word). On the BSP, we adopt a more
+		 * lenient approach, in the interests of enhancing
+		 * debuggability and reducing fragility.
+		 * If "lapic_error_count_threshold" error interrupts
+		 * occur within "lapic_error_time_threshold" absolute
+		 * time units, we mask the error vector and log. The
+		 * error interrupts themselves are likely
+		 * side effects of issues which are beyond the purview of
+		 * the local APIC interrupt handler, however. The Error
+		 * Status Register value (the illegal destination
+		 * vector code is one observed in practice) indicates
+		 * the immediate cause of the error.
+		 */
+		esr = lapic_esr_read();
+		lapic_dump();
+
+		if ((debug_boot_arg && (lapic_dont_panic == FALSE)) ||
+			cpu_number() != master_cpu) {
+			panic("Local APIC error, ESR: %d\n", esr);
+		}
+
+		if (cpu_number() == master_cpu) {
+			uint64_t abstime = mach_absolute_time();
+			if ((abstime - lapic_last_master_error) < lapic_error_time_threshold) {
+				if (lapic_master_error_count++ > lapic_error_count_threshold) {
+					lapic_errors_masked = TRUE;
+					LAPIC_WRITE(LVT_ERROR, LAPIC_READ(LVT_ERROR) | LAPIC_LVT_MASKED);
+					printf("Local APIC: errors masked\n");
+				}
+			}
+			else {
+				lapic_last_master_error = abstime;
+				lapic_master_error_count = 0;
+			}
+			printf("Local APIC error on master CPU, ESR: %d, error count this run: %d\n", esr, lapic_master_error_count);
+		}
+
+		_lapic_end_of_interrupt();
+		retval = 1;
+		break;
+	case LAPIC_SPURIOUS_INTERRUPT:
+		kprintf("SPIV\n");
+		/* No EOI required here */
+		retval = 1;
+		break;
+	case LAPIC_PMC_SW_INTERRUPT: 
+		{
+		}
+		break;
+	case LAPIC_KICK_INTERRUPT:
+		_lapic_end_of_interrupt();
+		retval = 1;
+		break;
+	}
+
+	return retval;
+}
+
+void
+lapic_smm_restore(void)
+{
+	boolean_t state;
+
+	if (lapic_os_enabled == FALSE)
+		return;
+
+	state = ml_set_interrupts_enabled(FALSE);
+
+ 	if (LAPIC_ISR_IS_SET(LAPIC_REDUCED_INTERRUPT_BASE, TIMER)) {
+		/*
+		 * Bogus SMI handler enables interrupts but does not know about
+		 * local APIC interrupt sources. When APIC timer counts down to
+		 * zero while in SMM, local APIC will end up waiting for an EOI
+		 * but no interrupt was delivered to the OS.
+ 		 */
+		_lapic_end_of_interrupt();
+
+		/*
+		 * timer is one-shot, trigger another quick countdown to trigger
+		 * another timer interrupt.
+		 */
+		if (LAPIC_READ(TIMER_CURRENT_COUNT) == 0) {
+			LAPIC_WRITE(TIMER_INITIAL_COUNT, 1);
+		}
+
+		kprintf("lapic_smm_restore\n");
+	}
+
+	ml_set_interrupts_enabled(state);
+}
+
+void
+lapic_send_ipi(int cpu, int vector)
+{
+	boolean_t	state;
+
+	if (vector < lapic_interrupt_base)
+		vector += lapic_interrupt_base;
+
+	state = ml_set_interrupts_enabled(FALSE);
+
+	/* Wait for pending outgoing send to complete */
+	while (LAPIC_READ_ICR() & LAPIC_ICR_DS_PENDING) {
+		cpu_pause();
+	}
+
+	LAPIC_WRITE_ICR(cpu_to_lapic[cpu], vector | LAPIC_ICR_DM_FIXED);
+
+	(void) ml_set_interrupts_enabled(state);
+}
+
+/*
+ * The following interfaces are privately exported to AICPM.
+ */
+
+boolean_t
+lapic_is_interrupt_pending(void)
+{
+	int		i;
+
+	for (i = 0; i < 8; i += 1) {
+		if ((LAPIC_READ_OFFSET(IRR_BASE, i) != 0) ||
+		    (LAPIC_READ_OFFSET(ISR_BASE, i) != 0))
+			return (TRUE);
+	}
+
+	return (FALSE);
+}
+
+boolean_t
+lapic_is_interrupting(uint8_t vector)
+{
+	int		i;
+	int		bit;
+	uint32_t	irr;
+	uint32_t	isr;
+
+	i = vector / 32;
+	bit = 1 << (vector % 32);
+
+	irr = LAPIC_READ_OFFSET(IRR_BASE, i);
+	isr = LAPIC_READ_OFFSET(ISR_BASE, i);
+
+	if ((irr | isr) & bit)
+		return (TRUE);
+
+	return (FALSE);
+}
+
+void
+lapic_interrupt_counts(uint64_t intrs[256])
+{
+	int		i;
+	int		j;
+	int		bit;
+	uint32_t	irr;
+	uint32_t	isr;
+
+	if (intrs == NULL)
+		return;
+
+	for (i = 0; i < 8; i += 1) {
+		irr = LAPIC_READ_OFFSET(IRR_BASE, i);
+		isr = LAPIC_READ_OFFSET(ISR_BASE, i);
+
+		if ((isr | irr) == 0)
+			continue;
+
+		for (j = (i == 0) ? 16 : 0; j < 32; j += 1) {
+			bit = (32 * i) + j;
+			if ((isr | irr) & (1 << j))
+				intrs[bit] += 1;
+		}
+	}
+}
+
+void
+lapic_disable_timer(void)
+{
+	uint32_t	lvt_timer;
+
+	/*
+         * If we're in deadline timer mode,
+	 * simply clear the deadline timer, otherwise
+	 * mask the timer interrupt and clear the countdown.
+         */
+	lvt_timer = LAPIC_READ(LVT_TIMER);
+	if (lvt_timer & LAPIC_LVT_TSC_DEADLINE) {
+		wrmsr64(MSR_IA32_TSC_DEADLINE, 0);
+	} else {
+		LAPIC_WRITE(LVT_TIMER, lvt_timer | LAPIC_LVT_MASKED);
+		LAPIC_WRITE(TIMER_INITIAL_COUNT, 0);
+		lvt_timer = LAPIC_READ(LVT_TIMER);
+	}
+}
+
+/* SPI returning the CMCI vector */
+uint8_t
+lapic_get_cmci_vector(void)
+{
+	uint8_t	cmci_vector = 0;
+#if CONFIG_MCA
+	/* CMCI, if available */
+	if (mca_is_cmci_present())
+		cmci_vector = LAPIC_VECTOR(CMCI);
+#endif
+	return cmci_vector;
+}
+
+#if DEBUG
+extern void lapic_trigger_MC(void);
+void
+lapic_trigger_MC(void)
+{
+	/* A 64-bit access to any register will do it. */
+	volatile uint64_t dummy = *(volatile uint64_t *) (volatile void *) LAPIC_MMIO(ID);
+	dummy++;
+}
+#endif
diff -Nur xnu-3247.1.106/osfmk/i386/mp_desc.c xnu-3247.1.106-AnV/osfmk/i386/mp_desc.c
--- xnu-3247.1.106/osfmk/i386/mp_desc.c	2015-12-06 01:32:37.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/mp_desc.c	2015-12-13 17:08:10.000000000 +0100
@@ -67,6 +67,7 @@
 #include <vm/vm_kern.h>
 #include <vm/vm_map.h>
 
+#include <i386/cpuid.h>
 #include <i386/bit_routines.h>
 #include <i386/mp_desc.h>
 #include <i386/misc_protos.h>
@@ -544,9 +545,13 @@
 static void
 fast_syscall_init64(__unused cpu_data_t *cdp)
 {
-	wrmsr64(MSR_IA32_SYSENTER_CS, SYSENTER_CS); 
-	wrmsr64(MSR_IA32_SYSENTER_EIP, (uintptr_t) hi64_sysenter);
-	wrmsr64(MSR_IA32_SYSENTER_ESP, current_sstk());
+    if (IsIntelCPU())
+    {
+        wrmsr64(MSR_IA32_SYSENTER_CS, SYSENTER_CS);
+        wrmsr64(MSR_IA32_SYSENTER_EIP, (uintptr_t) hi64_sysenter);
+        wrmsr64(MSR_IA32_SYSENTER_ESP, current_sstk());
+    }
+
 	/* Enable syscall/sysret */
 	wrmsr64(MSR_IA32_EFER, rdmsr64(MSR_IA32_EFER) | MSR_IA32_EFER_SCE);
 
diff -Nur xnu-3247.1.106/osfmk/i386/mtrr.c xnu-3247.1.106-AnV/osfmk/i386/mtrr.c
--- xnu-3247.1.106/osfmk/i386/mtrr.c	2015-12-06 01:32:37.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/mtrr.c	2015-12-13 17:08:10.000000000 +0100
@@ -313,6 +313,25 @@
  * barrier synchronization among all processors. This function is
  * called from the rendezvous IPI handler, and mtrr_update_cpu().
  */
+//enum {
+//    PAT_UC = 0,             /* uncached */
+//    PAT_WC = 1,             /* Write combining */
+//    PAT_WT = 4,             /* Write Through */
+//    PAT_WP = 5,             /* Write Protected */
+//    PAT_WB = 6,             /* Write Back (default) */
+//    PAT_UC_MINUS = 7,       /* UC, but can be overriden by MTRR */
+//};
+//#define PAT(x, y)       ((uint64_t)PAT_ ## y << ((x)*8))
+
+//Bronzovka: For artifacts: i fixed here - work or don't work :)))
+#define	PATENTRY(n, type)	(type << ((n) * 8))
+#define	PAT_UC		0x0ULL
+#define	PAT_WC		0x1ULL
+#define	PAT_WT		0x4ULL
+#define	PAT_WP		0x5ULL
+#define	PAT_WB		0x6ULL
+#define	PAT_UCMINUS	0x7ULL
+
 static void
 mtrr_update_action(void * cache_control_type)
 {
@@ -341,8 +360,20 @@
 		/* Change PA6 attribute field to WC */
 		uint64_t pat = rdmsr64(MSR_IA32_CR_PAT);
 		DBG("CPU%d PAT: was 0x%016llx\n", get_cpu_number(), pat);
-		pat &= ~(0x0FULL << 48);
-		pat |=  (0x01ULL << 48);
+        if (IsIntelCPU())
+        {
+            pat &= ~(0x0FULL << 48);
+            pat |=  (0x01ULL << 48);
+        } else {
+            //Bronzovka: modified pat...
+            /* We change WT to WC. Leave all other entries the default values. */
+            pat = PATENTRY(0, PAT_WB) | PATENTRY(1, PAT_WC) |
+                PATENTRY(2, PAT_UCMINUS) | PATENTRY(3, PAT_UC) |
+                PATENTRY(4, PAT_WB) | PATENTRY(5, PAT_WC) |
+                PATENTRY(6, PAT_UCMINUS) | PATENTRY(7, PAT_UC);
+
+            //pat = PAT(0, WB) | PAT(1, WC) | PAT(2, UC_MINUS) | PAT(3, UC) | PAT(4, WB) | PAT(5, WC) | PAT(6, UC_MINUS) | PAT(7, UC);
+        }
 		wrmsr64(MSR_IA32_CR_PAT, pat);
 		DBG("CPU%d PAT: is  0x%016llx\n",
 		    get_cpu_number(), rdmsr64(MSR_IA32_CR_PAT));
@@ -689,8 +720,18 @@
 	DBG("CPU%d PAT: was 0x%016llx\n", get_cpu_number(), pat);
 
 	/* Change PA6 attribute field to WC if required */
+    if (IsIntelCPU())
+    {
 	if ((pat & ~(0x0FULL << 48)) != (0x01ULL << 48)) {
 		mtrr_update_action(CACHE_CONTROL_PAT);
 	}
+    } else {
+        if ((pat = PATENTRY(0, PAT_WB) | PATENTRY(1, PAT_WC) |
+             PATENTRY(2, PAT_UCMINUS) | PATENTRY(3, PAT_UC) |
+             PATENTRY(4, PAT_WB) | PATENTRY(5, PAT_WC) |
+             PATENTRY(6, PAT_UCMINUS) | PATENTRY(7, PAT_UC))) {
+            mtrr_update_action(CACHE_CONTROL_PAT);
+        }
+    }
 	ml_set_interrupts_enabled(istate);
 }
diff -Nur xnu-3247.1.106/osfmk/i386/pmCPU.c xnu-3247.1.106-AnV/osfmk/i386/pmCPU.c
--- xnu-3247.1.106/osfmk/i386/pmCPU.c	2015-12-06 01:32:38.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/pmCPU.c	2015-12-13 17:08:10.000000000 +0100
@@ -911,6 +911,12 @@
 pmKextRegister(uint32_t version, pmDispatch_t *cpuFuncs,
     pmCallBacks_t *callbacks)
 {
+    /* AnV Software:	Set it anyway and don't panic please...
+     + 				This causes issues for SleepEnabler.kext...
+     + 				Warn about version mismatch though... */
+   	if (version != PM_DISPATCH_VERSION)
+ 		printf("Warning: Version mis-match between Kernel and CPU PM (0x%8X != 0x%8X)\n", version, PM_DISPATCH_VERSION);
+
 	if (callbacks != NULL && version == PM_DISPATCH_VERSION) {
 		callbacks->setRTCPop            = setPop;
 		callbacks->resyncDeadlines      = pmReSyncDeadlines;
@@ -939,13 +945,11 @@
 		callbacks->IsInterrupting	= lapic_is_interrupting;
 		callbacks->InterruptStats	= lapic_interrupt_counts;
 		callbacks->DisableApicTimer	= lapic_disable_timer;
-	} else {
-		panic("Version mis-match between Kernel and CPU PM");
 	}
 
 	if (cpuFuncs != NULL) {
 		if (pmDispatch) {
-			panic("Attempt to re-register power management interface--AICPM present in xcpm mode? %p->%p", pmDispatch, cpuFuncs);
+			printf("Attempt to re-register power management interface--AICPM present in xcpm mode? %p->%p\n", pmDispatch, cpuFuncs);
 		}
 
 		pmDispatch = cpuFuncs;
diff -Nur xnu-3247.1.106/osfmk/i386/pmCPU.c.orig xnu-3247.1.106-AnV/osfmk/i386/pmCPU.c.orig
--- xnu-3247.1.106/osfmk/i386/pmCPU.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/pmCPU.c.orig	2015-12-06 01:32:38.000000000 +0100
@@ -0,0 +1,988 @@
+/*
+ * Copyright (c) 2004-2011 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+
+/*
+ * CPU-specific power management support.
+ *
+ * Implements the "wrappers" to the KEXT.
+ */
+#include <i386/asm.h>
+#include <i386/machine_cpu.h>
+#include <i386/mp.h>
+#include <i386/machine_routines.h>
+#include <i386/proc_reg.h>
+#include <i386/pmap.h>
+#include <i386/misc_protos.h>
+#include <kern/machine.h>
+#include <kern/pms.h>
+#include <kern/processor.h>
+#include <kern/timer_queue.h>
+#include <i386/cpu_threads.h>
+#include <i386/pmCPU.h>
+#include <i386/cpuid.h>
+#include <i386/rtclock_protos.h>
+#include <kern/sched_prim.h>
+#include <i386/lapic.h>
+#include <i386/pal_routines.h>
+#include <sys/kdebug.h>
+#include <i386/tsc.h>
+
+extern int disableConsoleOutput;
+
+#define DELAY_UNSET		0xFFFFFFFFFFFFFFFFULL
+
+uint64_t cpu_itime_bins[CPU_ITIME_BINS] = {16* NSEC_PER_USEC, 32* NSEC_PER_USEC, 64* NSEC_PER_USEC, 128* NSEC_PER_USEC, 256* NSEC_PER_USEC, 512* NSEC_PER_USEC, 1024* NSEC_PER_USEC, 2048* NSEC_PER_USEC, 4096* NSEC_PER_USEC, 8192* NSEC_PER_USEC, 16384* NSEC_PER_USEC, 32768* NSEC_PER_USEC};
+uint64_t *cpu_rtime_bins = &cpu_itime_bins[0];
+
+/*
+ * The following is set when the KEXT loads and initializes.
+ */
+pmDispatch_t	*pmDispatch	= NULL;
+
+uint32_t		pmInitDone		= 0;
+static boolean_t	earlyTopology		= FALSE;
+static uint64_t		earlyMaxBusDelay	= DELAY_UNSET;
+static uint64_t		earlyMaxIntDelay	= DELAY_UNSET;
+
+/*
+ * Initialize the Cstate change code.
+ */
+void
+power_management_init(void)
+{
+    if (pmDispatch != NULL && pmDispatch->cstateInit != NULL)
+	(*pmDispatch->cstateInit)();
+}
+
+static inline void machine_classify_interval(uint64_t interval, uint64_t *bins, uint64_t *binvals, uint32_t nbins) {
+	uint32_t i;
+ 	for (i = 0; i < nbins; i++) {
+ 		if (interval < binvals[i]) {
+ 			bins[i]++;
+ 			break;
+ 		}
+ 	}
+}
+
+uint64_t	idle_pending_timers_processed;
+uint32_t	idle_entry_timer_processing_hdeadline_threshold = 5000000;
+
+/*
+ * Called when the CPU is idle.  It calls into the power management kext
+ * to determine the best way to idle the CPU.
+ */
+void
+machine_idle(void)
+{
+	cpu_data_t		*my_cpu		= current_cpu_datap();
+	__unused uint32_t	cnum = my_cpu->cpu_number;
+	uint64_t		ctime, rtime, itime;
+#if CST_DEMOTION_DEBUG
+	processor_t		cproc = my_cpu->cpu_processor;
+	uint64_t		cwakeups = PROCESSOR_DATA(cproc, wakeups_issued_total);
+#endif /* CST_DEMOTION_DEBUG */
+	uint64_t esdeadline, ehdeadline;
+	boolean_t do_process_pending_timers = FALSE;
+
+	ctime = mach_absolute_time();
+	esdeadline = my_cpu->rtclock_timer.queue.earliest_soft_deadline;
+	ehdeadline = my_cpu->rtclock_timer.deadline;
+/* Determine if pending timers exist */    
+	if ((ctime >= esdeadline) && (ctime < ehdeadline) &&
+	    ((ehdeadline - ctime) < idle_entry_timer_processing_hdeadline_threshold)) {
+		idle_pending_timers_processed++;
+		do_process_pending_timers = TRUE;
+		goto machine_idle_exit;
+	} else {
+		TCOAL_DEBUG(0xCCCC0000, ctime, my_cpu->rtclock_timer.queue.earliest_soft_deadline, my_cpu->rtclock_timer.deadline, idle_pending_timers_processed, 0);
+	}
+    
+	my_cpu->lcpu.state = LCPU_IDLE;
+	DBGLOG(cpu_handle, cpu_number(), MP_IDLE);
+	MARK_CPU_IDLE(cnum);
+
+	rtime = ctime - my_cpu->cpu_ixtime;
+
+	my_cpu->cpu_rtime_total += rtime;
+	machine_classify_interval(rtime, &my_cpu->cpu_rtimes[0], &cpu_rtime_bins[0], CPU_RTIME_BINS);
+#if CST_DEMOTION_DEBUG
+	uint32_t cl = 0, ch = 0;
+	uint64_t c3res, c6res, c7res;
+	rdmsr_carefully(MSR_IA32_CORE_C3_RESIDENCY, &cl, &ch);
+	c3res = ((uint64_t)ch << 32) | cl;
+	rdmsr_carefully(MSR_IA32_CORE_C6_RESIDENCY, &cl, &ch);
+	c6res = ((uint64_t)ch << 32) | cl;
+	rdmsr_carefully(MSR_IA32_CORE_C7_RESIDENCY, &cl, &ch);
+	c7res = ((uint64_t)ch << 32) | cl;
+#endif
+
+	if (pmInitDone) {
+		/*
+		 * Handle case where ml_set_maxbusdelay() or ml_set_maxintdelay()
+		 * were called prior to the CPU PM kext being registered.  We do
+		 * this here since we know at this point the values will be first
+		 * used since idle is where the decisions using these values is made.
+		 */
+		if (earlyMaxBusDelay != DELAY_UNSET)
+			ml_set_maxbusdelay((uint32_t)(earlyMaxBusDelay & 0xFFFFFFFF));
+		if (earlyMaxIntDelay != DELAY_UNSET)
+			ml_set_maxintdelay(earlyMaxIntDelay);
+	}
+
+	if (pmInitDone
+	    && pmDispatch != NULL
+	    && pmDispatch->MachineIdle != NULL)
+		(*pmDispatch->MachineIdle)(0x7FFFFFFFFFFFFFFFULL);
+	else {
+		/*
+		 * If no power management, re-enable interrupts and halt.
+		 * This will keep the CPU from spinning through the scheduler
+		 * and will allow at least some minimal power savings (but it
+		 * cause problems in some MP configurations w.r.t. the APIC
+		 * stopping during a GV3 transition).
+		 */
+		pal_hlt();
+		/* Once woken, re-disable interrupts. */
+		pal_cli();
+	}
+
+	/*
+	 * Mark the CPU as running again.
+	 */
+	MARK_CPU_ACTIVE(cnum);
+	DBGLOG(cpu_handle, cnum, MP_UNIDLE);
+	my_cpu->lcpu.state = LCPU_RUN;
+	uint64_t ixtime = my_cpu->cpu_ixtime = mach_absolute_time();
+	itime = ixtime - ctime;
+	my_cpu->cpu_idle_exits++;
+        my_cpu->cpu_itime_total += itime;
+    	machine_classify_interval(itime, &my_cpu->cpu_itimes[0], &cpu_itime_bins[0], CPU_ITIME_BINS);
+#if CST_DEMOTION_DEBUG
+	cl = ch = 0;
+	rdmsr_carefully(MSR_IA32_CORE_C3_RESIDENCY, &cl, &ch);
+	c3res = (((uint64_t)ch << 32) | cl) - c3res;
+	rdmsr_carefully(MSR_IA32_CORE_C6_RESIDENCY, &cl, &ch);
+	c6res = (((uint64_t)ch << 32) | cl) - c6res;
+	rdmsr_carefully(MSR_IA32_CORE_C7_RESIDENCY, &cl, &ch);
+	c7res = (((uint64_t)ch << 32) | cl) - c7res;
+
+	uint64_t ndelta = itime - tmrCvt(c3res + c6res + c7res, tscFCvtt2n);
+	KERNEL_DEBUG_CONSTANT(0xcead0000, ndelta, itime, c7res, c6res, c3res);
+	if ((itime > 1000000) && (ndelta > 250000))
+		KERNEL_DEBUG_CONSTANT(0xceae0000, ndelta, itime, c7res, c6res, c3res);
+#endif
+
+	machine_idle_exit:
+	/*
+	 * Re-enable interrupts.
+	 */
+
+	pal_sti();
+
+	if (do_process_pending_timers) {
+		TCOAL_DEBUG(0xBBBB0000 | DBG_FUNC_START, ctime, esdeadline, ehdeadline, idle_pending_timers_processed, 0);
+
+		/* Adjust to reflect that this isn't truly a package idle exit */
+		__sync_fetch_and_sub(&my_cpu->lcpu.package->num_idle, 1);
+		lapic_timer_swi(); /* Trigger software timer interrupt */
+		__sync_fetch_and_add(&my_cpu->lcpu.package->num_idle, 1);
+
+		TCOAL_DEBUG(0xBBBB0000 | DBG_FUNC_END, ctime, esdeadline, idle_pending_timers_processed, 0, 0);
+	}
+#if CST_DEMOTION_DEBUG
+	uint64_t nwakeups = PROCESSOR_DATA(cproc, wakeups_issued_total);
+
+	if ((nwakeups == cwakeups) && (topoParms.nLThreadsPerPackage == my_cpu->lcpu.package->num_idle)) {
+		KERNEL_DEBUG_CONSTANT(0xceaa0000, cwakeups, 0, 0, 0, 0);
+	}
+#endif    
+}
+
+/*
+ * Called when the CPU is to be halted.  It will choose the best C-State
+ * to be in.
+ */
+void
+pmCPUHalt(uint32_t reason)
+{
+    cpu_data_t	*cpup	= current_cpu_datap();
+
+    switch (reason) {
+    case PM_HALT_DEBUG:
+	cpup->lcpu.state = LCPU_PAUSE;
+	pal_stop_cpu(FALSE);
+	break;
+
+    case PM_HALT_PANIC:
+	cpup->lcpu.state = LCPU_PAUSE;
+	pal_stop_cpu(TRUE);
+	break;
+
+    case PM_HALT_NORMAL:
+    case PM_HALT_SLEEP:
+    default:
+        pal_cli();
+
+	if (pmInitDone
+	    && pmDispatch != NULL
+	    && pmDispatch->pmCPUHalt != NULL) {
+	    /*
+	     * Halt the CPU (and put it in a low power state.
+	     */
+	    (*pmDispatch->pmCPUHalt)();
+
+	    /*
+	     * We've exited halt, so get the CPU schedulable again.
+	     * - by calling the fast init routine for a slave, or
+	     * - by returning if we're the master processor.
+	     */
+	    if (cpup->cpu_number != master_cpu) {
+		i386_init_slave_fast();
+		panic("init_slave_fast returned");
+	    }
+	} else
+	{
+	    /*
+	     * If no power managment and a processor is taken off-line,
+	     * then invalidate the cache and halt it (it will not be able
+	     * to be brought back on-line without resetting the CPU).
+	     */
+	    __asm__ volatile ("wbinvd");
+	    cpup->lcpu.state = LCPU_HALT;
+	    pal_stop_cpu(FALSE);
+
+	    panic("back from Halt");
+	}
+
+	break;
+    }
+}
+
+void
+pmMarkAllCPUsOff(void)
+{
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->markAllCPUsOff != NULL)
+	(*pmDispatch->markAllCPUsOff)();
+}
+
+static void
+pmInitComplete(void)
+{
+    if (earlyTopology
+	&& pmDispatch != NULL
+	&& pmDispatch->pmCPUStateInit != NULL) {
+	(*pmDispatch->pmCPUStateInit)();
+	earlyTopology = FALSE;
+    }
+    pmInitDone = 1;
+}
+
+x86_lcpu_t *
+pmGetLogicalCPU(int cpu)
+{
+    return(cpu_to_lcpu(cpu));
+}
+
+x86_lcpu_t *
+pmGetMyLogicalCPU(void)
+{
+    cpu_data_t	*cpup	= current_cpu_datap();
+
+    return(&cpup->lcpu);
+}
+
+static x86_core_t *
+pmGetCore(int cpu)
+{
+    return(cpu_to_core(cpu));
+}
+
+static x86_core_t *
+pmGetMyCore(void)
+{
+    cpu_data_t	*cpup	= current_cpu_datap();
+
+    return(cpup->lcpu.core);
+}
+
+static x86_die_t *
+pmGetDie(int cpu)
+{
+    return(cpu_to_die(cpu));
+}
+
+static x86_die_t *
+pmGetMyDie(void)
+{
+    cpu_data_t	*cpup	= current_cpu_datap();
+
+    return(cpup->lcpu.die);
+}
+
+static x86_pkg_t *
+pmGetPackage(int cpu)
+{
+    return(cpu_to_package(cpu));
+}
+
+static x86_pkg_t *
+pmGetMyPackage(void)
+{
+    cpu_data_t	*cpup	= current_cpu_datap();
+
+    return(cpup->lcpu.package);
+}
+
+static void
+pmLockCPUTopology(int lock)
+{
+    if (lock) {
+	simple_lock(&x86_topo_lock);
+    } else {
+	simple_unlock(&x86_topo_lock);
+    }
+}
+
+/*
+ * Called to get the next deadline that has been set by the
+ * power management code.
+ * Note: a return of 0 from AICPM and this routine signifies
+ * that no deadline is set.
+ */
+uint64_t
+pmCPUGetDeadline(cpu_data_t *cpu)
+{
+    uint64_t	deadline	= 0;
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->GetDeadline != NULL)
+	deadline = (*pmDispatch->GetDeadline)(&cpu->lcpu);
+
+    return(deadline);
+}
+
+/*
+ * Called to determine if the supplied deadline or the power management
+ * deadline is sooner.  Returns which ever one is first.
+ */
+
+uint64_t
+pmCPUSetDeadline(cpu_data_t *cpu, uint64_t deadline)
+{
+   if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->SetDeadline != NULL)
+	deadline = (*pmDispatch->SetDeadline)(&cpu->lcpu, deadline);
+
+    return(deadline);
+}
+
+/*
+ * Called when a power management deadline expires.
+ */
+void
+pmCPUDeadline(cpu_data_t *cpu)
+{
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->Deadline != NULL)
+	(*pmDispatch->Deadline)(&cpu->lcpu);
+}
+
+/*
+ * Called to get a CPU out of idle.
+ */
+boolean_t
+pmCPUExitIdle(cpu_data_t *cpu)
+{
+    boolean_t		do_ipi;
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->exitIdle != NULL)
+	do_ipi = (*pmDispatch->exitIdle)(&cpu->lcpu);
+    else
+	do_ipi = TRUE;
+
+    return(do_ipi);
+}
+
+kern_return_t
+pmCPUExitHalt(int cpu)
+{
+    kern_return_t	rc	= KERN_INVALID_ARGUMENT;
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->exitHalt != NULL)
+	rc = pmDispatch->exitHalt(cpu_to_lcpu(cpu));
+
+    return(rc);
+}
+
+kern_return_t
+pmCPUExitHaltToOff(int cpu)
+{
+    kern_return_t	rc	= KERN_SUCCESS;
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->exitHaltToOff != NULL)
+	rc = pmDispatch->exitHaltToOff(cpu_to_lcpu(cpu));
+
+    return(rc);
+}
+
+/*
+ * Called to initialize the power management structures for the CPUs.
+ */
+void
+pmCPUStateInit(void)
+{
+    if (pmDispatch != NULL && pmDispatch->pmCPUStateInit != NULL)
+	(*pmDispatch->pmCPUStateInit)();
+    else
+	earlyTopology = TRUE;
+}
+
+/*
+ * Called when a CPU is being restarted after being powered off (as in S3).
+ */
+void
+pmCPUMarkRunning(cpu_data_t *cpu)
+{
+    cpu_data_t	*cpup	= current_cpu_datap();
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->markCPURunning != NULL)
+	(*pmDispatch->markCPURunning)(&cpu->lcpu);
+    else
+	cpup->lcpu.state = LCPU_RUN;
+}
+
+/*
+ * Called to get/set CPU power management state.
+ */
+int
+pmCPUControl(uint32_t cmd, void *datap)
+{
+    int		rc	= -1;
+
+    if (pmDispatch != NULL
+	&& pmDispatch->pmCPUControl != NULL)
+	rc = (*pmDispatch->pmCPUControl)(cmd, datap);
+
+    return(rc);
+}
+
+/*
+ * Called to save the timer state used by power management prior
+ * to "sleeping".
+ */
+void
+pmTimerSave(void)
+{
+    if (pmDispatch != NULL
+	&& pmDispatch->pmTimerStateSave != NULL)
+	(*pmDispatch->pmTimerStateSave)();
+}
+
+/*
+ * Called to restore the timer state used by power management after
+ * waking from "sleep".
+ */
+void
+pmTimerRestore(void)
+{
+    if (pmDispatch != NULL
+	&& pmDispatch->pmTimerStateRestore != NULL)
+	(*pmDispatch->pmTimerStateRestore)();
+}
+
+/*
+ * Set the worst-case time for the C4 to C2 transition.
+ * No longer does anything.
+ */
+void
+ml_set_maxsnoop(__unused uint32_t maxdelay)
+{
+}
+
+
+/*
+ * Get the worst-case time for the C4 to C2 transition.  Returns nanoseconds.
+ */
+unsigned
+ml_get_maxsnoop(void)
+{
+    uint64_t	max_snoop	= 0;
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->getMaxSnoop != NULL)
+	max_snoop = pmDispatch->getMaxSnoop();
+
+    return((unsigned)(max_snoop & 0xffffffff));
+}
+
+
+uint32_t
+ml_get_maxbusdelay(void)
+{
+    uint64_t	max_delay	= 0;
+
+    if (pmInitDone
+	&& pmDispatch != NULL
+	&& pmDispatch->getMaxBusDelay != NULL)
+	max_delay = pmDispatch->getMaxBusDelay();
+
+    return((uint32_t)(max_delay & 0xffffffff));
+}
+
+/*
+ * Advertise a memory access latency tolerance of "mdelay" ns
+ */
+void
+ml_set_maxbusdelay(uint32_t mdelay)
+{
+    uint64_t	maxdelay	= mdelay;
+
+    if (pmDispatch != NULL
+	&& pmDispatch->setMaxBusDelay != NULL) {
+	earlyMaxBusDelay = DELAY_UNSET;
+	pmDispatch->setMaxBusDelay(maxdelay);
+    } else
+	earlyMaxBusDelay = maxdelay;
+}
+
+uint64_t
+ml_get_maxintdelay(void)
+{
+    uint64_t	max_delay	= 0;
+
+    if (pmDispatch != NULL
+	&& pmDispatch->getMaxIntDelay != NULL)
+	max_delay = pmDispatch->getMaxIntDelay();
+
+    return(max_delay);
+}
+
+/*
+ * Set the maximum delay allowed for an interrupt.
+ */
+void
+ml_set_maxintdelay(uint64_t mdelay)
+{
+    if (pmDispatch != NULL
+	&& pmDispatch->setMaxIntDelay != NULL) {
+	earlyMaxIntDelay = DELAY_UNSET;
+	pmDispatch->setMaxIntDelay(mdelay);
+    } else
+	earlyMaxIntDelay = mdelay;
+}
+
+boolean_t
+ml_get_interrupt_prewake_applicable()
+{
+    boolean_t applicable = FALSE;
+
+    if (pmInitDone 
+	&& pmDispatch != NULL
+	&& pmDispatch->pmInterruptPrewakeApplicable != NULL)
+	applicable = pmDispatch->pmInterruptPrewakeApplicable();
+
+    return applicable;
+}
+
+/*
+ * Put a CPU into "safe" mode with respect to power.
+ *
+ * Some systems cannot operate at a continuous "normal" speed without
+ * exceeding the thermal design.  This is called per-CPU to place the
+ * CPUs into a "safe" operating mode.
+ */
+void
+pmSafeMode(x86_lcpu_t *lcpu, uint32_t flags)
+{
+    if (pmDispatch != NULL
+	&& pmDispatch->pmCPUSafeMode != NULL)
+	pmDispatch->pmCPUSafeMode(lcpu, flags);
+    else {
+	/*
+	 * Do something reasonable if the KEXT isn't present.
+	 *
+	 * We only look at the PAUSE and RESUME flags.  The other flag(s)
+	 * will not make any sense without the KEXT, so just ignore them.
+	 *
+	 * We set the CPU's state to indicate that it's halted.  If this
+	 * is the CPU we're currently running on, then spin until the
+	 * state becomes non-halted.
+	 */
+	if (flags & PM_SAFE_FL_PAUSE) {
+	    lcpu->state = LCPU_PAUSE;
+	    if (lcpu == x86_lcpu()) {
+		while (lcpu->state == LCPU_PAUSE)
+		    cpu_pause();
+	    }
+	}
+	
+	/*
+	 * Clear the halted flag for the specified CPU, that will
+	 * get it out of it's spin loop.
+	 */
+	if (flags & PM_SAFE_FL_RESUME) {
+	    lcpu->state = LCPU_RUN;
+	}
+    }
+}
+
+static uint32_t		saved_run_count = 0;
+
+void
+machine_run_count(uint32_t count)
+{
+    if (pmDispatch != NULL
+	&& pmDispatch->pmSetRunCount != NULL)
+	pmDispatch->pmSetRunCount(count);
+    else
+	saved_run_count = count;
+}
+
+processor_t
+machine_choose_processor(processor_set_t pset,
+			 processor_t preferred)
+{
+    int		startCPU;
+    int		endCPU;
+    int		preferredCPU;
+    int		chosenCPU;
+
+    if (!pmInitDone)
+	return(preferred);
+
+    if (pset == NULL) {
+	startCPU = -1;
+	endCPU = -1;
+    } else {
+	startCPU = pset->cpu_set_low;
+	endCPU = pset->cpu_set_hi;
+    }
+
+    if (preferred == NULL)
+	preferredCPU = -1;
+    else
+	preferredCPU = preferred->cpu_id;
+
+    if (pmDispatch != NULL
+	&& pmDispatch->pmChooseCPU != NULL) {
+	chosenCPU = pmDispatch->pmChooseCPU(startCPU, endCPU, preferredCPU);
+
+	if (chosenCPU == -1)
+	    return(NULL);
+	return(cpu_datap(chosenCPU)->cpu_processor);
+    }
+
+    return(preferred);
+}
+
+static int
+pmThreadGetUrgency(uint64_t *rt_period, uint64_t *rt_deadline)
+{
+	int             urgency;
+	uint64_t        arg1, arg2;
+
+	urgency = thread_get_urgency(current_processor()->next_thread, &arg1, &arg2);
+
+	if (urgency == THREAD_URGENCY_REAL_TIME) {
+		if (rt_period != NULL)
+			*rt_period = arg1;
+		
+		if (rt_deadline != NULL)
+			*rt_deadline = arg2;
+	}
+
+	return(urgency);
+}
+
+#if	DEBUG
+uint32_t	urgency_stats[64][THREAD_URGENCY_MAX];
+#endif
+
+#define		URGENCY_NOTIFICATION_ASSERT_NS (5 * 1000 * 1000)
+uint64_t	urgency_notification_assert_abstime_threshold, urgency_notification_max_recorded;
+
+void
+thread_tell_urgency(int urgency,
+    uint64_t rt_period,
+    uint64_t rt_deadline,
+    uint64_t sched_latency,
+    thread_t nthread)
+{
+	uint64_t	urgency_notification_time_start, delta;
+	boolean_t	urgency_assert = (urgency_notification_assert_abstime_threshold != 0);
+	assert(get_preemption_level() > 0 || ml_get_interrupts_enabled() == FALSE);
+#if	DEBUG
+	urgency_stats[cpu_number() % 64][urgency]++;
+#endif
+	if (!pmInitDone
+	    || pmDispatch == NULL
+	    || pmDispatch->pmThreadTellUrgency == NULL)
+		return;
+
+	SCHED_DEBUG_PLATFORM_KERNEL_DEBUG_CONSTANT(MACHDBG_CODE(DBG_MACH_SCHED,MACH_URGENCY) | DBG_FUNC_START, urgency, rt_period, rt_deadline, sched_latency, 0);
+
+	if (__improbable((urgency_assert == TRUE)))
+		urgency_notification_time_start = mach_absolute_time();
+
+	current_cpu_datap()->cpu_nthread = nthread;
+	pmDispatch->pmThreadTellUrgency(urgency, rt_period, rt_deadline);
+
+	if (__improbable((urgency_assert == TRUE))) {
+		delta = mach_absolute_time() - urgency_notification_time_start;
+
+		if (__improbable(delta > urgency_notification_max_recorded)) {
+			/* This is not synchronized, but it doesn't matter
+			 * if we (rarely) miss an event, as it is statistically
+			 * unlikely that it will never recur.
+			 */
+			urgency_notification_max_recorded = delta;
+
+			if (__improbable((delta > urgency_notification_assert_abstime_threshold) && !machine_timeout_suspended()))
+				panic("Urgency notification callout %p exceeded threshold, 0x%llx abstime units", pmDispatch->pmThreadTellUrgency, delta);
+		}
+	}
+
+	SCHED_DEBUG_PLATFORM_KERNEL_DEBUG_CONSTANT(MACHDBG_CODE(DBG_MACH_SCHED,MACH_URGENCY) | DBG_FUNC_END, urgency, rt_period, rt_deadline, 0, 0);
+}
+
+void
+machine_thread_going_on_core(__unused thread_t      new_thread,
+							 __unused int           urgency,
+							 __unused uint64_t      sched_latency)
+{
+}
+
+void
+machine_thread_going_off_core(__unused thread_t old_thread, __unused boolean_t thread_terminating)
+{
+}
+
+void
+machine_max_runnable_latency(__unused uint64_t bg_max_latency,
+							 __unused uint64_t default_max_latency,
+							 __unused uint64_t realtime_max_latency)
+{
+}
+
+void
+machine_work_interval_notify(__unused thread_t thread,
+							 __unused uint64_t work_interval_id,
+							 __unused uint64_t start_abstime,
+							 __unused uint64_t finish_abstime,
+							 __unused uint64_t deadline_abstime,
+							 __unused uint64_t next_start_abstime,
+							 __unused uint16_t urgency,
+							 __unused uint32_t flags)
+{
+}
+
+void
+active_rt_threads(boolean_t active)
+{
+    if (!pmInitDone
+	|| pmDispatch == NULL
+	|| pmDispatch->pmActiveRTThreads == NULL)
+	return;
+
+    pmDispatch->pmActiveRTThreads(active);
+}
+
+static uint32_t
+pmGetSavedRunCount(void)
+{
+    return(saved_run_count);
+}
+
+/*
+ * Returns the root of the package tree.
+ */
+x86_pkg_t *
+pmGetPkgRoot(void)
+{
+    return(x86_pkgs);
+}
+
+static boolean_t
+pmCPUGetHibernate(int cpu)
+{
+    return(cpu_datap(cpu)->cpu_hibernate);
+}
+
+processor_t
+pmLCPUtoProcessor(int lcpu)
+{
+    return(cpu_datap(lcpu)->cpu_processor);
+}
+
+static void
+pmReSyncDeadlines(int cpu)
+{
+    static boolean_t	registered	= FALSE;
+
+    if (!registered) {
+	PM_interrupt_register(&timer_resync_deadlines);
+	registered = TRUE;
+    }
+
+    if ((uint32_t)cpu == current_cpu_datap()->lcpu.cpu_num)
+	timer_resync_deadlines();
+    else
+	cpu_PM_interrupt(cpu);
+}
+
+static void
+pmSendIPI(int cpu)
+{
+    lapic_send_ipi(cpu, LAPIC_PM_INTERRUPT);
+}
+
+static void
+pmGetNanotimeInfo(pm_rtc_nanotime_t *rtc_nanotime)
+{
+	/*
+	 * Make sure that nanotime didn't change while we were reading it.
+	 */
+	do {
+		rtc_nanotime->generation = pal_rtc_nanotime_info.generation; /* must be first */
+		rtc_nanotime->tsc_base = pal_rtc_nanotime_info.tsc_base;
+		rtc_nanotime->ns_base = pal_rtc_nanotime_info.ns_base;
+		rtc_nanotime->scale = pal_rtc_nanotime_info.scale;
+		rtc_nanotime->shift = pal_rtc_nanotime_info.shift;
+	} while(pal_rtc_nanotime_info.generation != 0
+		&& rtc_nanotime->generation != pal_rtc_nanotime_info.generation);
+}
+
+uint32_t
+pmTimerQueueMigrate(int target_cpu)
+{
+    /* Call the etimer code to do this. */
+    return (target_cpu != cpu_number())
+		? timer_queue_migrate_cpu(target_cpu)
+		: 0;
+}
+
+
+/*
+ * Called by the power management kext to register itself and to get the
+ * callbacks it might need into other kernel functions.  This interface
+ * is versioned to allow for slight mis-matches between the kext and the
+ * kernel.
+ */
+void
+pmKextRegister(uint32_t version, pmDispatch_t *cpuFuncs,
+    pmCallBacks_t *callbacks)
+{
+	if (callbacks != NULL && version == PM_DISPATCH_VERSION) {
+		callbacks->setRTCPop            = setPop;
+		callbacks->resyncDeadlines      = pmReSyncDeadlines;
+		callbacks->initComplete         = pmInitComplete;
+		callbacks->GetLCPU              = pmGetLogicalCPU;
+		callbacks->GetCore              = pmGetCore;
+		callbacks->GetDie               = pmGetDie;
+		callbacks->GetPackage           = pmGetPackage;
+		callbacks->GetMyLCPU            = pmGetMyLogicalCPU;
+		callbacks->GetMyCore            = pmGetMyCore;
+		callbacks->GetMyDie             = pmGetMyDie;
+		callbacks->GetMyPackage         = pmGetMyPackage;
+		callbacks->GetPkgRoot           = pmGetPkgRoot;
+		callbacks->LockCPUTopology      = pmLockCPUTopology;
+		callbacks->GetHibernate         = pmCPUGetHibernate;
+		callbacks->LCPUtoProcessor      = pmLCPUtoProcessor;
+		callbacks->ThreadBind           = thread_bind;
+		callbacks->GetSavedRunCount     = pmGetSavedRunCount;
+		callbacks->GetNanotimeInfo	= pmGetNanotimeInfo;
+		callbacks->ThreadGetUrgency	= pmThreadGetUrgency;
+		callbacks->RTCClockAdjust	= rtc_clock_adjust;
+		callbacks->timerQueueMigrate    = pmTimerQueueMigrate;
+		callbacks->topoParms            = &topoParms;
+		callbacks->pmSendIPI		= pmSendIPI;
+		callbacks->InterruptPending	= lapic_is_interrupt_pending;
+		callbacks->IsInterrupting	= lapic_is_interrupting;
+		callbacks->InterruptStats	= lapic_interrupt_counts;
+		callbacks->DisableApicTimer	= lapic_disable_timer;
+	} else {
+		panic("Version mis-match between Kernel and CPU PM");
+	}
+
+	if (cpuFuncs != NULL) {
+		if (pmDispatch) {
+			panic("Attempt to re-register power management interface--AICPM present in xcpm mode? %p->%p", pmDispatch, cpuFuncs);
+		}
+
+		pmDispatch = cpuFuncs;
+
+		if (earlyTopology
+		    && pmDispatch->pmCPUStateInit != NULL) {
+			(*pmDispatch->pmCPUStateInit)();
+			earlyTopology = FALSE;
+		}
+
+		if (pmDispatch->pmIPIHandler != NULL) {
+			lapic_set_pm_func((i386_intr_func_t)pmDispatch->pmIPIHandler);
+		}
+	}
+}
+
+/*
+ * Unregisters the power management functions from the kext.
+ */
+void
+pmUnRegister(pmDispatch_t *cpuFuncs)
+{
+    if (cpuFuncs != NULL && pmDispatch == cpuFuncs) {
+	pmDispatch = NULL;
+    }
+}
+
+void machine_track_platform_idle(boolean_t entry) {
+	cpu_data_t		*my_cpu		= current_cpu_datap();
+
+	if (entry) {
+		(void)__sync_fetch_and_add(&my_cpu->lcpu.package->num_idle, 1);
+	}
+ 	else {
+ 		uint32_t nidle = __sync_fetch_and_sub(&my_cpu->lcpu.package->num_idle, 1);
+ 		if (nidle == topoParms.nLThreadsPerPackage) {
+ 			my_cpu->lcpu.package->package_idle_exits++;
+ 		}
+ 	}
+}
diff -Nur xnu-3247.1.106/osfmk/i386/rtclock.c xnu-3247.1.106-AnV/osfmk/i386/rtclock.c
--- xnu-3247.1.106/osfmk/i386/rtclock.c	2015-12-06 01:32:38.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/rtclock.c	2015-12-13 17:08:10.000000000 +0100
@@ -74,6 +74,7 @@
 int		rtclock_init(void);
 
 uint64_t	tsc_rebase_abs_time = 0;
+uint32_t	rtclock_boot_frequency = 0;
 
 static void	rtc_set_timescale(uint64_t cycles);
 static uint64_t	rtc_export_speed(uint64_t cycles);
@@ -231,14 +232,50 @@
 rtc_clock_stepping(__unused uint32_t new_frequency,
 		   __unused uint32_t old_frequency)
 {
-	panic("rtc_clock_stepping unsupported");
+	/* mercurysquad: Re-implemented 2009-05-24
+  	 * note that after stepping, we restore the ns base to the saved value
+  	 * but there might have been a finite interval during which the stepping
+   	 * took place, which will be unaccounted for. FIXME: use another timer
+   	 * source to correct for this
+     * AnV: Mods for 10.7 and above done
+     */
+  	pal_rtc_nanotime_t	*rntp = &pal_rtc_nanotime_info;
+   	boolean_t	istate;
+
+   	istate = ml_set_interrupts_enabled(FALSE);
+
+    rntp->ns_base = rtc_nanotime_read(); // save current ns base
+    rntp->tsc_base = rdtsc64();
+    
+    ml_set_interrupts_enabled(istate);
 }
 
 void
 rtc_clock_stepped(__unused uint32_t new_frequency,
 		  __unused uint32_t old_frequency)
 {
-	panic("rtc_clock_stepped unsupported");
+	/* mercurysquad: Re-implement 2009-05-24
+   	 * Originally written by Turbo to use 'slow' clock calculation method
+  	 * Updated to use scaled-tsc fast calculations
+     * AnV: Mods for 10.7 and above done
+     */
+   	pal_rtc_nanotime_t	*rntp = &pal_rtc_nanotime_info;
+   	uint64_t	cycles, ns_base;
+   	boolean_t	istate;
+
+   	if (rtclock_boot_frequency == 0) /* first step since boot time */
+   		rtclock_boot_frequency = old_frequency;
+
+   	cycles = (new_frequency * tscFreq / rtclock_boot_frequency);
+   	ns_base = rntp->ns_base;
+
+   	istate = ml_set_interrupts_enabled(FALSE);
+
+   	rntp->scale = (uint32_t)(((uint64_t)NSEC_PER_SEC << 32) / (cycles  * RTCLOCK_SCALE_UP_BY));
+   	rntp->shift = 32; // just to be safe, though this was already set at bootup
+   	rtc_nanotime_init(ns_base);
+
+   	ml_set_interrupts_enabled(istate);
 }
 
 /*
@@ -265,6 +302,306 @@
 	rtc_nanotime_init(base);
 }
 
+////////////////////////////////////////////////////////////////////////////////////
+/* Addon From Bronya 10.10 Ver
+ *
+ * Reference Relevant information:
+ * http://forge.voodooprojects.org/svn/chameleon/trunk/i386/libsaio/cpu.c
+ * http://forge.voodooprojects.org/svn/chameleon/trunk/i386/libsaio/cpu.h
+ *
+ * Enable or disable timer 2.
+ * Port 0x61 controls timer 2:
+ *   bit 0 gates the clock,
+ *   bit 1 gates output to speaker.
+ *
+ * Enable or disable timer 2.
+ * Port 0x61 controls timer 2:
+ *   bit 0 gates the clock,
+ *   bit 1 gates output to speaker.
+ */
+
+#define CLKNUM      1193182     /* formerly 1193167 */
+
+uint64_t rdtsc32(void);
+uint64_t rdtsc32()
+{
+    unsigned a, d;
+    __asm__ __volatile__("rdtsc" : "=a" (a), "=d" (d));
+    return ((uint64_t)a) | (((uint64_t)d) << 32);
+}
+
+inline static void enable_PIT2(void)
+{
+    asm volatile(
+                 " inb   $0x61,%%al      \n\t"
+                 " and   $0xFC,%%al       \n\t"
+                 " or    $1,%%al         \n\t"
+                 " outb  %%al,$0x61      \n\t"
+                 : : : "%al" );
+}
+
+inline static void disable_PIT2(void)
+{
+    asm volatile(
+                 " inb   $0x61,%%al      \n\t"
+                 " and   $0xFC,%%al      \n\t"
+                 " outb  %%al,$0x61      \n\t"
+                 : : : "%al" );
+}
+
+inline static void set_PIT2(int value)
+{
+    /*
+     * First, tell the clock we are going to write 16 bits to the counter
+     *   and enable one-shot mode (command 0xB8 to port 0x43)
+     * Then write the two bytes into the PIT2 clock register (port 0x42).
+     * Loop until the value is "realized" in the clock,
+     * this happens on the next tick.
+     */
+    asm volatile(
+                 " movb  $0xB8,%%al      \n\t"
+                 " outb	%%al,$0x43	\n\t"
+                 " movb	%%dl,%%al	\n\t"
+                 " outb	%%al,$0x42	\n\t"
+                 " movb	%%dh,%%al	\n\t"
+                 " outb	%%al,$0x42	\n"
+                 "1:	  inb	$0x42,%%al	\n\t"
+                 " inb	$0x42,%%al	\n\t"
+                 " cmp	%%al,%%dh	\n\t"
+                 " jne	1b"
+                 : : "d"(value) : "%al");
+}
+
+static inline  uint32_t get_PIT2(uint16_t *value)
+{
+    register uint32_t   result;
+    /*
+     * This routine first latches the time (command 0x80 to port 0x43),
+     * then gets the time stamp so we know how long the read will take later.
+     * Read (from port 0x42) and return the current value of the timer.
+     */
+#ifdef  __x86_64__
+    asm volatile(
+                 " xorq  %%rcx,%%rcx     \n\t"
+                 " movb  $0x80,%%al      \n\t"
+                 " outb  %%al,$0x43      \n\t"
+                 " rdtsc                 \n\t"
+                 " pushq  %%rax          \n\t"
+                 " inb   $0x42,%%al      \n\t"
+                 " movb  %%al,%%cl       \n\t"
+                 " inb   $0x42,%%al      \n\t"
+                 " movb  %%al,%%ch       \n\t"
+                 " popq  %%rax   "
+                 : "=A"(result), "=c"(*value));
+
+#else /* __i386__ */
+    asm volatile(
+                 " xorl  %%ecx,%%ecx     \n\t"
+                 " movb  $0x80,%%al      \n\t"
+                 " outb  %%al,$0x43      \n\t"
+                 " rdtsc                 \n\t"
+                 " pushl %%eax           \n\t"
+                 " inb   $0x42,%%al      \n\t"
+                 " movb  %%al,%%cl       \n\t"
+                 " inb   $0x42,%%al      \n\t"
+                 " movb  %%al,%%ch       \n\t"
+                 " popl  %%eax   "
+                 : "=A"(result), "=c"(*value));
+#endif
+    
+    return result;
+}
+
+
+inline uint32_t  clockspeed_rdtsc(void);
+inline uint32_t  clockspeed_rdtsc()
+{
+    uint32_t out;
+#ifdef  __x86_64__
+    __asm__ volatile (
+                      "rdtsc\n"
+                      "shl $32,%%rdx\n"
+                      "or %%rdx,%%rax\n"
+                      : "=a" (out)
+                      :
+                      : "%rdx"
+                      );
+#else /* __i386__ */
+    __asm__ volatile (
+                      "rdtsc\n"
+                      "shl $32,%%edx\n"
+                      "or %%edx,%%eax\n"
+                      : "=a" (out)
+                      :
+                      : "%edx"
+                      );
+#endif
+    return out;
+}
+
+/* timeRDTSC()
+ * This routine sets up PIT counter 2 to count down 1/20 of a second.
+ * It pauses until the value is latched in the counter
+ * and then reads the time stamp counter to return to the caller.
+ */
+uint64_t timeRDTSC(void)
+{
+    int		attempts = 0;
+    uint16_t	latchTime;
+    uint64_t	saveTime,intermediate;
+    uint16_t timerValue,lastValue;
+    boolean_t   int_enabled;
+    /*
+     * Table of correction factors to account for
+     *   - timer counter quantization errors, and
+     *   - undercounts 0..5
+     */
+#define	SAMPLE_CLKS_EXACT	(((double) CLKNUM) / 20.0)
+#define	SAMPLE_CLKS_INT		((int) CLKNUM / 20)
+#define SAMPLE_NSECS		(2000000000LL)
+#define SAMPLE_MULTIPLIER	(((double)SAMPLE_NSECS)*SAMPLE_CLKS_EXACT)
+#define ROUND64(x)		((uint64_t)((x) + 0.5))
+    uint64_t	scale[6] = {
+        ROUND64(SAMPLE_MULTIPLIER/(double)(SAMPLE_CLKS_INT-0)),
+        ROUND64(SAMPLE_MULTIPLIER/(double)(SAMPLE_CLKS_INT-1)),
+        ROUND64(SAMPLE_MULTIPLIER/(double)(SAMPLE_CLKS_INT-2)),
+        ROUND64(SAMPLE_MULTIPLIER/(double)(SAMPLE_CLKS_INT-3)),
+        ROUND64(SAMPLE_MULTIPLIER/(double)(SAMPLE_CLKS_INT-4)),
+        ROUND64(SAMPLE_MULTIPLIER/(double)(SAMPLE_CLKS_INT-5))
+    };
+    
+    int_enabled = ml_set_interrupts_enabled(FALSE);
+    
+restart:
+    if (attempts >= 2)
+        // panic("timeRDTSC() calibation failed with %d attempts\n", attempts);
+        attempts++;
+    enable_PIT2();      // turn on PIT2
+    set_PIT2(0);	// reset timer 2 to be zero
+    latchTime = clockspeed_rdtsc();	// get the time stamp to time
+    latchTime = get_PIT2(&timerValue) - latchTime; // time how long this takes
+    set_PIT2(SAMPLE_CLKS_INT);	// set up the timer for (almost) 1/20th a second
+    saveTime = clockspeed_rdtsc();	// now time how long a 20th a second is...
+    get_PIT2(&lastValue);
+    //printf("lastValue   %u\n",lastValue);
+    get_PIT2(&lastValue);	// read twice, first value may be unreliable
+    //printf("lastValue   %u\n",lastValue);
+    printf("");
+    do {
+        intermediate = get_PIT2(&timerValue);
+        //printf("timerValue   %d\n",timerValue);
+        if (timerValue > lastValue)
+        {
+            set_PIT2(0);
+            disable_PIT2();
+            goto restart;
+        }
+        lastValue = timerValue;
+    } while (timerValue > 5);
+    kprintf("timerValue   %u\n",timerValue);
+    kprintf("intermediate 0x%016llx\n",intermediate);
+    kprintf("saveTime     0x%016llx\n",saveTime);
+    
+    intermediate -= saveTime;		// raw count for about 1/20 second
+    intermediate *= scale[timerValue];	// rescale measured time spent
+    intermediate /= SAMPLE_NSECS;	// so its exactly 1/20 a second
+    intermediate += latchTime;		// add on our save fudge
+    
+    set_PIT2(0);			// reset timer 2 to be zero
+    disable_PIT2();     		// turn off PIT 2
+    
+    ml_set_interrupts_enabled(int_enabled);
+    return intermediate;
+}
+
+
+static uint64_t	rtc_set_cyc_per_sec(uint64_t cycles);
+#define RTC_FAST_DENOM	0xFFFFFFFF
+
+inline static uint32_t create_mul_quant_GHZ(int shift, uint32_t quant)
+{
+    return (uint32_t)((((uint64_t)NSEC_PER_SEC/20) << shift) / quant);
+}
+
+struct	{
+    mach_timespec_t			calend_offset;
+    boolean_t			calend_is_set;
+    
+    int64_t				calend_adjtotal;
+    int32_t				calend_adjdelta;
+    
+    uint32_t			boottime;
+    
+    mach_timebase_info_data_t	timebase_const;
+    
+    decl_simple_lock_data(,lock)	/* real-time clock device lock */
+} rtclock;
+
+uint32_t		rtc_quant_shift;	/* clock to nanos right shift */
+uint32_t		rtc_quant_scale;	/* clock to nanos multiplier */
+uint64_t		rtc_cyc_per_sec;	/* processor cycles per sec */
+uint64_t		rtc_cycle_count;	/* clocks in 1/20th second */
+//uint64_t cpuFreq;
+
+static uint64_t rtc_set_cyc_per_sec(uint64_t cycles)
+{
+    
+    if (cycles > (NSEC_PER_SEC/20))
+    {
+        // we can use just a "fast" multiply to get nanos
+        rtc_quant_shift = 32;
+        rtc_quant_scale = create_mul_quant_GHZ(rtc_quant_shift, (uint32_t)cycles);
+        rtclock.timebase_const.numer = rtc_quant_scale; // timeRDTSC is 1/20
+        rtclock.timebase_const.denom = (uint32_t)RTC_FAST_DENOM;
+    }
+    else
+    {
+        rtc_quant_shift = 26;
+        rtc_quant_scale = create_mul_quant_GHZ(rtc_quant_shift, (uint32_t)cycles);
+        rtclock.timebase_const.numer = NSEC_PER_SEC/20; // timeRDTSC is 1/20
+        rtclock.timebase_const.denom = (uint32_t)cycles;
+    }
+
+    rtc_cyc_per_sec = cycles*20;	// multiply it by 20 and we are done..
+    // BUT we also want to calculate...
+    
+    cycles = ((rtc_cyc_per_sec + (UI_CPUFREQ_ROUNDING_FACTOR/2))
+              / UI_CPUFREQ_ROUNDING_FACTOR)
+    * UI_CPUFREQ_ROUNDING_FACTOR;
+    
+    /*
+     * Set current measured speed.
+     */
+    if (cycles >= 0x100000000ULL)
+    {
+        gPEClockFrequencyInfo.cpu_clock_rate_hz = 0xFFFFFFFFUL;
+    }
+    else
+    {
+        gPEClockFrequencyInfo.cpu_clock_rate_hz = (unsigned long)cycles;
+    }
+
+    gPEClockFrequencyInfo.cpu_frequency_hz = cycles;
+    
+    printf("[RTCLOCK] frequency %llu (%llu)\n", cycles, rtc_cyc_per_sec);
+    return(rtc_cyc_per_sec);
+}
+
+uint64_t  cpuFreq()
+{
+    uint64_t cycles64;
+    rtc_cycle_count = timeRDTSC();
+    //printf("timetsc %llu\n",rtc_cycle_count);
+    cycles64 = rtc_set_cyc_per_sec(rtc_cycle_count);
+    return cycles64;
+}
+
+uint64_t cpuRealFreq()
+{
+    return cpuFreq();
+}
+////////////////////////////////////////////////////////////////////////////////////
+
 /*
  * rtclock_early_init() is called very early at boot to
  * establish mach_absolute_time() and set it to zero.
@@ -537,6 +874,9 @@
 	uint64_t		nanoseconds,
 	uint64_t		*result)
 {
+    if (nanoseconds == ~(0ULL))
+    *result = 2ULL;
+    else
 	*result = nanoseconds;
 }
 
diff -Nur xnu-3247.1.106/osfmk/i386/rtclock.c.orig xnu-3247.1.106-AnV/osfmk/i386/rtclock.c.orig
--- xnu-3247.1.106/osfmk/i386/rtclock.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/rtclock.c.orig	2015-12-06 01:32:38.000000000 +0100
@@ -0,0 +1,552 @@
+/*
+ * Copyright (c) 2000-2012 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+
+/*
+ *	File:		i386/rtclock.c
+ *	Purpose:	Routines for handling the machine dependent
+ *			real-time clock. Historically, this clock is
+ *			generated by the Intel 8254 Programmable Interval
+ *			Timer, but local apic timers are now used for
+ *			this purpose with the master time reference being
+ *			the cpu clock counted by the timestamp MSR.
+ */
+
+
+#include <mach/mach_types.h>
+
+#include <kern/cpu_data.h>
+#include <kern/cpu_number.h>
+#include <kern/clock.h>
+#include <kern/host_notify.h>
+#include <kern/macro_help.h>
+#include <kern/misc_protos.h>
+#include <kern/spl.h>
+#include <kern/assert.h>
+#include <kern/timer_queue.h>
+#include <mach/vm_prot.h>
+#include <vm/pmap.h>
+#include <vm/vm_kern.h>		/* for kernel_map */
+#include <architecture/i386/pio.h>
+#include <i386/machine_cpu.h>
+#include <i386/cpuid.h>
+#include <i386/cpu_threads.h>
+#include <i386/mp.h>
+#include <i386/machine_routines.h>
+#include <i386/pal_routines.h>
+#include <i386/proc_reg.h>
+#include <i386/misc_protos.h>
+#include <pexpert/pexpert.h>
+#include <machine/limits.h>
+#include <machine/commpage.h>
+#include <sys/kdebug.h>
+#include <i386/tsc.h>
+#include <i386/rtclock_protos.h>
+#define UI_CPUFREQ_ROUNDING_FACTOR	10000000
+
+int		rtclock_init(void);
+
+uint64_t	tsc_rebase_abs_time = 0;
+
+static void	rtc_set_timescale(uint64_t cycles);
+static uint64_t	rtc_export_speed(uint64_t cycles);
+
+void
+rtc_timer_start(void)
+{
+	/*
+	 * Force a complete re-evaluation of timer deadlines.
+	 */
+	x86_lcpu()->rtcDeadline = EndOfAllTime;
+	timer_resync_deadlines();
+}
+
+static inline uint32_t
+_absolutetime_to_microtime(uint64_t abstime, clock_sec_t *secs, clock_usec_t *microsecs)
+{
+	uint32_t remain;
+	*secs = abstime / (uint64_t)NSEC_PER_SEC;
+	remain = (uint32_t)(abstime % (uint64_t)NSEC_PER_SEC);
+	*microsecs = remain / NSEC_PER_USEC;
+	return remain;
+}
+
+static inline void
+_absolutetime_to_nanotime(uint64_t abstime, clock_sec_t *secs, clock_usec_t *nanosecs)
+{
+	*secs = abstime / (uint64_t)NSEC_PER_SEC;
+	*nanosecs = (clock_usec_t)(abstime % (uint64_t)NSEC_PER_SEC);
+}
+
+/*
+ * Nanotime/mach_absolutime_time
+ * -----------------------------
+ * The timestamp counter (TSC) - which counts cpu clock cycles and can be read
+ * efficiently by the kernel and in userspace - is the reference for all timing.
+ * The cpu clock rate is platform-dependent and may stop or be reset when the
+ * processor is napped/slept.  As a result, nanotime is the software abstraction
+ * used to maintain a monotonic clock, adjusted from an outside reference as needed.
+ *
+ * The kernel maintains nanotime information recording:
+ * 	- the ratio of tsc to nanoseconds
+ *	  with this ratio expressed as a 32-bit scale and shift
+ *	  (power of 2 divider);
+ *	- { tsc_base, ns_base } pair of corresponding timestamps.
+ *
+ * The tuple {tsc_base, ns_base, scale, shift} is exported in the commpage 
+ * for the userspace nanotime routine to read.
+ *
+ * All of the routines which update the nanotime data are non-reentrant.  This must
+ * be guaranteed by the caller.
+ */
+static inline void
+rtc_nanotime_set_commpage(pal_rtc_nanotime_t *rntp)
+{
+	commpage_set_nanotime(rntp->tsc_base, rntp->ns_base, rntp->scale, rntp->shift);
+}
+
+/*
+ * rtc_nanotime_init:
+ *
+ * Intialize the nanotime info from the base time.
+ */
+static inline void
+_rtc_nanotime_init(pal_rtc_nanotime_t *rntp, uint64_t base)
+{
+	uint64_t	tsc = rdtsc64();
+
+	_pal_rtc_nanotime_store(tsc, base, rntp->scale, rntp->shift, rntp);
+}
+
+static void
+rtc_nanotime_init(uint64_t base)
+{
+	_rtc_nanotime_init(&pal_rtc_nanotime_info, base);
+	rtc_nanotime_set_commpage(&pal_rtc_nanotime_info);
+}
+
+/*
+ * rtc_nanotime_init_commpage:
+ *
+ * Call back from the commpage initialization to
+ * cause the commpage data to be filled in once the
+ * commpages have been created.
+ */
+void
+rtc_nanotime_init_commpage(void)
+{
+	spl_t			s = splclock();
+
+	rtc_nanotime_set_commpage(&pal_rtc_nanotime_info);
+	splx(s);
+}
+
+/*
+ * rtc_nanotime_read:
+ *
+ * Returns the current nanotime value, accessable from any
+ * context.
+ */
+static inline uint64_t
+rtc_nanotime_read(void)
+{
+	return	_rtc_nanotime_read(&pal_rtc_nanotime_info);
+}
+
+/*
+ * rtc_clock_napped:
+ *
+ * Invoked from power management when we exit from a low C-State (>= C4)
+ * and the TSC has stopped counting.  The nanotime data is updated according
+ * to the provided value which represents the new value for nanotime.
+ */
+void
+rtc_clock_napped(uint64_t base, uint64_t tsc_base)
+{
+	pal_rtc_nanotime_t	*rntp = &pal_rtc_nanotime_info;
+	uint64_t	oldnsecs;
+	uint64_t	newnsecs;
+	uint64_t	tsc;
+
+	assert(!ml_get_interrupts_enabled());
+	tsc = rdtsc64();
+	oldnsecs = rntp->ns_base + _rtc_tsc_to_nanoseconds(tsc - rntp->tsc_base, rntp);
+	newnsecs = base + _rtc_tsc_to_nanoseconds(tsc - tsc_base, rntp);
+	
+	/*
+	 * Only update the base values if time using the new base values
+	 * is later than the time using the old base values.
+	 */
+	if (oldnsecs < newnsecs) {
+	    _pal_rtc_nanotime_store(tsc_base, base, rntp->scale, rntp->shift, rntp);
+	    rtc_nanotime_set_commpage(rntp);
+	}
+}
+
+/*
+ * Invoked from power management to correct the SFLM TSC entry drift problem:
+ * a small delta is added to the tsc_base.  This is equivalent to nudgin time
+ * backwards.  We require this to be on the order of a TSC quantum which won't
+ * cause callers of mach_absolute_time() to see time going backwards!
+ */
+void
+rtc_clock_adjust(uint64_t tsc_base_delta)
+{
+    pal_rtc_nanotime_t	*rntp = &pal_rtc_nanotime_info;
+
+    assert(!ml_get_interrupts_enabled());
+    assert(tsc_base_delta < 100ULL);	/* i.e. it's small */
+    _rtc_nanotime_adjust(tsc_base_delta, rntp);
+    rtc_nanotime_set_commpage(rntp);
+}
+
+void
+rtc_clock_stepping(__unused uint32_t new_frequency,
+		   __unused uint32_t old_frequency)
+{
+	panic("rtc_clock_stepping unsupported");
+}
+
+void
+rtc_clock_stepped(__unused uint32_t new_frequency,
+		  __unused uint32_t old_frequency)
+{
+	panic("rtc_clock_stepped unsupported");
+}
+
+/*
+ * rtc_sleep_wakeup:
+ *
+ * Invoked from power management when we have awoken from a sleep (S3)
+ * and the TSC has been reset, or from Deep Idle (S0) sleep when the TSC
+ * has progressed.  The nanotime data is updated based on the passed-in value.
+ *
+ * The caller must guarantee non-reentrancy.
+ */
+void
+rtc_sleep_wakeup(
+	uint64_t		base)
+{
+    	/* Set fixed configuration for lapic timers */
+	rtc_timer->rtc_config();
+
+	/*
+	 * Reset nanotime.
+	 * The timestamp counter will have been reset
+	 * but nanotime (uptime) marches onward.
+	 */
+	rtc_nanotime_init(base);
+}
+
+/*
+ * rtclock_early_init() is called very early at boot to
+ * establish mach_absolute_time() and set it to zero.
+ */
+void
+rtclock_early_init(void)
+{
+	assert(tscFreq);
+	rtc_set_timescale(tscFreq);
+}
+
+/*
+ * Initialize the real-time clock device.
+ * In addition, various variables used to support the clock are initialized.
+ */
+int
+rtclock_init(void)
+{
+	uint64_t	cycles;
+
+	assert(!ml_get_interrupts_enabled());
+
+	if (cpu_number() == master_cpu) {
+
+		assert(tscFreq);
+
+		/*
+		 * Adjust and set the exported cpu speed.
+		 */
+		cycles = rtc_export_speed(tscFreq);
+
+		/*
+		 * Set min/max to actual.
+		 * ACPI may update these later if speed-stepping is detected.
+		 */
+		gPEClockFrequencyInfo.cpu_frequency_min_hz = cycles;
+		gPEClockFrequencyInfo.cpu_frequency_max_hz = cycles;
+
+		rtc_timer_init();
+		clock_timebase_init();
+		ml_init_lock_timeout();
+		ml_init_delay_spin_threshold(10);
+	}
+
+    	/* Set fixed configuration for lapic timers */
+	rtc_timer->rtc_config();
+	rtc_timer_start();
+
+	return (1);
+}
+
+// utility routine 
+// Code to calculate how many processor cycles are in a second...
+
+static void
+rtc_set_timescale(uint64_t cycles)
+{
+	pal_rtc_nanotime_t	*rntp = &pal_rtc_nanotime_info;
+	uint32_t    shift = 0;
+    
+	/* the "scale" factor will overflow unless cycles>SLOW_TSC_THRESHOLD */
+    
+	while ( cycles <= SLOW_TSC_THRESHOLD) {
+		shift++;
+		cycles <<= 1;
+	}
+	
+	rntp->scale = (uint32_t)(((uint64_t)NSEC_PER_SEC << 32) / cycles);
+
+	rntp->shift = shift;
+
+	/*
+	 * On some platforms, the TSC is not reset at warm boot. But the
+	 * rebase time must be relative to the current boot so we can't use
+	 * mach_absolute_time(). Instead, we convert the TSC delta since boot
+	 * to nanoseconds.
+	 */
+	if (tsc_rebase_abs_time == 0)
+		tsc_rebase_abs_time = _rtc_tsc_to_nanoseconds(
+						rdtsc64() - tsc_at_boot, rntp);
+
+	rtc_nanotime_init(0);
+}
+
+static uint64_t
+rtc_export_speed(uint64_t cyc_per_sec)
+{
+	pal_rtc_nanotime_t	*rntp = &pal_rtc_nanotime_info;
+	uint64_t	cycles;
+
+	if (rntp->shift != 0 )
+		printf("Slow TSC, rtc_nanotime.shift == %d\n", rntp->shift);
+    
+	/* Round: */
+        cycles = ((cyc_per_sec + (UI_CPUFREQ_ROUNDING_FACTOR/2))
+			/ UI_CPUFREQ_ROUNDING_FACTOR)
+				* UI_CPUFREQ_ROUNDING_FACTOR;
+
+	/*
+	 * Set current measured speed.
+	 */
+        if (cycles >= 0x100000000ULL) {
+            gPEClockFrequencyInfo.cpu_clock_rate_hz = 0xFFFFFFFFUL;
+        } else {
+            gPEClockFrequencyInfo.cpu_clock_rate_hz = (unsigned long)cycles;
+        }
+        gPEClockFrequencyInfo.cpu_frequency_hz = cycles;
+
+	kprintf("[RTCLOCK] frequency %llu (%llu)\n", cycles, cyc_per_sec);
+	return(cycles);
+}
+
+void
+clock_get_system_microtime(
+	clock_sec_t			*secs,
+	clock_usec_t		*microsecs)
+{
+	uint64_t	now = rtc_nanotime_read();
+
+	_absolutetime_to_microtime(now, secs, microsecs);
+}
+
+void
+clock_get_system_nanotime(
+	clock_sec_t			*secs,
+	clock_nsec_t		*nanosecs)
+{
+	uint64_t	now = rtc_nanotime_read();
+
+	_absolutetime_to_nanotime(now, secs, nanosecs);
+}
+
+void
+clock_gettimeofday_set_commpage(
+	uint64_t				abstime,
+	uint64_t				epoch,
+	uint64_t				offset,
+	clock_sec_t				*secs,
+	clock_usec_t			*microsecs)
+{
+	uint64_t	now = abstime + offset;
+	uint32_t	remain;
+
+	remain = _absolutetime_to_microtime(now, secs, microsecs);
+
+	*secs += (clock_sec_t)epoch;
+
+	commpage_set_timestamp(abstime - remain, *secs);
+}
+
+void
+clock_timebase_info(
+	mach_timebase_info_t	info)
+{
+	info->numer = info->denom =  1;
+}	
+
+/*
+ * Real-time clock device interrupt.
+ */
+void
+rtclock_intr(
+	x86_saved_state_t	*tregs)
+{
+        uint64_t	rip;
+	boolean_t	user_mode = FALSE;
+
+	assert(get_preemption_level() > 0);
+	assert(!ml_get_interrupts_enabled());
+
+	if (is_saved_state64(tregs) == TRUE) {
+	        x86_saved_state64_t	*regs;
+		  
+		regs = saved_state64(tregs);
+
+		if (regs->isf.cs & 0x03)
+			user_mode = TRUE;
+		rip = regs->isf.rip;
+	} else {
+	        x86_saved_state32_t	*regs;
+
+		regs = saved_state32(tregs);
+
+		if (regs->cs & 0x03)
+		        user_mode = TRUE;
+		rip = regs->eip;
+	}
+
+	/* call the generic etimer */
+	timer_intr(user_mode, rip);
+}
+
+
+/*
+ *	Request timer pop from the hardware 
+ */
+
+uint64_t
+setPop(uint64_t time)
+{
+	uint64_t	now;
+	uint64_t	pop;
+
+	/* 0 and EndOfAllTime are special-cases for "clear the timer" */
+	if (time == 0 || time == EndOfAllTime ) {
+		time = EndOfAllTime;
+		now = 0;
+		pop = rtc_timer->rtc_set(0, 0);
+	} else {
+		now = rtc_nanotime_read();	/* The time in nanoseconds */
+		pop = rtc_timer->rtc_set(time, now);
+	}
+
+	/* Record requested and actual deadlines set */
+	x86_lcpu()->rtcDeadline = time;
+	x86_lcpu()->rtcPop	= pop;
+
+	return pop - now;
+}
+
+uint64_t
+mach_absolute_time(void)
+{
+	return rtc_nanotime_read();
+}
+
+uint64_t
+mach_approximate_time(void)
+{
+	return rtc_nanotime_read();
+}
+
+void
+clock_interval_to_absolutetime_interval(
+	uint32_t		interval,
+	uint32_t		scale_factor,
+	uint64_t		*result)
+{
+	*result = (uint64_t)interval * scale_factor;
+}
+
+void
+absolutetime_to_microtime(
+	uint64_t			abstime,
+	clock_sec_t			*secs,
+	clock_usec_t		*microsecs)
+{
+	_absolutetime_to_microtime(abstime, secs, microsecs);
+}
+
+void
+nanotime_to_absolutetime(
+	clock_sec_t			secs,
+	clock_nsec_t		nanosecs,
+	uint64_t			*result)
+{
+	*result = ((uint64_t)secs * NSEC_PER_SEC) + nanosecs;
+}
+
+void
+absolutetime_to_nanoseconds(
+	uint64_t		abstime,
+	uint64_t		*result)
+{
+	*result = abstime;
+}
+
+void
+nanoseconds_to_absolutetime(
+	uint64_t		nanoseconds,
+	uint64_t		*result)
+{
+	*result = nanoseconds;
+}
+
+void
+machine_delay_until(
+	uint64_t interval,
+	uint64_t		deadline)
+{
+	(void)interval;
+	while (mach_absolute_time() < deadline) {
+		cpu_pause();
+	} 
+}
diff -Nur xnu-3247.1.106/osfmk/i386/rtclock_protos.h xnu-3247.1.106-AnV/osfmk/i386/rtclock_protos.h
--- xnu-3247.1.106/osfmk/i386/rtclock_protos.h	2015-12-06 01:32:38.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/rtclock_protos.h	2015-12-13 17:08:10.000000000 +0100
@@ -40,6 +40,14 @@
 #ifndef _I386_RTCLOCK_PROTOS_H_
 #define _I386_RTCLOCK_PROTOS_H_
 
+extern uint64_t cpuRealFreq(void);
+extern uint64_t timeRDTSC(void);
+extern uint64_t cpuFreq(void);
+
+#define	RTCLOCK_SCALE_UP_BITS	4		/* mercurysquad: Refer to rtclock.c */
+#define	RTCLOCK_SCALE_UP_BY	(1 << RTCLOCK_SCALE_UP_BITS)
+#define	RTCLOCK_SCALE_UP_MASK	(RTCLOCK_SCALE_UP_BY - 1)
+
 typedef struct pal_rtc_nanotime pal_rtc_nanotime_t;
 extern uint64_t tsc_rebase_abs_time;
 
diff -Nur xnu-3247.1.106/osfmk/i386/trap.c xnu-3247.1.106-AnV/osfmk/i386/trap.c
--- xnu-3247.1.106/osfmk/i386/trap.c	2015-12-06 01:32:39.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/trap.c	2015-12-13 17:27:36.000000000 +0100
@@ -116,8 +116,11 @@
  * Forward declarations
  */
 static void user_page_fault_continue(kern_return_t kret);
-static void panic_trap(x86_saved_state64_t *saved_state, uint32_t pl);
-static void set_recovery_ip(x86_saved_state64_t *saved_state, vm_offset_t ip);
+void panic_trap(x86_saved_state64_t *saved_state, uint32_t pl);
+void set_recovery_ip(x86_saved_state64_t *saved_state, vm_offset_t ip);
+
+extern unsigned char opemu_ktrap(x86_saved_state_t *state);
+extern void opemu_utrap(x86_saved_state_t *state);
 
 volatile perfCallback perfTrapHook = NULL; /* Pointer to CHUD trap hook routine */
 
@@ -764,6 +767,9 @@
 		 *
 		 * fall through...
 		 */
+		case T_INVALID_OPCODE:
+			opemu_ktrap(state);
+			return;
 	    default:
 		/*
 		 * Exception 15 is reserved but some chips may generate it
@@ -795,7 +801,7 @@
 }
 
 
-static void
+void
 set_recovery_ip(x86_saved_state64_t  *saved_state, vm_offset_t ip)
 {
         saved_state->isf.rip = ip;
@@ -804,7 +810,7 @@
 
 
 
-static void
+void
 panic_trap(x86_saved_state64_t *regs, uint32_t pl)
 {
 	const char	*trapname = "Unknown";
@@ -1031,6 +1037,7 @@
 		break;
 
 	    case T_INVALID_OPCODE:
+	    opemu_utrap(saved_state);
 		exc = EXC_BAD_INSTRUCTION;
 		code = EXC_I386_INVOP;
 		break;
diff -Nur xnu-3247.1.106/osfmk/i386/tsc.c xnu-3247.1.106-AnV/osfmk/i386/tsc.c
--- xnu-3247.1.106/osfmk/i386/tsc.c	2015-12-06 01:32:39.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/tsc.c	2015-12-13 18:34:43.000000000 +0100
@@ -63,6 +63,10 @@
 #include <sys/kdebug.h>
 #include <pexpert/device_tree.h>
 
+static const char	FSB_Frequency_prop[] = "FSBFrequency";
+static const char   	FSB_CPUFrequency_prop[] = "CPUFrequency";
+static const char	TSC_at_boot_prop[]   = "InitialTSC";
+
 uint64_t	busFCvtt2n = 0;
 uint64_t	busFCvtn2t = 0;
 uint64_t	tscFreq = 0;
@@ -130,6 +134,171 @@
 	return frequency;
 }
 
+static uint64_t
+EFI_CPU_Frequency(void)
+{
+   	uint64_t	frequency = 0;
+    DTEntry		entry;
+    void		*value;
+    unsigned int	size;
+
+    uint8_t  dummyvar;
+
+    if (PE_parse_boot_argn("-cpuEFI", &dummyvar, sizeof(dummyvar)))
+    {
+        if (DTLookupEntry(0, "/efi/platform", &entry) != kSuccess)
+        {
+            kprintf("EFI_CPU_Frequency: didn't find /efi/platform\n");
+            return 0;
+        }
+        if (DTGetProperty(entry,FSB_CPUFrequency_prop,&value,&size) != kSuccess)
+        {
+            kprintf("EFI_CPU_Frequency: property %s not found\n", FSB_Frequency_prop);
+            return 0;
+        }
+        if (size == sizeof(uint64_t))
+        {
+            frequency = *(uint64_t *) value;
+            kprintf("EFI_CPU_Frequency: read %s value: %llu\n", FSB_Frequency_prop, frequency);
+            if (!(10*Mega < frequency && frequency < 50*Giga))
+            {
+                kprintf("EFI_Fake_MSR: value out of range\n");
+                frequency = 0;
+            }
+        }
+        else
+        {
+            kprintf("EFI_CPU_Frequency: unexpected size %d\n", size);
+        }
+        return frequency;
+    }
+
+    if(IsAmdCPU())
+    {
+        frequency = cpuRealFreq();
+    }
+    
+    return frequency;
+
+}
+
+/*
+ * Convert the cpu frequency info into a 'fake' MSR198h in Intel format
+ */
+static uint64_t
+getFakeMSR(uint64_t frequency, uint64_t bFreq)
+{
+    uint64_t fakeMSR = 0ull;
+    uint64_t multi = 0;
+
+    if (frequency == 0 || bFreq == 0)
+        return 0;
+
+    multi = frequency / (bFreq / 1000); // = multi*1000
+    // divide by 1000, rounding up if it was x.75 or more
+    // Example: 12900 will get rounded to 13150/1000 = 13
+    //          but 12480 will be 12730/1000 = 12
+    fakeMSR = (multi + 250) / 1000;
+    fakeMSR <<= 40; // push multiplier into bits 44 to 40
+
+    // If fractional part was within (0.25, 0.75), set N/2
+    if ((multi % 1000 > 250) && (multi % 1000 < 750))
+        fakeMSR |= (1ull << 46);
+
+    return fakeMSR;
+}
+
+static uint64_t GetCpuMultiplayer()
+{
+    uint64_t prfsts;
+    uint64_t cpuFreq;
+    busFreq = 200 * Mega;
+    // prfsts		= rdmsr64(AMD_COFVID_STS);
+    cpuFreq = EFI_CPU_Frequency();
+    prfsts = getFakeMSR(cpuFreq, busFreq);
+    tscGranularity = bitfield(prfsts, 44, 40);
+
+    return tscGranularity;
+}
+
+static uint64_t
+EFI_FSB_frequency(void)
+{
+	uint64_t	frequency = 0;
+	DTEntry		entry;
+	void		*value;
+	unsigned int	size;
+
+	if (DTLookupEntry(0, "/efi/platform", &entry) != kSuccess) {
+		kprintf("EFI_FSB_frequency: didn't find /efi/platform\n");
+		return 0;
+	}
+	if (DTGetProperty(entry,FSB_Frequency_prop,&value,&size) != kSuccess) {
+		kprintf("EFI_FSB_frequency: property %s not found\n",
+			FSB_Frequency_prop);
+		return 0;
+	}
+	if (size == sizeof(uint64_t)) {
+		frequency = *(uint64_t *) value;
+		kprintf("EFI_FSB_frequency: read %s value: %llu\n",
+			FSB_Frequency_prop, frequency);
+		if (!(90*Mega < frequency && frequency < 20*Giga)) {
+			kprintf("EFI_FSB_frequency: value out of range\n");
+			frequency = 0;
+		}
+	} else {
+		kprintf("EFI_FSB_frequency: unexpected size %d\n", size);
+	}
+
+	/*
+	 * While we're here, see if EFI published an initial TSC value.
+	 */
+	if (DTGetProperty(entry,TSC_at_boot_prop,&value,&size) == kSuccess) {
+		if (size == sizeof(uint64_t)) {
+			tsc_at_boot = *(uint64_t *) value;
+			kprintf("EFI_FSB_frequency: read %s value: %llu\n",
+				TSC_at_boot_prop, tsc_at_boot);
+		}
+	}
+
+	return frequency;
+}
+
+static uint64_t
+Detect_FSB_frequency(void)
+{
+    //char **argv;
+    // int voltage = 1;
+    //voltage = atoi(argv[1]);
+
+    //busFreq = EFI_FSB_frequency();
+    int  res;
+    uint64_t cpuMult;
+    uint64_t cpuFreq;
+
+    //If fsb boot parameter exists
+    if (PE_parse_boot_argn("fsb", &res,sizeof(res))) return res * Mega;
+
+    //Else try to autodetect
+    cpuMult	= GetCpuMultiplayer();
+    cpuFreq = EFI_CPU_Frequency();//Detect_CPU_Frequency();
+
+    printf("FSB Detection: calculated Mult %lld, cpuFreq %lld \n", cpuMult, cpuFreq);
+
+    if ((cpuMult == 0) || (cpuFreq == 0))
+    {
+        //  if (DetectFSB_NonClocked()) return DetectFSB_NonClocked() * Mega;
+        return 200 * Mega;
+    }
+
+    uint64_t freq = cpuFreq / cpuMult;
+
+    if (freq)
+        return freq;
+
+    return 200 * Mega;
+}
+
 /*
  * Initialize the various conversion factors needed by code referencing
  * the TSC.
@@ -139,6 +308,8 @@
 {
 	boolean_t	N_by_2_bus_ratio = FALSE;
 
+    if (IsIntelCPU())
+    {
 	if (cpuid_vmm_present()) {
 		kprintf("VMM vendor %u TSC frequency %u KHz bus frequency %u KHz\n",
 				cpuid_vmm_info()->cpuid_vmm_family,
@@ -164,6 +335,8 @@
 		}
 	}
 
+     if (IsIntelCPU())
+     {
 	switch (cpuid_cpufamily()) {
 	default: {
 		uint64_t msr_flex_ratio;
@@ -202,16 +375,196 @@
 
 		busFreq = EFI_get_frequency("FSBFrequency");
 	    }
+
+            /* AnV - Adapted from old code */
+            case CPU_FAMILY_PENTIUM_4:
+            {
+                uint64_t cpuFreq;
+                uint64_t prfsts;
+
+                if (cpuid_info()->cpuid_model < 2)
+                {
+                    /* This uses the CPU frequency exported into EFI by the bootloader */
+                    cpuFreq = EFI_CPU_Frequency();
+                    prfsts  = getFakeMSR(cpuFreq, busFreq);
+                    tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                    N_by_2_bus_ratio = (boolean_t)(prfsts & bit(46));
+                }
+                else if (cpuid_info()->cpuid_model == 2)
+                {
+                    prfsts      = rdmsr64(0x2C); // TODO: Add to header
+                    tscGranularity  = bitfield(prfsts, 31, 24);
+                }
+                else
+                {
+                    prfsts = rdmsr64(IA32_PERF_STS);
+                    tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                    N_by_2_bus_ratio = (prfsts & bit(46)) != 0;
+                }
+                /****** Addon boot Argn ******/
+                if (PE_parse_boot_argn("busratio", &tscGranularity, sizeof(tscGranularity)))
+                {
+                    if (tscGranularity == 0) tscGranularity = 1; // avoid div by zero
+                    N_by_2_bus_ratio = (tscGranularity > 30) && ((tscGranularity % 10) != 0);
+                    if (N_by_2_bus_ratio) tscGranularity /= 10; // Scale it back to normal
+                }
+                /****** boot Argn END ******/
+                break;
+            }
+
+            /* AnV - Adapted from old code */
+            case CPU_FAMILY_PENTIUM_M:
+            {
+                uint64_t cpuFreq;
+                uint64_t prfsts;
+
+                if (cpuid_info()->cpuid_model >= 0xD)
+                {
+                    /* Pentium M or Core and above can use Apple method*/
+                    prfsts = rdmsr64(IA32_PERF_STS);
+                    tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                    N_by_2_bus_ratio = (prfsts & bit(46)) != 0;
+                }
+                else
+                {
+                    /* Other Pentium class CPU, use safest option */
+                    /* This uses the CPU frequency exported into EFI by the bootloader */
+                    cpuFreq = EFI_CPU_Frequency();
+                    prfsts  = getFakeMSR(cpuFreq, busFreq);
+                    tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                    N_by_2_bus_ratio = (boolean_t)(prfsts & bit(46));
+                }
+                /****** Addon boot Argn ******/
+                if (PE_parse_boot_argn("busratio", &tscGranularity, sizeof(tscGranularity)))
+                {
+                    if (tscGranularity == 0) tscGranularity = 1; // avoid div by zero
+                    N_by_2_bus_ratio = (tscGranularity > 30) && ((tscGranularity % 10) != 0);
+                    if (N_by_2_bus_ratio) tscGranularity /= 10; // Scale it back to normal
+                }
+                /****** boot Argn END ******/
+                break;
+            }
 	}
+     } else {
+        switch (cpuid_info()->cpuid_family)
+        {
+            /*** AMD K8 Family ***/
+            case 15: /*** AMD K8 Ext.Family 0x0f=15 ***/
+            {
+                 uint64_t prfsts = 0;
+                uint64_t cpuFreq = 0;
+                busFreq = Detect_FSB_frequency();
+                prfsts      = rdmsr64(AMD_PERF_STS);
+                printf("rtclock_init: K8 MSR 0x%x \n", AMD_PERF_STS);
+
+                cpuFreq = EFI_CPU_Frequency();
+                prfsts = getFakeMSR(cpuFreq, busFreq);
+                tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                N_by_2_bus_ratio = (prfsts & bit(0))!=0; // FIXME: This is experimental!
+
+                /****** Addon boot Argn ******/
+                if (PE_parse_boot_argn("busratio", &tscGranularity, sizeof(tscGranularity)))
+                {
+                    if (tscGranularity == 0) tscGranularity = 1; // avoid div by zero
+                    N_by_2_bus_ratio = (tscGranularity > 30) && ((tscGranularity % 10) != 0);
+                    if (N_by_2_bus_ratio) tscGranularity /= 10; /* Scale it back to normal */
+                }
+                /****** Addon END ******/
+            }
+            break;
+
+            /*** AMD K10 Family ***/
+            case 16: /*** AMD K10 Ext.Family 0x10=16 ***/
+            case 18: /*** AMD K10 Ext.Family 0x12=18 ***/
+            {
+                uint64_t prfsts = 0;
+                uint64_t cpuFreq = 0;
+                busFreq = Detect_FSB_frequency();
+                prfsts      = rdmsr64(AMD_COFVID_STS);
+                printf("rtclock_init: K10 MSR 0x%x \n ", AMD_COFVID_STS);
+
+                cpuFreq = EFI_CPU_Frequency();
+                prfsts = getFakeMSR(cpuFreq, busFreq);
+                tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                N_by_2_bus_ratio = (prfsts & bit(46))!= 0;
+
+                /****** Addon boot Argn ******/
+                if (PE_parse_boot_argn("busratio", &tscGranularity, sizeof(tscGranularity)))
+                {
+                    if (tscGranularity == 0) tscGranularity = 1; // avoid div by zero
+                    N_by_2_bus_ratio = (tscGranularity > 30) && ((tscGranularity % 10) != 0);
+                    if (N_by_2_bus_ratio) tscGranularity /= 10; /* Scale it back to normal */
+                }
+                /****** Addon END ******/
+            }
+            break;
+
+            /*** AMD APU Family ***/
+            case 6:  /*** AMD APU Ext.Family 0x6=6 ***/
+            case 20: /*** AMD APU Ext.Family 0x14=20 ***/
+            case 21: /*** AMD A8/A10 Ext.Family 0x15=21 ***/
+            case 22: /*** AMD APU Athlon 5350 Ext.Family 0x16=22 ***/
+            {
+                uint64_t prfsts = 0;
+                uint64_t cpuFreq = 0;
+                uint64_t cpu_mhz;
+
+                busFreq = Detect_FSB_frequency();
+                prfsts      = rdmsr64(AMD_COFVID_STS);
+                printf("rtclock_init: K10.5 MSR 0x%x \n ", AMD_COFVID_STS);
+
+                cpuFreq = EFI_CPU_Frequency();
+                prfsts = getFakeMSR(cpuFreq, busFreq);
+                tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+                N_by_2_bus_ratio= (prfsts & bit(46))!=0;
+
+                /****** Addon boot Argn ******/
+                if (PE_parse_boot_argn("busratio", &tscGranularity, sizeof(tscGranularity)))
+                {
+                    if (tscGranularity == 0) tscGranularity = 1; // avoid div by zero
+                    N_by_2_bus_ratio = (tscGranularity > 30) && ((tscGranularity % 10) != 0);
+                    if (N_by_2_bus_ratio) tscGranularity /= 10; /* Scale it back to normal */
+                }
+                /****** Addon END ******/
+                cpu_mhz = tscGranularity * EFI_FSB_frequency();
+            }
+            break;
+
+            default:
+            {
+                uint64_t prfsts;
+                uint64_t cpuFreq = 0;
+
+                busFreq = EFI_FSB_frequency();
+
+                cpuFreq = EFI_CPU_Frequency();
+                prfsts = getFakeMSR(cpuFreq, busFreq);
+                tscGranularity = (uint32_t)bitfield(prfsts, 44, 40);
+
+                /****** Addon boot Argn ******/
+                if (PE_parse_boot_argn("busratio", &tscGranularity, sizeof(tscGranularity)))
+                {
+                    if (tscGranularity == 0) tscGranularity = 1; // avoid div by zero
+                    N_by_2_bus_ratio = (tscGranularity > 30) && ((tscGranularity % 10) != 0);
+                    if (N_by_2_bus_ratio) tscGranularity /= 10; // Scale it back to normal
+                }
+                /****** Addon END ******/
+            }
+        }
+    }
+    /* AMD CPU END */
 
 	if (busFreq != 0) {
 		busFCvtt2n = ((1 * Giga) << 32) / busFreq;
 		busFCvtn2t = 0xFFFFFFFFFFFFFFFFULL / busFCvtt2n;
 	} else {
-		panic("tsc_init: EFI not supported!\n");
+		/* Instead of panicking, set a default FSB frequency */
+	        /* AnV - Modded to 200 Mhz default... (for AMD) */
+   		busFreq = 200*Mega;
+  		kprintf("rtclock_init: Setting fsb to %u MHz\n", (uint32_t) (busFreq/Mega));
 	}
 
-	kprintf(" BUS: Frequency = %6d.%06dMHz, "
+	printf(" BUS: Frequency = %6d.%06dMHz, "
 		"cvtt2n = %08X.%08X, cvtn2t = %08X.%08X\n",
 		(uint32_t)(busFreq / Mega),
 		(uint32_t)(busFreq % Mega), 
@@ -247,7 +600,7 @@
 		bus2tsc = tmrCvt(busFCvtt2n, tscFCvtn2t);
 	}
 
-	kprintf(" TSC: Frequency = %6d.%06dMHz, "
+	printf(" TSC: Frequency = %6d.%06dMHz, "
 		"cvtt2n = %08X.%08X, cvtn2t = %08X.%08X, gran = %lld%s\n",
 		(uint32_t)(tscFreq / Mega),
 		(uint32_t)(tscFreq % Mega), 
diff -Nur xnu-3247.1.106/osfmk/i386/tsc.h xnu-3247.1.106-AnV/osfmk/i386/tsc.h
--- xnu-3247.1.106/osfmk/i386/tsc.h	2015-12-06 01:32:39.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/tsc.h	2015-12-13 17:08:10.000000000 +0100
@@ -44,6 +44,11 @@
 #define IA32_PERF_STS		0x198
 #define	SLOW_TSC_THRESHOLD	1000067800	/* if slower, nonzero shift required in nanotime() algorithm */
 
+#define AMD_PERF_STS	0xC0010042	/* AMD's version of the MSR */
+#define AMD_COFVID_STS 0xC0010071
+#define AMD_PSTATE0_STS 0xC0010064
+/** #define BASE_NHM_CLOCK_SOURCE_AMD 	200000000ULL **/
+
 #ifndef ASSEMBLER
 extern uint64_t	busFCvtt2n;
 extern uint64_t	busFCvtn2t;
diff -Nur xnu-3247.1.106/osfmk/i386/vmx/vmx_cpu.c xnu-3247.1.106-AnV/osfmk/i386/vmx/vmx_cpu.c
--- xnu-3247.1.106/osfmk/i386/vmx/vmx_cpu.c	2015-12-06 01:32:39.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/i386/vmx/vmx_cpu.c	2015-12-13 17:08:10.000000000 +0100
@@ -71,7 +71,7 @@
    vmx_is_cr0_valid()
 	Is CR0 valid for executing VMXON on this CPU?
    -------------------------------------------------------------------------- */
-static inline boolean_t
+static inline boolean_t __unused
 vmx_is_cr0_valid(vmx_specs_t *specs)
 {
 	uintptr_t cr0 = get_cr0();
@@ -82,7 +82,7 @@
    vmx_is_cr4_valid()
 	Is CR4 valid for executing VMXON on this CPU?
    -------------------------------------------------------------------------- */
-static inline boolean_t
+static inline boolean_t __unused
 vmx_is_cr4_valid(vmx_specs_t *specs)
 {
 	uintptr_t cr4 = get_cr4();
diff -Nur xnu-3247.1.106/osfmk/kern/debug.c xnu-3247.1.106-AnV/osfmk/kern/debug.c
--- xnu-3247.1.106/osfmk/kern/debug.c	2015-12-06 01:32:43.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/kern/debug.c	2015-12-13 17:37:14.000000000 +0100
@@ -102,7 +102,7 @@
 unsigned int 	disable_debug_output = TRUE;
 unsigned int 	systemLogDiags = FALSE;
 unsigned int 	panicDebugging = FALSE;
-unsigned int	logPanicDataToScreen = FALSE;
+unsigned int	logPanicDataToScreen = TRUE;
 unsigned int	kdebug_serial = FALSE;
 
 int mach_assert = 1;
@@ -245,7 +245,7 @@
 #endif
 }
 
-#if defined(__i386__) || defined(__x86_64__)
+/*#if defined(__i386__) || defined(__x86_64__)
 #define panic_stop()	pmCPUHalt(PM_HALT_PANIC)
 #define panic_safe()	pmSafeMode(x86_lcpu(), PM_SAFE_FL_SAFE)
 #define panic_normal()	pmSafeMode(x86_lcpu(), PM_SAFE_FL_NORMAL)
@@ -253,7 +253,11 @@
 #define panic_stop()	{ while (1) ; }
 #define panic_safe()
 #define panic_normal()
-#endif
+#endif*/
+/* AnV - Don't reboot on panic */
+#define panic_stop()	{ while (1) ; }
+#define panic_safe()
+#define panic_normal()
 
 /*
  * Prevent CPP from breaking the definition below,
diff -Nur xnu-3247.1.106/osfmk/kern/page_decrypt.c xnu-3247.1.106-AnV/osfmk/kern/page_decrypt.c
--- xnu-3247.1.106/osfmk/kern/page_decrypt.c	2015-12-06 01:32:45.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/kern/page_decrypt.c	2015-12-13 17:08:10.000000000 +0100
@@ -32,29 +32,495 @@
 #include <kern/task.h>
 #include <machine/commpage.h>
 
+#include "../../bsd/crypto/blowfish/blowfish.h"
+#include "../../bsd/crypto/blowfish/bf_locl.h"
+
+static const BF_KEY bf_init= {
+    {
+        0x243f6a88L, 0x85a308d3L, 0x13198a2eL, 0x03707344L,
+        0xa4093822L, 0x299f31d0L, 0x082efa98L, 0xec4e6c89L,
+        0x452821e6L, 0x38d01377L, 0xbe5466cfL, 0x34e90c6cL,
+        0xc0ac29b7L, 0xc97c50ddL, 0x3f84d5b5L, 0xb5470917L,
+        0x9216d5d9L, 0x8979fb1b
+    },{
+        0xd1310ba6L, 0x98dfb5acL, 0x2ffd72dbL, 0xd01adfb7L,
+        0xb8e1afedL, 0x6a267e96L, 0xba7c9045L, 0xf12c7f99L,
+        0x24a19947L, 0xb3916cf7L, 0x0801f2e2L, 0x858efc16L,
+        0x636920d8L, 0x71574e69L, 0xa458fea3L, 0xf4933d7eL,
+        0x0d95748fL, 0x728eb658L, 0x718bcd58L, 0x82154aeeL,
+        0x7b54a41dL, 0xc25a59b5L, 0x9c30d539L, 0x2af26013L,
+        0xc5d1b023L, 0x286085f0L, 0xca417918L, 0xb8db38efL,
+        0x8e79dcb0L, 0x603a180eL, 0x6c9e0e8bL, 0xb01e8a3eL,
+        0xd71577c1L, 0xbd314b27L, 0x78af2fdaL, 0x55605c60L,
+        0xe65525f3L, 0xaa55ab94L, 0x57489862L, 0x63e81440L,
+        0x55ca396aL, 0x2aab10b6L, 0xb4cc5c34L, 0x1141e8ceL,
+        0xa15486afL, 0x7c72e993L, 0xb3ee1411L, 0x636fbc2aL,
+        0x2ba9c55dL, 0x741831f6L, 0xce5c3e16L, 0x9b87931eL,
+        0xafd6ba33L, 0x6c24cf5cL, 0x7a325381L, 0x28958677L,
+        0x3b8f4898L, 0x6b4bb9afL, 0xc4bfe81bL, 0x66282193L,
+        0x61d809ccL, 0xfb21a991L, 0x487cac60L, 0x5dec8032L,
+        0xef845d5dL, 0xe98575b1L, 0xdc262302L, 0xeb651b88L,
+        0x23893e81L, 0xd396acc5L, 0x0f6d6ff3L, 0x83f44239L,
+        0x2e0b4482L, 0xa4842004L, 0x69c8f04aL, 0x9e1f9b5eL,
+        0x21c66842L, 0xf6e96c9aL, 0x670c9c61L, 0xabd388f0L,
+        0x6a51a0d2L, 0xd8542f68L, 0x960fa728L, 0xab5133a3L,
+        0x6eef0b6cL, 0x137a3be4L, 0xba3bf050L, 0x7efb2a98L,
+        0xa1f1651dL, 0x39af0176L, 0x66ca593eL, 0x82430e88L,
+        0x8cee8619L, 0x456f9fb4L, 0x7d84a5c3L, 0x3b8b5ebeL,
+        0xe06f75d8L, 0x85c12073L, 0x401a449fL, 0x56c16aa6L,
+        0x4ed3aa62L, 0x363f7706L, 0x1bfedf72L, 0x429b023dL,
+        0x37d0d724L, 0xd00a1248L, 0xdb0fead3L, 0x49f1c09bL,
+        0x075372c9L, 0x80991b7bL, 0x25d479d8L, 0xf6e8def7L,
+        0xe3fe501aL, 0xb6794c3bL, 0x976ce0bdL, 0x04c006baL,
+        0xc1a94fb6L, 0x409f60c4L, 0x5e5c9ec2L, 0x196a2463L,
+        0x68fb6fafL, 0x3e6c53b5L, 0x1339b2ebL, 0x3b52ec6fL,
+        0x6dfc511fL, 0x9b30952cL, 0xcc814544L, 0xaf5ebd09L,
+        0xbee3d004L, 0xde334afdL, 0x660f2807L, 0x192e4bb3L,
+        0xc0cba857L, 0x45c8740fL, 0xd20b5f39L, 0xb9d3fbdbL,
+        0x5579c0bdL, 0x1a60320aL, 0xd6a100c6L, 0x402c7279L,
+        0x679f25feL, 0xfb1fa3ccL, 0x8ea5e9f8L, 0xdb3222f8L,
+        0x3c7516dfL, 0xfd616b15L, 0x2f501ec8L, 0xad0552abL,
+        0x323db5faL, 0xfd238760L, 0x53317b48L, 0x3e00df82L,
+        0x9e5c57bbL, 0xca6f8ca0L, 0x1a87562eL, 0xdf1769dbL,
+        0xd542a8f6L, 0x287effc3L, 0xac6732c6L, 0x8c4f5573L,
+        0x695b27b0L, 0xbbca58c8L, 0xe1ffa35dL, 0xb8f011a0L,
+        0x10fa3d98L, 0xfd2183b8L, 0x4afcb56cL, 0x2dd1d35bL,
+        0x9a53e479L, 0xb6f84565L, 0xd28e49bcL, 0x4bfb9790L,
+        0xe1ddf2daL, 0xa4cb7e33L, 0x62fb1341L, 0xcee4c6e8L,
+        0xef20cadaL, 0x36774c01L, 0xd07e9efeL, 0x2bf11fb4L,
+        0x95dbda4dL, 0xae909198L, 0xeaad8e71L, 0x6b93d5a0L,
+        0xd08ed1d0L, 0xafc725e0L, 0x8e3c5b2fL, 0x8e7594b7L,
+        0x8ff6e2fbL, 0xf2122b64L, 0x8888b812L, 0x900df01cL,
+        0x4fad5ea0L, 0x688fc31cL, 0xd1cff191L, 0xb3a8c1adL,
+        0x2f2f2218L, 0xbe0e1777L, 0xea752dfeL, 0x8b021fa1L,
+        0xe5a0cc0fL, 0xb56f74e8L, 0x18acf3d6L, 0xce89e299L,
+        0xb4a84fe0L, 0xfd13e0b7L, 0x7cc43b81L, 0xd2ada8d9L,
+        0x165fa266L, 0x80957705L, 0x93cc7314L, 0x211a1477L,
+        0xe6ad2065L, 0x77b5fa86L, 0xc75442f5L, 0xfb9d35cfL,
+        0xebcdaf0cL, 0x7b3e89a0L, 0xd6411bd3L, 0xae1e7e49L,
+        0x00250e2dL, 0x2071b35eL, 0x226800bbL, 0x57b8e0afL,
+        0x2464369bL, 0xf009b91eL, 0x5563911dL, 0x59dfa6aaL,
+        0x78c14389L, 0xd95a537fL, 0x207d5ba2L, 0x02e5b9c5L,
+        0x83260376L, 0x6295cfa9L, 0x11c81968L, 0x4e734a41L,
+        0xb3472dcaL, 0x7b14a94aL, 0x1b510052L, 0x9a532915L,
+        0xd60f573fL, 0xbc9bc6e4L, 0x2b60a476L, 0x81e67400L,
+        0x08ba6fb5L, 0x571be91fL, 0xf296ec6bL, 0x2a0dd915L,
+        0xb6636521L, 0xe7b9f9b6L, 0xff34052eL, 0xc5855664L,
+        0x53b02d5dL, 0xa99f8fa1L, 0x08ba4799L, 0x6e85076aL,
+        0x4b7a70e9L, 0xb5b32944L, 0xdb75092eL, 0xc4192623L,
+        0xad6ea6b0L, 0x49a7df7dL, 0x9cee60b8L, 0x8fedb266L,
+        0xecaa8c71L, 0x699a17ffL, 0x5664526cL, 0xc2b19ee1L,
+        0x193602a5L, 0x75094c29L, 0xa0591340L, 0xe4183a3eL,
+        0x3f54989aL, 0x5b429d65L, 0x6b8fe4d6L, 0x99f73fd6L,
+        0xa1d29c07L, 0xefe830f5L, 0x4d2d38e6L, 0xf0255dc1L,
+        0x4cdd2086L, 0x8470eb26L, 0x6382e9c6L, 0x021ecc5eL,
+        0x09686b3fL, 0x3ebaefc9L, 0x3c971814L, 0x6b6a70a1L,
+        0x687f3584L, 0x52a0e286L, 0xb79c5305L, 0xaa500737L,
+        0x3e07841cL, 0x7fdeae5cL, 0x8e7d44ecL, 0x5716f2b8L,
+        0xb03ada37L, 0xf0500c0dL, 0xf01c1f04L, 0x0200b3ffL,
+        0xae0cf51aL, 0x3cb574b2L, 0x25837a58L, 0xdc0921bdL,
+        0xd19113f9L, 0x7ca92ff6L, 0x94324773L, 0x22f54701L,
+        0x3ae5e581L, 0x37c2dadcL, 0xc8b57634L, 0x9af3dda7L,
+        0xa9446146L, 0x0fd0030eL, 0xecc8c73eL, 0xa4751e41L,
+        0xe238cd99L, 0x3bea0e2fL, 0x3280bba1L, 0x183eb331L,
+        0x4e548b38L, 0x4f6db908L, 0x6f420d03L, 0xf60a04bfL,
+        0x2cb81290L, 0x24977c79L, 0x5679b072L, 0xbcaf89afL,
+        0xde9a771fL, 0xd9930810L, 0xb38bae12L, 0xdccf3f2eL,
+        0x5512721fL, 0x2e6b7124L, 0x501adde6L, 0x9f84cd87L,
+        0x7a584718L, 0x7408da17L, 0xbc9f9abcL, 0xe94b7d8cL,
+        0xec7aec3aL, 0xdb851dfaL, 0x63094366L, 0xc464c3d2L,
+        0xef1c1847L, 0x3215d908L, 0xdd433b37L, 0x24c2ba16L,
+        0x12a14d43L, 0x2a65c451L, 0x50940002L, 0x133ae4ddL,
+        0x71dff89eL, 0x10314e55L, 0x81ac77d6L, 0x5f11199bL,
+        0x043556f1L, 0xd7a3c76bL, 0x3c11183bL, 0x5924a509L,
+        0xf28fe6edL, 0x97f1fbfaL, 0x9ebabf2cL, 0x1e153c6eL,
+        0x86e34570L, 0xeae96fb1L, 0x860e5e0aL, 0x5a3e2ab3L,
+        0x771fe71cL, 0x4e3d06faL, 0x2965dcb9L, 0x99e71d0fL,
+        0x803e89d6L, 0x5266c825L, 0x2e4cc978L, 0x9c10b36aL,
+        0xc6150ebaL, 0x94e2ea78L, 0xa5fc3c53L, 0x1e0a2df4L,
+        0xf2f74ea7L, 0x361d2b3dL, 0x1939260fL, 0x19c27960L,
+        0x5223a708L, 0xf71312b6L, 0xebadfe6eL, 0xeac31f66L,
+        0xe3bc4595L, 0xa67bc883L, 0xb17f37d1L, 0x018cff28L,
+        0xc332ddefL, 0xbe6c5aa5L, 0x65582185L, 0x68ab9802L,
+        0xeecea50fL, 0xdb2f953bL, 0x2aef7dadL, 0x5b6e2f84L,
+        0x1521b628L, 0x29076170L, 0xecdd4775L, 0x619f1510L,
+        0x13cca830L, 0xeb61bd96L, 0x0334fe1eL, 0xaa0363cfL,
+        0xb5735c90L, 0x4c70a239L, 0xd59e9e0bL, 0xcbaade14L,
+        0xeecc86bcL, 0x60622ca7L, 0x9cab5cabL, 0xb2f3846eL,
+        0x648b1eafL, 0x19bdf0caL, 0xa02369b9L, 0x655abb50L,
+        0x40685a32L, 0x3c2ab4b3L, 0x319ee9d5L, 0xc021b8f7L,
+        0x9b540b19L, 0x875fa099L, 0x95f7997eL, 0x623d7da8L,
+        0xf837889aL, 0x97e32d77L, 0x11ed935fL, 0x16681281L,
+        0x0e358829L, 0xc7e61fd6L, 0x96dedfa1L, 0x7858ba99L,
+        0x57f584a5L, 0x1b227263L, 0x9b83c3ffL, 0x1ac24696L,
+        0xcdb30aebL, 0x532e3054L, 0x8fd948e4L, 0x6dbc3128L,
+        0x58ebf2efL, 0x34c6ffeaL, 0xfe28ed61L, 0xee7c3c73L,
+        0x5d4a14d9L, 0xe864b7e3L, 0x42105d14L, 0x203e13e0L,
+        0x45eee2b6L, 0xa3aaabeaL, 0xdb6c4f15L, 0xfacb4fd0L,
+        0xc742f442L, 0xef6abbb5L, 0x654f3b1dL, 0x41cd2105L,
+        0xd81e799eL, 0x86854dc7L, 0xe44b476aL, 0x3d816250L,
+        0xcf62a1f2L, 0x5b8d2646L, 0xfc8883a0L, 0xc1c7b6a3L,
+        0x7f1524c3L, 0x69cb7492L, 0x47848a0bL, 0x5692b285L,
+        0x095bbf00L, 0xad19489dL, 0x1462b174L, 0x23820e00L,
+        0x58428d2aL, 0x0c55f5eaL, 0x1dadf43eL, 0x233f7061L,
+        0x3372f092L, 0x8d937e41L, 0xd65fecf1L, 0x6c223bdbL,
+        0x7cde3759L, 0xcbee7460L, 0x4085f2a7L, 0xce77326eL,
+        0xa6078084L, 0x19f8509eL, 0xe8efd855L, 0x61d99735L,
+        0xa969a7aaL, 0xc50c06c2L, 0x5a04abfcL, 0x800bcadcL,
+        0x9e447a2eL, 0xc3453484L, 0xfdd56705L, 0x0e1e9ec9L,
+        0xdb73dbd3L, 0x105588cdL, 0x675fda79L, 0xe3674340L,
+        0xc5c43465L, 0x713e38d8L, 0x3d28f89eL, 0xf16dff20L,
+        0x153e21e7L, 0x8fb03d4aL, 0xe6e39f2bL, 0xdb83adf7L,
+        0xe93d5a68L, 0x948140f7L, 0xf64c261cL, 0x94692934L,
+        0x411520f7L, 0x7602d4f7L, 0xbcf46b2eL, 0xd4a20068L,
+        0xd4082471L, 0x3320f46aL, 0x43b7d4b7L, 0x500061afL,
+        0x1e39f62eL, 0x97244546L, 0x14214f74L, 0xbf8b8840L,
+        0x4d95fc1dL, 0x96b591afL, 0x70f4ddd3L, 0x66a02f45L,
+        0xbfbc09ecL, 0x03bd9785L, 0x7fac6dd0L, 0x31cb8504L,
+        0x96eb27b3L, 0x55fd3941L, 0xda2547e6L, 0xabca0a9aL,
+        0x28507825L, 0x530429f4L, 0x0a2c86daL, 0xe9b66dfbL,
+        0x68dc1462L, 0xd7486900L, 0x680ec0a4L, 0x27a18deeL,
+        0x4f3ffea2L, 0xe887ad8cL, 0xb58ce006L, 0x7af4d6b6L,
+        0xaace1e7cL, 0xd3375fecL, 0xce78a399L, 0x406b2a42L,
+        0x20fe9e35L, 0xd9f385b9L, 0xee39d7abL, 0x3b124e8bL,
+        0x1dc9faf7L, 0x4b6d1856L, 0x26a36631L, 0xeae397b2L,
+        0x3a6efa74L, 0xdd5b4332L, 0x6841e7f7L, 0xca7820fbL,
+        0xfb0af54eL, 0xd8feb397L, 0x454056acL, 0xba489527L,
+        0x55533a3aL, 0x20838d87L, 0xfe6ba9b7L, 0xd096954bL,
+        0x55a867bcL, 0xa1159a58L, 0xcca92963L, 0x99e1db33L,
+        0xa62a4a56L, 0x3f3125f9L, 0x5ef47e1cL, 0x9029317cL,
+        0xfdf8e802L, 0x04272f70L, 0x80bb155cL, 0x05282ce3L,
+        0x95c11548L, 0xe4c66d22L, 0x48c1133fL, 0xc70f86dcL,
+        0x07f9c9eeL, 0x41041f0fL, 0x404779a4L, 0x5d886e17L,
+        0x325f51ebL, 0xd59bc0d1L, 0xf2bcc18fL, 0x41113564L,
+        0x257b7834L, 0x602a9c60L, 0xdff8e8a3L, 0x1f636c1bL,
+        0x0e12b4c2L, 0x02e1329eL, 0xaf664fd1L, 0xcad18115L,
+        0x6b2395e0L, 0x333e92e1L, 0x3b240b62L, 0xeebeb922L,
+        0x85b2a20eL, 0xe6ba0d99L, 0xde720c8cL, 0x2da2f728L,
+        0xd0127845L, 0x95b794fdL, 0x647d0862L, 0xe7ccf5f0L,
+        0x5449a36fL, 0x877d48faL, 0xc39dfd27L, 0xf33e8d1eL,
+        0x0a476341L, 0x992eff74L, 0x3a6f6eabL, 0xf4f8fd37L,
+        0xa812dc60L, 0xa1ebddf8L, 0x991be14cL, 0xdb6e6b0dL,
+        0xc67b5510L, 0x6d672c37L, 0x2765d43bL, 0xdcd0e804L,
+        0xf1290dc7L, 0xcc00ffa3L, 0xb5390f92L, 0x690fed0bL,
+        0x667b9ffbL, 0xcedb7d9cL, 0xa091cf0bL, 0xd9155ea3L,
+        0xbb132f88L, 0x515bad24L, 0x7b9479bfL, 0x763bd6ebL,
+        0x37392eb3L, 0xcc115979L, 0x8026e297L, 0xf42e312dL,
+        0x6842ada7L, 0xc66a2b3bL, 0x12754cccL, 0x782ef11cL,
+        0x6a124237L, 0xb79251e7L, 0x06a1bbe6L, 0x4bfb6350L,
+        0x1a6b1018L, 0x11caedfaL, 0x3d25bdd8L, 0xe2e1c3c9L,
+        0x44421659L, 0x0a121386L, 0xd90cec6eL, 0xd5abea2aL,
+        0x64af674eL, 0xda86a85fL, 0xbebfe988L, 0x64e4c3feL,
+        0x9dbc8057L, 0xf0f7c086L, 0x60787bf8L, 0x6003604dL,
+        0xd1fd8346L, 0xf6381fb0L, 0x7745ae04L, 0xd736fcccL,
+        0x83426b33L, 0xf01eab71L, 0xb0804187L, 0x3c005e5fL,
+        0x77a057beL, 0xbde8ae24L, 0x55464299L, 0xbf582e61L,
+        0x4e58f48fL, 0xf2ddfda2L, 0xf474ef38L, 0x8789bdc2L,
+        0x5366f9c3L, 0xc8b38e74L, 0xb475f255L, 0x46fcd9b9L,
+        0x7aeb2661L, 0x8b1ddf84L, 0x846a0e79L, 0x915f95e2L,
+        0x466e598eL, 0x20b45770L, 0x8cd55591L, 0xc902de4cL,
+        0xb90bace1L, 0xbb8205d0L, 0x11a86248L, 0x7574a99eL,
+        0xb77f19b6L, 0xe0a9dc09L, 0x662d09a1L, 0xc4324633L,
+        0xe85a1f02L, 0x09f0be8cL, 0x4a99a025L, 0x1d6efe10L,
+        0x1ab93d1dL, 0x0ba5a4dfL, 0xa186f20fL, 0x2868f169L,
+        0xdcb7da83L, 0x573906feL, 0xa1e2ce9bL, 0x4fcd7f52L,
+        0x50115e01L, 0xa70683faL, 0xa002b5c4L, 0x0de6d027L,
+        0x9af88c27L, 0x773f8641L, 0xc3604c06L, 0x61a806b5L,
+        0xf0177a28L, 0xc0f586e0L, 0x006058aaL, 0x30dc7d62L,
+        0x11e69ed7L, 0x2338ea63L, 0x53c2dd94L, 0xc2c21634L,
+        0xbbcbee56L, 0x90bcb6deL, 0xebfc7da1L, 0xce591d76L,
+        0x6f05e409L, 0x4b7c0188L, 0x39720a3dL, 0x7c927c24L,
+        0x86e3725fL, 0x724d9db9L, 0x1ac15bb4L, 0xd39eb8fcL,
+        0xed545578L, 0x08fca5b5L, 0xd83d7cd3L, 0x4dad0fc4L,
+        0x1e50ef5eL, 0xb161e6f8L, 0xa28514d9L, 0x6c51133cL,
+        0x6fd5c7e7L, 0x56e14ec4L, 0x362abfceL, 0xddc6c837L,
+        0xd79a3234L, 0x92638212L, 0x670efa8eL, 0x406000e0L,
+        0x3a39ce37L, 0xd3faf5cfL, 0xabc27737L, 0x5ac52d1bL,
+        0x5cb0679eL, 0x4fa33742L, 0xd3822740L, 0x99bc9bbeL,
+        0xd5118e9dL, 0xbf0f7315L, 0xd62d1c7eL, 0xc700c47bL,
+        0xb78c1b6bL, 0x21a19045L, 0xb26eb1beL, 0x6a366eb4L,
+        0x5748ab2fL, 0xbc946e79L, 0xc6a376d2L, 0x6549c2c8L,
+        0x530ff8eeL, 0x468dde7dL, 0xd5730a1dL, 0x4cd04dc6L,
+        0x2939bbdbL, 0xa9ba4650L, 0xac9526e8L, 0xbe5ee304L,
+        0xa1fad5f0L, 0x6a2d519aL, 0x63ef8ce2L, 0x9a86ee22L,
+        0xc089c2b8L, 0x43242ef6L, 0xa51e03aaL, 0x9cf2d0a4L,
+        0x83c061baL, 0x9be96a4dL, 0x8fe51550L, 0xba645bd6L,
+        0x2826a2f9L, 0xa73a3ae1L, 0x4ba99586L, 0xef5562e9L,
+        0xc72fefd3L, 0xf752f7daL, 0x3f046f69L, 0x77fa0a59L,
+        0x80e4a915L, 0x87b08601L, 0x9b09e6adL, 0x3b3ee593L,
+        0xe990fd5aL, 0x9e34d797L, 0x2cf0b7d9L, 0x022b8b51L,
+        0x96d5ac3aL, 0x017da67dL, 0xd1cf3ed6L, 0x7c7d2d28L,
+        0x1f9f25cfL, 0xadf2b89bL, 0x5ad6b472L, 0x5a88f54cL,
+        0xe029ac71L, 0xe019a5e6L, 0x47b0acfdL, 0xed93fa9bL,
+        0xe8d3c48dL, 0x283b57ccL, 0xf8d56629L, 0x79132e28L,
+        0x785f0191L, 0xed756055L, 0xf7960e44L, 0xe3d35e8cL,
+        0x15056dd4L, 0x88f46dbaL, 0x03a16125L, 0x0564f0bdL,
+        0xc3eb9e15L, 0x3c9057a2L, 0x97271aecL, 0xa93a072aL,
+        0x1b3f6d9bL, 0x1e6321f5L, 0xf59c66fbL, 0x26dcf319L,
+        0x7533d928L, 0xb155fdf5L, 0x03563482L, 0x8aba3cbbL,
+        0x28517711L, 0xc20ad9f8L, 0xabcc5167L, 0xccad925fL,
+        0x4de81751L, 0x3830dc8eL, 0x379d5862L, 0x9320f991L,
+        0xea7a90c2L, 0xfb3e7bceL, 0x5121ce64L, 0x774fbe32L,
+        0xa8b6e37eL, 0xc3293d46L, 0x48de5369L, 0x6413e680L,
+        0xa2ae0810L, 0xdd6db224L, 0x69852dfdL, 0x09072166L,
+        0xb39a460aL, 0x6445c0ddL, 0x586cdecfL, 0x1c20c8aeL,
+        0x5bbef7ddL, 0x1b588d40L, 0xccd2017fL, 0x6bb4e3bbL,
+        0xdda26a7eL, 0x3a59ff45L, 0x3e350a44L, 0xbcb4cdd5L,
+        0x72eacea8L, 0xfa6484bbL, 0x8d6612aeL, 0xbf3c6f47L,
+        0xd29be463L, 0x542f5d9eL, 0xaec2771bL, 0xf64e6370L,
+        0x740e0d8dL, 0xe75b1357L, 0xf8721671L, 0xaf537d5dL,
+        0x4040cb08L, 0x4eb4e2ccL, 0x34d2466aL, 0x0115af84L,
+        0xe1b00428L, 0x95983a1dL, 0x06b89fb4L, 0xce6ea048L, 
+        0x6f3f3b82L, 0x3520ab82L, 0x011a1d4bL, 0x277227f8L, 
+        0x611560b1L, 0xe7933fdcL, 0xbb3a792bL, 0x344525bdL, 
+        0xa08839e1L, 0x51ce794bL, 0x2f32c9b7L, 0xa01fbac9L, 
+        0xe01cc87eL, 0xbcc7d1f6L, 0xcf0111c3L, 0xa1e8aac7L, 
+        0x1a908749L, 0xd44fbd9aL, 0xd0dadecbL, 0xd50ada38L, 
+        0x0339c32aL, 0xc6913667L, 0x8df9317cL, 0xe0b12b4fL, 
+        0xf79e59b7L, 0x43f5bb3aL, 0xf2d519ffL, 0x27d9459cL, 
+        0xbf97222cL, 0x15e6fc2aL, 0x0f91fc71L, 0x9b941525L, 
+        0xfae59361L, 0xceb69cebL, 0xc2a86459L, 0x12baa8d1L, 
+        0xb6c1075eL, 0xe3056a0cL, 0x10d25065L, 0xcb03a442L, 
+        0xe0ec6e0eL, 0x1698db3bL, 0x4c98a0beL, 0x3278e964L, 
+        0x9f1f9532L, 0xe0d392dfL, 0xd3a0342bL, 0x8971f21eL, 
+        0x1b0a7441L, 0x4ba3348cL, 0xc5be7120L, 0xc37632d8L, 
+        0xdf359f8dL, 0x9b992f2eL, 0xe60b6f47L, 0x0fe3f11dL, 
+        0xe54cda54L, 0x1edad891L, 0xce6279cfL, 0xcd3e7e6fL, 
+        0x1618b166L, 0xfd2c1d05L, 0x848fd2c5L, 0xf6fb2299L, 
+        0xf523f357L, 0xa6327623L, 0x93a83531L, 0x56cccd02L, 
+        0xacf08162L, 0x5a75ebb5L, 0x6e163697L, 0x88d273ccL, 
+        0xde966292L, 0x81b949d0L, 0x4c50901bL, 0x71c65614L, 
+        0xe6c6c7bdL, 0x327a140aL, 0x45e1d006L, 0xc3f27b9aL, 
+        0xc9aa53fdL, 0x62a80f00L, 0xbb25bfe2L, 0x35bdd2f6L, 
+        0x71126905L, 0xb2040222L, 0xb6cbcf7cL, 0xcd769c2bL, 
+        0x53113ec0L, 0x1640e3d3L, 0x38abbd60L, 0x2547adf0L, 
+        0xba38209cL, 0xf746ce76L, 0x77afa1c5L, 0x20756060L, 
+        0x85cbfe4eL, 0x8ae88dd8L, 0x7aaaf9b0L, 0x4cf9aa7eL, 
+        0x1948c25cL, 0x02fb8a8cL, 0x01c36ae4L, 0xd6ebe1f9L, 
+        0x90d4f869L, 0xa65cdea0L, 0x3f09252dL, 0xc208e69fL, 
+        0xb74e6132L, 0xce77e25bL, 0x578fdfe3L, 0x3ac372e6L, 
+    }
+};
+
+inline void BF_encrypt(BF_LONG *data, const BF_KEY *key)
+{
+    register BF_LONG l,r;
+    register const BF_LONG *p,*s;
+    
+    p=key->P;
+    s= &(key->S[0]);
+    l=data[0];
+    r=data[1];
+    
+    l^=p[0];
+    BF_ENC(r,l,s,p[ 1]);
+    BF_ENC(l,r,s,p[ 2]);
+    BF_ENC(r,l,s,p[ 3]);
+    BF_ENC(l,r,s,p[ 4]);
+    BF_ENC(r,l,s,p[ 5]);
+    BF_ENC(l,r,s,p[ 6]);
+    BF_ENC(r,l,s,p[ 7]);
+    BF_ENC(l,r,s,p[ 8]);
+    BF_ENC(r,l,s,p[ 9]);
+    BF_ENC(l,r,s,p[10]);
+    BF_ENC(r,l,s,p[11]);
+    BF_ENC(l,r,s,p[12]);
+    BF_ENC(r,l,s,p[13]);
+    BF_ENC(l,r,s,p[14]);
+    BF_ENC(r,l,s,p[15]);
+    BF_ENC(l,r,s,p[16]);
+    r^=p[BF_ROUNDS+1];
+    
+    data[1]=l&0xffffffffL;
+    data[0]=r&0xffffffffL;
+}
+
+inline void BF_set_key(BF_KEY *key, int len, const unsigned char *data)
+{
+    int i;
+    BF_LONG *p,ri,in[2];
+    const unsigned char *d,*end;
+    
+    
+    memcpy(key,&bf_init,sizeof(BF_KEY));
+    p=key->P;
+    
+    if (len > ((BF_ROUNDS+2)*4)) len=(BF_ROUNDS+2)*4;
+    
+    d=data;
+    end= &(data[len]);
+    for (i=0; i<(BF_ROUNDS+2); i++)
+    {
+        ri= *(d++);
+        if (d >= end) d=data;
+        
+        ri<<=8;
+        ri|= *(d++);
+        if (d >= end) d=data;
+        
+        ri<<=8;
+        ri|= *(d++);
+        if (d >= end) d=data;
+        
+        ri<<=8;
+        ri|= *(d++);
+        if (d >= end) d=data;
+        
+        p[i]^=ri;
+    }
+    
+    in[0]=0L;
+    in[1]=0L;
+    for (i=0; i<(BF_ROUNDS+2); i+=2)
+    {
+        BF_encrypt(in,key);
+        p[i  ]=in[0];
+        p[i+1]=in[1];
+    }
+    
+    p=key->S;
+    for (i=0; i<4*256; i+=2)
+    {
+        BF_encrypt(in,key);
+        p[i  ]=in[0];
+        p[i+1]=in[1];
+    }
+}
+
+inline void BF_decrypt(BF_LONG *data, const BF_KEY *key)
+{
+    register BF_LONG l,r;
+    register const BF_LONG *p,*s;
+    
+    p=key->P;
+    s= &(key->S[0]);
+    l=data[0];
+    r=data[1];
+    
+    l^=p[BF_ROUNDS+1];
+    BF_ENC(r,l,s,p[16]);
+    BF_ENC(l,r,s,p[15]);
+    BF_ENC(r,l,s,p[14]);
+    BF_ENC(l,r,s,p[13]);
+    BF_ENC(r,l,s,p[12]);
+    BF_ENC(l,r,s,p[11]);
+    BF_ENC(r,l,s,p[10]);
+    BF_ENC(l,r,s,p[ 9]);
+    BF_ENC(r,l,s,p[ 8]);
+    BF_ENC(l,r,s,p[ 7]);
+    BF_ENC(r,l,s,p[ 6]);
+    BF_ENC(l,r,s,p[ 5]);
+    BF_ENC(r,l,s,p[ 4]);
+    BF_ENC(l,r,s,p[ 3]);
+    BF_ENC(r,l,s,p[ 2]);
+    BF_ENC(l,r,s,p[ 1]);
+    r^=p[0];
+    
+    data[1]=l&0xffffffffL;
+    data[0]=r&0xffffffffL;
+}
+
+/* BLOWFISH DECRYPT from OpenSSL.
+ */
+
+void BF_cbc_decrypt(const unsigned char *in, unsigned char *out, long length,
+					const BF_KEY *schedule, unsigned char *ivec)
+{
+    register BF_LONG tin0,tin1;
+    register BF_LONG tout0,tout1,xor0,xor1;
+    register long l=length;
+    BF_LONG tin[2];
+
+    n2l(ivec,xor0);
+    n2l(ivec,xor1);
+    ivec-=8;
+    for (l-=8; l>=0; l-=8)
+    {
+        n2l(in,tin0);
+        n2l(in,tin1);
+        tin[0]=tin0;
+        tin[1]=tin1;
+        BF_decrypt(tin,(BF_KEY *)schedule);
+        tout0=tin[0]^xor0;
+        tout1=tin[1]^xor1;
+        l2n(tout0,out);
+        l2n(tout1,out);
+        xor0=tin0;
+        xor1=tin1;
+    }
+    if (l != -8)
+    {
+        n2l(in,tin0);
+        n2l(in,tin1);
+        tin[0]=tin0;
+        tin[1]=tin1;
+        BF_decrypt(tin,(BF_KEY *)schedule);
+        tout0=tin[0]^xor0;
+        tout1=tin[1]^xor1;
+        l2nn(tout0,tout1,out,l+8);
+        xor0=tin0;
+        xor1=tin1;
+    }
+    l2n(xor0,ivec);
+    l2n(xor1,ivec);
+
+    tin0=tin1=tout0=tout1=xor0=xor1=0;
+    tin[0]=tin[1]=0;
+}
+
+/* Perform in-kernel memory decryption of a page. Since Snow Leopard they use BLOWFISH.
+ * "ops" is treated as a value, and it indicates which decryption type should be attempted.
+ */
+int ink_decrypt(const void* from, void *to, unsigned long long __unused src_offset, __unused void *  ops);
+
+int ink_decrypt(const void* from, void *to, unsigned long long __unused src_offset, __unused void *  ops)
+{
+    BF_KEY key;
+    unsigned char null_ivec[32] = {0,};
+	static  unsigned char *plain_key ;
+	static  unsigned char plain_keys[65] = "ourhardworkbythesewordsguardedpleasedontsteal(c)AppleComputerInc";
+	plain_key = plain_keys;
+    BF_set_key(&key, 64, (unsigned char *)plain_key);
+    BF_cbc_decrypt(from, to, PAGE_SIZE, &key, null_ivec);
+    return KERN_SUCCESS;
+}
+
 static dsmos_page_transform_hook_t dsmos_hook = NULL;
 
 void
 dsmos_page_transform_hook(dsmos_page_transform_hook_t hook)
 {
+    unsigned char builtindecrypt = 0;
 
-	printf("DSMOS has arrived\n");
-	/* set the hook now - new callers will run with it */
-	dsmos_hook = hook;
+    if (PE_parse_boot_argn("-nodecryptor", &builtindecrypt, sizeof(builtindecrypt)))
+    {
+        printf("DSMOS has arrived\n");
+        /* set the hook now - new callers will run with it */
+        dsmos_hook = hook;
+    } else {
+        printf("Built-in decrypter used\n");
+        dsmos_hook = ink_decrypt;
+    }
 }
 
 int
 dsmos_page_transform(const void* from, void *to, unsigned long long src_offset, void *ops)
 {
 	static boolean_t first_wait = TRUE;
+    unsigned char builtindecrypt = 0;
 
 	if (dsmos_hook == NULL) {
-		if (first_wait) {
-			first_wait = FALSE;
-			printf("Waiting for DSMOS...\n");
-		}
-		return KERN_ABORTED;
-	}
+        if (PE_parse_boot_argn("-nodecryptor", &builtindecrypt, sizeof(builtindecrypt)))
+        {
+            if (first_wait) {
+                first_wait = FALSE;
+                printf("Waiting for DSMOS...\n");
+            }
+            return KERN_ABORTED;
+        } else {
+            dsmos_hook = ink_decrypt;
+        }
+    }
 	return (*dsmos_hook) (from, to, src_offset, ops);
 }
 
diff -Nur xnu-3247.1.106/osfmk/kern/voodoo_assert.h xnu-3247.1.106-AnV/osfmk/kern/voodoo_assert.h
--- xnu-3247.1.106/osfmk/kern/voodoo_assert.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/kern/voodoo_assert.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,11 @@
+#ifndef _VOODOO_ASSERT_H
+#define _VOODOO_ASSERT_H
+
+#include <kern/debug.h>
+
+#define ASSERT(expr) do { if (!(expr)) panic("%s: failed assertion '%s'", \
+                                                      __FUNCTION__, #expr); } while (0)
+
+#define BUG(msg) panic("%s: %s\n", __FUNCTION__, #msg)
+
+#endif
\ No newline at end of file
diff -Nur xnu-3247.1.106/osfmk/vm/vm_fault.c xnu-3247.1.106-AnV/osfmk/vm/vm_fault.c
--- xnu-3247.1.106/osfmk/vm/vm_fault.c	2015-12-06 01:33:09.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/vm/vm_fault.c	2015-12-13 17:08:10.000000000 +0100
@@ -2926,11 +2926,6 @@
 				       "*** INVALID PAGE ***\n",
 				       (long long)vaddr);
 			}
-#if !SECURE_KERNEL
-			if (cs_enforcement_panic) {
-				panic("CODESIGNING: panicking on invalid page\n");
-			}
-#endif
 		}
 		
 	} else {
@@ -6141,12 +6136,12 @@
 	if (page->cs_validated || page->cs_tainted) {
 		return;
 	}
-
+/*
 	if (page->slid) {
 		panic("vm_page_validate_cs(%p): page is slid\n", page);
 	}
 	assert(!page->slid);
-
+*/
 #if CHECK_CS_VALIDATION_BITMAP	
 	if ( vnode_pager_cs_check_validation_bitmap( page->object->pager, trunc_page(page->offset + page->object->paging_offset), CS_BITMAP_CHECK ) == KERN_SUCCESS) {
 		page->cs_validated = TRUE;
@@ -6186,9 +6181,9 @@
 				  &ksize,
 				  &koffset,
 				  &need_unmap);
-	if (kr != KERN_SUCCESS) {
+	/* if (kr != KERN_SUCCESS) {
 		panic("vm_page_validate_cs: could not map page: 0x%x\n", kr);
-	}
+	} */
 	kaddr = CAST_DOWN(vm_offset_t, koffset);
 
 	/* validate the mapped page */
diff -Nur xnu-3247.1.106/osfmk/vm/vm_fault.c.orig xnu-3247.1.106-AnV/osfmk/vm/vm_fault.c.orig
--- xnu-3247.1.106/osfmk/vm/vm_fault.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/vm/vm_fault.c.orig	2015-12-06 01:33:09.000000000 +0100
@@ -0,0 +1,6292 @@
+/*
+ * Copyright (c) 2000-2009 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+/* 
+ * Mach Operating System
+ * Copyright (c) 1991,1990,1989,1988,1987 Carnegie Mellon University
+ * All Rights Reserved.
+ * 
+ * Permission to use, copy, modify and distribute this software and its
+ * documentation is hereby granted, provided that both the copyright
+ * notice and this permission notice appear in all copies of the
+ * software, derivative works or modified versions, and any portions
+ * thereof, and that both notices appear in supporting documentation.
+ * 
+ * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
+ * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR
+ * ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
+ * 
+ * Carnegie Mellon requests users of this software to return to
+ * 
+ *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU
+ *  School of Computer Science
+ *  Carnegie Mellon University
+ *  Pittsburgh PA 15213-3890
+ * 
+ * any improvements or extensions that they make and grant Carnegie Mellon
+ * the rights to redistribute these changes.
+ */
+/*
+ */
+/*
+ *	File:	vm_fault.c
+ *	Author:	Avadis Tevanian, Jr., Michael Wayne Young
+ *
+ *	Page fault handling module.
+ */
+
+#include <mach_cluster_stats.h>
+#include <mach_pagemap.h>
+#include <libkern/OSAtomic.h>
+
+#include <mach/mach_types.h>
+#include <mach/kern_return.h>
+#include <mach/message.h>	/* for error codes */
+#include <mach/vm_param.h>
+#include <mach/vm_behavior.h>
+#include <mach/memory_object.h>
+				/* For memory_object_data_{request,unlock} */
+#include <mach/sdt.h>
+
+#include <kern/kern_types.h>
+#include <kern/host_statistics.h>
+#include <kern/counters.h>
+#include <kern/task.h>
+#include <kern/thread.h>
+#include <kern/sched_prim.h>
+#include <kern/host.h>
+#include <kern/xpr.h>
+#include <kern/mach_param.h>
+#include <kern/macro_help.h>
+#include <kern/zalloc.h>
+#include <kern/misc_protos.h>
+
+#include <vm/vm_compressor.h>
+#include <vm/vm_compressor_pager.h>
+#include <vm/vm_fault.h>
+#include <vm/vm_map.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+#include <vm/vm_kern.h>
+#include <vm/pmap.h>
+#include <vm/vm_pageout.h>
+#include <vm/vm_protos.h>
+#include <vm/vm_external.h>
+#include <vm/memory_object.h>
+#include <vm/vm_purgeable_internal.h>	/* Needed by some vm_page.h macros */
+#include <vm/vm_shared_region.h>
+
+#include <sys/codesign.h>
+
+#include <libsa/sys/timers.h>	/* for struct timespec */
+
+#define VM_FAULT_CLASSIFY	0
+
+#define TRACEFAULTPAGE 0 /* (TEST/DEBUG) */
+
+unsigned int	vm_object_pagein_throttle = 16;
+
+/*
+ * We apply a hard throttle to the demand zero rate of tasks that we believe are running out of control which 
+ * kicks in when swap space runs out.  64-bit programs have massive address spaces and can leak enormous amounts
+ * of memory if they're buggy and can run the system completely out of swap space.  If this happens, we
+ * impose a hard throttle on them to prevent them from taking the last bit of memory left.  This helps
+ * keep the UI active so that the user has a chance to kill the offending task before the system 
+ * completely hangs.
+ *
+ * The hard throttle is only applied when the system is nearly completely out of swap space and is only applied
+ * to tasks that appear to be bloated.  When swap runs out, any task using more than vm_hard_throttle_threshold
+ * will be throttled.  The throttling is done by giving the thread that's trying to demand zero a page a
+ * delay of HARD_THROTTLE_DELAY microseconds before being allowed to try the page fault again.
+ */
+
+extern void throttle_lowpri_io(int);
+
+uint64_t vm_hard_throttle_threshold;
+
+
+
+#define NEED_TO_HARD_THROTTLE_THIS_TASK()	(vm_wants_task_throttled(current_task()) ||	\
+						 (vm_page_free_count < vm_page_throttle_limit && \
+						  proc_get_effective_thread_policy(current_thread(), TASK_POLICY_IO) > THROTTLE_LEVEL_THROTTLED))
+
+
+#define HARD_THROTTLE_DELAY	5000	/* 5000 us == 5 ms */
+#define SOFT_THROTTLE_DELAY	200	/* 200 us == .2 ms */
+
+#define	VM_PAGE_CREATION_THROTTLE_PERIOD_SECS	6
+#define	VM_PAGE_CREATION_THROTTLE_RATE_PER_SEC	20000
+
+
+boolean_t current_thread_aborted(void);
+
+/* Forward declarations of internal routines. */
+static kern_return_t vm_fault_wire_fast(
+				vm_map_t	map,
+				vm_map_offset_t	va,
+				vm_prot_t       prot,
+				vm_map_entry_t	entry,
+				pmap_t		pmap,
+				vm_map_offset_t	pmap_addr,
+				ppnum_t		*physpage_p);
+
+static kern_return_t vm_fault_internal(
+		vm_map_t	map,
+		vm_map_offset_t	vaddr,
+		vm_prot_t	caller_prot,
+		boolean_t	change_wiring,
+		int             interruptible,
+		pmap_t		pmap,
+		vm_map_offset_t	pmap_addr,
+		ppnum_t		*physpage_p);
+
+static void vm_fault_copy_cleanup(
+				vm_page_t	page,
+				vm_page_t	top_page);
+
+static void vm_fault_copy_dst_cleanup(
+				vm_page_t	page);
+
+#if	VM_FAULT_CLASSIFY
+extern void vm_fault_classify(vm_object_t	object,
+			  vm_object_offset_t	offset,
+			  vm_prot_t		fault_type);
+
+extern void vm_fault_classify_init(void);
+#endif
+
+unsigned long vm_pmap_enter_blocked = 0;
+unsigned long vm_pmap_enter_retried = 0;
+
+unsigned long vm_cs_validates = 0;
+unsigned long vm_cs_revalidates = 0;
+unsigned long vm_cs_query_modified = 0;
+unsigned long vm_cs_validated_dirtied = 0;
+unsigned long vm_cs_bitmap_validated = 0;
+
+void vm_pre_fault(vm_map_offset_t);
+
+extern int not_in_kdp;
+extern char *kdp_compressor_decompressed_page;
+extern addr64_t	kdp_compressor_decompressed_page_paddr;
+extern ppnum_t	kdp_compressor_decompressed_page_ppnum;
+
+/*
+ *	Routine:	vm_fault_init
+ *	Purpose:
+ *		Initialize our private data structures.
+ */
+void
+vm_fault_init(void)
+{
+	int i, vm_compressor_temp;
+	boolean_t need_default_val = TRUE;
+	/*
+	 * Choose a value for the hard throttle threshold based on the amount of ram.  The threshold is
+	 * computed as a percentage of available memory, and the percentage used is scaled inversely with
+	 * the amount of memory.  The percentage runs between 10% and 35%.  We use 35% for small memory systems
+	 * and reduce the value down to 10% for very large memory configurations.  This helps give us a
+	 * definition of a memory hog that makes more sense relative to the amount of ram in the machine.
+	 * The formula here simply uses the number of gigabytes of ram to adjust the percentage.
+	 */
+
+	vm_hard_throttle_threshold = sane_size * (35 - MIN((int)(sane_size / (1024*1024*1024)), 25)) / 100;
+
+	/*
+	 * Configure compressed pager behavior. A boot arg takes precedence over a device tree entry.
+	 */
+
+	if (PE_parse_boot_argn("vm_compressor", &vm_compressor_temp, sizeof (vm_compressor_temp))) {
+		for ( i = 0; i < VM_PAGER_MAX_MODES; i++) {
+			if (vm_compressor_temp > 0 && 
+			    ((vm_compressor_temp & ( 1 << i)) == vm_compressor_temp)) {
+				need_default_val = FALSE;
+				vm_compressor_mode = vm_compressor_temp;
+				break;
+			}
+		}
+		if (need_default_val)
+			printf("Ignoring \"vm_compressor\" boot arg %d\n", vm_compressor_temp);
+	} 
+	if (need_default_val) {
+		/* If no boot arg or incorrect boot arg, try device tree. */
+		PE_get_default("kern.vm_compressor", &vm_compressor_mode, sizeof(vm_compressor_mode));
+	}
+	PE_parse_boot_argn("vm_compressor_threads", &vm_compressor_thread_count, sizeof (vm_compressor_thread_count));
+
+	if (PE_parse_boot_argn("vm_compressor_immediate", &vm_compressor_temp, sizeof (vm_compressor_temp)))
+		vm_compressor_immediate_preferred_override = TRUE;
+	else {
+		if (PE_get_default("kern.vm_compressor_immediate", &vm_compressor_temp, sizeof(vm_compressor_temp)))
+			vm_compressor_immediate_preferred_override = TRUE;
+	}
+	if (vm_compressor_immediate_preferred_override == TRUE) {
+		if (vm_compressor_temp)
+			vm_compressor_immediate_preferred = TRUE;
+		else
+			vm_compressor_immediate_preferred = FALSE;
+	}
+	printf("\"vm_compressor_mode\" is %d\n", vm_compressor_mode);
+}
+
+/*
+ *	Routine:	vm_fault_cleanup
+ *	Purpose:
+ *		Clean up the result of vm_fault_page.
+ *	Results:
+ *		The paging reference for "object" is released.
+ *		"object" is unlocked.
+ *		If "top_page" is not null,  "top_page" is
+ *		freed and the paging reference for the object
+ *		containing it is released.
+ *
+ *	In/out conditions:
+ *		"object" must be locked.
+ */
+void
+vm_fault_cleanup(
+	register vm_object_t	object,
+	register vm_page_t	top_page)
+{
+	vm_object_paging_end(object);
+ 	vm_object_unlock(object);
+
+	if (top_page != VM_PAGE_NULL) {
+	        object = top_page->object;
+
+		vm_object_lock(object);
+		VM_PAGE_FREE(top_page);
+		vm_object_paging_end(object);
+		vm_object_unlock(object);
+	}
+}
+
+#if	MACH_CLUSTER_STATS
+#define MAXCLUSTERPAGES 16
+struct {
+	unsigned long pages_in_cluster;
+	unsigned long pages_at_higher_offsets;
+	unsigned long pages_at_lower_offsets;
+} cluster_stats_in[MAXCLUSTERPAGES];
+#define CLUSTER_STAT(clause)	clause
+#define CLUSTER_STAT_HIGHER(x)	\
+	((cluster_stats_in[(x)].pages_at_higher_offsets)++)
+#define CLUSTER_STAT_LOWER(x)	\
+	 ((cluster_stats_in[(x)].pages_at_lower_offsets)++)
+#define CLUSTER_STAT_CLUSTER(x)	\
+	((cluster_stats_in[(x)].pages_in_cluster)++)
+#else	/* MACH_CLUSTER_STATS */
+#define CLUSTER_STAT(clause)
+#endif	/* MACH_CLUSTER_STATS */
+
+#define ALIGNED(x) (((x) & (PAGE_SIZE_64 - 1)) == 0)
+
+
+boolean_t	vm_page_deactivate_behind = TRUE;
+/* 
+ * default sizes given VM_BEHAVIOR_DEFAULT reference behavior 
+ */
+#define VM_DEFAULT_DEACTIVATE_BEHIND_WINDOW	128
+#define VM_DEFAULT_DEACTIVATE_BEHIND_CLUSTER	16		/* don't make this too big... */
+                                                                /* we use it to size an array on the stack */
+
+int vm_default_behind = VM_DEFAULT_DEACTIVATE_BEHIND_WINDOW;
+
+#define MAX_SEQUENTIAL_RUN	(1024 * 1024 * 1024)
+
+/*
+ * vm_page_is_sequential
+ *
+ * Determine if sequential access is in progress
+ * in accordance with the behavior specified.
+ * Update state to indicate current access pattern.
+ *
+ * object must have at least the shared lock held
+ */
+static
+void
+vm_fault_is_sequential(
+	vm_object_t		object,
+	vm_object_offset_t	offset,
+	vm_behavior_t		behavior)
+{
+        vm_object_offset_t	last_alloc;
+	int			sequential;
+	int			orig_sequential;
+
+        last_alloc = object->last_alloc;
+	sequential = object->sequential;
+	orig_sequential = sequential;
+
+	switch (behavior) {
+	case VM_BEHAVIOR_RANDOM:
+	        /*
+		 * reset indicator of sequential behavior
+		 */
+	        sequential = 0;
+	        break;
+
+	case VM_BEHAVIOR_SEQUENTIAL:
+	        if (offset && last_alloc == offset - PAGE_SIZE_64) {
+		        /*
+			 * advance indicator of sequential behavior
+			 */
+		        if (sequential < MAX_SEQUENTIAL_RUN)
+			        sequential += PAGE_SIZE;
+		} else {
+		        /*
+			 * reset indicator of sequential behavior
+			 */
+		        sequential = 0;
+		}
+	        break;
+
+	case VM_BEHAVIOR_RSEQNTL:
+	        if (last_alloc && last_alloc == offset + PAGE_SIZE_64) {
+		        /*
+			 * advance indicator of sequential behavior
+			 */
+		        if (sequential > -MAX_SEQUENTIAL_RUN)
+			        sequential -= PAGE_SIZE;
+		} else {
+		        /*
+			 * reset indicator of sequential behavior
+			 */
+		        sequential = 0;
+		}
+	        break;
+
+	case VM_BEHAVIOR_DEFAULT:
+	default:
+	        if (offset && last_alloc == (offset - PAGE_SIZE_64)) {
+		        /*
+			 * advance indicator of sequential behavior
+			 */
+		        if (sequential < 0)
+			        sequential = 0;
+		        if (sequential < MAX_SEQUENTIAL_RUN)
+			        sequential += PAGE_SIZE;
+
+		} else if (last_alloc && last_alloc == (offset + PAGE_SIZE_64)) {
+		        /*
+			 * advance indicator of sequential behavior
+			 */
+		        if (sequential > 0)
+			        sequential = 0;
+		        if (sequential > -MAX_SEQUENTIAL_RUN)
+			        sequential -= PAGE_SIZE;
+		} else {
+		        /*
+			 * reset indicator of sequential behavior
+			 */
+		        sequential = 0;
+		}
+	        break;
+	}
+	if (sequential != orig_sequential) {
+	        if (!OSCompareAndSwap(orig_sequential, sequential, (UInt32 *)&object->sequential)) {
+		        /*
+			 * if someone else has already updated object->sequential
+			 * don't bother trying to update it or object->last_alloc
+			 */
+		        return;
+		}
+	}
+	/*
+	 * I'd like to do this with a OSCompareAndSwap64, but that
+	 * doesn't exist for PPC...  however, it shouldn't matter
+	 * that much... last_alloc is maintained so that we can determine
+	 * if a sequential access pattern is taking place... if only
+	 * one thread is banging on this object, no problem with the unprotected
+	 * update... if 2 or more threads are banging away, we run the risk of
+	 * someone seeing a mangled update... however, in the face of multiple
+	 * accesses, no sequential access pattern can develop anyway, so we
+	 * haven't lost any real info.
+	 */
+	object->last_alloc = offset;
+}
+
+
+int vm_page_deactivate_behind_count = 0;
+
+/*
+ * vm_page_deactivate_behind
+ *
+ * Determine if sequential access is in progress
+ * in accordance with the behavior specified.  If
+ * so, compute a potential page to deactivate and
+ * deactivate it.
+ *
+ * object must be locked.
+ *
+ * return TRUE if we actually deactivate a page
+ */
+static
+boolean_t
+vm_fault_deactivate_behind(
+	vm_object_t		object,
+	vm_object_offset_t	offset,
+	vm_behavior_t		behavior)
+{
+	int		n;
+	int		pages_in_run = 0;
+	int		max_pages_in_run = 0;
+	int		sequential_run;
+	int		sequential_behavior = VM_BEHAVIOR_SEQUENTIAL;
+	vm_object_offset_t	run_offset = 0;
+	vm_object_offset_t	pg_offset = 0;
+	vm_page_t	m;
+	vm_page_t	page_run[VM_DEFAULT_DEACTIVATE_BEHIND_CLUSTER];
+
+	pages_in_run = 0;
+#if TRACEFAULTPAGE
+	dbgTrace(0xBEEF0018, (unsigned int) object, (unsigned int) vm_fault_deactivate_behind);	/* (TEST/DEBUG) */
+#endif
+
+	if (object == kernel_object || vm_page_deactivate_behind == FALSE) {
+		/*
+		 * Do not deactivate pages from the kernel object: they
+		 * are not intended to become pageable.
+		 * or we've disabled the deactivate behind mechanism
+		 */
+		return FALSE;
+	}
+	if ((sequential_run = object->sequential)) {
+		  if (sequential_run < 0) {
+		          sequential_behavior = VM_BEHAVIOR_RSEQNTL;
+			  sequential_run = 0 - sequential_run;
+		  } else {
+		          sequential_behavior = VM_BEHAVIOR_SEQUENTIAL;
+		  }
+	}
+	switch (behavior) {
+	case VM_BEHAVIOR_RANDOM:
+		break;
+	case VM_BEHAVIOR_SEQUENTIAL:
+	        if (sequential_run >= (int)PAGE_SIZE) {
+			run_offset = 0 - PAGE_SIZE_64;
+			max_pages_in_run = 1;
+		}
+		break;
+	case VM_BEHAVIOR_RSEQNTL:
+	        if (sequential_run >= (int)PAGE_SIZE) {
+			run_offset = PAGE_SIZE_64;
+			max_pages_in_run = 1;
+		}
+		break;
+	case VM_BEHAVIOR_DEFAULT:
+	default:
+	{	vm_object_offset_t behind = vm_default_behind * PAGE_SIZE_64;
+
+	        /*
+		 * determine if the run of sequential accesss has been
+		 * long enough on an object with default access behavior
+		 * to consider it for deactivation
+		 */
+		if ((uint64_t)sequential_run >= behind && (sequential_run % (VM_DEFAULT_DEACTIVATE_BEHIND_CLUSTER * PAGE_SIZE)) == 0) {
+			/*
+			 * the comparisons between offset and behind are done
+			 * in this kind of odd fashion in order to prevent wrap around
+			 * at the end points
+			 */
+		        if (sequential_behavior == VM_BEHAVIOR_SEQUENTIAL) {
+			        if (offset >= behind) {
+					run_offset = 0 - behind;
+					pg_offset = PAGE_SIZE_64;
+					max_pages_in_run = VM_DEFAULT_DEACTIVATE_BEHIND_CLUSTER;
+				}
+			} else {
+			        if (offset < -behind) {
+					run_offset = behind;
+					pg_offset = 0 - PAGE_SIZE_64;
+					max_pages_in_run = VM_DEFAULT_DEACTIVATE_BEHIND_CLUSTER;
+				}
+			}
+		}
+		break;
+	}
+	}
+        for (n = 0; n < max_pages_in_run; n++) {
+		m = vm_page_lookup(object, offset + run_offset + (n * pg_offset));
+
+		if (m && !m->laundry && !m->busy && !m->no_cache && !m->throttled && !m->fictitious && !m->absent) {
+			page_run[pages_in_run++] = m;
+
+			/*
+			 * by not passing in a pmap_flush_context we will forgo any TLB flushing, local or otherwise...
+			 *
+			 * a TLB flush isn't really needed here since at worst we'll miss the reference bit being
+			 * updated in the PTE if a remote processor still has this mapping cached in its TLB when the
+			 * new reference happens. If no futher references happen on the page after that remote TLB flushes
+			 * we'll see a clean, non-referenced page when it eventually gets pulled out of the inactive queue
+			 * by pageout_scan, which is just fine since the last reference would have happened quite far
+			 * in the past (TLB caches don't hang around for very long), and of course could just as easily
+			 * have happened before we did the deactivate_behind.
+			 */
+			pmap_clear_refmod_options(m->phys_page, VM_MEM_REFERENCED, PMAP_OPTIONS_NOFLUSH, (void *)NULL);
+		}
+	}
+	if (pages_in_run) {
+		vm_page_lockspin_queues();
+
+		for (n = 0; n < pages_in_run; n++) {
+
+			m = page_run[n];
+
+			vm_page_deactivate_internal(m, FALSE);
+
+			vm_page_deactivate_behind_count++;
+#if TRACEFAULTPAGE
+			dbgTrace(0xBEEF0019, (unsigned int) object, (unsigned int) m);	/* (TEST/DEBUG) */
+#endif
+		}
+		vm_page_unlock_queues();
+
+		return TRUE;
+	}
+	return FALSE;
+}
+
+
+#if (DEVELOPMENT || DEBUG)
+uint32_t	vm_page_creation_throttled_hard = 0;
+uint32_t	vm_page_creation_throttled_soft = 0;
+uint64_t	vm_page_creation_throttle_avoided = 0;
+#endif /* DEVELOPMENT || DEBUG */
+
+static int
+vm_page_throttled(boolean_t page_kept)
+{
+        clock_sec_t     elapsed_sec;
+        clock_sec_t     tv_sec;
+        clock_usec_t    tv_usec;
+	
+	thread_t thread = current_thread();
+	
+	if (thread->options & TH_OPT_VMPRIV)
+		return (0);
+
+	if (thread->t_page_creation_throttled) {
+		thread->t_page_creation_throttled = 0;
+		
+		if (page_kept == FALSE)
+			goto no_throttle;
+	}
+	if (NEED_TO_HARD_THROTTLE_THIS_TASK()) {
+#if (DEVELOPMENT || DEBUG)
+		thread->t_page_creation_throttled_hard++;
+		OSAddAtomic(1, &vm_page_creation_throttled_hard);
+#endif /* DEVELOPMENT || DEBUG */
+		return (HARD_THROTTLE_DELAY);
+	}
+
+	if ((vm_page_free_count < vm_page_throttle_limit || ((COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) && SWAPPER_NEEDS_TO_UNTHROTTLE())) &&
+	    thread->t_page_creation_count > (VM_PAGE_CREATION_THROTTLE_PERIOD_SECS * VM_PAGE_CREATION_THROTTLE_RATE_PER_SEC)) {
+		
+		if (vm_page_free_wanted == 0 && vm_page_free_wanted_privileged == 0) {
+#if (DEVELOPMENT || DEBUG)
+			OSAddAtomic64(1, &vm_page_creation_throttle_avoided);
+#endif
+			goto no_throttle;
+		}
+		clock_get_system_microtime(&tv_sec, &tv_usec);
+
+		elapsed_sec = tv_sec - thread->t_page_creation_time;
+
+		if (elapsed_sec <= VM_PAGE_CREATION_THROTTLE_PERIOD_SECS ||
+		    (thread->t_page_creation_count / elapsed_sec) >= VM_PAGE_CREATION_THROTTLE_RATE_PER_SEC) {
+
+			if (elapsed_sec >= (3 * VM_PAGE_CREATION_THROTTLE_PERIOD_SECS)) {
+				/*
+				 * we'll reset our stats to give a well behaved app
+				 * that was unlucky enough to accumulate a bunch of pages
+				 * over a long period of time a chance to get out of
+				 * the throttled state... we reset the counter and timestamp
+				 * so that if it stays under the rate limit for the next second
+				 * it will be back in our good graces... if it exceeds it, it 
+				 * will remain in the throttled state
+				 */
+				thread->t_page_creation_time = tv_sec;
+				thread->t_page_creation_count = VM_PAGE_CREATION_THROTTLE_RATE_PER_SEC * (VM_PAGE_CREATION_THROTTLE_PERIOD_SECS - 1);
+			}
+			++vm_page_throttle_count;
+
+			thread->t_page_creation_throttled = 1;
+
+			if ((COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) && HARD_THROTTLE_LIMIT_REACHED()) {
+#if (DEVELOPMENT || DEBUG)
+				thread->t_page_creation_throttled_hard++;
+				OSAddAtomic(1, &vm_page_creation_throttled_hard);
+#endif /* DEVELOPMENT || DEBUG */
+				return (HARD_THROTTLE_DELAY);
+			} else {
+#if (DEVELOPMENT || DEBUG)
+				thread->t_page_creation_throttled_soft++;
+				OSAddAtomic(1, &vm_page_creation_throttled_soft);
+#endif /* DEVELOPMENT || DEBUG */
+				return (SOFT_THROTTLE_DELAY);
+			}
+		}
+		thread->t_page_creation_time = tv_sec;
+		thread->t_page_creation_count = 0;
+	}
+no_throttle:
+	thread->t_page_creation_count++;
+
+	return (0);
+}
+
+
+/*
+ * check for various conditions that would
+ * prevent us from creating a ZF page...
+ * cleanup is based on being called from vm_fault_page
+ *
+ * object must be locked
+ * object == m->object
+ */
+static vm_fault_return_t
+vm_fault_check(vm_object_t object, vm_page_t m, vm_page_t first_m, boolean_t interruptible_state, boolean_t page_throttle)
+{
+	int throttle_delay;
+
+        if (object->shadow_severed ||
+	    VM_OBJECT_PURGEABLE_FAULT_ERROR(object)) {
+	        /*
+		 * Either:
+		 * 1. the shadow chain was severed,
+		 * 2. the purgeable object is volatile or empty and is marked
+		 *    to fault on access while volatile.
+		 * Just have to return an error at this point
+		 */
+	        if (m != VM_PAGE_NULL)
+		        VM_PAGE_FREE(m);
+		vm_fault_cleanup(object, first_m);
+
+		thread_interrupt_level(interruptible_state);
+
+		return (VM_FAULT_MEMORY_ERROR);
+	}
+	if (vm_backing_store_low) {
+	        /*
+		 * are we protecting the system from
+		 * backing store exhaustion.  If so
+		 * sleep unless we are privileged.
+		 */
+	        if (!(current_task()->priv_flags & VM_BACKING_STORE_PRIV)) {
+
+			if (m != VM_PAGE_NULL)
+			        VM_PAGE_FREE(m);
+			vm_fault_cleanup(object, first_m);
+
+		        assert_wait((event_t)&vm_backing_store_low, THREAD_UNINT);
+
+			thread_block(THREAD_CONTINUE_NULL);
+			thread_interrupt_level(interruptible_state);
+
+			return (VM_FAULT_RETRY);
+		}
+	}
+	if (page_throttle == TRUE) {
+		if ((throttle_delay = vm_page_throttled(FALSE))) {
+			/*
+			 * we're throttling zero-fills...
+			 * treat this as if we couldn't grab a page
+			 */
+			if (m != VM_PAGE_NULL)
+				VM_PAGE_FREE(m);
+			vm_fault_cleanup(object, first_m);
+
+			VM_DEBUG_EVENT(vmf_check_zfdelay, VMF_CHECK_ZFDELAY, DBG_FUNC_NONE, throttle_delay, 0, 0, 0);
+
+			delay(throttle_delay);
+
+			if (current_thread_aborted()) {
+				thread_interrupt_level(interruptible_state);
+				return VM_FAULT_INTERRUPTED;
+			}
+			thread_interrupt_level(interruptible_state);
+
+			return (VM_FAULT_MEMORY_SHORTAGE);
+		}
+	}
+	return (VM_FAULT_SUCCESS);
+}
+
+
+/*
+ * do the work to zero fill a page and
+ * inject it into the correct paging queue
+ *
+ * m->object must be locked
+ * page queue lock must NOT be held
+ */
+static int
+vm_fault_zero_page(vm_page_t m, boolean_t no_zero_fill)
+{
+        int my_fault = DBG_ZERO_FILL_FAULT;
+
+	/*
+	 * This is is a zero-fill page fault...
+	 *
+	 * Checking the page lock is a waste of
+	 * time;  this page was absent, so
+	 * it can't be page locked by a pager.
+	 *
+	 * we also consider it undefined
+	 * with respect to instruction
+	 * execution.  i.e. it is the responsibility
+	 * of higher layers to call for an instruction
+	 * sync after changing the contents and before
+	 * sending a program into this area.  We 
+	 * choose this approach for performance
+	 */
+	m->pmapped = TRUE;
+
+	m->cs_validated = FALSE;
+	m->cs_tainted = FALSE;
+	m->cs_nx = FALSE;
+
+	if (no_zero_fill == TRUE) {
+		my_fault = DBG_NZF_PAGE_FAULT;
+
+		if (m->absent && m->busy)
+			return (my_fault);
+	} else {
+		vm_page_zero_fill(m);
+
+		VM_STAT_INCR(zero_fill_count);
+		DTRACE_VM2(zfod, int, 1, (uint64_t *), NULL);
+	}
+	assert(!m->laundry);
+	assert(m->object != kernel_object);
+	//assert(m->pageq.next == NULL && m->pageq.prev == NULL);
+
+	if (!VM_DYNAMIC_PAGING_ENABLED(memory_manager_default) &&
+		(m->object->purgable == VM_PURGABLE_DENY ||
+		 m->object->purgable == VM_PURGABLE_NONVOLATILE ||
+		 m->object->purgable == VM_PURGABLE_VOLATILE )) {
+
+		vm_page_lockspin_queues();
+
+		if (!VM_DYNAMIC_PAGING_ENABLED(memory_manager_default)) {
+			assert(!VM_PAGE_WIRED(m));
+
+			/*
+			 * can't be on the pageout queue since we don't
+			 * have a pager to try and clean to
+			 */
+			assert(!m->pageout_queue);
+
+			vm_page_queues_remove(m);
+			vm_page_check_pageable_safe(m);
+			queue_enter(&vm_page_queue_throttled, m, vm_page_t, pageq);
+			m->throttled = TRUE;
+			vm_page_throttled_count++;
+		}
+		vm_page_unlock_queues();
+	}
+	return (my_fault);
+}
+
+
+/*
+ *	Routine:	vm_fault_page
+ *	Purpose:
+ *		Find the resident page for the virtual memory
+ *		specified by the given virtual memory object
+ *		and offset.
+ *	Additional arguments:
+ *		The required permissions for the page is given
+ *		in "fault_type".  Desired permissions are included
+ *		in "protection".
+ *		fault_info is passed along to determine pagein cluster 
+ *		limits... it contains the expected reference pattern,
+ *		cluster size if available, etc...
+ *
+ *		If the desired page is known to be resident (for
+ *		example, because it was previously wired down), asserting
+ *		the "unwiring" parameter will speed the search.
+ *
+ *		If the operation can be interrupted (by thread_abort
+ *		or thread_terminate), then the "interruptible"
+ *		parameter should be asserted.
+ *
+ *	Results:
+ *		The page containing the proper data is returned
+ *		in "result_page".
+ *
+ *	In/out conditions:
+ *		The source object must be locked and referenced,
+ *		and must donate one paging reference.  The reference
+ *		is not affected.  The paging reference and lock are
+ *		consumed.
+ *
+ *		If the call succeeds, the object in which "result_page"
+ *		resides is left locked and holding a paging reference.
+ *		If this is not the original object, a busy page in the
+ *		original object is returned in "top_page", to prevent other
+ *		callers from pursuing this same data, along with a paging
+ *		reference for the original object.  The "top_page" should
+ *		be destroyed when this guarantee is no longer required.
+ *		The "result_page" is also left busy.  It is not removed
+ *		from the pageout queues.
+ *	Special Case:
+ *		A return value of VM_FAULT_SUCCESS_NO_PAGE means that the 
+ *		fault succeeded but there's no VM page (i.e. the VM object
+ * 		does not actually hold VM pages, but device memory or
+ *		large pages).  The object is still locked and we still hold a
+ *		paging_in_progress reference.
+ */
+unsigned int vm_fault_page_blocked_access = 0;
+unsigned int vm_fault_page_forced_retry = 0;
+
+vm_fault_return_t
+vm_fault_page(
+	/* Arguments: */
+	vm_object_t	first_object,	/* Object to begin search */
+	vm_object_offset_t first_offset,	/* Offset into object */
+	vm_prot_t	fault_type,	/* What access is requested */
+	boolean_t	must_be_resident,/* Must page be resident? */
+	boolean_t	caller_lookup,	/* caller looked up page */
+	/* Modifies in place: */
+	vm_prot_t	*protection,	/* Protection for mapping */
+	vm_page_t	*result_page,	/* Page found, if successful */
+	/* Returns: */
+	vm_page_t	*top_page,	/* Page in top object, if
+					 * not result_page.  */
+	int             *type_of_fault, /* if non-null, fill in with type of fault
+					 * COW, zero-fill, etc... returned in trace point */
+	/* More arguments: */
+	kern_return_t	*error_code,	/* code if page is in error */
+	boolean_t	no_zero_fill,	/* don't zero fill absent pages */
+	boolean_t	data_supply,	/* treat as data_supply if 
+					 * it is a write fault and a full
+					 * page is provided */
+	vm_object_fault_info_t fault_info)
+{
+	vm_page_t		m;
+	vm_object_t		object;
+	vm_object_offset_t	offset;
+	vm_page_t		first_m;
+	vm_object_t		next_object;
+	vm_object_t		copy_object;
+	boolean_t		look_for_page;
+	boolean_t		force_fault_retry = FALSE;
+	vm_prot_t		access_required = fault_type;
+	vm_prot_t		wants_copy_flag;
+	CLUSTER_STAT(int pages_at_higher_offsets;)
+	CLUSTER_STAT(int pages_at_lower_offsets;)
+	kern_return_t		wait_result;
+	boolean_t		interruptible_state;
+	boolean_t		data_already_requested = FALSE;
+	vm_behavior_t		orig_behavior;
+	vm_size_t		orig_cluster_size;
+	vm_fault_return_t	error;
+	int			my_fault;
+	uint32_t		try_failed_count;
+	int			interruptible; /* how may fault be interrupted? */
+	int			external_state = VM_EXTERNAL_STATE_UNKNOWN;
+	memory_object_t		pager;
+	vm_fault_return_t	retval;
+
+/*
+ * MACH page map - an optional optimization where a bit map is maintained
+ * by the VM subsystem for internal objects to indicate which pages of
+ * the object currently reside on backing store.  This existence map
+ * duplicates information maintained by the vnode pager.  It is 
+ * created at the time of the first pageout against the object, i.e. 
+ * at the same time pager for the object is created.  The optimization
+ * is designed to eliminate pager interaction overhead, if it is 
+ * 'known' that the page does not exist on backing store.
+ *
+ * MUST_ASK_PAGER() evaluates to TRUE if the page specified by object/offset is 
+ * either marked as paged out in the existence map for the object or no 
+ * existence map exists for the object.  MUST_ASK_PAGER() is one of the
+ * criteria in the decision to invoke the pager.   It is also used as one
+ * of the criteria to terminate the scan for adjacent pages in a clustered
+ * pagein operation.  Note that MUST_ASK_PAGER() always evaluates to TRUE for
+ * permanent objects.  Note also that if the pager for an internal object 
+ * has not been created, the pager is not invoked regardless of the value 
+ * of MUST_ASK_PAGER() and that clustered pagein scans are only done on an object
+ * for which a pager has been created.
+ *
+ * PAGED_OUT() evaluates to TRUE if the page specified by the object/offset
+ * is marked as paged out in the existence map for the object.  PAGED_OUT()
+ * PAGED_OUT() is used to determine if a page has already been pushed
+ * into a copy object in order to avoid a redundant page out operation.
+ */
+#if MACH_PAGEMAP
+#define MUST_ASK_PAGER(o, f, s)					\
+	((vm_external_state_get((o)->existence_map, (f))	\
+	  != VM_EXTERNAL_STATE_ABSENT) &&			\
+	 (s = (VM_COMPRESSOR_PAGER_STATE_GET((o), (f))))	\
+	 != VM_EXTERNAL_STATE_ABSENT)
+#define PAGED_OUT(o, f)						\
+	((vm_external_state_get((o)->existence_map, (f))	\
+	  == VM_EXTERNAL_STATE_EXISTS) ||			\
+	 (VM_COMPRESSOR_PAGER_STATE_GET((o), (f))		\
+	  == VM_EXTERNAL_STATE_EXISTS))
+#else /* MACH_PAGEMAP */
+#define MUST_ASK_PAGER(o, f, s)					\
+	((s = VM_COMPRESSOR_PAGER_STATE_GET((o), (f))) != VM_EXTERNAL_STATE_ABSENT)
+#define PAGED_OUT(o, f) \
+	(VM_COMPRESSOR_PAGER_STATE_GET((o), (f)) == VM_EXTERNAL_STATE_EXISTS)
+#endif /* MACH_PAGEMAP */
+
+/*
+ *	Recovery actions
+ */
+#define RELEASE_PAGE(m)					\
+	MACRO_BEGIN					\
+	PAGE_WAKEUP_DONE(m);				\
+	if (!m->active && !m->inactive && !m->throttled) {		\
+		vm_page_lockspin_queues();				\
+		if (!m->active && !m->inactive && !m->throttled) {	\
+			if (COMPRESSED_PAGER_IS_ACTIVE)	\
+                                vm_page_deactivate(m);                  \
+                        else						\
+				vm_page_activate(m);			\
+		}							\
+		vm_page_unlock_queues();				\
+	}								\
+	MACRO_END
+
+#if TRACEFAULTPAGE
+	dbgTrace(0xBEEF0002, (unsigned int) first_object, (unsigned int) first_offset);	/* (TEST/DEBUG) */
+#endif
+
+	interruptible = fault_info->interruptible;
+	interruptible_state = thread_interrupt_level(interruptible);
+ 
+	/*
+	 *	INVARIANTS (through entire routine):
+	 *
+	 *	1)	At all times, we must either have the object
+	 *		lock or a busy page in some object to prevent
+	 *		some other thread from trying to bring in
+	 *		the same page.
+	 *
+	 *		Note that we cannot hold any locks during the
+	 *		pager access or when waiting for memory, so
+	 *		we use a busy page then.
+	 *
+	 *	2)	To prevent another thread from racing us down the
+	 *		shadow chain and entering a new page in the top
+	 *		object before we do, we must keep a busy page in
+	 *		the top object while following the shadow chain.
+	 *
+	 *	3)	We must increment paging_in_progress on any object
+	 *		for which we have a busy page before dropping
+	 *		the object lock
+	 *
+	 *	4)	We leave busy pages on the pageout queues.
+	 *		If the pageout daemon comes across a busy page,
+	 *		it will remove the page from the pageout queues.
+	 */
+
+	object = first_object;
+	offset = first_offset;
+	first_m = VM_PAGE_NULL;
+	access_required = fault_type;
+
+
+	XPR(XPR_VM_FAULT,
+		"vm_f_page: obj 0x%X, offset 0x%X, type %d, prot %d\n",
+		object, offset, fault_type, *protection, 0);
+
+	/*
+	 * default type of fault
+	 */
+	my_fault = DBG_CACHE_HIT_FAULT;
+
+	while (TRUE) {
+#if TRACEFAULTPAGE
+		dbgTrace(0xBEEF0003, (unsigned int) 0, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+		if (!object->alive) {
+		        /*
+			 * object is no longer valid
+			 * clean up and return error
+			 */
+			vm_fault_cleanup(object, first_m);
+			thread_interrupt_level(interruptible_state);
+
+			return (VM_FAULT_MEMORY_ERROR);
+		}
+
+		if (!object->pager_created && object->phys_contiguous) {
+			/*
+			 * A physically-contiguous object without a pager:
+			 * must be a "large page" object.  We do not deal
+			 * with VM pages for this object.
+			 */
+			caller_lookup = FALSE;
+			m = VM_PAGE_NULL;
+			goto phys_contig_object;
+		}
+
+		if (object->blocked_access) {
+			/*
+			 * Access to this VM object has been blocked.
+			 * Replace our "paging_in_progress" reference with
+			 * a "activity_in_progress" reference and wait for
+			 * access to be unblocked.
+			 */
+			caller_lookup = FALSE; /* no longer valid after sleep */
+			vm_object_activity_begin(object);
+			vm_object_paging_end(object);
+			while (object->blocked_access) {
+				vm_object_sleep(object,
+						VM_OBJECT_EVENT_UNBLOCKED,
+						THREAD_UNINT);
+			}
+			vm_fault_page_blocked_access++;
+			vm_object_paging_begin(object);
+			vm_object_activity_end(object);
+		}
+
+		/*
+		 * See whether the page at 'offset' is resident
+		 */
+		if (caller_lookup == TRUE) {
+			/*
+			 * The caller has already looked up the page
+			 * and gave us the result in "result_page".
+			 * We can use this for the first lookup but
+			 * it loses its validity as soon as we unlock
+			 * the object.
+			 */
+			m = *result_page;
+			caller_lookup = FALSE; /* no longer valid after that */
+		} else {
+			m = vm_page_lookup(object, offset);
+		}
+#if TRACEFAULTPAGE
+		dbgTrace(0xBEEF0004, (unsigned int) m, (unsigned int) object);	/* (TEST/DEBUG) */
+#endif
+		if (m != VM_PAGE_NULL) {
+
+			if (m->busy) {
+			        /*
+				 * The page is being brought in,
+				 * wait for it and then retry.
+				 */
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF0005, (unsigned int) m, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+				wait_result = PAGE_SLEEP(object, m, interruptible);
+
+				XPR(XPR_VM_FAULT,
+				    "vm_f_page: block busy obj 0x%X, offset 0x%X, page 0x%X\n",
+				    object, offset,
+				    m, 0, 0);
+				counter(c_vm_fault_page_block_busy_kernel++);
+
+				if (wait_result != THREAD_AWAKENED) {
+					vm_fault_cleanup(object, first_m);
+					thread_interrupt_level(interruptible_state);
+
+					if (wait_result == THREAD_RESTART)
+						return (VM_FAULT_RETRY);
+					else
+						return (VM_FAULT_INTERRUPTED);
+				}
+				continue;
+			}
+			if (m->laundry) {
+				m->pageout = FALSE;
+
+				if (!m->cleaning) 
+					vm_pageout_steal_laundry(m, FALSE);
+			}
+			if (m->phys_page == vm_page_guard_addr) {
+				/*
+				 * Guard page: off limits !
+				 */
+				if (fault_type == VM_PROT_NONE) {
+					/*
+					 * The fault is not requesting any
+					 * access to the guard page, so it must
+					 * be just to wire or unwire it.
+					 * Let's pretend it succeeded...
+					 */
+					m->busy = TRUE;
+					*result_page = m;
+					assert(first_m == VM_PAGE_NULL);
+					*top_page = first_m;
+					if (type_of_fault)
+						*type_of_fault = DBG_GUARD_FAULT;
+					thread_interrupt_level(interruptible_state);
+					return VM_FAULT_SUCCESS;
+				} else {
+					/*
+					 * The fault requests access to the
+					 * guard page: let's deny that !
+					 */
+					vm_fault_cleanup(object, first_m);
+					thread_interrupt_level(interruptible_state);
+					return VM_FAULT_MEMORY_ERROR;
+				}
+			}
+
+			if (m->error) {
+			        /*
+				 * The page is in error, give up now.
+				 */
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF0006, (unsigned int) m, (unsigned int) error_code);	/* (TEST/DEBUG) */
+#endif
+				if (error_code)
+				        *error_code = KERN_MEMORY_ERROR;
+				VM_PAGE_FREE(m);
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_MEMORY_ERROR);
+			}
+			if (m->restart) {
+			        /*
+				 * The pager wants us to restart
+				 * at the top of the chain,
+				 * typically because it has moved the
+				 * page to another pager, then do so.
+				 */
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF0007, (unsigned int) m, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+				VM_PAGE_FREE(m);
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_RETRY);
+			}
+			if (m->absent) {
+			        /*
+				 * The page isn't busy, but is absent,
+				 * therefore it's deemed "unavailable".
+				 *
+				 * Remove the non-existent page (unless it's
+				 * in the top object) and move on down to the
+				 * next object (if there is one).
+				 */
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF0008, (unsigned int) m, (unsigned int) object->shadow);	/* (TEST/DEBUG) */
+#endif
+				next_object = object->shadow;
+
+				if (next_object == VM_OBJECT_NULL) {
+					/*
+					 * Absent page at bottom of shadow
+					 * chain; zero fill the page we left
+					 * busy in the first object, and free
+					 * the absent page.
+					 */
+					assert(!must_be_resident);
+
+					/*
+					 * check for any conditions that prevent
+					 * us from creating a new zero-fill page
+					 * vm_fault_check will do all of the 
+					 * fault cleanup in the case of an error condition
+					 * including resetting the thread_interrupt_level
+					 */
+					error = vm_fault_check(object, m, first_m, interruptible_state, (type_of_fault == NULL) ? TRUE : FALSE);
+
+					if (error != VM_FAULT_SUCCESS)
+					        return (error);
+
+					XPR(XPR_VM_FAULT,
+					    "vm_f_page: zero obj 0x%X, off 0x%X, page 0x%X, first_obj 0x%X\n",
+						object, offset,
+						m,
+						first_object, 0);
+
+					if (object != first_object) {
+					        /*
+						 * free the absent page we just found
+						 */
+						VM_PAGE_FREE(m);
+
+						/*
+						 * drop reference and lock on current object
+						 */
+						vm_object_paging_end(object);
+						vm_object_unlock(object);
+
+						/*
+						 * grab the original page we 
+						 * 'soldered' in place and
+						 * retake lock on 'first_object'
+						 */
+						m = first_m;
+						first_m = VM_PAGE_NULL;
+
+						object = first_object;
+						offset = first_offset;
+
+						vm_object_lock(object);
+					} else {
+					        /*
+						 * we're going to use the absent page we just found
+						 * so convert it to a 'busy' page
+						 */
+					        m->absent = FALSE;
+						m->busy = TRUE;
+					}
+					if (fault_info->mark_zf_absent && no_zero_fill == TRUE)
+						m->absent = TRUE;
+					/*
+					 * zero-fill the page and put it on
+					 * the correct paging queue
+					 */
+					my_fault = vm_fault_zero_page(m, no_zero_fill);
+
+					break;
+				} else {
+					if (must_be_resident)
+						vm_object_paging_end(object);
+					else if (object != first_object) {
+						vm_object_paging_end(object);
+						VM_PAGE_FREE(m);
+					} else {
+						first_m = m;
+						m->absent = FALSE;
+						m->busy = TRUE;
+
+						vm_page_lockspin_queues();
+
+						assert(!m->pageout_queue);
+						vm_page_queues_remove(m);
+
+						vm_page_unlock_queues();
+					}
+					XPR(XPR_VM_FAULT,
+					    "vm_f_page: unavail obj 0x%X, off 0x%X, next_obj 0x%X, newoff 0x%X\n",
+						object, offset,
+						next_object,
+						offset+object->vo_shadow_offset,0);
+
+					offset += object->vo_shadow_offset;
+					fault_info->lo_offset += object->vo_shadow_offset;
+					fault_info->hi_offset += object->vo_shadow_offset;
+					access_required = VM_PROT_READ;
+
+					vm_object_lock(next_object);
+					vm_object_unlock(object);
+					object = next_object;
+					vm_object_paging_begin(object);
+					
+					/*
+					 * reset to default type of fault
+					 */
+					my_fault = DBG_CACHE_HIT_FAULT;
+
+					continue;
+				}
+			}
+			if ((m->cleaning)
+			    && ((object != first_object) || (object->copy != VM_OBJECT_NULL))
+			    && (fault_type & VM_PROT_WRITE)) {
+				/*
+				 * This is a copy-on-write fault that will
+				 * cause us to revoke access to this page, but
+				 * this page is in the process of being cleaned
+				 * in a clustered pageout. We must wait until
+				 * the cleaning operation completes before
+				 * revoking access to the original page,
+				 * otherwise we might attempt to remove a
+				 * wired mapping.
+				 */
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF0009, (unsigned int) m, (unsigned int) offset);	/* (TEST/DEBUG) */
+#endif
+				XPR(XPR_VM_FAULT,
+				    "vm_f_page: cleaning obj 0x%X, offset 0x%X, page 0x%X\n",
+					object, offset,
+					m, 0, 0);
+				/*
+				 * take an extra ref so that object won't die
+				 */
+				vm_object_reference_locked(object);
+
+				vm_fault_cleanup(object, first_m);
+				
+				counter(c_vm_fault_page_block_backoff_kernel++);
+				vm_object_lock(object);
+				assert(object->ref_count > 0);
+
+				m = vm_page_lookup(object, offset);
+
+				if (m != VM_PAGE_NULL && m->cleaning) {
+					PAGE_ASSERT_WAIT(m, interruptible);
+
+					vm_object_unlock(object);
+					wait_result = thread_block(THREAD_CONTINUE_NULL);
+					vm_object_deallocate(object);
+
+					goto backoff;
+				} else {
+					vm_object_unlock(object);
+
+					vm_object_deallocate(object);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_RETRY);
+				}
+			}
+			if (type_of_fault == NULL && m->speculative &&
+			    !(fault_info != NULL && fault_info->stealth)) {
+			        /*
+				 * If we were passed a non-NULL pointer for
+				 * "type_of_fault", than we came from
+				 * vm_fault... we'll let it deal with
+				 * this condition, since it
+				 * needs to see m->speculative to correctly
+				 * account the pageins, otherwise...
+				 * take it off the speculative queue, we'll
+				 * let the caller of vm_fault_page deal
+				 * with getting it onto the correct queue
+				 *
+				 * If the caller specified in fault_info that
+				 * it wants a "stealth" fault, we also leave
+				 * the page in the speculative queue.
+				 */
+			        vm_page_lockspin_queues();
+				if (m->speculative)
+					vm_page_queues_remove(m);
+			        vm_page_unlock_queues();
+			}
+
+			if (m->encrypted) {
+				/*
+				 * ENCRYPTED SWAP:
+				 * the user needs access to a page that we
+				 * encrypted before paging it out.
+				 * Decrypt the page now.
+				 * Keep it busy to prevent anyone from
+				 * accessing it during the decryption.
+				 */
+				m->busy = TRUE;
+				vm_page_decrypt(m, 0);
+				assert(object == m->object);
+				assert(m->busy);
+				PAGE_WAKEUP_DONE(m);
+
+				/*
+				 * Retry from the top, in case
+				 * something changed while we were
+				 * decrypting.
+				 */
+				continue;
+			}
+			ASSERT_PAGE_DECRYPTED(m);
+
+			if (m->object->code_signed) {
+				/*
+				 * CODE SIGNING:
+				 * We just paged in a page from a signed
+				 * memory object but we don't need to
+				 * validate it now.  We'll validate it if
+				 * when it gets mapped into a user address
+				 * space for the first time or when the page
+				 * gets copied to another object as a result
+				 * of a copy-on-write.
+				 */
+			}
+
+			/*
+			 * We mark the page busy and leave it on
+			 * the pageout queues.  If the pageout
+			 * deamon comes across it, then it will
+			 * remove the page from the queue, but not the object
+			 */
+#if TRACEFAULTPAGE
+			dbgTrace(0xBEEF000B, (unsigned int) m, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+			XPR(XPR_VM_FAULT,
+			    "vm_f_page: found page obj 0x%X, offset 0x%X, page 0x%X\n",
+				object, offset, m, 0, 0);
+			assert(!m->busy);
+			assert(!m->absent);
+
+			m->busy = TRUE;
+			break;
+		}
+		
+
+		/*
+		 * we get here when there is no page present in the object at
+		 * the offset we're interested in... we'll allocate a page
+		 * at this point if the pager associated with
+		 * this object can provide the data or we're the top object...
+		 * object is locked;  m == NULL
+		 */
+		if (must_be_resident) {
+			if (fault_type == VM_PROT_NONE &&
+			    object == kernel_object) {
+				/*
+				 * We've been called from vm_fault_unwire()
+				 * while removing a map entry that was allocated
+				 * with KMA_KOBJECT and KMA_VAONLY.  This page
+				 * is not present and there's nothing more to
+				 * do here (nothing to unwire).
+				 */
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return VM_FAULT_MEMORY_ERROR;
+			}
+
+			goto dont_look_for_page;
+		}
+
+#if !MACH_PAGEMAP
+		data_supply = FALSE;
+#endif /* !MACH_PAGEMAP */
+
+		look_for_page =	(object->pager_created && (MUST_ASK_PAGER(object, offset, external_state) == TRUE) && !data_supply);
+		
+#if TRACEFAULTPAGE
+		dbgTrace(0xBEEF000C, (unsigned int) look_for_page, (unsigned int) object);	/* (TEST/DEBUG) */
+#endif
+		if (!look_for_page && object == first_object && !object->phys_contiguous) {
+			/*
+			 * Allocate a new page for this object/offset pair as a placeholder
+			 */
+			m = vm_page_grab();
+#if TRACEFAULTPAGE
+			dbgTrace(0xBEEF000D, (unsigned int) m, (unsigned int) object);	/* (TEST/DEBUG) */
+#endif
+			if (m == VM_PAGE_NULL) {
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_MEMORY_SHORTAGE);
+			}
+
+			if (fault_info && fault_info->batch_pmap_op == TRUE) {
+				vm_page_insert_internal(m, object, offset, VM_KERN_MEMORY_NONE, FALSE, TRUE, TRUE, FALSE, NULL);
+			} else {
+				vm_page_insert(m, object, offset);
+			}
+		}
+		if (look_for_page) {
+			kern_return_t	rc;
+			int		my_fault_type;
+
+			/*
+			 *	If the memory manager is not ready, we
+			 *	cannot make requests.
+			 */
+			if (!object->pager_ready) {
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF000E, (unsigned int) 0, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+				if (m != VM_PAGE_NULL)
+				        VM_PAGE_FREE(m);
+
+				XPR(XPR_VM_FAULT,
+				"vm_f_page: ready wait obj 0x%X, offset 0x%X\n",
+					object, offset, 0, 0, 0);
+
+				/*
+				 * take an extra ref so object won't die
+				 */
+				vm_object_reference_locked(object);
+				vm_fault_cleanup(object, first_m);
+				counter(c_vm_fault_page_block_backoff_kernel++);
+
+				vm_object_lock(object);
+				assert(object->ref_count > 0);
+
+				if (!object->pager_ready) {
+					wait_result = vm_object_assert_wait(object, VM_OBJECT_EVENT_PAGER_READY, interruptible);
+
+					vm_object_unlock(object);
+					if (wait_result == THREAD_WAITING)
+						wait_result = thread_block(THREAD_CONTINUE_NULL);
+					vm_object_deallocate(object);
+
+					goto backoff;
+				} else {
+					vm_object_unlock(object);
+					vm_object_deallocate(object);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_RETRY);
+				}
+			}
+			if (!object->internal && !object->phys_contiguous && object->paging_in_progress > vm_object_pagein_throttle) {
+				/*
+				 * If there are too many outstanding page
+				 * requests pending on this external object, we
+				 * wait for them to be resolved now.
+				 */
+#if TRACEFAULTPAGE
+				dbgTrace(0xBEEF0010, (unsigned int) m, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+				if (m != VM_PAGE_NULL)
+					VM_PAGE_FREE(m);
+				/*
+				 * take an extra ref so object won't die
+				 */
+				vm_object_reference_locked(object);
+
+				vm_fault_cleanup(object, first_m);
+
+				counter(c_vm_fault_page_block_backoff_kernel++);
+
+				vm_object_lock(object);
+				assert(object->ref_count > 0);
+
+				if (object->paging_in_progress >= vm_object_pagein_throttle) {
+				        vm_object_assert_wait(object, VM_OBJECT_EVENT_PAGING_ONLY_IN_PROGRESS, interruptible);
+
+					vm_object_unlock(object);
+					wait_result = thread_block(THREAD_CONTINUE_NULL);
+					vm_object_deallocate(object);
+
+					goto backoff;
+				} else {
+					vm_object_unlock(object);
+					vm_object_deallocate(object);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_RETRY);
+				}
+			}
+			if (object->internal &&
+			    (COMPRESSED_PAGER_IS_ACTIVE
+			     || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE)) {
+				int compressed_count_delta;
+
+				if (m == VM_PAGE_NULL) {
+					/*
+					 * Allocate a new page for this object/offset pair as a placeholder
+					 */
+					m = vm_page_grab();
+#if TRACEFAULTPAGE
+					dbgTrace(0xBEEF000D, (unsigned int) m, (unsigned int) object);	/* (TEST/DEBUG) */
+#endif
+					if (m == VM_PAGE_NULL) {
+
+						vm_fault_cleanup(object, first_m);
+						thread_interrupt_level(interruptible_state);
+
+						return (VM_FAULT_MEMORY_SHORTAGE);
+					}
+
+					m->absent = TRUE;
+					if (fault_info && fault_info->batch_pmap_op == TRUE) {
+						vm_page_insert_internal(m, object, offset, VM_KERN_MEMORY_NONE, FALSE, TRUE, TRUE, FALSE, NULL);
+					} else {
+						vm_page_insert(m, object, offset);
+					}
+				}
+				assert(m->busy);
+					
+				m->absent = TRUE;
+				pager = object->pager;
+
+				assert(object->paging_in_progress > 0);
+				vm_object_unlock(object);
+
+				rc = vm_compressor_pager_get(
+					pager,
+					offset + object->paging_offset,
+					m->phys_page,
+					&my_fault_type,
+					0,
+					&compressed_count_delta);
+
+				if (type_of_fault == NULL) {
+					int	throttle_delay;
+
+					/*
+					 * we weren't called from vm_fault, so we
+					 * need to apply page creation throttling
+					 * do it before we re-acquire any locks
+					 */
+					if (my_fault_type == DBG_COMPRESSOR_FAULT) {
+						if ((throttle_delay = vm_page_throttled(TRUE))) {
+							VM_DEBUG_EVENT(vmf_compressordelay, VMF_COMPRESSORDELAY, DBG_FUNC_NONE, throttle_delay, 0, 1, 0);
+							delay(throttle_delay);
+						}
+					}
+				}
+				vm_object_lock(object);
+				assert(object->paging_in_progress > 0);
+
+				vm_compressor_pager_count(
+					pager,
+					compressed_count_delta,
+					FALSE, /* shared_lock */
+					object);
+
+				switch (rc) {
+				case KERN_SUCCESS:
+					m->absent = FALSE;
+					m->dirty = TRUE;
+					if ((m->object->wimg_bits &
+					     VM_WIMG_MASK) !=
+					    VM_WIMG_USE_DEFAULT) {
+						/*
+						 * If the page is not cacheable,
+						 * we can't let its contents
+						 * linger in the data cache
+						 * after the decompression.
+						 */
+						pmap_sync_page_attributes_phys(
+							m->phys_page);
+					} else {
+						m->written_by_kernel = TRUE;
+					}
+
+					/*
+					 * If the object is purgeable, its
+					 * owner's purgeable ledgers have been
+					 * updated in vm_page_insert() but the
+					 * page was also accounted for in a
+					 * "compressed purgeable" ledger, so
+					 * update that now.
+					 */
+					if ((object->purgable !=
+					     VM_PURGABLE_DENY) &&
+					    (object->vo_purgeable_owner !=
+					     NULL)) {
+						/*
+						 * One less compressed
+						 * purgeable page.
+						 */
+						vm_purgeable_compressed_update(
+							object,
+							-1);
+					}
+
+					break;
+				case KERN_MEMORY_FAILURE:
+					m->unusual = TRUE;
+					m->error = TRUE;
+					m->absent = FALSE;
+					break;
+				case KERN_MEMORY_ERROR:
+					assert(m->absent);
+					break;
+				default:
+					panic("vm_fault_page(): unexpected "
+					      "error %d from "
+					      "vm_compressor_pager_get()\n",
+					      rc);
+				}
+				PAGE_WAKEUP_DONE(m);
+
+				rc = KERN_SUCCESS;
+				goto data_requested;
+			}
+			my_fault_type = DBG_PAGEIN_FAULT;
+		
+			if (m != VM_PAGE_NULL) {
+				VM_PAGE_FREE(m);
+				m = VM_PAGE_NULL;
+			}
+
+#if TRACEFAULTPAGE
+			dbgTrace(0xBEEF0012, (unsigned int) object, (unsigned int) 0);	/* (TEST/DEBUG) */
+#endif
+
+			/*
+			 * It's possible someone called vm_object_destroy while we weren't
+			 * holding the object lock.  If that has happened, then bail out 
+			 * here.
+			 */
+
+			pager = object->pager;
+
+			if (pager == MEMORY_OBJECT_NULL) {
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+				return VM_FAULT_MEMORY_ERROR;
+			}
+
+			/*
+			 * We have an absent page in place for the faulting offset,
+			 * so we can release the object lock.
+			 */
+
+			vm_object_unlock(object);
+
+			/*
+			 * If this object uses a copy_call strategy,
+			 * and we are interested in a copy of this object
+			 * (having gotten here only by following a
+			 * shadow chain), then tell the memory manager
+			 * via a flag added to the desired_access
+			 * parameter, so that it can detect a race
+			 * between our walking down the shadow chain
+			 * and its pushing pages up into a copy of
+			 * the object that it manages.
+			 */
+			if (object->copy_strategy == MEMORY_OBJECT_COPY_CALL && object != first_object)
+				wants_copy_flag = VM_PROT_WANTS_COPY;
+			else
+				wants_copy_flag = VM_PROT_NONE;
+
+			XPR(XPR_VM_FAULT,
+			    "vm_f_page: data_req obj 0x%X, offset 0x%X, page 0x%X, acc %d\n",
+				object, offset, m,
+				access_required | wants_copy_flag, 0);
+
+			if (object->copy == first_object) {
+				/*
+				 * if we issue the memory_object_data_request in
+				 * this state, we are subject to a deadlock with
+				 * the underlying filesystem if it is trying to
+				 * shrink the file resulting in a push of pages
+				 * into the copy object...  that push will stall
+				 * on the placeholder page, and if the pushing thread
+				 * is holding a lock that is required on the pagein
+				 * path (such as a truncate lock), we'll deadlock...
+				 * to avoid this potential deadlock, we throw away
+				 * our placeholder page before calling memory_object_data_request
+				 * and force this thread to retry the vm_fault_page after
+				 * we have issued the I/O.  the second time through this path
+				 * we will find the page already in the cache (presumably still
+				 * busy waiting for the I/O to complete) and then complete
+				 * the fault w/o having to go through memory_object_data_request again
+				 */
+				assert(first_m != VM_PAGE_NULL);
+				assert(first_m->object == first_object);
+					
+				vm_object_lock(first_object);
+				VM_PAGE_FREE(first_m);
+				vm_object_paging_end(first_object);
+				vm_object_unlock(first_object);
+
+				first_m = VM_PAGE_NULL;
+				force_fault_retry = TRUE;
+
+				vm_fault_page_forced_retry++;
+			}
+
+			if (data_already_requested == TRUE) {
+				orig_behavior = fault_info->behavior;
+				orig_cluster_size = fault_info->cluster_size;
+
+				fault_info->behavior = VM_BEHAVIOR_RANDOM;
+				fault_info->cluster_size = PAGE_SIZE;
+			}
+			/*
+			 * Call the memory manager to retrieve the data.
+			 */
+			rc = memory_object_data_request(
+				pager,
+				offset + object->paging_offset,
+				PAGE_SIZE,
+				access_required | wants_copy_flag,
+				(memory_object_fault_info_t)fault_info);
+
+			if (data_already_requested == TRUE) {
+				fault_info->behavior = orig_behavior;
+				fault_info->cluster_size = orig_cluster_size;
+			} else
+				data_already_requested = TRUE;
+
+			DTRACE_VM2(maj_fault, int, 1, (uint64_t *), NULL);
+#if TRACEFAULTPAGE
+			dbgTrace(0xBEEF0013, (unsigned int) object, (unsigned int) rc);	/* (TEST/DEBUG) */
+#endif
+			vm_object_lock(object);
+
+		data_requested:
+			if (rc != KERN_SUCCESS) {
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return ((rc == MACH_SEND_INTERRUPTED) ?
+					VM_FAULT_INTERRUPTED :
+					VM_FAULT_MEMORY_ERROR);
+			} else {
+				clock_sec_t     tv_sec;
+				clock_usec_t    tv_usec;
+
+				if (my_fault_type == DBG_PAGEIN_FAULT) {
+					clock_get_system_microtime(&tv_sec, &tv_usec);
+					current_thread()->t_page_creation_time = tv_sec;
+					current_thread()->t_page_creation_count = 0;
+				}
+			}
+			if ((interruptible != THREAD_UNINT) && (current_thread()->sched_flags & TH_SFLAG_ABORT)) {
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_INTERRUPTED);
+			}
+			if (force_fault_retry == TRUE) {
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_RETRY);
+			}
+			if (m == VM_PAGE_NULL && object->phys_contiguous) {
+				/*
+				 * No page here means that the object we
+				 * initially looked up was "physically 
+				 * contiguous" (i.e. device memory).  However,
+				 * with Virtual VRAM, the object might not
+				 * be backed by that device memory anymore,
+				 * so we're done here only if the object is
+				 * still "phys_contiguous".
+				 * Otherwise, if the object is no longer
+				 * "phys_contiguous", we need to retry the
+				 * page fault against the object's new backing
+				 * store (different memory object).
+				 */
+			phys_contig_object:
+				goto done;
+			}
+			/*
+			 * potentially a pagein fault
+			 * if we make it through the state checks
+			 * above, than we'll count it as such
+			 */
+			my_fault = my_fault_type;
+
+			/*
+			 * Retry with same object/offset, since new data may
+			 * be in a different page (i.e., m is meaningless at
+			 * this point).
+			 */
+			continue;
+		}
+dont_look_for_page:
+		/*
+		 * We get here if the object has no pager, or an existence map 
+		 * exists and indicates the page isn't present on the pager
+		 * or we're unwiring a page.  If a pager exists, but there
+		 * is no existence map, then the m->absent case above handles
+		 * the ZF case when the pager can't provide the page
+		 */
+#if TRACEFAULTPAGE
+		dbgTrace(0xBEEF0014, (unsigned int) object, (unsigned int) m);	/* (TEST/DEBUG) */
+#endif
+		if (object == first_object)
+			first_m = m;
+		else
+			assert(m == VM_PAGE_NULL);
+
+		XPR(XPR_VM_FAULT,
+		    "vm_f_page: no pager obj 0x%X, offset 0x%X, page 0x%X, next_obj 0x%X\n",
+			object, offset, m,
+			object->shadow, 0);
+
+		next_object = object->shadow;
+
+		if (next_object == VM_OBJECT_NULL) {
+			/*
+			 * we've hit the bottom of the shadown chain,
+			 * fill the page in the top object with zeros.
+			 */
+			assert(!must_be_resident);
+
+			if (object != first_object) {
+				vm_object_paging_end(object);
+				vm_object_unlock(object);
+
+				object = first_object;
+				offset = first_offset;
+				vm_object_lock(object);
+			}
+			m = first_m;
+			assert(m->object == object);
+			first_m = VM_PAGE_NULL;
+
+			/*
+			 * check for any conditions that prevent
+			 * us from creating a new zero-fill page
+			 * vm_fault_check will do all of the 
+			 * fault cleanup in the case of an error condition
+			 * including resetting the thread_interrupt_level
+			 */
+			error = vm_fault_check(object, m, first_m, interruptible_state, (type_of_fault == NULL) ? TRUE : FALSE);
+
+			if (error != VM_FAULT_SUCCESS)
+			        return (error);
+
+			if (m == VM_PAGE_NULL) {
+				m = vm_page_grab();
+
+				if (m == VM_PAGE_NULL) {
+					vm_fault_cleanup(object, VM_PAGE_NULL);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_MEMORY_SHORTAGE);
+				}
+				vm_page_insert(m, object, offset);
+			}
+			if (fault_info->mark_zf_absent && no_zero_fill == TRUE)
+				m->absent = TRUE;
+
+			my_fault = vm_fault_zero_page(m, no_zero_fill);
+
+			break;
+
+		} else {
+		        /*
+			 * Move on to the next object.  Lock the next
+			 * object before unlocking the current one.
+			 */
+			if ((object != first_object) || must_be_resident)
+				vm_object_paging_end(object);
+
+			offset += object->vo_shadow_offset;
+			fault_info->lo_offset += object->vo_shadow_offset;
+			fault_info->hi_offset += object->vo_shadow_offset;
+			access_required = VM_PROT_READ;
+
+			vm_object_lock(next_object);
+			vm_object_unlock(object);
+
+			object = next_object;
+			vm_object_paging_begin(object);
+		}
+	}
+
+	/*
+	 *	PAGE HAS BEEN FOUND.
+	 *
+	 *	This page (m) is:
+	 *		busy, so that we can play with it;
+	 *		not absent, so that nobody else will fill it;
+	 *		possibly eligible for pageout;
+	 *
+	 *	The top-level page (first_m) is:
+	 *		VM_PAGE_NULL if the page was found in the
+	 *		 top-level object;
+	 *		busy, not absent, and ineligible for pageout.
+	 *
+	 *	The current object (object) is locked.  A paging
+	 *	reference is held for the current and top-level
+	 *	objects.
+	 */
+
+#if TRACEFAULTPAGE
+	dbgTrace(0xBEEF0015, (unsigned int) object, (unsigned int) m);	/* (TEST/DEBUG) */
+#endif
+#if	EXTRA_ASSERTIONS
+	assert(m->busy && !m->absent);
+	assert((first_m == VM_PAGE_NULL) ||
+	       (first_m->busy && !first_m->absent &&
+		!first_m->active && !first_m->inactive));
+#endif	/* EXTRA_ASSERTIONS */
+
+	/*
+	 * ENCRYPTED SWAP:
+	 * If we found a page, we must have decrypted it before we
+	 * get here...
+	 */
+	ASSERT_PAGE_DECRYPTED(m);
+
+	XPR(XPR_VM_FAULT,
+	    "vm_f_page: FOUND obj 0x%X, off 0x%X, page 0x%X, 1_obj 0x%X, 1_m 0x%X\n",
+		object, offset, m,
+		first_object, first_m);
+
+	/*
+	 * If the page is being written, but isn't
+	 * already owned by the top-level object,
+	 * we have to copy it into a new page owned
+	 * by the top-level object.
+	 */
+	if (object != first_object) {
+
+#if TRACEFAULTPAGE
+		dbgTrace(0xBEEF0016, (unsigned int) object, (unsigned int) fault_type);	/* (TEST/DEBUG) */
+#endif
+	    	if (fault_type & VM_PROT_WRITE) {
+			vm_page_t copy_m;
+
+			/*
+			 * We only really need to copy if we
+			 * want to write it.
+			 */
+			assert(!must_be_resident);
+
+			/*
+			 * are we protecting the system from
+			 * backing store exhaustion.  If so
+			 * sleep unless we are privileged.
+			 */
+			if (vm_backing_store_low) {
+				if (!(current_task()->priv_flags & VM_BACKING_STORE_PRIV)) {
+
+					RELEASE_PAGE(m);
+					vm_fault_cleanup(object, first_m);
+
+					assert_wait((event_t)&vm_backing_store_low, THREAD_UNINT);
+
+					thread_block(THREAD_CONTINUE_NULL);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_RETRY);
+				}
+			}
+			/*
+			 * If we try to collapse first_object at this
+			 * point, we may deadlock when we try to get
+			 * the lock on an intermediate object (since we
+			 * have the bottom object locked).  We can't
+			 * unlock the bottom object, because the page
+			 * we found may move (by collapse) if we do.
+			 *
+			 * Instead, we first copy the page.  Then, when
+			 * we have no more use for the bottom object,
+			 * we unlock it and try to collapse.
+			 *
+			 * Note that we copy the page even if we didn't
+			 * need to... that's the breaks.
+			 */
+
+			/*
+			 * Allocate a page for the copy
+			 */
+			copy_m = vm_page_grab();
+
+			if (copy_m == VM_PAGE_NULL) {
+				RELEASE_PAGE(m);
+
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_MEMORY_SHORTAGE);
+			}
+			XPR(XPR_VM_FAULT,
+			    "vm_f_page: page_copy obj 0x%X, offset 0x%X, m 0x%X, copy_m 0x%X\n",
+				object, offset,
+				m, copy_m, 0);
+
+			vm_page_copy(m, copy_m);
+
+			/*
+			 * If another map is truly sharing this
+			 * page with us, we have to flush all
+			 * uses of the original page, since we
+			 * can't distinguish those which want the
+			 * original from those which need the
+			 * new copy.
+			 *
+			 * XXXO If we know that only one map has
+			 * access to this page, then we could
+			 * avoid the pmap_disconnect() call.
+			 */
+			if (m->pmapped)
+			        pmap_disconnect(m->phys_page);
+
+			if (m->clustered) {
+				VM_PAGE_COUNT_AS_PAGEIN(m);
+				VM_PAGE_CONSUME_CLUSTERED(m);
+			}
+			assert(!m->cleaning);
+
+			/*
+			 * We no longer need the old page or object.
+			 */
+			RELEASE_PAGE(m);
+
+			vm_object_paging_end(object);
+			vm_object_unlock(object);
+
+			my_fault = DBG_COW_FAULT;
+			VM_STAT_INCR(cow_faults);
+			DTRACE_VM2(cow_fault, int, 1, (uint64_t *), NULL);
+			current_task()->cow_faults++;
+
+			object = first_object;
+			offset = first_offset;
+
+			vm_object_lock(object);
+			/*
+			 * get rid of the place holder
+			 * page that we soldered in earlier
+			 */
+			VM_PAGE_FREE(first_m);
+			first_m = VM_PAGE_NULL;
+			
+			/*
+			 * and replace it with the
+			 * page we just copied into
+			 */
+			assert(copy_m->busy);
+			vm_page_insert(copy_m, object, offset);
+			SET_PAGE_DIRTY(copy_m, TRUE);
+
+			m = copy_m;
+			/*
+			 * Now that we've gotten the copy out of the
+			 * way, let's try to collapse the top object.
+			 * But we have to play ugly games with
+			 * paging_in_progress to do that...
+			 */     
+			vm_object_paging_end(object); 
+			vm_object_collapse(object, offset, TRUE);
+			vm_object_paging_begin(object);
+
+		} else
+		    	*protection &= (~VM_PROT_WRITE);
+	}
+	/*
+	 * Now check whether the page needs to be pushed into the
+	 * copy object.  The use of asymmetric copy on write for
+	 * shared temporary objects means that we may do two copies to
+	 * satisfy the fault; one above to get the page from a
+	 * shadowed object, and one here to push it into the copy.
+	 */
+	try_failed_count = 0;
+
+	while ((copy_object = first_object->copy) != VM_OBJECT_NULL) {
+		vm_object_offset_t	copy_offset;
+		vm_page_t		copy_m;
+
+#if TRACEFAULTPAGE
+		dbgTrace(0xBEEF0017, (unsigned int) copy_object, (unsigned int) fault_type);	/* (TEST/DEBUG) */
+#endif
+		/*
+		 * If the page is being written, but hasn't been
+		 * copied to the copy-object, we have to copy it there.
+		 */
+		if ((fault_type & VM_PROT_WRITE) == 0) {
+			*protection &= ~VM_PROT_WRITE;
+			break;
+		}
+
+		/*
+		 * If the page was guaranteed to be resident,
+		 * we must have already performed the copy.
+		 */
+		if (must_be_resident)
+			break;
+
+		/*
+		 * Try to get the lock on the copy_object.
+		 */
+		if (!vm_object_lock_try(copy_object)) {
+
+			vm_object_unlock(object);
+			try_failed_count++;
+
+			mutex_pause(try_failed_count);	/* wait a bit */
+			vm_object_lock(object);
+
+			continue;
+		}
+		try_failed_count = 0;
+
+		/*
+		 * Make another reference to the copy-object,
+		 * to keep it from disappearing during the
+		 * copy.
+		 */
+		vm_object_reference_locked(copy_object);
+
+		/*
+		 * Does the page exist in the copy?
+		 */
+		copy_offset = first_offset - copy_object->vo_shadow_offset;
+
+		if (copy_object->vo_size <= copy_offset)
+			/*
+			 * Copy object doesn't cover this page -- do nothing.
+			 */
+			;
+		else if ((copy_m = vm_page_lookup(copy_object, copy_offset)) != VM_PAGE_NULL) {
+			/*
+			 * Page currently exists in the copy object
+			 */
+			if (copy_m->busy) {
+				/*
+				 * If the page is being brought
+				 * in, wait for it and then retry.
+				 */
+				RELEASE_PAGE(m);
+
+				/*
+				 * take an extra ref so object won't die
+				 */
+				vm_object_reference_locked(copy_object);
+				vm_object_unlock(copy_object);
+				vm_fault_cleanup(object, first_m);
+				counter(c_vm_fault_page_block_backoff_kernel++);
+
+				vm_object_lock(copy_object);
+				assert(copy_object->ref_count > 0);
+				VM_OBJ_RES_DECR(copy_object);
+				vm_object_lock_assert_exclusive(copy_object);
+				copy_object->ref_count--;
+				assert(copy_object->ref_count > 0);
+				copy_m = vm_page_lookup(copy_object, copy_offset);
+				/*
+				 * ENCRYPTED SWAP:
+				 * it's OK if the "copy_m" page is encrypted,
+				 * because we're not moving it nor handling its
+				 * contents.
+				 */
+				if (copy_m != VM_PAGE_NULL && copy_m->busy) {
+					PAGE_ASSERT_WAIT(copy_m, interruptible);
+
+					vm_object_unlock(copy_object);
+					wait_result = thread_block(THREAD_CONTINUE_NULL);
+					vm_object_deallocate(copy_object);
+
+					goto backoff;
+				} else {
+					vm_object_unlock(copy_object);
+					vm_object_deallocate(copy_object);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_RETRY);
+				}
+			}
+		}
+		else if (!PAGED_OUT(copy_object, copy_offset)) {
+			/*
+			 * If PAGED_OUT is TRUE, then the page used to exist
+			 * in the copy-object, and has already been paged out.
+			 * We don't need to repeat this. If PAGED_OUT is
+			 * FALSE, then either we don't know (!pager_created,
+			 * for example) or it hasn't been paged out.
+			 * (VM_EXTERNAL_STATE_UNKNOWN||VM_EXTERNAL_STATE_ABSENT)
+			 * We must copy the page to the copy object.
+			 */
+
+			if (vm_backing_store_low) {
+			        /*
+				 * we are protecting the system from
+				 * backing store exhaustion.  If so
+				 * sleep unless we are privileged.
+				 */
+				if (!(current_task()->priv_flags & VM_BACKING_STORE_PRIV)) {
+					assert_wait((event_t)&vm_backing_store_low, THREAD_UNINT);
+
+					RELEASE_PAGE(m);
+					VM_OBJ_RES_DECR(copy_object);
+					vm_object_lock_assert_exclusive(copy_object);
+					copy_object->ref_count--;
+					assert(copy_object->ref_count > 0);
+
+					vm_object_unlock(copy_object);
+					vm_fault_cleanup(object, first_m);
+					thread_block(THREAD_CONTINUE_NULL);
+					thread_interrupt_level(interruptible_state);
+
+					return (VM_FAULT_RETRY);
+				}
+			}
+			/*
+			 * Allocate a page for the copy
+			 */
+			copy_m = vm_page_alloc(copy_object, copy_offset);
+
+			if (copy_m == VM_PAGE_NULL) {
+				RELEASE_PAGE(m);
+
+				VM_OBJ_RES_DECR(copy_object);
+				vm_object_lock_assert_exclusive(copy_object);
+				copy_object->ref_count--;
+				assert(copy_object->ref_count > 0);
+
+				vm_object_unlock(copy_object);
+				vm_fault_cleanup(object, first_m);
+				thread_interrupt_level(interruptible_state);
+
+				return (VM_FAULT_MEMORY_SHORTAGE);
+			}
+			/*
+			 * Must copy page into copy-object.
+			 */
+			vm_page_copy(m, copy_m);
+			
+			/*
+			 * If the old page was in use by any users
+			 * of the copy-object, it must be removed
+			 * from all pmaps.  (We can't know which
+			 * pmaps use it.)
+			 */
+			if (m->pmapped)
+			        pmap_disconnect(m->phys_page);
+
+			if (m->clustered) {
+				VM_PAGE_COUNT_AS_PAGEIN(m);
+				VM_PAGE_CONSUME_CLUSTERED(m);
+			}
+			/*
+			 * If there's a pager, then immediately
+			 * page out this page, using the "initialize"
+			 * option.  Else, we use the copy.
+			 */
+		 	if ((!copy_object->pager_ready)
+#if MACH_PAGEMAP
+			    || vm_external_state_get(copy_object->existence_map, copy_offset) == VM_EXTERNAL_STATE_ABSENT
+#endif
+			    || VM_COMPRESSOR_PAGER_STATE_GET(copy_object, copy_offset) == VM_EXTERNAL_STATE_ABSENT
+			    ) {
+
+				vm_page_lockspin_queues();
+				assert(!m->cleaning);
+				vm_page_activate(copy_m);
+				vm_page_unlock_queues();
+
+				SET_PAGE_DIRTY(copy_m, TRUE);
+				PAGE_WAKEUP_DONE(copy_m);
+
+			} else if (copy_object->internal &&
+				   (DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE)) {
+				/*
+				 * For internal objects check with the pager to see
+				 * if the page already exists in the backing store.
+				 * If yes, then we can drop the copy page. If not,
+				 * then we'll activate it, mark it dirty and keep it
+				 * around.
+				 */
+				
+				kern_return_t kr = KERN_SUCCESS;
+
+				memory_object_t	copy_pager = copy_object->pager;
+				assert(copy_pager != MEMORY_OBJECT_NULL);
+				vm_object_paging_begin(copy_object);
+
+				vm_object_unlock(copy_object);
+
+				kr = memory_object_data_request(
+					copy_pager,
+					copy_offset + copy_object->paging_offset,
+					0, /* Only query the pager. */
+					VM_PROT_READ,
+					NULL);
+				
+				vm_object_lock(copy_object);
+
+				vm_object_paging_end(copy_object);
+
+				/*
+				 * Since we dropped the copy_object's lock,
+				 * check whether we'll have to deallocate 
+				 * the hard way.
+				 */
+				if ((copy_object->shadow != object) || (copy_object->ref_count == 1)) {
+					vm_object_unlock(copy_object);
+					vm_object_deallocate(copy_object);
+					vm_object_lock(object);
+
+					continue;
+				}
+				if (kr == KERN_SUCCESS) {
+					/*
+					 * The pager has the page. We don't want to overwrite
+					 * that page by sending this one out to the backing store.
+					 * So we drop the copy page.
+					 */
+					VM_PAGE_FREE(copy_m);
+
+				} else {
+					/*
+					 * The pager doesn't have the page. We'll keep this one
+					 * around in the copy object. It might get sent out to 
+					 * the backing store under memory pressure.	 
+					 */
+					vm_page_lockspin_queues();
+					assert(!m->cleaning);
+					vm_page_activate(copy_m);
+					vm_page_unlock_queues();
+
+					SET_PAGE_DIRTY(copy_m, TRUE);
+					PAGE_WAKEUP_DONE(copy_m);
+				} 
+			} else {
+				
+				assert(copy_m->busy == TRUE);
+				assert(!m->cleaning);
+
+				/*
+				 * dirty is protected by the object lock
+				 */
+				SET_PAGE_DIRTY(copy_m, TRUE);
+
+				/*
+				 * The page is already ready for pageout:
+				 * not on pageout queues and busy.
+				 * Unlock everything except the
+				 * copy_object itself.
+				 */
+				vm_object_unlock(object);
+
+				/*
+				 * Write the page to the copy-object,
+				 * flushing it from the kernel.
+				 */
+				vm_pageout_initialize_page(copy_m);
+
+				/*
+				 * Since the pageout may have
+				 * temporarily dropped the
+				 * copy_object's lock, we
+				 * check whether we'll have
+				 * to deallocate the hard way.
+				 */
+				if ((copy_object->shadow != object) || (copy_object->ref_count == 1)) {
+					vm_object_unlock(copy_object);
+					vm_object_deallocate(copy_object);
+					vm_object_lock(object);
+
+					continue;
+				}
+				/*
+				 * Pick back up the old object's
+				 * lock.  [It is safe to do so,
+				 * since it must be deeper in the
+				 * object tree.]
+				 */
+				vm_object_lock(object);
+			}
+
+			/*
+			 * Because we're pushing a page upward
+			 * in the object tree, we must restart
+			 * any faults that are waiting here.
+			 * [Note that this is an expansion of
+			 * PAGE_WAKEUP that uses the THREAD_RESTART
+			 * wait result].  Can't turn off the page's
+			 * busy bit because we're not done with it.
+			 */
+			if (m->wanted) {
+				m->wanted = FALSE;
+				thread_wakeup_with_result((event_t) m, THREAD_RESTART);
+			}
+		}
+		/*
+		 * The reference count on copy_object must be
+		 * at least 2: one for our extra reference,
+		 * and at least one from the outside world
+		 * (we checked that when we last locked
+		 * copy_object).
+		 */
+		vm_object_lock_assert_exclusive(copy_object);
+		copy_object->ref_count--;
+		assert(copy_object->ref_count > 0);
+
+		VM_OBJ_RES_DECR(copy_object);	
+		vm_object_unlock(copy_object);
+
+		break;
+	}
+
+done:
+	*result_page = m;
+	*top_page = first_m;
+
+	XPR(XPR_VM_FAULT,
+		"vm_f_page: DONE obj 0x%X, offset 0x%X, m 0x%X, first_m 0x%X\n",
+		object, offset, m, first_m, 0);
+
+	if (m != VM_PAGE_NULL) {
+		retval = VM_FAULT_SUCCESS;
+
+		if (my_fault == DBG_PAGEIN_FAULT) {
+
+			VM_PAGE_COUNT_AS_PAGEIN(m);
+
+			if (m->object->internal)
+				my_fault = DBG_PAGEIND_FAULT;
+			else
+				my_fault = DBG_PAGEINV_FAULT;
+
+		        /*
+			 * evaluate access pattern and update state
+			 * vm_fault_deactivate_behind depends on the
+			 * state being up to date
+			 */
+		        vm_fault_is_sequential(object, offset, fault_info->behavior);
+
+			vm_fault_deactivate_behind(object, offset, fault_info->behavior);
+		} else if (my_fault == DBG_COMPRESSOR_FAULT || my_fault == DBG_COMPRESSOR_SWAPIN_FAULT) {
+
+			VM_STAT_INCR(decompressions);
+		}
+		if (type_of_fault)
+		        *type_of_fault = my_fault;
+	} else {
+		retval = VM_FAULT_SUCCESS_NO_VM_PAGE;
+		assert(first_m == VM_PAGE_NULL);
+		assert(object == first_object);
+	}
+
+	thread_interrupt_level(interruptible_state);
+
+#if TRACEFAULTPAGE
+	dbgTrace(0xBEEF001A, (unsigned int) VM_FAULT_SUCCESS, 0);	/* (TEST/DEBUG) */
+#endif
+	return retval;
+
+backoff:
+	thread_interrupt_level(interruptible_state);
+
+	if (wait_result == THREAD_INTERRUPTED)
+		return (VM_FAULT_INTERRUPTED);
+	return (VM_FAULT_RETRY);
+
+#undef	RELEASE_PAGE
+}
+
+
+
+/*
+ * CODE SIGNING:
+ * When soft faulting a page, we have to validate the page if:
+ * 1. the page is being mapped in user space
+ * 2. the page hasn't already been found to be "tainted"
+ * 3. the page belongs to a code-signed object
+ * 4. the page has not been validated yet or has been mapped for write.
+ */
+#define VM_FAULT_NEED_CS_VALIDATION(pmap, page)				\
+	((pmap) != kernel_pmap /*1*/ &&					\
+	 !(page)->cs_tainted /*2*/ &&					\
+	 (page)->object->code_signed /*3*/ &&				\
+	 (!(page)->cs_validated || (page)->wpmapped /*4*/))
+
+
+/*
+ * page queue lock must NOT be held
+ * m->object must be locked
+ *
+ * NOTE: m->object could be locked "shared" only if we are called
+ * from vm_fault() as part of a soft fault.  If so, we must be
+ * careful not to modify the VM object in any way that is not
+ * legal under a shared lock...
+ */
+extern int proc_selfpid(void);
+extern char *proc_name_address(void *p);
+unsigned long cs_enter_tainted_rejected = 0;
+unsigned long cs_enter_tainted_accepted = 0;
+kern_return_t
+vm_fault_enter(vm_page_t m,
+	       pmap_t pmap,
+	       vm_map_offset_t vaddr,
+	       vm_prot_t prot,
+	       vm_prot_t caller_prot,
+	       boolean_t wired,
+	       boolean_t change_wiring,
+	       boolean_t no_cache,
+	       boolean_t cs_bypass,
+	       __unused int	 user_tag,
+	       int	 pmap_options,
+	       boolean_t *need_retry,
+	       int *type_of_fault)
+{
+	kern_return_t	kr, pe_result;
+	boolean_t	previously_pmapped = m->pmapped;
+	boolean_t	must_disconnect = 0;
+	boolean_t	map_is_switched, map_is_switch_protected;
+	int		cs_enforcement_enabled;
+	vm_prot_t       fault_type;
+	
+	fault_type = change_wiring ? VM_PROT_NONE : caller_prot;
+
+	vm_object_lock_assert_held(m->object);
+#if DEBUG
+	lck_mtx_assert(&vm_page_queue_lock, LCK_MTX_ASSERT_NOTOWNED);
+#endif /* DEBUG */
+
+	if (m->phys_page == vm_page_guard_addr) {
+		assert(m->fictitious);
+		return KERN_SUCCESS;
+	}
+
+	if (*type_of_fault == DBG_ZERO_FILL_FAULT) {
+
+		vm_object_lock_assert_exclusive(m->object);
+
+	} else if ((fault_type & VM_PROT_WRITE) == 0) {
+		/*
+		 * This is not a "write" fault, so we
+		 * might not have taken the object lock
+		 * exclusively and we might not be able
+		 * to update the "wpmapped" bit in
+		 * vm_fault_enter().
+		 * Let's just grant read access to
+		 * the page for now and we'll
+		 * soft-fault again if we need write
+		 * access later...
+		 */
+		prot &= ~VM_PROT_WRITE;
+	}
+	if (m->pmapped == FALSE) {
+
+		if (m->clustered) {
+			if (*type_of_fault == DBG_CACHE_HIT_FAULT) {
+				/*
+				 * found it in the cache, but this
+				 * is the first fault-in of the page (m->pmapped == FALSE)
+				 * so it must have come in as part of
+				 * a cluster... account 1 pagein against it
+				 */
+				if (m->object->internal)
+					*type_of_fault = DBG_PAGEIND_FAULT;
+				else
+					*type_of_fault = DBG_PAGEINV_FAULT;
+				
+				VM_PAGE_COUNT_AS_PAGEIN(m);
+			}
+			VM_PAGE_CONSUME_CLUSTERED(m);
+		}
+	}
+
+	if (*type_of_fault != DBG_COW_FAULT) {
+		DTRACE_VM2(as_fault, int, 1, (uint64_t *), NULL);
+
+		if (pmap == kernel_pmap) {
+			DTRACE_VM2(kernel_asflt, int, 1, (uint64_t *), NULL);
+		}
+	}
+
+	/* Validate code signature if necessary. */
+	if (VM_FAULT_NEED_CS_VALIDATION(pmap, m)) {
+		vm_object_lock_assert_exclusive(m->object);
+
+		if (m->cs_validated) {
+			vm_cs_revalidates++;
+		}
+
+		/* VM map is locked, so 1 ref will remain on VM object - 
+		 * so no harm if vm_page_validate_cs drops the object lock */
+		vm_page_validate_cs(m);
+	}
+
+#define page_immutable(m,prot) ((m)->cs_validated /*&& ((prot) & VM_PROT_EXECUTE)*/)
+#define page_nx(m) ((m)->cs_nx)
+
+	map_is_switched = ((pmap != vm_map_pmap(current_task()->map)) &&
+			   (pmap == vm_map_pmap(current_thread()->map)));
+	map_is_switch_protected = current_thread()->map->switch_protect;
+	
+	/* If the map is switched, and is switch-protected, we must protect
+	 * some pages from being write-faulted: immutable pages because by 
+	 * definition they may not be written, and executable pages because that
+	 * would provide a way to inject unsigned code.
+	 * If the page is immutable, we can simply return. However, we can't
+	 * immediately determine whether a page is executable anywhere. But,
+	 * we can disconnect it everywhere and remove the executable protection
+	 * from the current map. We do that below right before we do the 
+	 * PMAP_ENTER.
+	 */
+	cs_enforcement_enabled = cs_enforcement(NULL);
+
+	if(cs_enforcement_enabled && map_is_switched && 
+	   map_is_switch_protected && page_immutable(m, prot) && 
+	   (prot & VM_PROT_WRITE))
+	{
+		return KERN_CODESIGN_ERROR;
+	}
+
+	if (cs_enforcement_enabled && page_nx(m) && (prot & VM_PROT_EXECUTE)) {
+		if (cs_debug)
+			printf("page marked to be NX, not letting it be mapped EXEC\n");
+		return KERN_CODESIGN_ERROR;
+	}
+
+	/* A page could be tainted, or pose a risk of being tainted later.
+	 * Check whether the receiving process wants it, and make it feel
+	 * the consequences (that hapens in cs_invalid_page()).
+	 * For CS Enforcement, two other conditions will 
+	 * cause that page to be tainted as well: 
+	 * - pmapping an unsigned page executable - this means unsigned code;
+	 * - writeable mapping of a validated page - the content of that page
+	 *   can be changed without the kernel noticing, therefore unsigned
+	 *   code can be created
+	 */
+	if (!cs_bypass &&
+	    (m->cs_tainted ||
+	     (cs_enforcement_enabled &&
+	      (/* The page is unsigned and wants to be executable */
+	       (!m->cs_validated && (prot & VM_PROT_EXECUTE))  ||
+	       /* The page should be immutable, but is in danger of being modified
+		* This is the case where we want policy from the code directory -
+		* is the page immutable or not? For now we have to assume that 
+		* code pages will be immutable, data pages not.
+		* We'll assume a page is a code page if it has a code directory 
+		* and we fault for execution.
+		* That is good enough since if we faulted the code page for
+		* writing in another map before, it is wpmapped; if we fault
+		* it for writing in this map later it will also be faulted for executing 
+		* at the same time; and if we fault for writing in another map
+		* later, we will disconnect it from this pmap so we'll notice
+		* the change.
+		*/
+	      (page_immutable(m, prot) && ((prot & VM_PROT_WRITE) || m->wpmapped))
+	      ))
+		    )) 
+	{
+		/* We will have a tainted page. Have to handle the special case
+		 * of a switched map now. If the map is not switched, standard
+		 * procedure applies - call cs_invalid_page().
+		 * If the map is switched, the real owner is invalid already.
+		 * There is no point in invalidating the switching process since
+		 * it will not be executing from the map. So we don't call
+		 * cs_invalid_page() in that case. */
+		boolean_t reject_page;
+		if(map_is_switched) { 
+			assert(pmap==vm_map_pmap(current_thread()->map));
+			assert(!(prot & VM_PROT_WRITE) || (map_is_switch_protected == FALSE));
+			reject_page = FALSE;
+		} else {
+			if (cs_debug > 5)
+				printf("vm_fault: signed: %s validate: %s tainted: %s wpmapped: %s slid: %s prot: 0x%x\n", 
+				       m->object->code_signed ? "yes" : "no",
+				       m->cs_validated ? "yes" : "no",
+				       m->cs_tainted ? "yes" : "no",
+				       m->wpmapped ? "yes" : "no",
+				       m->slid ? "yes" : "no",
+				       (int)prot);
+			reject_page = cs_invalid_page((addr64_t) vaddr);
+		}
+		
+		if (reject_page) {
+			/* reject the invalid page: abort the page fault */
+			int			pid;
+			const char		*procname;
+			task_t			task;
+			vm_object_t		file_object, shadow;
+			vm_object_offset_t	file_offset;
+			char			*pathname, *filename;
+			vm_size_t		pathname_len, filename_len;
+			boolean_t		truncated_path;
+#define __PATH_MAX 1024
+			struct timespec		mtime, cs_mtime;
+
+			kr = KERN_CODESIGN_ERROR;
+			cs_enter_tainted_rejected++;
+
+			/* get process name and pid */
+			procname = "?";
+			task = current_task();
+			pid = proc_selfpid();
+			if (task->bsd_info != NULL)
+				procname = proc_name_address(task->bsd_info);
+
+			/* get file's VM object */
+			file_object = m->object;
+			file_offset = m->offset;
+			for (shadow = file_object->shadow;
+			     shadow != VM_OBJECT_NULL;
+			     shadow = file_object->shadow) {
+				vm_object_lock_shared(shadow);
+				if (file_object != m->object) {
+					vm_object_unlock(file_object);
+				}
+				file_offset += file_object->vo_shadow_offset;
+				file_object = shadow;
+			}
+
+			mtime.tv_sec = 0;
+			mtime.tv_nsec = 0;
+			cs_mtime.tv_sec = 0;
+			cs_mtime.tv_nsec = 0;
+
+			/* get file's pathname and/or filename */
+			pathname = NULL;
+			filename = NULL;
+			pathname_len = 0;
+			filename_len = 0;
+			truncated_path = FALSE;
+			/* no pager -> no file -> no pathname, use "<nil>" in that case */
+			if (file_object->pager != NULL) {
+				pathname = (char *)kalloc(__PATH_MAX * 2);
+				if (pathname) {
+					pathname[0] = '\0';
+					pathname_len = __PATH_MAX;
+					filename = pathname + pathname_len;
+					filename_len = __PATH_MAX;
+				}
+				vnode_pager_get_object_name(file_object->pager,
+							    pathname,
+							    pathname_len,
+							    filename,
+							    filename_len,
+							    &truncated_path);
+				if (pathname) {
+					/* safety first... */
+					pathname[__PATH_MAX-1] = '\0';
+					filename[__PATH_MAX-1] = '\0';
+				}
+				vnode_pager_get_object_mtime(file_object->pager,
+							     &mtime,
+							     &cs_mtime);
+			}
+			printf("CODE SIGNING: process %d[%s]: "
+			       "rejecting invalid page at address 0x%llx "
+			       "from offset 0x%llx in file \"%s%s%s\" "
+			       "(cs_mtime:%lu.%ld %s mtime:%lu.%ld) "
+			       "(signed:%d validated:%d tainted:%d "
+			       "wpmapped:%d slid:%d)\n",
+			       pid, procname, (addr64_t) vaddr,
+			       file_offset,
+			       (pathname ? pathname : "<nil>"),
+			       (truncated_path ? "/.../" : ""),
+			       (truncated_path ? filename : ""),
+			       cs_mtime.tv_sec, cs_mtime.tv_nsec,
+			       ((cs_mtime.tv_sec == mtime.tv_sec &&
+				 cs_mtime.tv_nsec == mtime.tv_nsec)
+				? "=="
+				: "!="),
+			       mtime.tv_sec, mtime.tv_nsec,
+			       m->object->code_signed,
+			       m->cs_validated,
+			       m->cs_tainted,
+			       m->wpmapped,
+			       m->slid);
+			if (file_object != m->object) {
+				vm_object_unlock(file_object);
+			}
+			if (pathname_len != 0) {
+				kfree(pathname, __PATH_MAX * 2);
+				pathname = NULL;
+				filename = NULL;
+			}
+		} else {
+			/* proceed with the invalid page */
+			kr = KERN_SUCCESS;
+			if (!m->cs_validated) {
+				/*
+				 * This page has not been validated, so it
+				 * must not belong to a code-signed object
+				 * and should not be forcefully considered
+				 * as tainted.
+				 * We're just concerned about it here because
+				 * we've been asked to "execute" it but that
+				 * does not mean that it should cause other
+				 * accesses to fail.
+				 * This happens when a debugger sets a
+				 * breakpoint and we then execute code in
+				 * that page.  Marking the page as "tainted"
+				 * would cause any inspection tool ("leaks",
+				 * "vmmap", "CrashReporter", ...) to get killed
+				 * due to code-signing violation on that page,
+				 * even though they're just reading it and not
+				 * executing from it.
+				 */
+				assert(!m->object->code_signed);
+			} else {
+				/*
+				 * Page might have been tainted before or not;
+				 * now it definitively is. If the page wasn't
+				 * tainted, we must disconnect it from all
+				 * pmaps later, to force existing mappings
+				 * through that code path for re-consideration
+				 * of the validity of that page.
+				 */
+				must_disconnect = !m->cs_tainted;
+				m->cs_tainted = TRUE;
+			}
+			cs_enter_tainted_accepted++;
+		}
+		if (kr != KERN_SUCCESS) {
+			if (cs_debug) {
+				printf("CODESIGNING: vm_fault_enter(0x%llx): "
+				       "*** INVALID PAGE ***\n",
+				       (long long)vaddr);
+			}
+#if !SECURE_KERNEL
+			if (cs_enforcement_panic) {
+				panic("CODESIGNING: panicking on invalid page\n");
+			}
+#endif
+		}
+		
+	} else {
+		/* proceed with the valid page */
+		kr = KERN_SUCCESS;
+	}
+
+	boolean_t	page_queues_locked = FALSE;
+#define __VM_PAGE_LOCKSPIN_QUEUES_IF_NEEDED()	\
+MACRO_BEGIN			    		\
+	if (! page_queues_locked) {		\
+		page_queues_locked = TRUE;	\
+		vm_page_lockspin_queues();	\
+	}					\
+MACRO_END
+#define __VM_PAGE_UNLOCK_QUEUES_IF_NEEDED()	\
+MACRO_BEGIN			    		\
+	if (page_queues_locked) {		\
+		page_queues_locked = FALSE;	\
+		vm_page_unlock_queues();	\
+	}					\
+MACRO_END
+
+	/*
+	 * Hold queues lock to manipulate
+	 * the page queues.  Change wiring
+	 * case is obvious.
+	 */
+	assert(m->compressor || m->object != compressor_object);
+	if (m->compressor) {
+		/*
+		 * Compressor pages are neither wired
+		 * nor pageable and should never change.
+		 */
+		assert(m->object == compressor_object);
+	} else if (change_wiring) {
+	        __VM_PAGE_LOCKSPIN_QUEUES_IF_NEEDED();
+
+		if (wired) {
+			if (kr == KERN_SUCCESS) {
+				vm_page_wire(m, VM_PROT_MEMORY_TAG(caller_prot), TRUE);
+			}
+		} else {
+		        vm_page_unwire(m, TRUE);
+		}
+		/* we keep the page queues lock, if we need it later */
+
+	} else {
+	        if (kr != KERN_SUCCESS) {
+		        __VM_PAGE_LOCKSPIN_QUEUES_IF_NEEDED();
+		        vm_page_deactivate(m);
+			/* we keep the page queues lock, if we need it later */
+		} else if (((!m->active && !m->inactive) ||
+			    m->clean_queue ||
+			    no_cache) &&
+			   !VM_PAGE_WIRED(m) && !m->throttled) {
+
+			if (vm_page_local_q &&
+			    !no_cache &&
+			    (*type_of_fault == DBG_COW_FAULT ||
+			     *type_of_fault == DBG_ZERO_FILL_FAULT) ) {
+				struct vpl	*lq;
+				uint32_t	lid;
+
+				__VM_PAGE_UNLOCK_QUEUES_IF_NEEDED();
+				vm_object_lock_assert_exclusive(m->object);
+
+				/*
+				 * we got a local queue to stuff this
+				 * new page on...
+				 * its safe to manipulate local and
+				 * local_id at this point since we're
+				 * behind an exclusive object lock and
+				 * the page is not on any global queue.
+				 *
+				 * we'll use the current cpu number to
+				 * select the queue note that we don't
+				 * need to disable preemption... we're
+				 * going to behind the local queue's
+				 * lock to do the real work
+				 */
+				lid = cpu_number();
+
+				lq = &vm_page_local_q[lid].vpl_un.vpl;
+
+				VPL_LOCK(&lq->vpl_lock);
+
+				vm_page_check_pageable_safe(m);
+				queue_enter(&lq->vpl_queue, m,
+					    vm_page_t, pageq);
+				m->local = TRUE;
+				m->local_id = lid;
+				lq->vpl_count++;
+					
+				if (m->object->internal)
+					lq->vpl_internal_count++;
+				else
+					lq->vpl_external_count++;
+
+				VPL_UNLOCK(&lq->vpl_lock);
+
+				if (lq->vpl_count > vm_page_local_q_soft_limit)
+				{
+					/*
+					 * we're beyond the soft limit
+					 * for the local queue
+					 * vm_page_reactivate_local will
+					 * 'try' to take the global page
+					 * queue lock... if it can't
+					 * that's ok... we'll let the
+					 * queue continue to grow up
+					 * to the hard limit... at that
+					 * point we'll wait for the
+					 * lock... once we've got the
+					 * lock, we'll transfer all of
+					 * the pages from the local
+					 * queue to the global active
+					 * queue
+					 */
+					vm_page_reactivate_local(lid, FALSE, FALSE);
+				}
+			} else {
+
+				__VM_PAGE_LOCKSPIN_QUEUES_IF_NEEDED();
+
+				/*
+				 * test again now that we hold the
+				 * page queue lock
+				 */
+				if (!VM_PAGE_WIRED(m)) {
+					if (m->clean_queue) {
+						vm_page_queues_remove(m);
+
+						vm_pageout_cleaned_reactivated++;
+						vm_pageout_cleaned_fault_reactivated++;
+					}
+
+					if ((!m->active &&
+					     !m->inactive) ||
+					    no_cache) {
+						/*
+						 * If this is a no_cache mapping
+						 * and the page has never been
+						 * mapped before or was
+						 * previously a no_cache page,
+						 * then we want to leave pages
+						 * in the speculative state so
+						 * that they can be readily
+						 * recycled if free memory runs
+						 * low.  Otherwise the page is
+						 * activated as normal. 
+						 */
+
+						if (no_cache &&
+						    (!previously_pmapped ||
+						     m->no_cache)) {
+							m->no_cache = TRUE;
+
+							if (!m->speculative) 
+								vm_page_speculate(m, FALSE);
+
+						} else if (!m->active &&
+							   !m->inactive) {
+
+							vm_page_activate(m);
+						}
+					}
+				}
+				/* we keep the page queues lock, if we need it later */
+			}
+		}
+	}
+	/* we're done with the page queues lock, if we ever took it */
+	__VM_PAGE_UNLOCK_QUEUES_IF_NEEDED();
+
+
+	/* If we have a KERN_SUCCESS from the previous checks, we either have
+	 * a good page, or a tainted page that has been accepted by the process.
+	 * In both cases the page will be entered into the pmap.
+	 * If the page is writeable, we need to disconnect it from other pmaps
+	 * now so those processes can take note.
+	 */
+	if (kr == KERN_SUCCESS) {
+
+	        /*
+		 * NOTE: we may only hold the vm_object lock SHARED
+		 * at this point, so we need the phys_page lock to 
+		 * properly serialize updating the pmapped and
+		 * xpmapped bits
+		 */
+		if ((prot & VM_PROT_EXECUTE) && !m->xpmapped) {
+
+			pmap_lock_phys_page(m->phys_page);
+			/*
+			 * go ahead and take the opportunity
+			 * to set 'pmapped' here so that we don't
+			 * need to grab this lock a 2nd time
+			 * just below
+			 */
+			m->pmapped = TRUE;
+			
+			if (!m->xpmapped) {
+
+				m->xpmapped = TRUE;
+
+				pmap_unlock_phys_page(m->phys_page);
+
+				if (!m->object->internal)
+					OSAddAtomic(1, &vm_page_xpmapped_external_count);
+
+				if ((COMPRESSED_PAGER_IS_ACTIVE) &&
+				    m->object->internal &&
+				    m->object->pager != NULL) {
+					/*
+					 * This page could have been
+					 * uncompressed by the
+					 * compressor pager and its
+					 * contents might be only in
+					 * the data cache.
+					 * Since it's being mapped for
+					 * "execute" for the fist time,
+					 * make sure the icache is in
+					 * sync.
+					 */
+					pmap_sync_page_data_phys(m->phys_page);
+				}
+			} else
+				pmap_unlock_phys_page(m->phys_page);
+		} else {
+			if (m->pmapped == FALSE) {
+				pmap_lock_phys_page(m->phys_page);
+				m->pmapped = TRUE;
+				pmap_unlock_phys_page(m->phys_page);
+			}
+		}
+		if (vm_page_is_slideable(m)) {
+			boolean_t was_busy = m->busy;
+
+			vm_object_lock_assert_exclusive(m->object);
+
+			m->busy = TRUE;
+			kr = vm_page_slide(m, 0);
+			assert(m->busy);
+			if(!was_busy) {
+				PAGE_WAKEUP_DONE(m);
+			}
+			if (kr != KERN_SUCCESS) {
+				/*
+				 * This page has not been slid correctly,
+				 * do not do the pmap_enter() !
+				 * Let vm_fault_enter() return the error
+				 * so the caller can fail the fault.
+				 */
+				goto after_the_pmap_enter;
+			}
+		}
+
+		if (fault_type & VM_PROT_WRITE) {
+
+			if (m->wpmapped == FALSE) {
+				vm_object_lock_assert_exclusive(m->object);
+
+				m->wpmapped = TRUE;
+			}
+			if (must_disconnect) {
+				/*
+				 * We can only get here 
+				 * because of the CSE logic
+				 */
+				assert(cs_enforcement_enabled);
+				pmap_disconnect(m->phys_page);
+				/* 
+				 * If we are faulting for a write, we can clear
+				 * the execute bit - that will ensure the page is
+				 * checked again before being executable, which
+				 * protects against a map switch.
+				 * This only happens the first time the page
+				 * gets tainted, so we won't get stuck here 
+				 * to make an already writeable page executable.
+				 */
+				if (!cs_bypass){
+					prot &= ~VM_PROT_EXECUTE;
+				}
+			}
+		}
+
+		/* Prevent a deadlock by not
+		 * holding the object lock if we need to wait for a page in
+		 * pmap_enter() - <rdar://problem/7138958> */
+		PMAP_ENTER_OPTIONS(pmap, vaddr, m, prot, fault_type, 0,
+				   wired,
+				   pmap_options | PMAP_OPTIONS_NOWAIT,
+				   pe_result);
+
+		if(pe_result == KERN_RESOURCE_SHORTAGE) {
+
+			if (need_retry) {
+				/*
+				 * this will be non-null in the case where we hold the lock
+				 * on the top-object in this chain... we can't just drop
+				 * the lock on the object we're inserting the page into
+				 * and recall the PMAP_ENTER since we can still cause
+				 * a deadlock if one of the critical paths tries to 
+				 * acquire the lock on the top-object and we're blocked
+				 * in PMAP_ENTER waiting for memory... our only recourse
+				 * is to deal with it at a higher level where we can 
+				 * drop both locks.
+				 */
+				*need_retry = TRUE;
+				vm_pmap_enter_retried++;
+				goto after_the_pmap_enter;
+			}
+			/* The nonblocking version of pmap_enter did not succeed.
+			 * and we don't need to drop other locks and retry
+			 * at the level above us, so 
+			 * use the blocking version instead. Requires marking
+			 * the page busy and unlocking the object */
+			boolean_t was_busy = m->busy;
+
+			vm_object_lock_assert_exclusive(m->object);
+
+			m->busy = TRUE;
+			vm_object_unlock(m->object);
+			
+			PMAP_ENTER_OPTIONS(pmap, vaddr, m, prot, fault_type,
+					   0, wired,
+			                   pmap_options, pe_result);
+				
+			/* Take the object lock again. */
+			vm_object_lock(m->object);
+			
+			/* If the page was busy, someone else will wake it up.
+			 * Otherwise, we have to do it now. */
+			assert(m->busy);
+			if(!was_busy) {
+				PAGE_WAKEUP_DONE(m);
+			}
+			vm_pmap_enter_blocked++;
+		}
+	}
+
+after_the_pmap_enter:
+	return kr;
+}
+
+void
+vm_pre_fault(vm_map_offset_t vaddr)
+{
+	if (pmap_find_phys(current_map()->pmap, vaddr) == 0) {
+
+		vm_fault(current_map(), /* map */
+			vaddr,		/* vaddr */
+			VM_PROT_READ, /* fault_type */
+			FALSE, /* change_wiring */
+			THREAD_UNINT, /* interruptible */
+			NULL, /* caller_pmap */
+			0 /* caller_pmap_addr */);
+	}
+}
+
+
+/*
+ *	Routine:	vm_fault
+ *	Purpose:
+ *		Handle page faults, including pseudo-faults
+ *		used to change the wiring status of pages.
+ *	Returns:
+ *		Explicit continuations have been removed.
+ *	Implementation:
+ *		vm_fault and vm_fault_page save mucho state
+ *		in the moral equivalent of a closure.  The state
+ *		structure is allocated when first entering vm_fault
+ *		and deallocated when leaving vm_fault.
+ */
+
+extern int _map_enter_debug;
+
+unsigned long vm_fault_collapse_total = 0;
+unsigned long vm_fault_collapse_skipped = 0;
+
+
+kern_return_t
+vm_fault(
+	vm_map_t	map,
+	vm_map_offset_t	vaddr,
+	vm_prot_t	fault_type,
+	boolean_t	change_wiring,
+	int		interruptible,
+	pmap_t		caller_pmap,
+	vm_map_offset_t	caller_pmap_addr)
+{
+	return vm_fault_internal(map, vaddr, fault_type, change_wiring,
+				 interruptible, caller_pmap, caller_pmap_addr,
+				 NULL);
+}
+
+
+kern_return_t
+vm_fault_internal(
+	vm_map_t	map,
+	vm_map_offset_t	vaddr,
+	vm_prot_t	caller_prot,
+	boolean_t	change_wiring,
+	int		interruptible,
+	pmap_t		caller_pmap,
+	vm_map_offset_t	caller_pmap_addr,
+	ppnum_t		*physpage_p)
+{
+	vm_map_version_t	version;	/* Map version for verificiation */
+	boolean_t		wired;		/* Should mapping be wired down? */
+	vm_object_t		object;		/* Top-level object */
+	vm_object_offset_t	offset;		/* Top-level offset */
+	vm_prot_t		prot;		/* Protection for mapping */
+	vm_object_t		old_copy_object; /* Saved copy object */
+	vm_page_t		result_page;	/* Result of vm_fault_page */
+	vm_page_t		top_page;	/* Placeholder page */
+	kern_return_t		kr;
+
+	vm_page_t		m;	/* Fast access to result_page */
+	kern_return_t		error_code;
+	vm_object_t		cur_object;
+	vm_object_offset_t	cur_offset;
+	vm_page_t		cur_m;
+	vm_object_t		new_object;
+	int                     type_of_fault;
+	pmap_t			pmap;
+	boolean_t		interruptible_state;
+	vm_map_t		real_map = map;
+	vm_map_t		original_map = map;
+	vm_prot_t		fault_type;
+	vm_prot_t		original_fault_type;
+	struct vm_object_fault_info fault_info;
+	boolean_t		need_collapse = FALSE;
+	boolean_t		need_retry = FALSE;
+	boolean_t		*need_retry_ptr = NULL;
+	int			object_lock_type = 0;
+	int			cur_object_lock_type;
+	vm_object_t		top_object = VM_OBJECT_NULL;
+	int			throttle_delay;
+	int			compressed_count_delta;
+
+
+	KERNEL_DEBUG_CONSTANT_IST(KDEBUG_TRACE, 
+	              (MACHDBG_CODE(DBG_MACH_VM, 2)) | DBG_FUNC_START,
+			      ((uint64_t)vaddr >> 32),
+			      vaddr,
+			      (map == kernel_map),
+			      0,
+			      0);
+
+	if (get_preemption_level() != 0) {
+	        KERNEL_DEBUG_CONSTANT_IST(KDEBUG_TRACE, 
+				      (MACHDBG_CODE(DBG_MACH_VM, 2)) | DBG_FUNC_END,
+				      ((uint64_t)vaddr >> 32),
+				      vaddr,
+				      KERN_FAILURE,
+				      0,
+				      0);
+
+		return (KERN_FAILURE);
+	}
+	
+	interruptible_state = thread_interrupt_level(interruptible);
+
+	fault_type = (change_wiring ? VM_PROT_NONE : caller_prot);
+
+	VM_STAT_INCR(faults);
+	current_task()->faults++;
+	original_fault_type = fault_type;
+
+	if (fault_type & VM_PROT_WRITE)
+	        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+	else
+	        object_lock_type = OBJECT_LOCK_SHARED;
+
+	cur_object_lock_type = OBJECT_LOCK_SHARED;
+
+RetryFault:
+	/*
+	 * assume we will hit a page in the cache
+	 * otherwise, explicitly override with
+	 * the real fault type once we determine it
+	 */
+	type_of_fault = DBG_CACHE_HIT_FAULT;
+
+	/*
+	 *	Find the backing store object and offset into
+	 *	it to begin the search.
+	 */
+	fault_type = original_fault_type;
+	map = original_map;
+	vm_map_lock_read(map);
+
+	kr = vm_map_lookup_locked(&map, vaddr, fault_type,
+				  object_lock_type, &version,
+				  &object, &offset, &prot, &wired,
+				  &fault_info,
+				  &real_map);
+
+
+	if (kr != KERN_SUCCESS) {
+		vm_map_unlock_read(map);
+		goto done;
+	}
+	pmap = real_map->pmap;
+	fault_info.interruptible = interruptible;
+	fault_info.stealth = FALSE;
+	fault_info.io_sync = FALSE;
+	fault_info.mark_zf_absent = FALSE;
+	fault_info.batch_pmap_op = FALSE;
+
+	/*
+	 * If the page is wired, we must fault for the current protection
+	 * value, to avoid further faults.
+	 */
+	if (wired) {
+		fault_type = prot | VM_PROT_WRITE;
+		/*
+		 * since we're treating this fault as a 'write'
+		 * we must hold the top object lock exclusively
+		 */
+		if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+		        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+			if (vm_object_lock_upgrade(object) == FALSE) {
+			        /*
+				 * couldn't upgrade, so explictly
+				 * take the lock exclusively
+				 */
+			        vm_object_lock(object);
+			}
+		}
+	}
+
+#if	VM_FAULT_CLASSIFY
+	/*
+	 *	Temporary data gathering code
+	 */
+	vm_fault_classify(object, offset, fault_type);
+#endif
+	/*
+	 *	Fast fault code.  The basic idea is to do as much as
+	 *	possible while holding the map lock and object locks.
+	 *      Busy pages are not used until the object lock has to
+	 *	be dropped to do something (copy, zero fill, pmap enter).
+	 *	Similarly, paging references aren't acquired until that
+	 *	point, and object references aren't used.
+	 *
+	 *	If we can figure out what to do
+	 *	(zero fill, copy on write, pmap enter) while holding
+	 *	the locks, then it gets done.  Otherwise, we give up,
+	 *	and use the original fault path (which doesn't hold
+	 *	the map lock, and relies on busy pages).
+	 *	The give up cases include:
+	 * 		- Have to talk to pager.
+	 *		- Page is busy, absent or in error.
+	 *		- Pager has locked out desired access.
+	 *		- Fault needs to be restarted.
+	 *		- Have to push page into copy object.
+	 *
+	 *	The code is an infinite loop that moves one level down
+	 *	the shadow chain each time.  cur_object and cur_offset
+	 * 	refer to the current object being examined. object and offset
+	 *	are the original object from the map.  The loop is at the
+	 *	top level if and only if object and cur_object are the same.
+	 *
+	 *	Invariants:  Map lock is held throughout.  Lock is held on
+	 *		original object and cur_object (if different) when
+	 *		continuing or exiting loop.
+	 *
+	 */
+
+
+	/*
+	 * If this page is to be inserted in a copy delay object
+	 * for writing, and if the object has a copy, then the
+	 * copy delay strategy is implemented in the slow fault page.
+	 */
+	if (object->copy_strategy == MEMORY_OBJECT_COPY_DELAY &&
+	    object->copy != VM_OBJECT_NULL && (fault_type & VM_PROT_WRITE))
+	        goto handle_copy_delay;
+
+	cur_object = object;
+	cur_offset = offset;
+
+	while (TRUE) {
+		if (!cur_object->pager_created &&
+		    cur_object->phys_contiguous) /* superpage */
+			break;
+
+		if (cur_object->blocked_access) {
+			/*
+			 * Access to this VM object has been blocked.
+			 * Let the slow path handle it.
+			 */
+			break;
+		}
+
+		m = vm_page_lookup(cur_object, cur_offset);
+
+		if (m != VM_PAGE_NULL) {
+			if (m->busy) {
+			        wait_result_t	result;
+
+				/*
+				 * in order to do the PAGE_ASSERT_WAIT, we must
+				 * have object that 'm' belongs to locked exclusively
+				 */
+				if (object != cur_object) {
+
+					if (cur_object_lock_type == OBJECT_LOCK_SHARED) {
+
+					        cur_object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+						if (vm_object_lock_upgrade(cur_object) == FALSE) {
+						        /*
+							 * couldn't upgrade so go do a full retry
+							 * immediately since we can no longer be
+							 * certain about cur_object (since we
+							 * don't hold a reference on it)...
+							 * first drop the top object lock
+							 */
+							vm_object_unlock(object);
+
+						        vm_map_unlock_read(map);
+							if (real_map != map)
+							        vm_map_unlock(real_map);
+
+							goto RetryFault;
+						}
+					}
+				} else if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+				        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+					if (vm_object_lock_upgrade(object) == FALSE) {
+					        /*
+						 * couldn't upgrade, so explictly take the lock
+						 * exclusively and go relookup the page since we
+						 * will have dropped the object lock and
+						 * a different thread could have inserted
+						 * a page at this offset
+						 * no need for a full retry since we're
+						 * at the top level of the object chain
+						 */
+					        vm_object_lock(object);
+
+						continue;
+					}
+				}
+				if (m->pageout_queue && m->object->internal && COMPRESSED_PAGER_IS_ACTIVE) {
+					/*
+					 * m->busy == TRUE and the object is locked exclusively
+					 * if m->pageout_queue == TRUE after we acquire the
+					 * queues lock, we are guaranteed that it is stable on
+					 * the pageout queue and therefore reclaimable
+					 *
+					 * NOTE: this is only true for the internal pageout queue
+					 * in the compressor world
+					 */
+					vm_page_lock_queues();
+
+					if (m->pageout_queue) {
+						vm_pageout_throttle_up(m);
+						vm_page_unlock_queues();
+
+						PAGE_WAKEUP_DONE(m);
+						goto reclaimed_from_pageout;
+					}
+					vm_page_unlock_queues();
+				}
+				if (object != cur_object)
+					vm_object_unlock(object);
+
+				vm_map_unlock_read(map);
+				if (real_map != map)
+				        vm_map_unlock(real_map);
+
+				result = PAGE_ASSERT_WAIT(m, interruptible);
+
+				vm_object_unlock(cur_object);
+
+				if (result == THREAD_WAITING) {
+				        result = thread_block(THREAD_CONTINUE_NULL);
+
+					counter(c_vm_fault_page_block_busy_kernel++);
+				}
+				if (result == THREAD_AWAKENED || result == THREAD_RESTART)
+				        goto RetryFault;
+
+				kr = KERN_ABORTED;
+				goto done;
+			}
+reclaimed_from_pageout:
+			if (m->laundry) {
+				if (object != cur_object) {
+					if (cur_object_lock_type == OBJECT_LOCK_SHARED) {
+						cur_object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+						vm_object_unlock(object);
+						vm_object_unlock(cur_object);
+
+						vm_map_unlock_read(map);
+						if (real_map != map)
+							vm_map_unlock(real_map);
+
+						goto RetryFault;
+					}
+
+				} else if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+					object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+					if (vm_object_lock_upgrade(object) == FALSE) {
+						/*
+						 * couldn't upgrade, so explictly take the lock
+						 * exclusively and go relookup the page since we
+						 * will have dropped the object lock and
+						 * a different thread could have inserted
+						 * a page at this offset
+						 * no need for a full retry since we're
+						 * at the top level of the object chain
+						 */
+						vm_object_lock(object);
+
+						continue;
+					}
+				}
+				m->pageout = FALSE;
+				
+				vm_pageout_steal_laundry(m, FALSE);
+			}
+
+			if (m->phys_page == vm_page_guard_addr) {
+				/*
+				 * Guard page: let the slow path deal with it
+				 */
+				break;
+			}
+			if (m->unusual && (m->error || m->restart || m->private || m->absent)) {
+			        /*
+				 * Unusual case... let the slow path deal with it
+				 */
+				break;
+			}
+			if (VM_OBJECT_PURGEABLE_FAULT_ERROR(m->object)) {
+				if (object != cur_object)
+					vm_object_unlock(object);
+				vm_map_unlock_read(map);
+				if (real_map != map)
+				        vm_map_unlock(real_map);
+				vm_object_unlock(cur_object);
+				kr = KERN_MEMORY_ERROR;
+				goto done;
+			}
+
+			if (m->encrypted) {
+				/*
+				 * ENCRYPTED SWAP:
+				 * We've soft-faulted (because it's not in the page
+				 * table) on an encrypted page.
+				 * Keep the page "busy" so that no one messes with
+				 * it during the decryption.
+				 * Release the extra locks we're holding, keep only
+				 * the page's VM object lock.
+				 *
+				 * in order to set 'busy' on 'm', we must
+				 * have object that 'm' belongs to locked exclusively
+				 */
+			        if (object != cur_object) {
+					vm_object_unlock(object);
+
+					if (cur_object_lock_type == OBJECT_LOCK_SHARED) {
+
+					        cur_object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+						if (vm_object_lock_upgrade(cur_object) == FALSE) {
+						        /*
+							 * couldn't upgrade so go do a full retry
+							 * immediately since we've already dropped
+							 * the top object lock associated with this page
+							 * and the current one got dropped due to the
+							 * failed upgrade... the state is no longer valid
+							 */
+						        vm_map_unlock_read(map);
+							if (real_map != map)
+							        vm_map_unlock(real_map);
+
+							goto RetryFault;
+						}
+					}
+				} else if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+				        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+					if (vm_object_lock_upgrade(object) == FALSE) {
+					        /*
+						 * couldn't upgrade, so explictly take the lock
+						 * exclusively and go relookup the page since we
+						 * will have dropped the object lock and
+						 * a different thread could have inserted
+						 * a page at this offset
+						 * no need for a full retry since we're
+						 * at the top level of the object chain
+						 */
+					        vm_object_lock(object);
+
+						continue;
+					}
+				}
+				m->busy = TRUE;
+
+				vm_map_unlock_read(map);
+				if (real_map != map) 
+					vm_map_unlock(real_map);
+
+				vm_page_decrypt(m, 0);
+
+				assert(m->busy);
+				PAGE_WAKEUP_DONE(m);
+
+				vm_object_unlock(cur_object);
+				/*
+				 * Retry from the top, in case anything
+				 * changed while we were decrypting...
+				 */
+				goto RetryFault;
+			}
+			ASSERT_PAGE_DECRYPTED(m);
+
+			if(vm_page_is_slideable(m)) {
+				/*
+				 * We might need to slide this page, and so,
+				 * we want to hold the VM object exclusively.
+				 */
+			        if (object != cur_object) {
+					if (cur_object_lock_type == OBJECT_LOCK_SHARED) {
+						vm_object_unlock(object);
+						vm_object_unlock(cur_object);
+
+					        cur_object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+						vm_map_unlock_read(map);
+						if (real_map != map)
+							vm_map_unlock(real_map);
+
+						goto RetryFault;
+					}
+				} else if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+					vm_object_unlock(object);
+				        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+					vm_map_unlock_read(map);
+					goto RetryFault;
+				}
+			}
+
+			if (VM_FAULT_NEED_CS_VALIDATION(map->pmap, m) ||
+			    (physpage_p != NULL && (prot & VM_PROT_WRITE))) {
+upgrade_for_validation:
+				/*
+				 * We might need to validate this page
+				 * against its code signature, so we
+				 * want to hold the VM object exclusively.
+				 */
+			        if (object != cur_object) {
+					if (cur_object_lock_type == OBJECT_LOCK_SHARED) {
+						vm_object_unlock(object);
+						vm_object_unlock(cur_object);
+
+					        cur_object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+						vm_map_unlock_read(map);
+						if (real_map != map)
+							vm_map_unlock(real_map);
+
+						goto RetryFault;
+					}
+
+				} else if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+				        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+					if (vm_object_lock_upgrade(object) == FALSE) {
+					        /*
+						 * couldn't upgrade, so explictly take the lock
+						 * exclusively and go relookup the page since we
+						 * will have dropped the object lock and
+						 * a different thread could have inserted
+						 * a page at this offset
+						 * no need for a full retry since we're
+						 * at the top level of the object chain
+						 */
+					        vm_object_lock(object);
+
+						continue;
+					}
+				}
+			}
+			/*
+			 *	Two cases of map in faults:
+			 *	    - At top level w/o copy object.
+			 *	    - Read fault anywhere.
+			 *		--> must disallow write.
+			 */
+
+			if (object == cur_object && object->copy == VM_OBJECT_NULL) {
+
+				goto FastPmapEnter;
+			}
+
+			if ((fault_type & VM_PROT_WRITE) == 0) {
+
+			  	if (object != cur_object) {
+				        /*
+					 * We still need to hold the top object
+					 * lock here to prevent a race between
+					 * a read fault (taking only "shared"
+					 * locks) and a write fault (taking
+					 * an "exclusive" lock on the top
+					 * object.
+					 * Otherwise, as soon as we release the
+					 * top lock, the write fault could
+					 * proceed and actually complete before
+					 * the read fault, and the copied page's
+					 * translation could then be overwritten
+					 * by the read fault's translation for
+					 * the original page.
+					 *
+					 * Let's just record what the top object
+					 * is and we'll release it later.
+					 */
+					top_object = object;
+
+					/*
+					 * switch to the object that has the new page
+					 */
+					object = cur_object;
+					object_lock_type = cur_object_lock_type;
+				}
+FastPmapEnter:
+				/*
+				 * prepare for the pmap_enter...
+				 * object and map are both locked
+				 * m contains valid data
+				 * object == m->object
+				 * cur_object == NULL or it's been unlocked
+				 * no paging references on either object or cur_object
+				 */
+				if (top_object != VM_OBJECT_NULL || object_lock_type != OBJECT_LOCK_EXCLUSIVE)
+					need_retry_ptr = &need_retry;
+				else
+					need_retry_ptr = NULL;
+
+				if (caller_pmap) {
+				        kr = vm_fault_enter(m,
+							    caller_pmap,
+							    caller_pmap_addr,
+							    prot,
+							    caller_prot,
+							    wired,
+							    change_wiring,
+							    fault_info.no_cache,
+							    fault_info.cs_bypass,
+							    fault_info.user_tag,
+							    fault_info.pmap_options,
+							    need_retry_ptr,
+							    &type_of_fault);
+				} else {
+				        kr = vm_fault_enter(m,
+							    pmap,
+							    vaddr,
+							    prot,
+							    caller_prot,
+							    wired,
+							    change_wiring,
+							    fault_info.no_cache,
+							    fault_info.cs_bypass,
+							    fault_info.user_tag,
+							    fault_info.pmap_options,
+							    need_retry_ptr,
+							    &type_of_fault);
+				}
+
+				if (kr == KERN_SUCCESS &&
+				    physpage_p != NULL) {
+					/* for vm_map_wire_and_extract() */
+					*physpage_p = m->phys_page;
+					if (prot & VM_PROT_WRITE) {
+						vm_object_lock_assert_exclusive(
+							m->object);
+						m->dirty = TRUE;
+					}
+				}
+
+				if (top_object != VM_OBJECT_NULL) {
+					/*
+					 * It's safe to drop the top object
+					 * now that we've done our
+					 * vm_fault_enter().  Any other fault
+					 * in progress for that virtual
+					 * address will either find our page
+					 * and translation or put in a new page
+					 * and translation.
+					 */
+					vm_object_unlock(top_object);
+					top_object = VM_OBJECT_NULL;
+				}
+
+				if (need_collapse == TRUE)
+				        vm_object_collapse(object, offset, TRUE);
+				
+				if (need_retry == FALSE &&
+				    (type_of_fault == DBG_PAGEIND_FAULT || type_of_fault == DBG_PAGEINV_FAULT || type_of_fault == DBG_CACHE_HIT_FAULT)) {
+				        /*
+					 * evaluate access pattern and update state
+					 * vm_fault_deactivate_behind depends on the
+					 * state being up to date
+					 */
+				        vm_fault_is_sequential(object, cur_offset, fault_info.behavior);
+
+					vm_fault_deactivate_behind(object, cur_offset, fault_info.behavior);
+				}
+				/*
+				 * That's it, clean up and return.
+				 */
+				if (m->busy)
+				        PAGE_WAKEUP_DONE(m);
+
+				vm_object_unlock(object);
+
+				vm_map_unlock_read(map);
+				if (real_map != map)
+					vm_map_unlock(real_map);
+
+				if (need_retry == TRUE) {
+					/*
+					 * vm_fault_enter couldn't complete the PMAP_ENTER...
+					 * at this point we don't hold any locks so it's safe
+					 * to ask the pmap layer to expand the page table to
+					 * accommodate this mapping... once expanded, we'll
+					 * re-drive the fault which should result in vm_fault_enter
+					 * being able to successfully enter the mapping this time around
+					 */
+					(void)pmap_enter_options(
+						pmap, vaddr, 0, 0, 0, 0, 0,
+						PMAP_OPTIONS_NOENTER, NULL);
+					
+					need_retry = FALSE;
+					goto RetryFault;
+				}
+				goto done;
+			}
+			/*
+			 * COPY ON WRITE FAULT
+			 */
+			assert(object_lock_type == OBJECT_LOCK_EXCLUSIVE);
+
+                        /*
+			 * If objects match, then
+			 * object->copy must not be NULL (else control
+			 * would be in previous code block), and we
+			 * have a potential push into the copy object
+			 * with which we can't cope with here.
+			 */
+			if (cur_object == object) {
+			        /*
+				 * must take the slow path to
+				 * deal with the copy push
+				 */
+				break;
+			}
+			
+			/*
+			 * This is now a shadow based copy on write
+			 * fault -- it requires a copy up the shadow
+			 * chain.
+			 */
+			
+			if ((cur_object_lock_type == OBJECT_LOCK_SHARED) &&
+			    VM_FAULT_NEED_CS_VALIDATION(NULL, m)) {
+				goto upgrade_for_validation;
+			}
+
+			/*
+			 * Allocate a page in the original top level
+			 * object. Give up if allocate fails.  Also
+			 * need to remember current page, as it's the
+			 * source of the copy.
+			 *
+			 * at this point we hold locks on both 
+			 * object and cur_object... no need to take
+			 * paging refs or mark pages BUSY since
+			 * we don't drop either object lock until
+			 * the page has been copied and inserted
+			 */
+			cur_m = m;
+			m = vm_page_grab();
+
+			if (m == VM_PAGE_NULL) {
+			        /*
+				 * no free page currently available...
+				 * must take the slow path
+				 */
+				break;
+			}
+			/*
+			 * Now do the copy.  Mark the source page busy...
+			 *
+			 *	NOTE: This code holds the map lock across
+			 *	the page copy.
+			 */
+			vm_page_copy(cur_m, m);
+			vm_page_insert(m, object, offset);
+			SET_PAGE_DIRTY(m, FALSE);
+
+			/*
+			 * Now cope with the source page and object
+			 */
+			if (object->ref_count > 1 && cur_m->pmapped)
+			        pmap_disconnect(cur_m->phys_page);
+			
+			if (cur_m->clustered) {
+				VM_PAGE_COUNT_AS_PAGEIN(cur_m);
+				VM_PAGE_CONSUME_CLUSTERED(cur_m);
+			}
+			need_collapse = TRUE;
+
+			if (!cur_object->internal &&
+			    cur_object->copy_strategy == MEMORY_OBJECT_COPY_DELAY) {
+			        /*
+				 * The object from which we've just
+				 * copied a page is most probably backed
+				 * by a vnode.  We don't want to waste too
+				 * much time trying to collapse the VM objects
+				 * and create a bottleneck when several tasks
+				 * map the same file.
+				 */
+			        if (cur_object->copy == object) {
+				        /*
+					 * Shared mapping or no COW yet.
+					 * We can never collapse a copy
+					 * object into its backing object.
+					 */
+				        need_collapse = FALSE;
+				} else if (cur_object->copy == object->shadow &&
+					   object->shadow->resident_page_count == 0) {
+				        /*
+					 * Shared mapping after a COW occurred.
+					 */
+				        need_collapse = FALSE;
+				}
+			}
+			vm_object_unlock(cur_object);
+
+			if (need_collapse == FALSE)
+			        vm_fault_collapse_skipped++;
+			vm_fault_collapse_total++;
+
+			type_of_fault = DBG_COW_FAULT;
+			VM_STAT_INCR(cow_faults);
+			DTRACE_VM2(cow_fault, int, 1, (uint64_t *), NULL);
+			current_task()->cow_faults++;
+
+			goto FastPmapEnter;
+
+		} else {
+			/*
+			 * No page at cur_object, cur_offset... m == NULL
+			 */
+			if (cur_object->pager_created) {
+				int	compressor_external_state = VM_EXTERNAL_STATE_UNKNOWN;
+
+			        if (MUST_ASK_PAGER(cur_object, cur_offset, compressor_external_state) == TRUE) {
+					int		my_fault_type;
+					int		c_flags = C_DONT_BLOCK;
+					boolean_t	insert_cur_object = FALSE;
+
+				        /*
+					 * May have to talk to a pager...
+					 * if so, take the slow path by
+					 * doing a 'break' from the while (TRUE) loop
+					 *
+					 * external_state will only be set to VM_EXTERNAL_STATE_EXISTS
+					 * if the compressor is active and the page exists there
+					 */
+					if (compressor_external_state != VM_EXTERNAL_STATE_EXISTS)
+						break;
+
+					if (map == kernel_map || real_map == kernel_map) {
+						/*
+						 * can't call into the compressor with the kernel_map
+						 * lock held, since the compressor may try to operate
+						 * on the kernel map in order to return an empty c_segment
+						 */
+						break;
+					}
+					if (object != cur_object) {
+						if (fault_type & VM_PROT_WRITE)
+							c_flags |= C_KEEP;
+						else
+							insert_cur_object = TRUE;
+					}
+					if (insert_cur_object == TRUE) {
+
+						if (cur_object_lock_type == OBJECT_LOCK_SHARED) {
+
+							cur_object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+							if (vm_object_lock_upgrade(cur_object) == FALSE) {
+								/*
+								 * couldn't upgrade so go do a full retry
+								 * immediately since we can no longer be
+								 * certain about cur_object (since we
+								 * don't hold a reference on it)...
+								 * first drop the top object lock
+								 */
+								vm_object_unlock(object);
+
+								vm_map_unlock_read(map);
+								if (real_map != map)
+									vm_map_unlock(real_map);
+
+								goto RetryFault;
+							}
+						}
+					} else if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+						object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+						if (object != cur_object) {
+							/*
+							 * we can't go for the upgrade on the top
+							 * lock since the upgrade may block waiting
+							 * for readers to drain... since we hold
+							 * cur_object locked at this point, waiting
+							 * for the readers to drain would represent
+							 * a lock order inversion since the lock order
+							 * for objects is the reference order in the
+							 * shadown chain
+							 */
+							vm_object_unlock(object);
+							vm_object_unlock(cur_object);
+
+							vm_map_unlock_read(map);
+							if (real_map != map)
+								vm_map_unlock(real_map);
+
+							goto RetryFault;
+						}
+						if (vm_object_lock_upgrade(object) == FALSE) {
+							/*
+							 * couldn't upgrade, so explictly take the lock
+							 * exclusively and go relookup the page since we
+							 * will have dropped the object lock and
+							 * a different thread could have inserted
+							 * a page at this offset
+							 * no need for a full retry since we're
+							 * at the top level of the object chain
+							 */
+							vm_object_lock(object);
+							
+							continue;
+						}
+					}
+					m = vm_page_grab();
+
+					if (m == VM_PAGE_NULL) {
+						/*
+						 * no free page currently available...
+						 * must take the slow path
+						 */
+						break;
+					}
+
+					/*
+					 * The object is and remains locked
+					 * so no need to take a
+					 * "paging_in_progress" reference.
+					 */
+					boolean_t shared_lock;
+					if ((object == cur_object &&
+					     object_lock_type == OBJECT_LOCK_EXCLUSIVE) ||
+					    (object != cur_object &&
+					     cur_object_lock_type == OBJECT_LOCK_EXCLUSIVE)) {
+						shared_lock = FALSE;
+					} else {
+						shared_lock = TRUE;
+					}
+
+					kr = vm_compressor_pager_get(
+						cur_object->pager,
+						(cur_offset +
+						 cur_object->paging_offset),
+						m->phys_page,
+						&my_fault_type,
+						c_flags,
+						&compressed_count_delta);
+
+					vm_compressor_pager_count(
+						cur_object->pager,
+						compressed_count_delta,
+						shared_lock,
+						cur_object);
+
+					if (kr != KERN_SUCCESS) {
+						vm_page_release(m);
+						break;
+					}
+					m->dirty = TRUE;
+
+					/*
+					 * If the object is purgeable, its
+					 * owner's purgeable ledgers will be
+					 * updated in vm_page_insert() but the
+					 * page was also accounted for in a
+					 * "compressed purgeable" ledger, so
+					 * update that now.
+					 */
+					if (object != cur_object &&
+					    !insert_cur_object) {
+						/*
+						 * We're not going to insert
+						 * the decompressed page into
+						 * the object it came from.
+						 *
+						 * We're dealing with a
+						 * copy-on-write fault on
+						 * "object".
+						 * We're going to decompress
+						 * the page directly into the
+						 * target "object" while
+						 * keepin the compressed
+						 * page for "cur_object", so
+						 * no ledger update in that
+						 * case.
+						 */
+					} else if ((cur_object->purgable ==
+						    VM_PURGABLE_DENY) ||
+						   (cur_object->vo_purgeable_owner ==
+						    NULL)) {
+						/*
+						 * "cur_object" is not purgeable
+						 * or is not owned, so no
+						 * purgeable ledgers to update.
+						 */
+					} else {
+						/*
+						 * One less compressed
+						 * purgeable page for
+						 * cur_object's owner.
+						 */
+						vm_purgeable_compressed_update(
+							cur_object,
+							-1);
+					}
+
+					if (insert_cur_object) {
+						vm_page_insert(m, cur_object, cur_offset);
+					} else {
+						vm_page_insert(m, object, offset);
+					}
+
+					if ((m->object->wimg_bits & VM_WIMG_MASK) != VM_WIMG_USE_DEFAULT) {
+                                                /*
+						 * If the page is not cacheable,
+						 * we can't let its contents
+						 * linger in the data cache
+						 * after the decompression.
+						 */
+						pmap_sync_page_attributes_phys(m->phys_page);
+					}
+
+					type_of_fault = my_fault_type;
+
+					VM_STAT_INCR(decompressions);
+
+					if (cur_object != object) {
+						if (insert_cur_object) {
+							top_object = object;
+							/*
+							 * switch to the object that has the new page
+							 */
+							object = cur_object;
+							object_lock_type = cur_object_lock_type;
+						} else {
+							vm_object_unlock(cur_object);
+							cur_object = object;
+						}
+					}
+					goto FastPmapEnter;
+				}
+				/*
+				 * existence map present and indicates
+				 * that the pager doesn't have this page
+				 */
+			}
+			if (cur_object->shadow == VM_OBJECT_NULL) {
+				/*
+				 * Zero fill fault.  Page gets
+				 * inserted into the original object.
+				 */
+				if (cur_object->shadow_severed ||
+				    VM_OBJECT_PURGEABLE_FAULT_ERROR(cur_object))
+				{
+					if (object != cur_object)
+					        vm_object_unlock(cur_object);
+					vm_object_unlock(object);
+
+					vm_map_unlock_read(map);
+					if (real_map != map)
+						vm_map_unlock(real_map);
+
+					kr = KERN_MEMORY_ERROR;
+					goto done;
+				}
+				if (vm_backing_store_low) {
+				        /*
+					 * we are protecting the system from
+					 * backing store exhaustion... 
+					 * must take the slow path if we're
+					 * not privileged
+					 */
+					if (!(current_task()->priv_flags & VM_BACKING_STORE_PRIV))
+					        break;
+				}
+			  	if (cur_object != object) {
+					vm_object_unlock(cur_object);
+
+					cur_object = object;
+				}
+				if (object_lock_type == OBJECT_LOCK_SHARED) {
+
+				        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+					if (vm_object_lock_upgrade(object) == FALSE) {
+					        /*
+						 * couldn't upgrade so do a full retry on the fault
+						 * since we dropped the object lock which
+						 * could allow another thread to insert
+						 * a page at this offset
+						 */
+					        vm_map_unlock_read(map);
+						if (real_map != map)
+						        vm_map_unlock(real_map);
+
+						goto RetryFault;
+					}
+				}
+				m = vm_page_alloc(object, offset);
+
+				if (m == VM_PAGE_NULL) {
+				        /*
+					 * no free page currently available...
+					 * must take the slow path
+					 */
+					break;
+				}
+
+				/*
+				 * Now zero fill page...
+				 * the page is probably going to 
+				 * be written soon, so don't bother
+				 * to clear the modified bit
+				 *
+				 *   NOTE: This code holds the map
+				 *   lock across the zero fill.
+				 */
+				type_of_fault = vm_fault_zero_page(m, map->no_zero_fill);
+
+				goto FastPmapEnter;
+		        }
+			/*
+			 * On to the next level in the shadow chain
+			 */
+			cur_offset += cur_object->vo_shadow_offset;
+			new_object = cur_object->shadow;
+
+			/*
+			 * take the new_object's lock with the indicated state
+			 */
+			if (cur_object_lock_type == OBJECT_LOCK_SHARED)
+			        vm_object_lock_shared(new_object);
+			else
+			        vm_object_lock(new_object);
+
+			if (cur_object != object)
+				vm_object_unlock(cur_object);
+
+			cur_object = new_object;
+
+			continue;
+		}
+	}
+	/*
+	 * Cleanup from fast fault failure.  Drop any object
+	 * lock other than original and drop map lock.
+	 */
+	if (object != cur_object)
+		vm_object_unlock(cur_object);
+
+	/*
+	 * must own the object lock exclusively at this point
+	 */
+	if (object_lock_type == OBJECT_LOCK_SHARED) {
+	        object_lock_type = OBJECT_LOCK_EXCLUSIVE;
+
+		if (vm_object_lock_upgrade(object) == FALSE) {
+		        /*
+			 * couldn't upgrade, so explictly
+			 * take the lock exclusively
+			 * no need to retry the fault at this
+			 * point since "vm_fault_page" will
+			 * completely re-evaluate the state
+			 */
+		        vm_object_lock(object);
+		}
+	}
+
+handle_copy_delay:
+	vm_map_unlock_read(map);
+	if (real_map != map)
+		vm_map_unlock(real_map);
+
+   	/*
+	 * Make a reference to this object to
+	 * prevent its disposal while we are messing with
+	 * it.  Once we have the reference, the map is free
+	 * to be diddled.  Since objects reference their
+	 * shadows (and copies), they will stay around as well.
+	 */
+	vm_object_reference_locked(object);
+	vm_object_paging_begin(object);
+
+	XPR(XPR_VM_FAULT,"vm_fault -> vm_fault_page\n",0,0,0,0,0);
+
+	error_code = 0;
+
+	result_page = VM_PAGE_NULL;
+	kr = vm_fault_page(object, offset, fault_type,
+			   (change_wiring && !wired),
+			   FALSE, /* page not looked up */
+			   &prot, &result_page, &top_page,
+			   &type_of_fault,
+			   &error_code, map->no_zero_fill,
+			   FALSE, &fault_info);
+
+	/*
+	 * if kr != VM_FAULT_SUCCESS, then the paging reference
+	 * has been dropped and the object unlocked... the ref_count
+	 * is still held
+	 *
+	 * if kr == VM_FAULT_SUCCESS, then the paging reference
+	 * is still held along with the ref_count on the original object
+	 *
+	 *	the object is returned locked with a paging reference
+	 *
+	 *	if top_page != NULL, then it's BUSY and the 
+	 *	object it belongs to has a paging reference
+	 *	but is returned unlocked
+	 */
+	if (kr != VM_FAULT_SUCCESS &&
+	    kr != VM_FAULT_SUCCESS_NO_VM_PAGE) {
+	        /*
+		 * we didn't succeed, lose the object reference immediately.
+		 */
+		vm_object_deallocate(object);
+
+		/*
+		 * See why we failed, and take corrective action.
+		 */
+		switch (kr) {
+		case VM_FAULT_MEMORY_SHORTAGE:
+			if (vm_page_wait((change_wiring) ? 
+					 THREAD_UNINT :
+					 THREAD_ABORTSAFE))
+				goto RetryFault;
+			/*
+			 * fall thru
+			 */
+		case VM_FAULT_INTERRUPTED:
+			kr = KERN_ABORTED;
+			goto done;
+		case VM_FAULT_RETRY:
+			goto RetryFault;
+		case VM_FAULT_MEMORY_ERROR:
+			if (error_code)
+				kr = error_code;
+			else
+				kr = KERN_MEMORY_ERROR;
+			goto done;
+		default:
+			panic("vm_fault: unexpected error 0x%x from "
+			      "vm_fault_page()\n", kr);
+		}
+	}
+	m = result_page;
+
+	if (m != VM_PAGE_NULL) {
+		assert((change_wiring && !wired) ?
+	   	    (top_page == VM_PAGE_NULL) :
+	   	    ((top_page == VM_PAGE_NULL) == (m->object == object)));
+	}
+
+	/*
+	 * What to do with the resulting page from vm_fault_page
+	 * if it doesn't get entered into the physical map:
+	 */
+#define RELEASE_PAGE(m)					\
+	MACRO_BEGIN					\
+	PAGE_WAKEUP_DONE(m);				\
+	if (!m->active && !m->inactive && !m->throttled) {		\
+		vm_page_lockspin_queues();				\
+		if (!m->active && !m->inactive && !m->throttled)	\
+			vm_page_activate(m);				\
+		vm_page_unlock_queues();				\
+	}								\
+	MACRO_END
+
+	/*
+	 * We must verify that the maps have not changed
+	 * since our last lookup.
+	 */
+	if (m != VM_PAGE_NULL) {
+		old_copy_object = m->object->copy;
+		vm_object_unlock(m->object);
+	} else {
+		old_copy_object = VM_OBJECT_NULL;
+		vm_object_unlock(object);
+	}
+
+	/*
+	 * no object locks are held at this point
+	 */
+	if ((map != original_map) || !vm_map_verify(map, &version)) {
+		vm_object_t		retry_object;
+		vm_object_offset_t	retry_offset;
+		vm_prot_t		retry_prot;
+
+		/*
+		 * To avoid trying to write_lock the map while another
+		 * thread has it read_locked (in vm_map_pageable), we
+		 * do not try for write permission.  If the page is
+		 * still writable, we will get write permission.  If it
+		 * is not, or has been marked needs_copy, we enter the
+		 * mapping without write permission, and will merely
+		 * take another fault.
+		 */
+		map = original_map;
+		vm_map_lock_read(map);
+
+		kr = vm_map_lookup_locked(&map, vaddr,
+					  fault_type & ~VM_PROT_WRITE,
+					  OBJECT_LOCK_EXCLUSIVE, &version,
+					  &retry_object, &retry_offset, &retry_prot,
+					  &wired,
+					  &fault_info,
+					  &real_map);
+		pmap = real_map->pmap;
+
+		if (kr != KERN_SUCCESS) {
+			vm_map_unlock_read(map);
+
+			if (m != VM_PAGE_NULL) {
+			        /*
+				 * retake the lock so that
+				 * we can drop the paging reference
+				 * in vm_fault_cleanup and do the
+				 * PAGE_WAKEUP_DONE in RELEASE_PAGE
+				 */
+				vm_object_lock(m->object);
+
+				RELEASE_PAGE(m);
+
+				vm_fault_cleanup(m->object, top_page);
+			} else {
+			        /*
+				 * retake the lock so that
+				 * we can drop the paging reference
+				 * in vm_fault_cleanup
+				 */
+			        vm_object_lock(object);
+
+			        vm_fault_cleanup(object, top_page);
+			}
+			vm_object_deallocate(object);
+
+			goto done;
+		}
+		vm_object_unlock(retry_object);
+
+		if ((retry_object != object) || (retry_offset != offset)) {
+
+			vm_map_unlock_read(map);
+			if (real_map != map)
+				vm_map_unlock(real_map);
+
+			if (m != VM_PAGE_NULL) {
+			        /*
+				 * retake the lock so that
+				 * we can drop the paging reference
+				 * in vm_fault_cleanup and do the
+				 * PAGE_WAKEUP_DONE in RELEASE_PAGE
+				 */
+			        vm_object_lock(m->object);
+
+				RELEASE_PAGE(m);
+
+				vm_fault_cleanup(m->object, top_page);
+			} else {
+			        /*
+				 * retake the lock so that
+				 * we can drop the paging reference
+				 * in vm_fault_cleanup
+				 */
+			        vm_object_lock(object);
+
+			        vm_fault_cleanup(object, top_page);
+			}
+			vm_object_deallocate(object);
+
+			goto RetryFault;
+		}
+		/*
+		 * Check whether the protection has changed or the object
+		 * has been copied while we left the map unlocked.
+		 */
+		prot &= retry_prot;
+	}
+	if (m != VM_PAGE_NULL) {
+		vm_object_lock(m->object);
+
+		if (m->object->copy != old_copy_object) {
+		        /*
+			 * The copy object changed while the top-level object
+			 * was unlocked, so take away write permission.
+			 */
+			prot &= ~VM_PROT_WRITE;
+		}
+	} else
+		vm_object_lock(object);
+
+	/*
+	 * If we want to wire down this page, but no longer have
+	 * adequate permissions, we must start all over.
+	 */
+	if (wired && (fault_type != (prot | VM_PROT_WRITE))) {
+
+		vm_map_verify_done(map, &version);
+		if (real_map != map)
+			vm_map_unlock(real_map);
+
+		if (m != VM_PAGE_NULL) {
+			RELEASE_PAGE(m);
+
+			vm_fault_cleanup(m->object, top_page);
+		} else
+		        vm_fault_cleanup(object, top_page);
+
+		vm_object_deallocate(object);
+
+		goto RetryFault;
+	}
+	if (m != VM_PAGE_NULL) {
+		/*
+		 * Put this page into the physical map.
+		 * We had to do the unlock above because pmap_enter
+		 * may cause other faults.  The page may be on
+		 * the pageout queues.  If the pageout daemon comes
+		 * across the page, it will remove it from the queues.
+		 */
+		if (caller_pmap) {
+			kr = vm_fault_enter(m,
+					    caller_pmap,
+					    caller_pmap_addr,
+					    prot,
+					    caller_prot,
+					    wired,
+					    change_wiring,
+					    fault_info.no_cache,
+					    fault_info.cs_bypass,
+					    fault_info.user_tag,
+					    fault_info.pmap_options,
+					    NULL,
+					    &type_of_fault);
+		} else {
+			kr = vm_fault_enter(m,
+					    pmap,
+					    vaddr,
+					    prot,
+					    caller_prot,
+					    wired,
+					    change_wiring,
+					    fault_info.no_cache,
+					    fault_info.cs_bypass,
+					    fault_info.user_tag,
+					    fault_info.pmap_options,
+					    NULL,
+					    &type_of_fault);
+		}
+		if (kr != KERN_SUCCESS) {
+			/* abort this page fault */
+			vm_map_verify_done(map, &version);
+			if (real_map != map)
+				vm_map_unlock(real_map);
+			PAGE_WAKEUP_DONE(m);
+			vm_fault_cleanup(m->object, top_page);
+			vm_object_deallocate(object);
+			goto done;
+		}
+		if (physpage_p != NULL) {
+			/* for vm_map_wire_and_extract() */
+			*physpage_p = m->phys_page;
+			if (prot & VM_PROT_WRITE) {
+				vm_object_lock_assert_exclusive(m->object);
+				m->dirty = TRUE;
+			}
+		}
+	} else {
+
+		vm_map_entry_t		entry;
+		vm_map_offset_t		laddr;
+		vm_map_offset_t		ldelta, hdelta;
+
+		/* 
+		 * do a pmap block mapping from the physical address
+		 * in the object 
+		 */
+
+#ifdef ppc
+		/* While we do not worry about execution protection in   */
+		/* general, certian pages may have instruction execution */
+		/* disallowed.  We will check here, and if not allowed   */
+		/* to execute, we return with a protection failure.      */
+
+		if ((fault_type & VM_PROT_EXECUTE) &&
+			(!pmap_eligible_for_execute((ppnum_t)(object->vo_shadow_offset >> 12)))) {
+
+			vm_map_verify_done(map, &version);
+
+			if (real_map != map)
+				vm_map_unlock(real_map);
+
+			vm_fault_cleanup(object, top_page);
+			vm_object_deallocate(object);
+
+			kr = KERN_PROTECTION_FAILURE;
+			goto done;
+		}
+#endif	/* ppc */
+
+		if (real_map != map)
+			vm_map_unlock(real_map);
+
+		if (original_map != map) {
+			vm_map_unlock_read(map);
+			vm_map_lock_read(original_map);
+			map = original_map;
+		}
+		real_map = map;
+
+		laddr = vaddr;
+		hdelta = 0xFFFFF000;
+		ldelta = 0xFFFFF000;
+
+		while (vm_map_lookup_entry(map, laddr, &entry)) {
+			if (ldelta > (laddr - entry->vme_start))
+				ldelta = laddr - entry->vme_start;
+			if (hdelta > (entry->vme_end - laddr))
+				hdelta = entry->vme_end - laddr;
+			if (entry->is_sub_map) {
+				
+				laddr = ((laddr - entry->vme_start) 
+					 + VME_OFFSET(entry));
+				vm_map_lock_read(VME_SUBMAP(entry));
+
+				if (map != real_map)
+					vm_map_unlock_read(map);
+				if (entry->use_pmap) {
+					vm_map_unlock_read(real_map);
+					real_map = VME_SUBMAP(entry);
+				}
+				map = VME_SUBMAP(entry);
+				
+			} else {
+				break;
+			}
+		}
+
+		if (vm_map_lookup_entry(map, laddr, &entry) && 
+		    (VME_OBJECT(entry) != NULL) &&
+		    (VME_OBJECT(entry) == object)) {
+			int superpage;
+
+			if (!object->pager_created &&
+			    object->phys_contiguous) {
+				superpage = VM_MEM_SUPERPAGE;
+			} else {
+				superpage = 0;
+			}
+
+			if (superpage && physpage_p) {
+				/* for vm_map_wire_and_extract() */
+				*physpage_p = (ppnum_t)
+					((((vm_map_offset_t)
+					   object->vo_shadow_offset)
+					  + VME_OFFSET(entry)
+					  + (laddr - entry->vme_start))
+					 >> PAGE_SHIFT);
+			}
+
+			if (caller_pmap) {
+				/*
+				 * Set up a block mapped area
+				 */
+				assert((uint32_t)((ldelta + hdelta) >> PAGE_SHIFT) == ((ldelta + hdelta) >> PAGE_SHIFT));
+				pmap_map_block(caller_pmap, 
+					       (addr64_t)(caller_pmap_addr - ldelta), 
+					       (ppnum_t)((((vm_map_offset_t) (VME_OBJECT(entry)->vo_shadow_offset)) +
+							  VME_OFFSET(entry) + (laddr - entry->vme_start) - ldelta) >> PAGE_SHIFT),
+					       (uint32_t)((ldelta + hdelta) >> PAGE_SHIFT), prot, 
+					       (VM_WIMG_MASK & (int)object->wimg_bits) | superpage, 0);
+			} else { 
+				/*
+				 * Set up a block mapped area
+				 */
+				assert((uint32_t)((ldelta + hdelta) >> PAGE_SHIFT) == ((ldelta + hdelta) >> PAGE_SHIFT));
+				pmap_map_block(real_map->pmap, 
+					       (addr64_t)(vaddr - ldelta), 
+					       (ppnum_t)((((vm_map_offset_t)(VME_OBJECT(entry)->vo_shadow_offset)) +
+							  VME_OFFSET(entry) + (laddr - entry->vme_start) - ldelta) >> PAGE_SHIFT),
+					       (uint32_t)((ldelta + hdelta) >> PAGE_SHIFT), prot, 
+					       (VM_WIMG_MASK & (int)object->wimg_bits) | superpage, 0);
+			}
+		}
+	}
+
+	/*
+	 * Unlock everything, and return
+	 */
+	vm_map_verify_done(map, &version);
+	if (real_map != map)
+		vm_map_unlock(real_map);
+
+	if (m != VM_PAGE_NULL) {
+		PAGE_WAKEUP_DONE(m);
+
+		vm_fault_cleanup(m->object, top_page);
+	} else
+	        vm_fault_cleanup(object, top_page);
+
+	vm_object_deallocate(object);
+
+#undef	RELEASE_PAGE
+
+	kr = KERN_SUCCESS;
+done:
+	thread_interrupt_level(interruptible_state);
+
+	/*
+	 * Only I/O throttle on faults which cause a pagein/swapin.
+	 */
+	if ((type_of_fault == DBG_PAGEIND_FAULT) || (type_of_fault == DBG_PAGEINV_FAULT) || (type_of_fault == DBG_COMPRESSOR_SWAPIN_FAULT)) {
+		throttle_lowpri_io(1);
+	} else {
+		if (kr == KERN_SUCCESS && type_of_fault != DBG_CACHE_HIT_FAULT && type_of_fault != DBG_GUARD_FAULT) {
+
+			if ((throttle_delay = vm_page_throttled(TRUE))) {
+
+				if (vm_debug_events) {
+					if (type_of_fault == DBG_COMPRESSOR_FAULT)
+						VM_DEBUG_EVENT(vmf_compressordelay, VMF_COMPRESSORDELAY, DBG_FUNC_NONE, throttle_delay, 0, 0, 0);
+					else if (type_of_fault == DBG_COW_FAULT)
+						VM_DEBUG_EVENT(vmf_cowdelay, VMF_COWDELAY, DBG_FUNC_NONE, throttle_delay, 0, 0, 0);
+					else
+						VM_DEBUG_EVENT(vmf_zfdelay, VMF_ZFDELAY, DBG_FUNC_NONE, throttle_delay, 0, 0, 0);
+				}
+				delay(throttle_delay);
+			}
+		}
+	}
+	KERNEL_DEBUG_CONSTANT_IST(KDEBUG_TRACE, 
+			      (MACHDBG_CODE(DBG_MACH_VM, 2)) | DBG_FUNC_END,
+			      ((uint64_t)vaddr >> 32),
+			      vaddr,
+			      kr,
+			      type_of_fault,
+			      0);
+
+	return (kr);
+}
+
+/*
+ *	vm_fault_wire:
+ *
+ *	Wire down a range of virtual addresses in a map.
+ */
+kern_return_t
+vm_fault_wire(
+	vm_map_t	map,
+	vm_map_entry_t	entry,
+	vm_prot_t       prot,
+	pmap_t		pmap,
+	vm_map_offset_t	pmap_addr,
+	ppnum_t		*physpage_p)
+{
+
+	register vm_map_offset_t	va;
+	register vm_map_offset_t	end_addr = entry->vme_end;
+	register kern_return_t	rc;
+
+	assert(entry->in_transition);
+
+	if ((VME_OBJECT(entry) != NULL) && 
+	    !entry->is_sub_map && 
+	    VME_OBJECT(entry)->phys_contiguous) {
+		return KERN_SUCCESS;
+	}
+
+	/*
+	 *	Inform the physical mapping system that the
+	 *	range of addresses may not fault, so that
+	 *	page tables and such can be locked down as well.
+	 */
+
+	pmap_pageable(pmap, pmap_addr, 
+		pmap_addr + (end_addr - entry->vme_start), FALSE);
+
+	/*
+	 *	We simulate a fault to get the page and enter it
+	 *	in the physical map.
+	 */
+
+	for (va = entry->vme_start; va < end_addr; va += PAGE_SIZE) {
+		rc = vm_fault_wire_fast(map, va, prot, entry, pmap, 
+					pmap_addr + (va - entry->vme_start),
+					physpage_p);
+		if (rc != KERN_SUCCESS) {
+			rc = vm_fault_internal(map, va, prot, TRUE, 
+					       ((pmap == kernel_pmap)
+						? THREAD_UNINT
+						: THREAD_ABORTSAFE), 
+					       pmap,
+					       (pmap_addr +
+						(va - entry->vme_start)),
+					       physpage_p);
+			DTRACE_VM2(softlock, int, 1, (uint64_t *), NULL);
+		}
+
+		if (rc != KERN_SUCCESS) {
+			struct vm_map_entry	tmp_entry = *entry;
+
+			/* unwire wired pages */
+			tmp_entry.vme_end = va;
+			vm_fault_unwire(map, 
+				&tmp_entry, FALSE, pmap, pmap_addr);
+
+			return rc;
+		}
+	}
+	return KERN_SUCCESS;
+}
+
+/*
+ *	vm_fault_unwire:
+ *
+ *	Unwire a range of virtual addresses in a map.
+ */
+void
+vm_fault_unwire(
+	vm_map_t	map,
+	vm_map_entry_t	entry,
+	boolean_t	deallocate,
+	pmap_t		pmap,
+	vm_map_offset_t	pmap_addr)
+{
+	register vm_map_offset_t	va;
+	register vm_map_offset_t	end_addr = entry->vme_end;
+	vm_object_t		object;
+	struct vm_object_fault_info fault_info;
+
+	object = (entry->is_sub_map) ? VM_OBJECT_NULL : VME_OBJECT(entry);
+
+	/*
+	 * If it's marked phys_contiguous, then vm_fault_wire() didn't actually
+	 * do anything since such memory is wired by default.  So we don't have
+	 * anything to undo here.
+	 */
+
+	if (object != VM_OBJECT_NULL && object->phys_contiguous)
+		return;
+
+	fault_info.interruptible = THREAD_UNINT;
+	fault_info.behavior = entry->behavior;
+	fault_info.user_tag = VME_ALIAS(entry);
+	fault_info.pmap_options = 0;
+	if (entry->iokit_acct ||
+	    (!entry->is_sub_map && !entry->use_pmap)) {
+		fault_info.pmap_options |= PMAP_OPTIONS_ALT_ACCT;
+	}
+	fault_info.lo_offset = VME_OFFSET(entry);
+	fault_info.hi_offset = (entry->vme_end - entry->vme_start) + VME_OFFSET(entry);
+	fault_info.no_cache = entry->no_cache;
+	fault_info.stealth = TRUE;
+	fault_info.io_sync = FALSE;
+	fault_info.cs_bypass = FALSE;
+	fault_info.mark_zf_absent = FALSE;
+	fault_info.batch_pmap_op = FALSE;
+
+	/*
+	 *	Since the pages are wired down, we must be able to
+	 *	get their mappings from the physical map system.
+	 */
+
+	for (va = entry->vme_start; va < end_addr; va += PAGE_SIZE) {
+
+		if (object == VM_OBJECT_NULL) {
+			if (pmap) {
+				pmap_change_wiring(pmap, 
+						   pmap_addr + (va - entry->vme_start), FALSE);
+			}
+			(void) vm_fault(map, va, VM_PROT_NONE, 
+					TRUE, THREAD_UNINT, pmap, pmap_addr);
+		} else {
+		 	vm_prot_t	prot;
+			vm_page_t	result_page;
+			vm_page_t	top_page;
+			vm_object_t	result_object;
+			vm_fault_return_t result;
+
+			if (end_addr - va > (vm_size_t) -1) {
+				/* 32-bit overflow */
+				fault_info.cluster_size = (vm_size_t) (0 - PAGE_SIZE);
+			} else {
+				fault_info.cluster_size = (vm_size_t) (end_addr - va);
+				assert(fault_info.cluster_size == end_addr - va);
+			}
+
+			do {
+				prot = VM_PROT_NONE;
+
+				vm_object_lock(object);
+				vm_object_paging_begin(object);
+				XPR(XPR_VM_FAULT,
+					"vm_fault_unwire -> vm_fault_page\n",
+					0,0,0,0,0);
+				result_page = VM_PAGE_NULL;
+			 	result = vm_fault_page(
+					object,
+					(VME_OFFSET(entry) +
+					 (va - entry->vme_start)),
+					VM_PROT_NONE, TRUE,
+					FALSE, /* page not looked up */
+					&prot, &result_page, &top_page,
+					(int *)0,
+					NULL, map->no_zero_fill, 
+					FALSE, &fault_info);
+			} while (result == VM_FAULT_RETRY);
+
+			/*
+			 * If this was a mapping to a file on a device that has been forcibly
+			 * unmounted, then we won't get a page back from vm_fault_page().  Just
+			 * move on to the next one in case the remaining pages are mapped from
+			 * different objects.  During a forced unmount, the object is terminated
+			 * so the alive flag will be false if this happens.  A forced unmount will
+			 * will occur when an external disk is unplugged before the user does an 
+			 * eject, so we don't want to panic in that situation.
+			 */
+
+			if (result == VM_FAULT_MEMORY_ERROR && !object->alive)
+				continue;
+
+			if (result == VM_FAULT_MEMORY_ERROR &&
+			    object == kernel_object) {
+				/*
+				 * This must have been allocated with
+				 * KMA_KOBJECT and KMA_VAONLY and there's
+				 * no physical page at this offset.
+				 * We're done (no page to free).
+				 */
+				assert(deallocate);
+				continue;
+			}
+
+			if (result != VM_FAULT_SUCCESS)
+				panic("vm_fault_unwire: failure");
+
+			result_object = result_page->object;
+
+			if (deallocate) {
+				assert(result_page->phys_page !=
+				       vm_page_fictitious_addr);
+				pmap_disconnect(result_page->phys_page);
+				VM_PAGE_FREE(result_page);
+			} else {
+				if ((pmap) && (result_page->phys_page != vm_page_guard_addr))
+					pmap_change_wiring(pmap, 
+					    pmap_addr + (va - entry->vme_start), FALSE);
+
+
+				if (VM_PAGE_WIRED(result_page)) {
+					vm_page_lockspin_queues();
+					vm_page_unwire(result_page, TRUE);
+					vm_page_unlock_queues();
+				}
+				if(entry->zero_wired_pages) {
+					pmap_zero_page(result_page->phys_page);
+					entry->zero_wired_pages = FALSE;
+				}
+
+				PAGE_WAKEUP_DONE(result_page);
+			}
+			vm_fault_cleanup(result_object, top_page);
+		}
+	}
+
+	/*
+	 *	Inform the physical mapping system that the range
+	 *	of addresses may fault, so that page tables and
+	 *	such may be unwired themselves.
+	 */
+
+	pmap_pageable(pmap, pmap_addr, 
+		pmap_addr + (end_addr - entry->vme_start), TRUE);
+
+}
+
+/*
+ *	vm_fault_wire_fast:
+ *
+ *	Handle common case of a wire down page fault at the given address.
+ *	If successful, the page is inserted into the associated physical map.
+ *	The map entry is passed in to avoid the overhead of a map lookup.
+ *
+ *	NOTE: the given address should be truncated to the
+ *	proper page address.
+ *
+ *	KERN_SUCCESS is returned if the page fault is handled; otherwise,
+ *	a standard error specifying why the fault is fatal is returned.
+ *
+ *	The map in question must be referenced, and remains so.
+ *	Caller has a read lock on the map.
+ *
+ *	This is a stripped version of vm_fault() for wiring pages.  Anything
+ *	other than the common case will return KERN_FAILURE, and the caller
+ *	is expected to call vm_fault().
+ */
+static kern_return_t
+vm_fault_wire_fast(
+	__unused vm_map_t	map,
+	vm_map_offset_t	va,
+	vm_prot_t       caller_prot,
+	vm_map_entry_t	entry,
+	pmap_t		pmap,
+	vm_map_offset_t	pmap_addr,
+	ppnum_t		*physpage_p)
+{
+	vm_object_t		object;
+	vm_object_offset_t	offset;
+	register vm_page_t	m;
+	vm_prot_t		prot;
+	thread_t           	thread = current_thread();
+	int			type_of_fault;
+	kern_return_t		kr;
+
+	VM_STAT_INCR(faults);
+
+	if (thread != THREAD_NULL && thread->task != TASK_NULL)
+	  thread->task->faults++;
+
+/*
+ *	Recovery actions
+ */
+
+#undef	RELEASE_PAGE
+#define RELEASE_PAGE(m)	{				\
+	PAGE_WAKEUP_DONE(m);				\
+	vm_page_lockspin_queues();			\
+	vm_page_unwire(m, TRUE);			\
+	vm_page_unlock_queues();			\
+}
+
+
+#undef	UNLOCK_THINGS
+#define UNLOCK_THINGS	{				\
+	vm_object_paging_end(object);			   \
+	vm_object_unlock(object);			   \
+}
+
+#undef	UNLOCK_AND_DEALLOCATE
+#define UNLOCK_AND_DEALLOCATE	{			\
+	UNLOCK_THINGS;					\
+	vm_object_deallocate(object);			\
+}
+/*
+ *	Give up and have caller do things the hard way.
+ */
+
+#define GIVE_UP {					\
+	UNLOCK_AND_DEALLOCATE;				\
+	return(KERN_FAILURE);				\
+}
+
+
+	/*
+	 *	If this entry is not directly to a vm_object, bail out.
+	 */
+	if (entry->is_sub_map) {
+		assert(physpage_p == NULL);
+		return(KERN_FAILURE);
+	}
+
+	/*
+	 *	Find the backing store object and offset into it.
+	 */
+
+	object = VME_OBJECT(entry);
+	offset = (va - entry->vme_start) + VME_OFFSET(entry);
+	prot = entry->protection;
+
+   	/*
+	 *	Make a reference to this object to prevent its
+	 *	disposal while we are messing with it.
+	 */
+
+	vm_object_lock(object);
+	vm_object_reference_locked(object);
+	vm_object_paging_begin(object);
+
+	/*
+	 *	INVARIANTS (through entire routine):
+	 *
+	 *	1)	At all times, we must either have the object
+	 *		lock or a busy page in some object to prevent
+	 *		some other thread from trying to bring in
+	 *		the same page.
+	 *
+	 *	2)	Once we have a busy page, we must remove it from
+	 *		the pageout queues, so that the pageout daemon
+	 *		will not grab it away.
+	 *
+	 */
+
+	/*
+	 *	Look for page in top-level object.  If it's not there or
+	 *	there's something going on, give up.
+	 * ENCRYPTED SWAP: use the slow fault path, since we'll need to
+	 * decrypt the page before wiring it down.
+	 */
+	m = vm_page_lookup(object, offset);
+	if ((m == VM_PAGE_NULL) || (m->busy) || (m->encrypted) ||
+	    (m->unusual && ( m->error || m->restart || m->absent))) {
+
+		GIVE_UP;
+	}
+	ASSERT_PAGE_DECRYPTED(m);
+
+	if (m->fictitious &&
+	    m->phys_page == vm_page_guard_addr) {
+		/*
+		 * Guard pages are fictitious pages and are never
+		 * entered into a pmap, so let's say it's been wired...
+		 */
+		kr = KERN_SUCCESS;
+		goto done;
+	}
+
+	/*
+	 *	Wire the page down now.  All bail outs beyond this
+	 *	point must unwire the page.  
+	 */
+
+	vm_page_lockspin_queues();
+	vm_page_wire(m, VM_PROT_MEMORY_TAG(caller_prot), TRUE);
+	vm_page_unlock_queues();
+
+	/*
+	 *	Mark page busy for other threads.
+	 */
+	assert(!m->busy);
+	m->busy = TRUE;
+	assert(!m->absent);
+
+	/*
+	 *	Give up if the page is being written and there's a copy object
+	 */
+	if ((object->copy != VM_OBJECT_NULL) && (prot & VM_PROT_WRITE)) {
+		RELEASE_PAGE(m);
+		GIVE_UP;
+	}
+
+	/*
+	 *	Put this page into the physical map.
+	 */
+	type_of_fault = DBG_CACHE_HIT_FAULT;
+	kr = vm_fault_enter(m,
+			    pmap,
+			    pmap_addr,
+			    prot,
+			    prot,
+			    TRUE,
+			    FALSE,
+			    FALSE,
+			    FALSE,
+			    VME_ALIAS(entry),
+			    ((entry->iokit_acct ||
+			      (!entry->is_sub_map && !entry->use_pmap))
+			     ? PMAP_OPTIONS_ALT_ACCT
+			     : 0),
+			    NULL,
+			    &type_of_fault);
+
+done:
+	/*
+	 *	Unlock everything, and return
+	 */
+
+	if (physpage_p) {
+		/* for vm_map_wire_and_extract() */
+		if (kr == KERN_SUCCESS) {
+			*physpage_p = m->phys_page;
+			if (prot & VM_PROT_WRITE) {
+				vm_object_lock_assert_exclusive(m->object);
+				m->dirty = TRUE;
+			}
+		} else {
+			*physpage_p = 0;
+		}
+	}
+
+	PAGE_WAKEUP_DONE(m);
+	UNLOCK_AND_DEALLOCATE;
+
+	return kr;
+
+}
+
+/*
+ *	Routine:	vm_fault_copy_cleanup
+ *	Purpose:
+ *		Release a page used by vm_fault_copy.
+ */
+
+static void
+vm_fault_copy_cleanup(
+	vm_page_t	page,
+	vm_page_t	top_page)
+{
+	vm_object_t	object = page->object;
+
+	vm_object_lock(object);
+	PAGE_WAKEUP_DONE(page);
+	if (!page->active && !page->inactive && !page->throttled) {
+		vm_page_lockspin_queues();
+		if (!page->active && !page->inactive && !page->throttled)
+			vm_page_activate(page);
+		vm_page_unlock_queues();
+	}
+	vm_fault_cleanup(object, top_page);
+}
+
+static void
+vm_fault_copy_dst_cleanup(
+	vm_page_t	page)
+{
+	vm_object_t	object;
+
+	if (page != VM_PAGE_NULL) {
+		object = page->object;
+		vm_object_lock(object);
+		vm_page_lockspin_queues();
+		vm_page_unwire(page, TRUE);
+		vm_page_unlock_queues();
+		vm_object_paging_end(object);	
+		vm_object_unlock(object);
+	}
+}
+
+/*
+ *	Routine:	vm_fault_copy
+ *
+ *	Purpose:
+ *		Copy pages from one virtual memory object to another --
+ *		neither the source nor destination pages need be resident.
+ *
+ *		Before actually copying a page, the version associated with
+ *		the destination address map wil be verified.
+ *
+ *	In/out conditions:
+ *		The caller must hold a reference, but not a lock, to
+ *		each of the source and destination objects and to the
+ *		destination map.
+ *
+ *	Results:
+ *		Returns KERN_SUCCESS if no errors were encountered in
+ *		reading or writing the data.  Returns KERN_INTERRUPTED if
+ *		the operation was interrupted (only possible if the
+ *		"interruptible" argument is asserted).  Other return values
+ *		indicate a permanent error in copying the data.
+ *
+ *		The actual amount of data copied will be returned in the
+ *		"copy_size" argument.  In the event that the destination map
+ *		verification failed, this amount may be less than the amount
+ *		requested.
+ */
+kern_return_t
+vm_fault_copy(
+	vm_object_t		src_object,
+	vm_object_offset_t	src_offset,
+	vm_map_size_t		*copy_size,		/* INOUT */
+	vm_object_t		dst_object,
+	vm_object_offset_t	dst_offset,
+	vm_map_t		dst_map,
+	vm_map_version_t	 *dst_version,
+	int			interruptible)
+{
+	vm_page_t		result_page;
+	
+	vm_page_t		src_page;
+	vm_page_t		src_top_page;
+	vm_prot_t		src_prot;
+
+	vm_page_t		dst_page;
+	vm_page_t		dst_top_page;
+	vm_prot_t		dst_prot;
+
+	vm_map_size_t		amount_left;
+	vm_object_t		old_copy_object;
+	kern_return_t		error = 0;
+	vm_fault_return_t	result;
+
+	vm_map_size_t		part_size;
+	struct vm_object_fault_info fault_info_src;
+	struct vm_object_fault_info fault_info_dst;
+
+	/*
+	 * In order not to confuse the clustered pageins, align
+	 * the different offsets on a page boundary.
+	 */
+
+#define	RETURN(x)					\
+	MACRO_BEGIN					\
+	*copy_size -= amount_left;			\
+	MACRO_RETURN(x);				\
+	MACRO_END
+
+	amount_left = *copy_size;
+
+	fault_info_src.interruptible = interruptible;
+	fault_info_src.behavior = VM_BEHAVIOR_SEQUENTIAL;
+	fault_info_src.user_tag  = 0;
+	fault_info_src.pmap_options = 0;
+	fault_info_src.lo_offset = vm_object_trunc_page(src_offset);
+	fault_info_src.hi_offset = fault_info_src.lo_offset + amount_left;
+	fault_info_src.no_cache   = FALSE;
+	fault_info_src.stealth = TRUE;
+	fault_info_src.io_sync = FALSE;
+	fault_info_src.cs_bypass = FALSE;
+	fault_info_src.mark_zf_absent = FALSE;
+	fault_info_src.batch_pmap_op = FALSE;
+
+	fault_info_dst.interruptible = interruptible;
+	fault_info_dst.behavior = VM_BEHAVIOR_SEQUENTIAL;
+	fault_info_dst.user_tag  = 0;
+	fault_info_dst.pmap_options = 0;
+	fault_info_dst.lo_offset = vm_object_trunc_page(dst_offset);
+	fault_info_dst.hi_offset = fault_info_dst.lo_offset + amount_left;
+	fault_info_dst.no_cache   = FALSE;
+	fault_info_dst.stealth = TRUE;
+	fault_info_dst.io_sync = FALSE;
+	fault_info_dst.cs_bypass = FALSE;
+	fault_info_dst.mark_zf_absent = FALSE;
+	fault_info_dst.batch_pmap_op = FALSE;
+
+	do { /* while (amount_left > 0) */
+		/*
+		 * There may be a deadlock if both source and destination
+		 * pages are the same. To avoid this deadlock, the copy must
+		 * start by getting the destination page in order to apply
+		 * COW semantics if any.
+		 */
+
+	RetryDestinationFault: ;
+
+		dst_prot = VM_PROT_WRITE|VM_PROT_READ;
+
+		vm_object_lock(dst_object);
+		vm_object_paging_begin(dst_object);
+
+		if (amount_left > (vm_size_t) -1) {
+			/* 32-bit overflow */
+			fault_info_dst.cluster_size = (vm_size_t) (0 - PAGE_SIZE);
+		} else {
+			fault_info_dst.cluster_size = (vm_size_t) amount_left;
+			assert(fault_info_dst.cluster_size == amount_left);
+		}
+
+		XPR(XPR_VM_FAULT,"vm_fault_copy -> vm_fault_page\n",0,0,0,0,0);
+		dst_page = VM_PAGE_NULL;
+		result = vm_fault_page(dst_object,
+				       vm_object_trunc_page(dst_offset),
+				       VM_PROT_WRITE|VM_PROT_READ,
+				       FALSE,
+				       FALSE, /* page not looked up */
+				       &dst_prot, &dst_page, &dst_top_page,
+				       (int *)0,
+				       &error,
+				       dst_map->no_zero_fill,
+				       FALSE, &fault_info_dst);
+		switch (result) {
+		case VM_FAULT_SUCCESS:
+			break;
+		case VM_FAULT_RETRY:
+			goto RetryDestinationFault;
+		case VM_FAULT_MEMORY_SHORTAGE:
+			if (vm_page_wait(interruptible))
+				goto RetryDestinationFault;
+			/* fall thru */
+		case VM_FAULT_INTERRUPTED:
+			RETURN(MACH_SEND_INTERRUPTED);
+		case VM_FAULT_SUCCESS_NO_VM_PAGE:
+			/* success but no VM page: fail the copy */
+			vm_object_paging_end(dst_object);
+			vm_object_unlock(dst_object);
+			/*FALLTHROUGH*/
+		case VM_FAULT_MEMORY_ERROR:
+			if (error)
+				return (error);
+			else
+				return(KERN_MEMORY_ERROR);
+		default:
+			panic("vm_fault_copy: unexpected error 0x%x from "
+			      "vm_fault_page()\n", result);
+		}
+		assert ((dst_prot & VM_PROT_WRITE) != VM_PROT_NONE);
+
+		old_copy_object = dst_page->object->copy;
+
+		/*
+		 * There exists the possiblity that the source and
+		 * destination page are the same.  But we can't
+		 * easily determine that now.  If they are the
+		 * same, the call to vm_fault_page() for the
+		 * destination page will deadlock.  To prevent this we
+		 * wire the page so we can drop busy without having
+		 * the page daemon steal the page.  We clean up the 
+		 * top page  but keep the paging reference on the object
+		 * holding the dest page so it doesn't go away.
+		 */
+
+		vm_page_lockspin_queues();
+		vm_page_wire(dst_page, VM_KERN_MEMORY_OSFMK, TRUE);
+		vm_page_unlock_queues();
+		PAGE_WAKEUP_DONE(dst_page);
+		vm_object_unlock(dst_page->object);
+
+		if (dst_top_page != VM_PAGE_NULL) {
+			vm_object_lock(dst_object);
+			VM_PAGE_FREE(dst_top_page);
+			vm_object_paging_end(dst_object);
+			vm_object_unlock(dst_object);
+		}
+
+	RetrySourceFault: ;
+
+		if (src_object == VM_OBJECT_NULL) {
+			/*
+			 *	No source object.  We will just
+			 *	zero-fill the page in dst_object.
+			 */
+			src_page = VM_PAGE_NULL;
+			result_page = VM_PAGE_NULL;
+		} else {
+			vm_object_lock(src_object);
+			src_page = vm_page_lookup(src_object,
+						  vm_object_trunc_page(src_offset));
+			if (src_page == dst_page) {
+				src_prot = dst_prot;
+				result_page = VM_PAGE_NULL;
+			} else {
+				src_prot = VM_PROT_READ;
+				vm_object_paging_begin(src_object);
+
+				if (amount_left > (vm_size_t) -1) {
+					/* 32-bit overflow */
+					fault_info_src.cluster_size = (vm_size_t) (0 - PAGE_SIZE);
+				} else {
+					fault_info_src.cluster_size = (vm_size_t) amount_left;
+					assert(fault_info_src.cluster_size == amount_left);
+				}
+
+				XPR(XPR_VM_FAULT,
+					"vm_fault_copy(2) -> vm_fault_page\n",
+					0,0,0,0,0);
+				result_page = VM_PAGE_NULL;
+				result = vm_fault_page(
+					src_object, 
+					vm_object_trunc_page(src_offset),
+					VM_PROT_READ, FALSE,
+					FALSE, /* page not looked up */
+					&src_prot, 
+					&result_page, &src_top_page,
+					(int *)0, &error, FALSE,
+					FALSE, &fault_info_src);
+
+				switch (result) {
+				case VM_FAULT_SUCCESS:
+					break;
+				case VM_FAULT_RETRY:
+					goto RetrySourceFault;
+				case VM_FAULT_MEMORY_SHORTAGE:
+					if (vm_page_wait(interruptible))
+						goto RetrySourceFault;
+					/* fall thru */
+				case VM_FAULT_INTERRUPTED:
+					vm_fault_copy_dst_cleanup(dst_page);
+					RETURN(MACH_SEND_INTERRUPTED);
+				case VM_FAULT_SUCCESS_NO_VM_PAGE:
+					/* success but no VM page: fail */
+					vm_object_paging_end(src_object);
+					vm_object_unlock(src_object);
+					/*FALLTHROUGH*/
+				case VM_FAULT_MEMORY_ERROR:
+					vm_fault_copy_dst_cleanup(dst_page);
+					if (error)
+						return (error);
+					else
+						return(KERN_MEMORY_ERROR);
+				default:
+					panic("vm_fault_copy(2): unexpected "
+					      "error 0x%x from "
+					      "vm_fault_page()\n", result);
+				}
+
+
+				assert((src_top_page == VM_PAGE_NULL) ==
+				       (result_page->object == src_object));
+			}
+			assert ((src_prot & VM_PROT_READ) != VM_PROT_NONE);
+			vm_object_unlock(result_page->object);
+		}
+
+		if (!vm_map_verify(dst_map, dst_version)) {
+			if (result_page != VM_PAGE_NULL && src_page != dst_page)
+				vm_fault_copy_cleanup(result_page, src_top_page);
+			vm_fault_copy_dst_cleanup(dst_page);
+			break;
+		}
+
+		vm_object_lock(dst_page->object);
+
+		if (dst_page->object->copy != old_copy_object) {
+			vm_object_unlock(dst_page->object);
+			vm_map_verify_done(dst_map, dst_version);
+			if (result_page != VM_PAGE_NULL && src_page != dst_page)
+				vm_fault_copy_cleanup(result_page, src_top_page);
+			vm_fault_copy_dst_cleanup(dst_page);
+			break;
+		}
+		vm_object_unlock(dst_page->object);
+
+		/*
+		 *	Copy the page, and note that it is dirty
+		 *	immediately.
+		 */
+
+		if (!page_aligned(src_offset) ||
+			!page_aligned(dst_offset) ||
+			!page_aligned(amount_left)) {
+
+			vm_object_offset_t	src_po,
+						dst_po;
+
+			src_po = src_offset - vm_object_trunc_page(src_offset);
+			dst_po = dst_offset - vm_object_trunc_page(dst_offset);
+
+			if (dst_po > src_po) {
+				part_size = PAGE_SIZE - dst_po;
+			} else {
+				part_size = PAGE_SIZE - src_po;
+			}
+			if (part_size > (amount_left)){
+				part_size = amount_left;
+			}
+
+			if (result_page == VM_PAGE_NULL) {
+				assert((vm_offset_t) dst_po == dst_po);
+				assert((vm_size_t) part_size == part_size);
+				vm_page_part_zero_fill(dst_page,
+						       (vm_offset_t) dst_po,
+						       (vm_size_t) part_size);
+			} else {
+				assert((vm_offset_t) src_po == src_po);
+				assert((vm_offset_t) dst_po == dst_po);
+				assert((vm_size_t) part_size == part_size);
+				vm_page_part_copy(result_page,
+						  (vm_offset_t) src_po,
+						  dst_page,
+						  (vm_offset_t) dst_po,
+						  (vm_size_t)part_size);
+				if(!dst_page->dirty){
+					vm_object_lock(dst_object);
+					SET_PAGE_DIRTY(dst_page, TRUE);
+					vm_object_unlock(dst_page->object);
+				}
+
+			}
+		} else {
+			part_size = PAGE_SIZE;
+
+			if (result_page == VM_PAGE_NULL)
+				vm_page_zero_fill(dst_page);
+			else{
+				vm_object_lock(result_page->object);
+				vm_page_copy(result_page, dst_page);
+				vm_object_unlock(result_page->object);
+
+				if(!dst_page->dirty){
+					vm_object_lock(dst_object);
+					SET_PAGE_DIRTY(dst_page, TRUE);
+					vm_object_unlock(dst_page->object);
+				}
+			}
+
+		}
+
+		/*
+		 *	Unlock everything, and return
+		 */
+
+		vm_map_verify_done(dst_map, dst_version);
+
+		if (result_page != VM_PAGE_NULL && src_page != dst_page)
+			vm_fault_copy_cleanup(result_page, src_top_page);
+		vm_fault_copy_dst_cleanup(dst_page);
+
+		amount_left -= part_size;
+		src_offset += part_size;
+		dst_offset += part_size;
+	} while (amount_left > 0);
+
+	RETURN(KERN_SUCCESS);
+#undef	RETURN
+
+	/*NOTREACHED*/	
+}
+
+#if	VM_FAULT_CLASSIFY
+/*
+ *	Temporary statistics gathering support.
+ */
+
+/*
+ *	Statistics arrays:
+ */
+#define VM_FAULT_TYPES_MAX	5
+#define	VM_FAULT_LEVEL_MAX	8
+
+int	vm_fault_stats[VM_FAULT_TYPES_MAX][VM_FAULT_LEVEL_MAX];
+
+#define	VM_FAULT_TYPE_ZERO_FILL	0
+#define	VM_FAULT_TYPE_MAP_IN	1
+#define	VM_FAULT_TYPE_PAGER	2
+#define	VM_FAULT_TYPE_COPY	3
+#define	VM_FAULT_TYPE_OTHER	4
+
+
+void
+vm_fault_classify(vm_object_t		object,
+		  vm_object_offset_t	offset,
+		  vm_prot_t		fault_type)
+{
+	int		type, level = 0;
+	vm_page_t	m;
+
+	while (TRUE) {
+		m = vm_page_lookup(object, offset);
+		if (m != VM_PAGE_NULL) {		
+		        if (m->busy || m->error || m->restart || m->absent) {
+				type = VM_FAULT_TYPE_OTHER;
+				break;
+			}
+			if (((fault_type & VM_PROT_WRITE) == 0) ||
+			    ((level == 0) && object->copy == VM_OBJECT_NULL)) {
+				type = VM_FAULT_TYPE_MAP_IN;
+				break;	
+			}
+			type = VM_FAULT_TYPE_COPY;
+			break;
+		}
+		else {
+			if (object->pager_created) {
+				type = VM_FAULT_TYPE_PAGER;
+				break;
+			}
+			if (object->shadow == VM_OBJECT_NULL) {
+				type = VM_FAULT_TYPE_ZERO_FILL;
+				break;
+		        }
+
+			offset += object->vo_shadow_offset;
+			object = object->shadow;
+			level++;
+			continue;
+		}
+	}
+
+	if (level > VM_FAULT_LEVEL_MAX)
+		level = VM_FAULT_LEVEL_MAX;
+
+	vm_fault_stats[type][level] += 1;
+
+	return;
+}
+
+/* cleanup routine to call from debugger */
+
+void
+vm_fault_classify_init(void)
+{
+	int type, level;
+
+	for (type = 0; type < VM_FAULT_TYPES_MAX; type++) {
+		for (level = 0; level < VM_FAULT_LEVEL_MAX; level++) {
+			vm_fault_stats[type][level] = 0;
+		}
+	}
+
+	return;
+}
+#endif	/* VM_FAULT_CLASSIFY */
+
+vm_offset_t
+kdp_lightweight_fault(vm_map_t map, vm_offset_t cur_target_addr, uint32_t *fault_results)
+{
+#pragma unused(map, cur_target_addr, fault_results)
+
+	return 0;
+#if 0
+	vm_map_entry_t	entry;
+	vm_object_t	object;
+	vm_offset_t	object_offset;
+	vm_page_t	m;
+	int		compressor_external_state, compressed_count_delta;
+	int		compressor_flags = (C_DONT_BLOCK | C_KEEP | C_KDP);
+	int		my_fault_type = VM_PROT_READ;
+	kern_return_t	kr;
+
+
+	if (not_in_kdp) {
+		panic("kdp_lightweight_fault called from outside of debugger context");
+	}
+
+	assert(map != VM_MAP_NULL);
+
+	assert((cur_target_addr & PAGE_MASK) == 0);
+	if ((cur_target_addr & PAGE_MASK) != 0) {
+		return 0;
+	}
+
+	if (kdp_lck_rw_lock_is_acquired_exclusive(&map->lock)) {
+		return 0;
+	}
+
+	if (!vm_map_lookup_entry(map, cur_target_addr, &entry)) {
+		return 0;
+	}
+
+	if (entry->is_sub_map) {
+		return 0;
+	}
+
+	object = VME_OBJECT(entry);
+	if (object == VM_OBJECT_NULL) {
+		return 0;
+	}
+
+	object_offset = cur_target_addr - entry->vme_start + VME_OFFSET(entry);
+
+	while (TRUE) {
+		if (kdp_lck_rw_lock_is_acquired_exclusive(&object->Lock)) {
+			return 0;
+		}
+
+		if (object->pager_created && (object->paging_in_progress ||
+			object->activity_in_progress)) {
+			return 0;
+		}
+
+		m = kdp_vm_page_lookup(object, object_offset);
+
+		if (m != VM_PAGE_NULL) {
+
+			if ((object->wimg_bits & VM_WIMG_MASK) != VM_WIMG_DEFAULT) {
+				return 0;
+			}
+
+			if (m->laundry || m->busy || m->pageout || m->absent || m->error || m->cleaning ||
+				m->overwriting || m->restart || m->unusual) {
+				return 0;
+			}
+
+			assert(!m->private);
+			if (m->private) {
+				return 0;
+			}
+
+			assert(!m->fictitious);
+			if (m->fictitious) {
+				return 0;
+			}
+
+			assert(!m->encrypted);
+			if (m->encrypted) {
+				return 0;
+			}
+
+			assert(!m->encrypted_cleaning);
+			if (m->encrypted_cleaning) {
+				return 0;
+			}
+
+			assert(!m->compressor);
+			if (m->compressor) {
+				return 0;
+			}
+
+			if (fault_results) {
+				*fault_results |= kThreadFaultedBT;
+			}
+			return ptoa(m->phys_page);
+		}
+
+		compressor_external_state = VM_EXTERNAL_STATE_UNKNOWN;
+
+		if (object->pager_created && MUST_ASK_PAGER(object, object_offset, compressor_external_state)) {
+			if (compressor_external_state == VM_EXTERNAL_STATE_EXISTS) {
+				kr = vm_compressor_pager_get(object->pager, (object_offset + object->paging_offset),
+								kdp_compressor_decompressed_page_ppnum, &my_fault_type,
+								compressor_flags, &compressed_count_delta);
+				if (kr == KERN_SUCCESS) {
+					if (fault_results) {
+						*fault_results |= kThreadDecompressedBT;
+					}
+					return kdp_compressor_decompressed_page_paddr;
+				} else {
+					return 0;
+				}
+			}
+		}
+
+		if (object->shadow == VM_OBJECT_NULL) {
+			return 0;
+		}
+
+		object_offset += object->vo_shadow_offset;
+		object = object->shadow;
+	}
+#endif /* 0 */
+}
+
+
+#define CODE_SIGNING_CHUNK_SIZE 4096
+void
+vm_page_validate_cs_mapped(
+	vm_page_t	page,
+	const void 	*kaddr)
+{
+	vm_object_t		object;
+	vm_object_offset_t	offset, offset_in_page;
+	kern_return_t		kr;
+	memory_object_t		pager;
+	void			*blobs;
+	boolean_t		validated;
+	unsigned		tainted;
+	int			num_chunks, num_chunks_validated;
+
+	assert(page->busy);
+	vm_object_lock_assert_exclusive(page->object);
+
+	if (!cs_validation) {
+		return;
+	}
+
+	if (page->wpmapped && !page->cs_tainted) {
+		/*
+		 * This page was mapped for "write" access sometime in the
+		 * past and could still be modifiable in the future.
+		 * Consider it tainted.
+		 * [ If the page was already found to be "tainted", no
+		 * need to re-validate. ]
+		 */
+		page->cs_validated = TRUE;
+		page->cs_tainted = TRUE;
+		if (cs_debug) {
+			printf("CODESIGNING: vm_page_validate_cs: "
+			       "page %p obj %p off 0x%llx "
+			       "was modified\n",
+			       page, page->object, page->offset);
+		}
+		vm_cs_validated_dirtied++;
+	}
+
+	if (page->cs_validated || page->cs_tainted) {
+		return;
+	}
+
+	vm_cs_validates++;
+
+	object = page->object;
+	assert(object->code_signed);
+	offset = page->offset;
+
+	if (!object->alive || object->terminating || object->pager == NULL) {
+		/*
+		 * The object is terminating and we don't have its pager
+		 * so we can't validate the data...
+		 */
+		return;
+	}
+	/*
+	 * Since we get here to validate a page that was brought in by
+	 * the pager, we know that this pager is all setup and ready
+	 * by now.
+	 */
+	assert(!object->internal);
+	assert(object->pager != NULL);
+	assert(object->pager_ready);
+
+	pager = object->pager;
+	assert(object->paging_in_progress);
+	kr = vnode_pager_get_object_cs_blobs(pager, &blobs);
+	if (kr != KERN_SUCCESS) {
+		blobs = NULL;
+	}
+
+	/* verify the SHA1 hash for this page */
+	num_chunks_validated = 0;
+	for (offset_in_page = 0, num_chunks = 0;
+	     offset_in_page < PAGE_SIZE_64;
+	     offset_in_page += CODE_SIGNING_CHUNK_SIZE, num_chunks++) {
+		tainted = 0;
+		validated = cs_validate_page(blobs,
+					     pager,
+					     (object->paging_offset +
+					      offset +
+					      offset_in_page),
+					     (const void *)((const char *)kaddr
+							    + offset_in_page),
+					     &tainted);
+		if (validated) {
+			num_chunks_validated++;
+		}
+		if (tainted & CS_VALIDATE_TAINTED) {
+			page->cs_tainted = TRUE;
+		} 
+		if (tainted & CS_VALIDATE_NX) {
+			page->cs_nx = TRUE;
+		}
+	}
+	/* page is validated only if all its chunks are */
+	if (num_chunks_validated == num_chunks) {
+		page->cs_validated = TRUE;
+	}
+}
+
+void
+vm_page_validate_cs(
+	vm_page_t	page)
+{
+	vm_object_t		object;
+	vm_object_offset_t	offset;
+	vm_map_offset_t		koffset;
+	vm_map_size_t		ksize;
+	vm_offset_t		kaddr;
+	kern_return_t		kr;
+	boolean_t		busy_page;
+	boolean_t		need_unmap;
+
+	vm_object_lock_assert_held(page->object);
+
+	if (!cs_validation) {
+		return;
+	}
+
+	if (page->wpmapped && !page->cs_tainted) {
+		vm_object_lock_assert_exclusive(page->object);
+
+		/*
+		 * This page was mapped for "write" access sometime in the
+		 * past and could still be modifiable in the future.
+		 * Consider it tainted.
+		 * [ If the page was already found to be "tainted", no
+		 * need to re-validate. ]
+		 */
+		page->cs_validated = TRUE;
+		page->cs_tainted = TRUE;
+		if (cs_debug) {
+			printf("CODESIGNING: vm_page_validate_cs: "
+			       "page %p obj %p off 0x%llx "
+			       "was modified\n",
+			       page, page->object, page->offset);
+		}
+		vm_cs_validated_dirtied++;
+	}
+
+	if (page->cs_validated || page->cs_tainted) {
+		return;
+	}
+
+	if (page->slid) {
+		panic("vm_page_validate_cs(%p): page is slid\n", page);
+	}
+	assert(!page->slid);
+
+#if CHECK_CS_VALIDATION_BITMAP	
+	if ( vnode_pager_cs_check_validation_bitmap( page->object->pager, trunc_page(page->offset + page->object->paging_offset), CS_BITMAP_CHECK ) == KERN_SUCCESS) {
+		page->cs_validated = TRUE;
+		page->cs_tainted = FALSE;
+		vm_cs_bitmap_validated++;
+		return;
+	}
+#endif
+	vm_object_lock_assert_exclusive(page->object);
+
+	object = page->object;
+	assert(object->code_signed);
+	offset = page->offset;
+
+	busy_page = page->busy;
+	if (!busy_page) {
+		/* keep page busy while we map (and unlock) the VM object */
+		page->busy = TRUE;
+	}
+	
+	/*
+	 * Take a paging reference on the VM object
+	 * to protect it from collapse or bypass,
+	 * and keep it from disappearing too.
+	 */
+	vm_object_paging_begin(object);
+
+	/* map the page in the kernel address space */
+	ksize = PAGE_SIZE_64;
+	koffset = 0;
+	need_unmap = FALSE;
+	kr = vm_paging_map_object(page,
+				  object,
+				  offset,
+				  VM_PROT_READ,
+				  FALSE, /* can't unlock object ! */
+				  &ksize,
+				  &koffset,
+				  &need_unmap);
+	if (kr != KERN_SUCCESS) {
+		panic("vm_page_validate_cs: could not map page: 0x%x\n", kr);
+	}
+	kaddr = CAST_DOWN(vm_offset_t, koffset);
+
+	/* validate the mapped page */
+	vm_page_validate_cs_mapped(page, (const void *) kaddr);
+
+#if CHECK_CS_VALIDATION_BITMAP	
+	if ( page->cs_validated == TRUE && page->cs_tainted == FALSE ) {
+		vnode_pager_cs_check_validation_bitmap( object->pager, trunc_page( offset + object->paging_offset), CS_BITMAP_SET );
+	}
+#endif
+	assert(page->busy);
+	assert(object == page->object);
+	vm_object_lock_assert_exclusive(object);
+
+	if (!busy_page) {
+		PAGE_WAKEUP_DONE(page);
+	}
+	if (need_unmap) {
+		/* unmap the map from the kernel address space */
+		vm_paging_unmap_object(object, koffset, koffset + ksize);
+		koffset = 0;
+		ksize = 0;
+		kaddr = 0;
+	}
+	vm_object_paging_end(object);
+}
+
+void
+vm_page_validate_cs_mapped_chunk(
+	vm_page_t	page,
+	const void 	*kaddr,
+	vm_offset_t	chunk_offset,
+	boolean_t	*validated_p,
+	unsigned	*tainted_p)
+{
+	vm_object_t		object;
+	vm_object_offset_t	offset, offset_in_page;
+	kern_return_t		kr;
+	memory_object_t		pager;
+	void			*blobs;
+	boolean_t		validated;
+	unsigned		tainted;
+
+	*validated_p = FALSE;
+	*tainted_p = 0;
+
+	assert(page->busy);
+	vm_object_lock_assert_exclusive(page->object);
+
+	if (!cs_validation) {
+		return;
+	}
+
+	object = page->object;
+	assert(object->code_signed);
+	offset = page->offset;
+
+	if (!object->alive || object->terminating || object->pager == NULL) {
+		/*
+		 * The object is terminating and we don't have its pager
+		 * so we can't validate the data...
+		 */
+		return;
+	}
+	/*
+	 * Since we get here to validate a page that was brought in by
+	 * the pager, we know that this pager is all setup and ready
+	 * by now.
+	 */
+	assert(!object->internal);
+	assert(object->pager != NULL);
+	assert(object->pager_ready);
+
+	pager = object->pager;
+	assert(object->paging_in_progress);
+	kr = vnode_pager_get_object_cs_blobs(pager, &blobs);
+	if (kr != KERN_SUCCESS) {
+		blobs = NULL;
+	}
+
+	/* verify the signature for this chunk */
+	offset_in_page = chunk_offset;
+	assert(offset_in_page < PAGE_SIZE);
+	assert((offset_in_page & (CODE_SIGNING_CHUNK_SIZE-1)) == 0);
+
+	tainted = 0;
+	validated = cs_validate_page(blobs,
+				     pager,
+				     (object->paging_offset +
+				      offset +
+				      offset_in_page),
+				     (const void *)((const char *)kaddr
+						    + offset_in_page),
+				     &tainted);
+	if (validated) {
+		*validated_p = TRUE;
+	}
+	if (tainted) {
+		*tainted_p = tainted;
+	}
+}
diff -Nur xnu-3247.1.106/osfmk/vm/vm_pageout.c xnu-3247.1.106-AnV/osfmk/vm/vm_pageout.c
--- xnu-3247.1.106/osfmk/vm/vm_pageout.c	2015-12-06 01:33:10.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/vm/vm_pageout.c	2015-12-13 17:08:10.000000000 +0100
@@ -6853,10 +6853,10 @@
 					 * so it will need to be
 					 * re-validated.
 					 */
-					if (m->slid) {
+					/*if (m->slid) {
 						panic("upl_commit_range(%p): page %p was slid\n",
 						      upl, m);
-					}
+					}*/
 					assert(!m->slid);
 					m->cs_validated = FALSE;
 #if DEVELOPMENT || DEBUG
@@ -10370,7 +10370,7 @@
 	return kr;
 }
 
-void inline memoryshot(unsigned int event, unsigned int control)
+void memoryshot(unsigned int event, unsigned int control)
 {
 	if (vm_debug_events) {
 		KERNEL_DEBUG_CONSTANT1((MACHDBG_CODE(DBG_MACH_VM_PRESSURE, event)) | control,
diff -Nur xnu-3247.1.106/osfmk/vm/vm_pageout.c.orig xnu-3247.1.106-AnV/osfmk/vm/vm_pageout.c.orig
--- xnu-3247.1.106/osfmk/vm/vm_pageout.c.orig	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/vm/vm_pageout.c.orig	2015-12-06 01:33:10.000000000 +0100
@@ -0,0 +1,10625 @@
+/*
+ * Copyright (c) 2000-2014 Apple Inc. All rights reserved.
+ *
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
+ * 
+ * This file contains Original Code and/or Modifications of Original Code
+ * as defined in and that are subject to the Apple Public Source License
+ * Version 2.0 (the 'License'). You may not use this file except in
+ * compliance with the License. The rights granted to you under the License
+ * may not be used to create, or enable the creation or redistribution of,
+ * unlawful or unlicensed copies of an Apple operating system, or to
+ * circumvent, violate, or enable the circumvention or violation of, any
+ * terms of an Apple operating system software license agreement.
+ * 
+ * Please obtain a copy of the License at
+ * http://www.opensource.apple.com/apsl/ and read it before using this file.
+ * 
+ * The Original Code and all software distributed under the License are
+ * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
+ * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
+ * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
+ * Please see the License for the specific language governing rights and
+ * limitations under the License.
+ * 
+ * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
+ */
+/*
+ * @OSF_COPYRIGHT@
+ */
+/* 
+ * Mach Operating System
+ * Copyright (c) 1991,1990,1989,1988,1987 Carnegie Mellon University
+ * All Rights Reserved.
+ * 
+ * Permission to use, copy, modify and distribute this software and its
+ * documentation is hereby granted, provided that both the copyright
+ * notice and this permission notice appear in all copies of the
+ * software, derivative works or modified versions, and any portions
+ * thereof, and that both notices appear in supporting documentation.
+ * 
+ * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
+ * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR
+ * ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
+ * 
+ * Carnegie Mellon requests users of this software to return to
+ * 
+ *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU
+ *  School of Computer Science
+ *  Carnegie Mellon University
+ *  Pittsburgh PA 15213-3890
+ * 
+ * any improvements or extensions that they make and grant Carnegie Mellon
+ * the rights to redistribute these changes.
+ */
+/*
+ */
+/*
+ *	File:	vm/vm_pageout.c
+ *	Author:	Avadis Tevanian, Jr., Michael Wayne Young
+ *	Date:	1985
+ *
+ *	The proverbial page-out daemon.
+ */
+
+#include <stdint.h>
+
+#include <debug.h>
+#include <mach_pagemap.h>
+#include <mach_cluster_stats.h>
+
+#include <mach/mach_types.h>
+#include <mach/memory_object.h>
+#include <mach/memory_object_default.h>
+#include <mach/memory_object_control_server.h>
+#include <mach/mach_host_server.h>
+#include <mach/upl.h>
+#include <mach/vm_map.h>
+#include <mach/vm_param.h>
+#include <mach/vm_statistics.h>
+#include <mach/sdt.h>
+
+#include <kern/kern_types.h>
+#include <kern/counters.h>
+#include <kern/host_statistics.h>
+#include <kern/machine.h>
+#include <kern/misc_protos.h>
+#include <kern/sched.h>
+#include <kern/thread.h>
+#include <kern/xpr.h>
+#include <kern/kalloc.h>
+
+#include <machine/vm_tuning.h>
+#include <machine/commpage.h>
+
+#include <vm/pmap.h>
+#include <vm/vm_compressor_pager.h>
+#include <vm/vm_fault.h>
+#include <vm/vm_map.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+#include <vm/vm_pageout.h>
+#include <vm/vm_protos.h> /* must be last */
+#include <vm/memory_object.h>
+#include <vm/vm_purgeable_internal.h>
+#include <vm/vm_shared_region.h>
+#include <vm/vm_compressor.h>
+
+#if CONFIG_PHANTOM_CACHE
+#include <vm/vm_phantom_cache.h>
+#endif
+/*
+ * ENCRYPTED SWAP:
+ */
+#include <libkern/crypto/aes.h>
+extern u_int32_t random(void);	/* from <libkern/libkern.h> */
+
+extern int cs_debug;
+
+#if UPL_DEBUG
+#include <libkern/OSDebug.h>
+#endif
+
+extern void m_drain(void);
+
+#if VM_PRESSURE_EVENTS
+extern unsigned int memorystatus_available_pages;
+extern unsigned int memorystatus_available_pages_pressure;
+extern unsigned int memorystatus_available_pages_critical;
+extern unsigned int memorystatus_frozen_count;
+extern unsigned int memorystatus_suspended_count;
+
+extern vm_pressure_level_t memorystatus_vm_pressure_level;
+int memorystatus_purge_on_warning = 2;
+int memorystatus_purge_on_urgent = 5;
+int memorystatus_purge_on_critical = 8;
+
+void vm_pressure_response(void);
+boolean_t vm_pressure_thread_running = FALSE;
+extern void consider_vm_pressure_events(void);
+
+#define MEMORYSTATUS_SUSPENDED_THRESHOLD  4
+#endif /* VM_PRESSURE_EVENTS */
+
+boolean_t	vm_pressure_changed = FALSE;
+
+#ifndef VM_PAGEOUT_BURST_ACTIVE_THROTTLE   /* maximum iterations of the active queue to move pages to inactive */
+#define VM_PAGEOUT_BURST_ACTIVE_THROTTLE  100
+#endif
+
+#ifndef VM_PAGEOUT_BURST_INACTIVE_THROTTLE  /* maximum iterations of the inactive queue w/o stealing/cleaning a page */
+#define VM_PAGEOUT_BURST_INACTIVE_THROTTLE 4096
+#endif
+
+#ifndef VM_PAGEOUT_DEADLOCK_RELIEF
+#define VM_PAGEOUT_DEADLOCK_RELIEF 100	/* number of pages to move to break deadlock */
+#endif
+
+#ifndef VM_PAGEOUT_INACTIVE_RELIEF
+#define VM_PAGEOUT_INACTIVE_RELIEF 50	/* minimum number of pages to move to the inactive q */
+#endif
+
+#ifndef	VM_PAGE_LAUNDRY_MAX
+#define	VM_PAGE_LAUNDRY_MAX	128UL	/* maximum pageouts on a given pageout queue */
+#endif	/* VM_PAGEOUT_LAUNDRY_MAX */
+
+#ifndef	VM_PAGEOUT_BURST_WAIT
+#define	VM_PAGEOUT_BURST_WAIT	10	/* milliseconds */
+#endif	/* VM_PAGEOUT_BURST_WAIT */
+
+#ifndef	VM_PAGEOUT_EMPTY_WAIT
+#define VM_PAGEOUT_EMPTY_WAIT	200	/* milliseconds */
+#endif	/* VM_PAGEOUT_EMPTY_WAIT */
+
+#ifndef	VM_PAGEOUT_DEADLOCK_WAIT
+#define VM_PAGEOUT_DEADLOCK_WAIT	300	/* milliseconds */
+#endif	/* VM_PAGEOUT_DEADLOCK_WAIT */
+
+#ifndef	VM_PAGEOUT_IDLE_WAIT
+#define VM_PAGEOUT_IDLE_WAIT	10	/* milliseconds */
+#endif	/* VM_PAGEOUT_IDLE_WAIT */
+
+#ifndef	VM_PAGEOUT_SWAP_WAIT
+#define VM_PAGEOUT_SWAP_WAIT	50	/* milliseconds */
+#endif	/* VM_PAGEOUT_SWAP_WAIT */
+
+#ifndef VM_PAGEOUT_PRESSURE_PAGES_CONSIDERED
+#define VM_PAGEOUT_PRESSURE_PAGES_CONSIDERED		1000	/* maximum pages considered before we issue a pressure event */
+#endif /* VM_PAGEOUT_PRESSURE_PAGES_CONSIDERED */
+
+#ifndef VM_PAGEOUT_PRESSURE_EVENT_MONITOR_SECS
+#define VM_PAGEOUT_PRESSURE_EVENT_MONITOR_SECS		5	/* seconds */
+#endif /* VM_PAGEOUT_PRESSURE_EVENT_MONITOR_SECS */
+
+unsigned int	vm_page_speculative_q_age_ms = VM_PAGE_SPECULATIVE_Q_AGE_MS;
+unsigned int	vm_page_speculative_percentage = 5;
+
+#ifndef VM_PAGE_SPECULATIVE_TARGET
+#define VM_PAGE_SPECULATIVE_TARGET(total) ((total) * 1 / (100 / vm_page_speculative_percentage))
+#endif /* VM_PAGE_SPECULATIVE_TARGET */
+
+
+#ifndef VM_PAGE_INACTIVE_HEALTHY_LIMIT
+#define VM_PAGE_INACTIVE_HEALTHY_LIMIT(total) ((total) * 1 / 200)
+#endif /* VM_PAGE_INACTIVE_HEALTHY_LIMIT */
+
+
+/*
+ *	To obtain a reasonable LRU approximation, the inactive queue
+ *	needs to be large enough to give pages on it a chance to be
+ *	referenced a second time.  This macro defines the fraction
+ *	of active+inactive pages that should be inactive.
+ *	The pageout daemon uses it to update vm_page_inactive_target.
+ *
+ *	If vm_page_free_count falls below vm_page_free_target and
+ *	vm_page_inactive_count is below vm_page_inactive_target,
+ *	then the pageout daemon starts running.
+ */
+
+#ifndef	VM_PAGE_INACTIVE_TARGET
+#define	VM_PAGE_INACTIVE_TARGET(avail)	((avail) * 1 / 2)
+#endif	/* VM_PAGE_INACTIVE_TARGET */
+
+/*
+ *	Once the pageout daemon starts running, it keeps going
+ *	until vm_page_free_count meets or exceeds vm_page_free_target.
+ */
+
+#ifndef	VM_PAGE_FREE_TARGET
+#define	VM_PAGE_FREE_TARGET(free)	(15 + (free) / 80)
+#endif	/* VM_PAGE_FREE_TARGET */
+
+
+/*
+ *	The pageout daemon always starts running once vm_page_free_count
+ *	falls below vm_page_free_min.
+ */
+
+#ifndef	VM_PAGE_FREE_MIN
+#define	VM_PAGE_FREE_MIN(free)		(10 + (free) / 100)
+#endif	/* VM_PAGE_FREE_MIN */
+
+#define VM_PAGE_FREE_RESERVED_LIMIT	1700
+#define VM_PAGE_FREE_MIN_LIMIT		3500
+#define VM_PAGE_FREE_TARGET_LIMIT	4000
+
+/*
+ *	When vm_page_free_count falls below vm_page_free_reserved,
+ *	only vm-privileged threads can allocate pages.  vm-privilege
+ *	allows the pageout daemon and default pager (and any other
+ *	associated threads needed for default pageout) to continue
+ *	operation by dipping into the reserved pool of pages.
+ */
+
+#ifndef	VM_PAGE_FREE_RESERVED
+#define	VM_PAGE_FREE_RESERVED(n)	\
+	((unsigned) (6 * VM_PAGE_LAUNDRY_MAX) + (n))
+#endif	/* VM_PAGE_FREE_RESERVED */
+
+/*
+ *	When we dequeue pages from the inactive list, they are
+ *	reactivated (ie, put back on the active queue) if referenced.
+ *	However, it is possible to starve the free list if other
+ *	processors are referencing pages faster than we can turn off
+ *	the referenced bit.  So we limit the number of reactivations
+ *	we will make per call of vm_pageout_scan().
+ */
+#define VM_PAGE_REACTIVATE_LIMIT_MAX 20000
+#ifndef	VM_PAGE_REACTIVATE_LIMIT
+#define	VM_PAGE_REACTIVATE_LIMIT(avail)	(MAX((avail) * 1 / 20,VM_PAGE_REACTIVATE_LIMIT_MAX))
+#endif	/* VM_PAGE_REACTIVATE_LIMIT */
+#define VM_PAGEOUT_INACTIVE_FORCE_RECLAIM	1000
+
+
+extern boolean_t hibernate_cleaning_in_progress;
+
+/*
+ * Exported variable used to broadcast the activation of the pageout scan
+ * Working Set uses this to throttle its use of pmap removes.  In this
+ * way, code which runs within memory in an uncontested context does
+ * not keep encountering soft faults.
+ */
+
+unsigned int	vm_pageout_scan_event_counter = 0;
+
+/*
+ * Forward declarations for internal routines.
+ */
+struct cq {
+	struct vm_pageout_queue *q;
+	void			*current_chead;
+	char			*scratch_buf;
+	int			id;
+};
+#define MAX_COMPRESSOR_THREAD_COUNT	8
+
+struct cq ciq[MAX_COMPRESSOR_THREAD_COUNT];
+
+void	*vm_pageout_immediate_chead;
+char	*vm_pageout_immediate_scratch_buf;
+
+
+#if VM_PRESSURE_EVENTS
+void vm_pressure_thread(void);
+
+boolean_t VM_PRESSURE_NORMAL_TO_WARNING(void);
+boolean_t VM_PRESSURE_WARNING_TO_CRITICAL(void);
+
+boolean_t VM_PRESSURE_WARNING_TO_NORMAL(void);
+boolean_t VM_PRESSURE_CRITICAL_TO_WARNING(void);
+#endif
+static void vm_pageout_garbage_collect(int);
+static void vm_pageout_iothread_continue(struct vm_pageout_queue *);
+static void vm_pageout_iothread_external(void);
+static void vm_pageout_iothread_internal(struct cq *cq);
+static void vm_pageout_adjust_io_throttles(struct vm_pageout_queue *, struct vm_pageout_queue *, boolean_t);
+
+extern void vm_pageout_continue(void);
+extern void vm_pageout_scan(void);
+
+static void	vm_pageout_immediate(vm_page_t, boolean_t);
+boolean_t	vm_compressor_immediate_preferred = FALSE;
+boolean_t	vm_compressor_immediate_preferred_override = FALSE;
+boolean_t	vm_restricted_to_single_processor = FALSE;
+
+static thread_t	vm_pageout_external_iothread = THREAD_NULL;
+static thread_t	vm_pageout_internal_iothread = THREAD_NULL;
+
+unsigned int vm_pageout_reserved_internal = 0;
+unsigned int vm_pageout_reserved_really = 0;
+
+unsigned int vm_pageout_swap_wait = 0;
+unsigned int vm_pageout_idle_wait = 0;		/* milliseconds */
+unsigned int vm_pageout_empty_wait = 0;		/* milliseconds */
+unsigned int vm_pageout_burst_wait = 0;		/* milliseconds */
+unsigned int vm_pageout_deadlock_wait = 0;	/* milliseconds */
+unsigned int vm_pageout_deadlock_relief = 0;
+unsigned int vm_pageout_inactive_relief = 0;
+unsigned int vm_pageout_burst_active_throttle = 0;
+unsigned int vm_pageout_burst_inactive_throttle = 0;
+
+int	vm_upl_wait_for_pages = 0;
+
+
+/*
+ *	These variables record the pageout daemon's actions:
+ *	how many pages it looks at and what happens to those pages.
+ *	No locking needed because only one thread modifies the variables.
+ */
+
+unsigned int vm_pageout_active = 0;		/* debugging */
+unsigned int vm_pageout_active_busy = 0;	/* debugging */
+unsigned int vm_pageout_inactive = 0;		/* debugging */
+unsigned int vm_pageout_inactive_throttled = 0;	/* debugging */
+unsigned int vm_pageout_inactive_forced = 0;	/* debugging */
+unsigned int vm_pageout_inactive_nolock = 0;	/* debugging */
+unsigned int vm_pageout_inactive_avoid = 0;	/* debugging */
+unsigned int vm_pageout_inactive_busy = 0;	/* debugging */
+unsigned int vm_pageout_inactive_error = 0;	/* debugging */
+unsigned int vm_pageout_inactive_absent = 0;	/* debugging */
+unsigned int vm_pageout_inactive_notalive = 0;	/* debugging */
+unsigned int vm_pageout_inactive_used = 0;	/* debugging */
+unsigned int vm_pageout_cache_evicted = 0;	/* debugging */
+unsigned int vm_pageout_inactive_clean = 0;	/* debugging */
+unsigned int vm_pageout_speculative_clean = 0;	/* debugging */
+
+unsigned int vm_pageout_freed_from_cleaned = 0;
+unsigned int vm_pageout_freed_from_speculative = 0;
+unsigned int vm_pageout_freed_from_inactive_clean = 0;
+
+unsigned int vm_pageout_enqueued_cleaned_from_inactive_clean = 0;
+unsigned int vm_pageout_enqueued_cleaned_from_inactive_dirty = 0;
+
+unsigned int vm_pageout_cleaned_reclaimed = 0;		/* debugging; how many cleaned pages are reclaimed by the pageout scan */
+unsigned int vm_pageout_cleaned_reactivated = 0;	/* debugging; how many cleaned pages are found to be referenced on pageout (and are therefore reactivated) */
+unsigned int vm_pageout_cleaned_reference_reactivated = 0;
+unsigned int vm_pageout_cleaned_volatile_reactivated = 0;
+unsigned int vm_pageout_cleaned_fault_reactivated = 0;
+unsigned int vm_pageout_cleaned_commit_reactivated = 0;	/* debugging; how many cleaned pages are found to be referenced on commit (and are therefore reactivated) */
+unsigned int vm_pageout_cleaned_busy = 0;
+unsigned int vm_pageout_cleaned_nolock = 0;
+
+unsigned int vm_pageout_inactive_dirty_internal = 0;	/* debugging */
+unsigned int vm_pageout_inactive_dirty_external = 0;	/* debugging */
+unsigned int vm_pageout_inactive_deactivated = 0;	/* debugging */
+unsigned int vm_pageout_inactive_anonymous = 0;	/* debugging */
+unsigned int vm_pageout_dirty_no_pager = 0;	/* debugging */
+unsigned int vm_pageout_purged_objects = 0;	/* used for sysctl vm stats */
+unsigned int vm_stat_discard = 0;		/* debugging */
+unsigned int vm_stat_discard_sent = 0;		/* debugging */
+unsigned int vm_stat_discard_failure = 0;	/* debugging */
+unsigned int vm_stat_discard_throttle = 0;	/* debugging */
+unsigned int vm_pageout_reactivation_limit_exceeded = 0;	/* debugging */
+unsigned int vm_pageout_catch_ups = 0;				/* debugging */
+unsigned int vm_pageout_inactive_force_reclaim = 0;	/* debugging */
+
+unsigned int vm_pageout_scan_reclaimed_throttled = 0;
+unsigned int vm_pageout_scan_active_throttled = 0;
+unsigned int vm_pageout_scan_inactive_throttled_internal = 0;
+unsigned int vm_pageout_scan_inactive_throttled_external = 0;
+unsigned int vm_pageout_scan_throttle = 0;			/* debugging */
+unsigned int vm_pageout_scan_burst_throttle = 0;		/* debugging */
+unsigned int vm_pageout_scan_empty_throttle = 0;		/* debugging */
+unsigned int vm_pageout_scan_swap_throttle = 0;		/* debugging */
+unsigned int vm_pageout_scan_deadlock_detected = 0;		/* debugging */
+unsigned int vm_pageout_scan_active_throttle_success = 0;	/* debugging */
+unsigned int vm_pageout_scan_inactive_throttle_success = 0;	/* debugging */
+unsigned int vm_pageout_inactive_external_forced_jetsam_count = 0;	/* debugging */
+unsigned int vm_pageout_scan_throttle_deferred = 0;		/* debugging */
+unsigned int vm_pageout_scan_yield_unthrottled = 0;		/* debugging */
+unsigned int vm_page_speculative_count_drifts = 0;
+unsigned int vm_page_speculative_count_drift_max = 0;
+
+
+/*
+ * Backing store throttle when BS is exhausted
+ */
+unsigned int	vm_backing_store_low = 0;
+
+unsigned int vm_pageout_out_of_line  = 0;
+unsigned int vm_pageout_in_place  = 0;
+
+unsigned int vm_page_steal_pageout_page = 0;
+
+/*
+ * ENCRYPTED SWAP:
+ * counters and statistics...
+ */
+unsigned long vm_page_decrypt_counter = 0;
+unsigned long vm_page_decrypt_for_upl_counter = 0;
+unsigned long vm_page_encrypt_counter = 0;
+unsigned long vm_page_encrypt_abort_counter = 0;
+unsigned long vm_page_encrypt_already_encrypted_counter = 0;
+boolean_t vm_pages_encrypted = FALSE; /* are there encrypted pages ? */
+
+struct	vm_pageout_queue vm_pageout_queue_internal;
+struct	vm_pageout_queue vm_pageout_queue_external;
+
+unsigned int vm_page_speculative_target = 0;
+
+vm_object_t 	vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+
+boolean_t (* volatile consider_buffer_cache_collect)(int) = NULL;
+
+#if DEVELOPMENT || DEBUG
+unsigned long vm_cs_validated_resets = 0;
+#endif
+
+int	vm_debug_events	= 0;
+
+#if CONFIG_MEMORYSTATUS
+#if !CONFIG_JETSAM
+extern boolean_t memorystatus_idle_exit_from_VM(void);
+#endif
+extern boolean_t memorystatus_kill_on_VM_page_shortage(boolean_t async);
+extern void memorystatus_on_pageout_scan_end(void);
+#endif
+
+/*
+ *	Routine:	vm_backing_store_disable
+ *	Purpose:
+ *		Suspend non-privileged threads wishing to extend
+ *		backing store when we are low on backing store
+ *		(Synchronized by caller)
+ */
+void
+vm_backing_store_disable(
+	boolean_t	disable)
+{
+	if(disable) {
+		vm_backing_store_low = 1;
+	} else {
+		if(vm_backing_store_low) {
+			vm_backing_store_low = 0;
+			thread_wakeup((event_t) &vm_backing_store_low);
+		}
+	}
+}
+
+
+#if MACH_CLUSTER_STATS
+unsigned long vm_pageout_cluster_dirtied = 0;
+unsigned long vm_pageout_cluster_cleaned = 0;
+unsigned long vm_pageout_cluster_collisions = 0;
+unsigned long vm_pageout_cluster_clusters = 0;
+unsigned long vm_pageout_cluster_conversions = 0;
+unsigned long vm_pageout_target_collisions = 0;
+unsigned long vm_pageout_target_page_dirtied = 0;
+unsigned long vm_pageout_target_page_freed = 0;
+#define CLUSTER_STAT(clause)	clause
+#else	/* MACH_CLUSTER_STATS */
+#define CLUSTER_STAT(clause)
+#endif	/* MACH_CLUSTER_STATS */
+
+/* 
+ *	Routine:	vm_pageout_object_terminate
+ *	Purpose:
+ *		Destroy the pageout_object, and perform all of the
+ *		required cleanup actions.
+ * 
+ *	In/Out conditions:
+ *		The object must be locked, and will be returned locked.
+ */
+void
+vm_pageout_object_terminate(
+	vm_object_t	object)
+{
+	vm_object_t	shadow_object;
+
+	/*
+	 * Deal with the deallocation (last reference) of a pageout object
+	 * (used for cleaning-in-place) by dropping the paging references/
+	 * freeing pages in the original object.
+	 */
+
+	assert(object->pageout);
+	shadow_object = object->shadow;
+	vm_object_lock(shadow_object);
+
+	while (!queue_empty(&object->memq)) {
+		vm_page_t 		p, m;
+		vm_object_offset_t	offset;
+
+		p = (vm_page_t) queue_first(&object->memq);
+
+		assert(p->private);
+		assert(p->pageout);
+		p->pageout = FALSE;
+		assert(!p->cleaning);
+		assert(!p->laundry);
+
+		offset = p->offset;
+		VM_PAGE_FREE(p);
+		p = VM_PAGE_NULL;
+
+		m = vm_page_lookup(shadow_object,
+			offset + object->vo_shadow_offset);
+
+		if(m == VM_PAGE_NULL)
+			continue;
+
+		assert((m->dirty) || (m->precious) ||
+				(m->busy && m->cleaning));
+
+		/*
+		 * Handle the trusted pager throttle.
+		 * Also decrement the burst throttle (if external).
+		 */
+		vm_page_lock_queues();
+		if (m->pageout_queue)
+			vm_pageout_throttle_up(m);
+
+		/*
+		 * Handle the "target" page(s). These pages are to be freed if
+		 * successfully cleaned. Target pages are always busy, and are
+		 * wired exactly once. The initial target pages are not mapped,
+		 * (so cannot be referenced or modified) but converted target
+		 * pages may have been modified between the selection as an
+		 * adjacent page and conversion to a target.
+		 */
+		if (m->pageout) {
+			assert(m->busy);
+			assert(m->wire_count == 1);
+			m->cleaning = FALSE;
+			m->encrypted_cleaning = FALSE;
+			m->pageout = FALSE;
+#if MACH_CLUSTER_STATS
+			if (m->wanted) vm_pageout_target_collisions++;
+#endif
+			/*
+			 * Revoke all access to the page. Since the object is
+			 * locked, and the page is busy, this prevents the page
+			 * from being dirtied after the pmap_disconnect() call
+			 * returns.
+			 *
+			 * Since the page is left "dirty" but "not modifed", we
+			 * can detect whether the page was redirtied during
+			 * pageout by checking the modify state.
+			 */
+			if (pmap_disconnect(m->phys_page) & VM_MEM_MODIFIED) {
+				SET_PAGE_DIRTY(m, FALSE);
+			} else {
+				m->dirty = FALSE;
+			}
+
+			if (m->dirty) {
+				CLUSTER_STAT(vm_pageout_target_page_dirtied++;)
+				vm_page_unwire(m, TRUE);	/* reactivates */
+				VM_STAT_INCR(reactivations);
+				PAGE_WAKEUP_DONE(m);
+			} else {
+				CLUSTER_STAT(vm_pageout_target_page_freed++;)
+				vm_page_free(m);/* clears busy, etc. */
+			}
+			vm_page_unlock_queues();
+			continue;
+		}
+		/*
+		 * Handle the "adjacent" pages. These pages were cleaned in
+		 * place, and should be left alone.
+		 * If prep_pin_count is nonzero, then someone is using the
+		 * page, so make it active.
+		 */
+		if (!m->active && !m->inactive && !m->throttled && !m->private) {
+			if (m->reference)
+				vm_page_activate(m);
+			else
+				vm_page_deactivate(m);
+		}
+		if (m->overwriting) {
+			/*
+			 * the (COPY_OUT_FROM == FALSE) request_page_list case
+			 */
+			if (m->busy) {
+				/*
+				 * We do not re-set m->dirty !
+				 * The page was busy so no extraneous activity
+				 * could have occurred. COPY_INTO is a read into the
+				 * new pages. CLEAN_IN_PLACE does actually write
+				 * out the pages but handling outside of this code
+				 * will take care of resetting dirty. We clear the
+				 * modify however for the Programmed I/O case.
+				 */
+				pmap_clear_modify(m->phys_page);
+
+				m->busy = FALSE;
+				m->absent = FALSE;
+			} else {
+				/*
+				 * alternate (COPY_OUT_FROM == FALSE) request_page_list case
+				 * Occurs when the original page was wired
+				 * at the time of the list request
+				 */
+				 assert(VM_PAGE_WIRED(m));
+				 vm_page_unwire(m, TRUE);	/* reactivates */
+			}
+			m->overwriting = FALSE;
+		} else {
+			/*
+			 * Set the dirty state according to whether or not the page was
+			 * modified during the pageout. Note that we purposefully do
+			 * NOT call pmap_clear_modify since the page is still mapped.
+			 * If the page were to be dirtied between the 2 calls, this
+			 * this fact would be lost. This code is only necessary to
+			 * maintain statistics, since the pmap module is always
+			 * consulted if m->dirty is false.
+			 */
+#if MACH_CLUSTER_STATS
+			m->dirty = pmap_is_modified(m->phys_page);
+
+			if (m->dirty)	vm_pageout_cluster_dirtied++;
+			else		vm_pageout_cluster_cleaned++;
+			if (m->wanted)	vm_pageout_cluster_collisions++;
+#else
+			m->dirty = FALSE;
+#endif
+		}
+		if (m->encrypted_cleaning == TRUE) {
+			m->encrypted_cleaning = FALSE;
+			m->busy = FALSE;
+		}
+		m->cleaning = FALSE;
+
+		/*
+		 * Wakeup any thread waiting for the page to be un-cleaning.
+		 */
+		PAGE_WAKEUP(m);
+		vm_page_unlock_queues();
+	}
+	/*
+	 * Account for the paging reference taken in vm_paging_object_allocate.
+	 */
+	vm_object_activity_end(shadow_object);
+	vm_object_unlock(shadow_object);
+
+	assert(object->ref_count == 0);
+	assert(object->paging_in_progress == 0);
+	assert(object->activity_in_progress == 0);
+	assert(object->resident_page_count == 0);
+	return;
+}
+
+/*
+ * Routine:	vm_pageclean_setup
+ *
+ * Purpose:	setup a page to be cleaned (made non-dirty), but not
+ *		necessarily flushed from the VM page cache.
+ *		This is accomplished by cleaning in place.
+ *
+ *		The page must not be busy, and new_object
+ *		must be locked.
+ *
+ */
+static void
+vm_pageclean_setup(
+	vm_page_t		m,
+	vm_page_t		new_m,
+	vm_object_t		new_object,
+	vm_object_offset_t	new_offset)
+{
+	assert(!m->busy);
+#if 0
+	assert(!m->cleaning);
+#endif
+
+	XPR(XPR_VM_PAGEOUT,
+    "vm_pageclean_setup, obj 0x%X off 0x%X page 0x%X new 0x%X new_off 0x%X\n",
+		m->object, m->offset, m, 
+		new_m, new_offset);
+
+	pmap_clear_modify(m->phys_page);
+
+	/*
+	 * Mark original page as cleaning in place.
+	 */
+	m->cleaning = TRUE;
+	SET_PAGE_DIRTY(m, FALSE);
+	m->precious = FALSE;
+
+	/*
+	 * Convert the fictitious page to a private shadow of
+	 * the real page.
+	 */
+	assert(new_m->fictitious);
+	assert(new_m->phys_page == vm_page_fictitious_addr);
+	new_m->fictitious = FALSE;
+	new_m->private = TRUE;
+	new_m->pageout = TRUE;
+	new_m->phys_page = m->phys_page;
+
+	vm_page_lockspin_queues();
+	vm_page_wire(new_m, VM_KERN_MEMORY_NONE, TRUE);
+	vm_page_unlock_queues();
+
+	vm_page_insert_wired(new_m, new_object, new_offset, VM_KERN_MEMORY_NONE);
+	assert(!new_m->wanted);
+	new_m->busy = FALSE;
+}
+
+/*
+ *	Routine:	vm_pageout_initialize_page
+ *	Purpose:
+ *		Causes the specified page to be initialized in
+ *		the appropriate memory object. This routine is used to push
+ *		pages into a copy-object when they are modified in the
+ *		permanent object.
+ *
+ *		The page is moved to a temporary object and paged out.
+ *
+ *	In/out conditions:
+ *		The page in question must not be on any pageout queues.
+ *		The object to which it belongs must be locked.
+ *		The page must be busy, but not hold a paging reference.
+ *
+ *	Implementation:
+ *		Move this page to a completely new object.
+ */
+void	
+vm_pageout_initialize_page(
+	vm_page_t	m)
+{
+	vm_object_t		object;
+	vm_object_offset_t	paging_offset;
+	memory_object_t		pager;
+
+	XPR(XPR_VM_PAGEOUT,
+		"vm_pageout_initialize_page, page 0x%X\n",
+		m, 0, 0, 0, 0);
+	assert(m->busy);
+
+	/*
+	 *	Verify that we really want to clean this page
+	 */
+	assert(!m->absent);
+	assert(!m->error);
+	assert(m->dirty);
+
+	/*
+	 *	Create a paging reference to let us play with the object.
+	 */
+	object = m->object;
+	paging_offset = m->offset + object->paging_offset;
+
+	if (m->absent || m->error || m->restart || (!m->dirty && !m->precious)) {
+		VM_PAGE_FREE(m);
+		panic("reservation without pageout?"); /* alan */
+		vm_object_unlock(object);
+
+		return;
+	}
+
+	/*
+	 * If there's no pager, then we can't clean the page.  This should 
+	 * never happen since this should be a copy object and therefore not
+	 * an external object, so the pager should always be there.
+	 */
+
+	pager = object->pager;
+
+	if (pager == MEMORY_OBJECT_NULL) {
+		VM_PAGE_FREE(m);
+		panic("missing pager for copy object");
+		return;
+	}
+
+	/*
+	 * set the page for future call to vm_fault_list_request
+	 */
+	pmap_clear_modify(m->phys_page);
+	SET_PAGE_DIRTY(m, FALSE);
+	m->pageout = TRUE;
+
+	/*
+	 * keep the object from collapsing or terminating
+	 */
+	vm_object_paging_begin(object);
+	vm_object_unlock(object);
+
+	/*
+	 *	Write the data to its pager.
+	 *	Note that the data is passed by naming the new object,
+	 *	not a virtual address; the pager interface has been
+	 *	manipulated to use the "internal memory" data type.
+	 *	[The object reference from its allocation is donated
+	 *	to the eventual recipient.]
+	 */
+	memory_object_data_initialize(pager, paging_offset, PAGE_SIZE);
+
+	vm_object_lock(object);
+	vm_object_paging_end(object);
+}
+
+#if	MACH_CLUSTER_STATS
+#define MAXCLUSTERPAGES	16
+struct {
+	unsigned long pages_in_cluster;
+	unsigned long pages_at_higher_offsets;
+	unsigned long pages_at_lower_offsets;
+} cluster_stats[MAXCLUSTERPAGES];
+#endif	/* MACH_CLUSTER_STATS */
+
+
+/*
+ * vm_pageout_cluster:
+ *
+ * Given a page, queue it to the appropriate I/O thread,
+ * which will page it out and attempt to clean adjacent pages
+ * in the same operation.
+ *
+ * The object and queues must be locked. We will take a
+ * paging reference to prevent deallocation or collapse when we
+ * release the object lock back at the call site.  The I/O thread
+ * is responsible for consuming this reference
+ *
+ * The page must not be on any pageout queue.
+ */
+
+int
+vm_pageout_cluster(vm_page_t m, boolean_t pageout, boolean_t immediate_ok, boolean_t keep_object_locked)
+{
+	vm_object_t	object = m->object;
+        struct		vm_pageout_queue *q;
+
+
+	XPR(XPR_VM_PAGEOUT,
+		"vm_pageout_cluster, object 0x%X offset 0x%X page 0x%X\n",
+		object, m->offset, m, 0, 0);
+
+	VM_PAGE_CHECK(m);
+#if DEBUG
+	lck_mtx_assert(&vm_page_queue_lock, LCK_MTX_ASSERT_OWNED);
+#endif
+	vm_object_lock_assert_exclusive(object);
+
+	/*
+	 * Only a certain kind of page is appreciated here.
+	 */
+	assert((m->dirty || m->precious) && (!VM_PAGE_WIRED(m)));
+	assert(!m->cleaning && !m->pageout && !m->laundry);
+#ifndef CONFIG_FREEZE
+	assert(!m->inactive && !m->active);
+	assert(!m->throttled);
+#endif
+
+	/*
+	 * protect the object from collapse or termination
+	 */
+	vm_object_activity_begin(object);
+
+	m->pageout = pageout;
+
+	if (object->internal == TRUE) {
+		if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
+			m->busy = TRUE;
+
+			if (vm_compressor_immediate_preferred == TRUE && immediate_ok == TRUE) {
+				if (keep_object_locked == FALSE)
+					vm_object_unlock(object);
+				vm_page_unlock_queues();
+
+				vm_pageout_immediate(m, keep_object_locked);
+
+				return (1);
+			}
+		}
+	        q = &vm_pageout_queue_internal;
+	} else
+	        q = &vm_pageout_queue_external;
+
+	/* 
+	 * pgo_laundry count is tied to the laundry bit
+	 */
+	m->laundry = TRUE;
+	q->pgo_laundry++;
+
+	m->pageout_queue = TRUE;
+	queue_enter(&q->pgo_pending, m, vm_page_t, pageq);
+	
+	if (q->pgo_idle == TRUE) {
+		q->pgo_idle = FALSE;
+		thread_wakeup((event_t) &q->pgo_pending);
+	}
+	VM_PAGE_CHECK(m);
+
+	return (0);
+}
+
+
+unsigned long vm_pageout_throttle_up_count = 0;
+
+/*
+ * A page is back from laundry or we are stealing it back from 
+ * the laundering state.  See if there are some pages waiting to
+ * go to laundry and if we can let some of them go now.
+ *
+ * Object and page queues must be locked.
+ */
+void
+vm_pageout_throttle_up(
+       vm_page_t       m)
+{
+       struct vm_pageout_queue *q;
+
+       assert(m->object != VM_OBJECT_NULL);
+       assert(m->object != kernel_object);
+
+#if DEBUG
+       lck_mtx_assert(&vm_page_queue_lock, LCK_MTX_ASSERT_OWNED);
+       vm_object_lock_assert_exclusive(m->object);
+#endif
+
+       vm_pageout_throttle_up_count++;
+
+       if (m->object->internal == TRUE)
+               q = &vm_pageout_queue_internal;
+       else
+               q = &vm_pageout_queue_external;
+
+       if (m->pageout_queue == TRUE) {
+
+	       queue_remove(&q->pgo_pending, m, vm_page_t, pageq);
+	       m->pageout_queue = FALSE;
+
+	       m->pageq.next = NULL;
+	       m->pageq.prev = NULL;
+
+	       vm_object_activity_end(m->object);
+       }
+       if (m->laundry == TRUE) {
+
+	       m->laundry = FALSE;
+	       q->pgo_laundry--;
+
+	       if (q->pgo_throttled == TRUE) {
+		       q->pgo_throttled = FALSE;
+                       thread_wakeup((event_t) &q->pgo_laundry);
+               }
+	       if (q->pgo_draining == TRUE && q->pgo_laundry == 0) {
+		       q->pgo_draining = FALSE;
+		       thread_wakeup((event_t) (&q->pgo_laundry+1));
+	       }
+	}
+}
+
+
+static void
+vm_pageout_throttle_up_batch(
+	struct vm_pageout_queue *q,
+	int		batch_cnt)
+{
+#if DEBUG
+       lck_mtx_assert(&vm_page_queue_lock, LCK_MTX_ASSERT_OWNED);
+#endif
+
+       vm_pageout_throttle_up_count += batch_cnt;
+
+       q->pgo_laundry -= batch_cnt;
+
+       if (q->pgo_throttled == TRUE) {
+	       q->pgo_throttled = FALSE;
+	       thread_wakeup((event_t) &q->pgo_laundry);
+       }
+       if (q->pgo_draining == TRUE && q->pgo_laundry == 0) {
+	       q->pgo_draining = FALSE;
+	       thread_wakeup((event_t) (&q->pgo_laundry+1));
+       }
+}
+
+
+
+/*
+ * VM memory pressure monitoring.
+ *
+ * vm_pageout_scan() keeps track of the number of pages it considers and
+ * reclaims, in the currently active vm_pageout_stat[vm_pageout_stat_now].
+ *
+ * compute_memory_pressure() is called every second from compute_averages()
+ * and moves "vm_pageout_stat_now" forward, to start accumulating the number
+ * of recalimed pages in a new vm_pageout_stat[] bucket.
+ *
+ * mach_vm_pressure_monitor() collects past statistics about memory pressure.
+ * The caller provides the number of seconds ("nsecs") worth of statistics
+ * it wants, up to 30 seconds.
+ * It computes the number of pages reclaimed in the past "nsecs" seconds and
+ * also returns the number of pages the system still needs to reclaim at this
+ * moment in time.
+ */
+#define VM_PAGEOUT_STAT_SIZE	31
+struct vm_pageout_stat {
+	unsigned int considered;
+	unsigned int reclaimed;
+} vm_pageout_stats[VM_PAGEOUT_STAT_SIZE] = {{0,0}, };
+unsigned int vm_pageout_stat_now = 0;
+unsigned int vm_memory_pressure = 0;
+
+#define VM_PAGEOUT_STAT_BEFORE(i) \
+	(((i) == 0) ? VM_PAGEOUT_STAT_SIZE - 1 : (i) - 1)
+#define VM_PAGEOUT_STAT_AFTER(i) \
+	(((i) == VM_PAGEOUT_STAT_SIZE - 1) ? 0 : (i) + 1)
+
+#if VM_PAGE_BUCKETS_CHECK
+int vm_page_buckets_check_interval = 10; /* in seconds */
+#endif /* VM_PAGE_BUCKETS_CHECK */
+
+/*
+ * Called from compute_averages().
+ */
+void
+compute_memory_pressure(
+	__unused void *arg)
+{
+	unsigned int vm_pageout_next;
+
+#if VM_PAGE_BUCKETS_CHECK
+	/* check the consistency of VM page buckets at regular interval */
+	static int counter = 0;
+	if ((++counter % vm_page_buckets_check_interval) == 0) {
+		vm_page_buckets_check();
+	}
+#endif /* VM_PAGE_BUCKETS_CHECK */
+
+	vm_memory_pressure =
+		vm_pageout_stats[VM_PAGEOUT_STAT_BEFORE(vm_pageout_stat_now)].reclaimed;
+
+	commpage_set_memory_pressure( vm_memory_pressure );
+
+	/* move "now" forward */
+	vm_pageout_next = VM_PAGEOUT_STAT_AFTER(vm_pageout_stat_now);
+	vm_pageout_stats[vm_pageout_next].considered = 0;
+	vm_pageout_stats[vm_pageout_next].reclaimed = 0;
+	vm_pageout_stat_now = vm_pageout_next;
+}
+
+
+/*
+ * IMPORTANT
+ * mach_vm_ctl_page_free_wanted() is called indirectly, via
+ * mach_vm_pressure_monitor(), when taking a stackshot. Therefore, 
+ * it must be safe in the restricted stackshot context. Locks and/or 
+ * blocking are not allowable.
+ */
+unsigned int
+mach_vm_ctl_page_free_wanted(void)
+{
+	unsigned int page_free_target, page_free_count, page_free_wanted;
+
+	page_free_target = vm_page_free_target;
+	page_free_count = vm_page_free_count;
+	if (page_free_target > page_free_count) {
+		page_free_wanted = page_free_target - page_free_count;
+	} else {
+		page_free_wanted = 0;
+	}
+
+	return page_free_wanted;
+}
+
+
+/*
+ * IMPORTANT:
+ * mach_vm_pressure_monitor() is called when taking a stackshot, with 
+ * wait_for_pressure FALSE, so that code path must remain safe in the
+ * restricted stackshot context. No blocking or locks are allowable.
+ * on that code path.
+ */
+
+kern_return_t
+mach_vm_pressure_monitor(
+	boolean_t	wait_for_pressure,
+	unsigned int	nsecs_monitored,
+	unsigned int	*pages_reclaimed_p,
+	unsigned int	*pages_wanted_p)
+{
+	wait_result_t	wr;
+	unsigned int	vm_pageout_then, vm_pageout_now;
+	unsigned int	pages_reclaimed;
+
+	/*
+	 * We don't take the vm_page_queue_lock here because we don't want
+	 * vm_pressure_monitor() to get in the way of the vm_pageout_scan()
+	 * thread when it's trying to reclaim memory.  We don't need fully
+	 * accurate monitoring anyway...
+	 */
+
+	if (wait_for_pressure) {
+		/* wait until there's memory pressure */
+		while (vm_page_free_count >= vm_page_free_target) {
+			wr = assert_wait((event_t) &vm_page_free_wanted,
+					 THREAD_INTERRUPTIBLE);
+			if (wr == THREAD_WAITING) {
+				wr = thread_block(THREAD_CONTINUE_NULL);
+			}
+			if (wr == THREAD_INTERRUPTED) {
+				return KERN_ABORTED;
+			}
+			if (wr == THREAD_AWAKENED) {
+				/*
+				 * The memory pressure might have already
+				 * been relieved but let's not block again
+				 * and let's report that there was memory
+				 * pressure at some point.
+				 */
+				break;
+			}
+		}
+	}
+
+	/* provide the number of pages the system wants to reclaim */
+	if (pages_wanted_p != NULL) {
+		*pages_wanted_p = mach_vm_ctl_page_free_wanted();
+	}
+
+	if (pages_reclaimed_p == NULL) {
+		return KERN_SUCCESS;
+	}
+
+	/* provide number of pages reclaimed in the last "nsecs_monitored" */
+	do {
+		vm_pageout_now = vm_pageout_stat_now;
+		pages_reclaimed = 0;
+		for (vm_pageout_then =
+			     VM_PAGEOUT_STAT_BEFORE(vm_pageout_now);
+		     vm_pageout_then != vm_pageout_now &&
+			     nsecs_monitored-- != 0;
+		     vm_pageout_then =
+			     VM_PAGEOUT_STAT_BEFORE(vm_pageout_then)) {
+			pages_reclaimed += vm_pageout_stats[vm_pageout_then].reclaimed;
+		}
+	} while (vm_pageout_now != vm_pageout_stat_now);
+	*pages_reclaimed_p = pages_reclaimed;
+
+	return KERN_SUCCESS;
+}
+
+
+
+static void
+vm_pageout_page_queue(queue_head_t *, int);
+
+/*
+ * condition variable used to make sure there is
+ * only a single sweep going on at a time
+ */
+boolean_t	vm_pageout_anonymous_pages_active = FALSE;
+
+
+void
+vm_pageout_anonymous_pages()
+{
+	if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
+
+		vm_page_lock_queues();
+
+		if (vm_pageout_anonymous_pages_active == TRUE) {
+			vm_page_unlock_queues();
+			return;
+		}
+		vm_pageout_anonymous_pages_active = TRUE;
+		vm_page_unlock_queues();
+
+		vm_pageout_page_queue(&vm_page_queue_throttled, vm_page_throttled_count);
+		vm_pageout_page_queue(&vm_page_queue_anonymous, vm_page_anonymous_count);
+		vm_pageout_page_queue(&vm_page_queue_active, vm_page_active_count);
+
+		vm_consider_swapping();
+
+		vm_page_lock_queues();
+		vm_pageout_anonymous_pages_active = FALSE;
+		vm_page_unlock_queues();
+	}
+}
+
+
+void
+vm_pageout_page_queue(queue_head_t *q, int qcount)
+{
+	vm_page_t	m;
+	vm_object_t	t_object = NULL;
+	vm_object_t	l_object = NULL;
+	vm_object_t	m_object = NULL;
+	int		delayed_unlock = 0;
+	int		try_failed_count = 0;
+	int		refmod_state;
+	int		pmap_options;
+	struct		vm_pageout_queue *iq;
+
+
+	iq = &vm_pageout_queue_internal;
+	
+	vm_page_lock_queues();
+
+	while (qcount && !queue_empty(q)) {
+
+		lck_mtx_assert(&vm_page_queue_lock, LCK_MTX_ASSERT_OWNED);
+
+		if (VM_PAGE_Q_THROTTLED(iq)) {
+
+		        if (l_object != NULL) {
+			        vm_object_unlock(l_object);
+				l_object = NULL;
+			}
+			iq->pgo_draining = TRUE;
+					
+			assert_wait((event_t) (&iq->pgo_laundry + 1), THREAD_INTERRUPTIBLE);
+			vm_page_unlock_queues();
+					
+			thread_block(THREAD_CONTINUE_NULL);
+			
+			vm_page_lock_queues();
+			delayed_unlock = 0;
+			continue;
+		}
+		m = (vm_page_t) queue_first(q);
+		m_object = m->object;
+
+		/*
+		 * check to see if we currently are working
+		 * with the same object... if so, we've
+		 * already got the lock
+		 */
+		if (m_object != l_object) {
+			if ( !m_object->internal) 
+				goto reenter_pg_on_q;
+
+		        /*
+			 * the object associated with candidate page is 
+			 * different from the one we were just working
+			 * with... dump the lock if we still own it
+			 */
+		        if (l_object != NULL) {
+			        vm_object_unlock(l_object);
+				l_object = NULL;
+			}
+			if (m_object != t_object)
+				try_failed_count = 0;
+
+			/*
+			 * Try to lock object; since we've alread got the
+			 * page queues lock, we can only 'try' for this one.
+			 * if the 'try' fails, we need to do a mutex_pause
+			 * to allow the owner of the object lock a chance to
+			 * run... 
+			 */
+			if ( !vm_object_lock_try_scan(m_object)) {
+
+				if (try_failed_count > 20) {
+					goto reenter_pg_on_q;
+				}
+				vm_page_unlock_queues();
+				mutex_pause(try_failed_count++);
+				vm_page_lock_queues();
+				delayed_unlock = 0;
+
+				t_object = m_object;
+				continue;
+			}
+			l_object = m_object;
+		}
+		if ( !m_object->alive || m->encrypted_cleaning || m->cleaning || m->laundry || m->busy || m->absent || m->error || m->pageout) {
+			/*
+			 * page is not to be cleaned
+			 * put it back on the head of its queue
+			 */
+			goto reenter_pg_on_q;
+		}
+		if (m->reference == FALSE && m->pmapped == TRUE) {
+			refmod_state = pmap_get_refmod(m->phys_page);
+		  
+			if (refmod_state & VM_MEM_REFERENCED)
+			        m->reference = TRUE;
+			if (refmod_state & VM_MEM_MODIFIED) {
+			        SET_PAGE_DIRTY(m, FALSE);
+			}
+		}
+		if (m->reference == TRUE) {
+			m->reference = FALSE;
+			pmap_clear_refmod_options(m->phys_page, VM_MEM_REFERENCED, PMAP_OPTIONS_NOFLUSH, (void *)NULL);
+			goto reenter_pg_on_q;
+		}
+		if (m->pmapped == TRUE) {
+			if (m->dirty || m->precious) {
+				pmap_options = PMAP_OPTIONS_COMPRESSOR;
+			} else {
+				pmap_options = PMAP_OPTIONS_COMPRESSOR_IFF_MODIFIED;
+			}
+			refmod_state = pmap_disconnect_options(m->phys_page, pmap_options, NULL);
+			if (refmod_state & VM_MEM_MODIFIED) {
+				SET_PAGE_DIRTY(m, FALSE);
+			}
+		}
+		if ( !m->dirty && !m->precious) {
+			vm_page_unlock_queues();
+			VM_PAGE_FREE(m);
+			vm_page_lock_queues();
+			delayed_unlock = 0;
+
+			goto next_pg;
+		}
+		if (!m_object->pager_initialized || m_object->pager == MEMORY_OBJECT_NULL)  {
+			
+			if (!m_object->pager_initialized) {
+
+				vm_page_unlock_queues();
+
+				vm_object_collapse(m_object, (vm_object_offset_t) 0, TRUE);
+
+				if (!m_object->pager_initialized)
+					vm_object_compressor_pager_create(m_object);
+
+				vm_page_lock_queues();
+				delayed_unlock = 0;
+			}
+			if (!m_object->pager_initialized || m_object->pager == MEMORY_OBJECT_NULL)
+				goto reenter_pg_on_q;
+			/*
+			 * vm_object_compressor_pager_create will drop the object lock
+			 * which means 'm' may no longer be valid to use
+			 */
+			continue;
+		}
+		/*
+		 * we've already factored out pages in the laundry which
+		 * means this page can't be on the pageout queue so it's
+		 * safe to do the vm_page_queues_remove
+		 */
+                assert(!m->pageout_queue);
+
+		vm_page_queues_remove(m);
+
+		lck_mtx_assert(&vm_page_queue_lock, LCK_MTX_ASSERT_OWNED);
+
+		vm_pageout_cluster(m, TRUE, FALSE, FALSE);
+
+		goto next_pg;
+
+reenter_pg_on_q:
+		queue_remove(q, m, vm_page_t, pageq);
+		queue_enter(q, m, vm_page_t, pageq);
+next_pg:
+		qcount--;
+		try_failed_count = 0;
+
+		if (delayed_unlock++ > 128) {
+
+			if (l_object != NULL) {
+				vm_object_unlock(l_object);
+				l_object = NULL;
+			}
+			lck_mtx_yield(&vm_page_queue_lock);
+			delayed_unlock = 0;
+		}
+	}
+	if (l_object != NULL) {
+		vm_object_unlock(l_object);
+		l_object = NULL;
+	}
+	vm_page_unlock_queues();
+}
+
+
+
+/*
+ * function in BSD to apply I/O throttle to the pageout thread
+ */
+extern void vm_pageout_io_throttle(void);
+
+/*
+ * Page States: Used below to maintain the page state
+ * before it's removed from it's Q. This saved state
+ * helps us do the right accounting in certain cases
+ */
+#define PAGE_STATE_SPECULATIVE		1
+#define PAGE_STATE_ANONYMOUS		2
+#define PAGE_STATE_INACTIVE		3
+#define PAGE_STATE_INACTIVE_FIRST	4
+#define PAGE_STATE_CLEAN      5
+
+
+#define VM_PAGEOUT_SCAN_HANDLE_REUSABLE_PAGE(m)                         \
+        MACRO_BEGIN                                                     \
+        /*                                                              \
+         * If a "reusable" page somehow made it back into               \
+         * the active queue, it's been re-used and is not               \
+         * quite re-usable.                                             \
+         * If the VM object was "all_reusable", consider it             \
+         * as "all re-used" instead of converting it to                 \
+         * "partially re-used", which could be expensive.               \
+         */                                                             \
+        if ((m)->reusable ||                                            \
+            (m)->object->all_reusable) {                                \
+                vm_object_reuse_pages((m)->object,                      \
+                                      (m)->offset,                      \
+                                      (m)->offset + PAGE_SIZE_64,       \
+                                      FALSE);                           \
+        }                                                               \
+        MACRO_END
+
+
+#define VM_PAGEOUT_DELAYED_UNLOCK_LIMIT  	64
+#define VM_PAGEOUT_DELAYED_UNLOCK_LIMIT_MAX	1024
+
+#define	FCS_IDLE		0
+#define FCS_DELAYED		1
+#define FCS_DEADLOCK_DETECTED	2
+
+struct flow_control {
+        int		state;
+        mach_timespec_t	ts;
+};
+
+uint32_t vm_pageout_considered_page = 0;
+uint32_t vm_page_filecache_min = 0;
+
+#define ANONS_GRABBED_LIMIT	2
+
+/*
+ *	vm_pageout_scan does the dirty work for the pageout daemon.
+ *	It returns with both vm_page_queue_free_lock and vm_page_queue_lock
+ *	held and vm_page_free_wanted == 0.
+ */
+void
+vm_pageout_scan(void)
+{
+	unsigned int loop_count = 0;
+	unsigned int inactive_burst_count = 0;
+	unsigned int active_burst_count = 0;
+	unsigned int reactivated_this_call;
+	unsigned int reactivate_limit;
+	vm_page_t   local_freeq = NULL;
+	int         local_freed = 0;
+	int         delayed_unlock;
+	int	    delayed_unlock_limit = 0;
+	int	    refmod_state = 0;
+        int	vm_pageout_deadlock_target = 0;
+	struct	vm_pageout_queue *iq;
+	struct	vm_pageout_queue *eq;
+        struct	vm_speculative_age_q *sq;
+	struct  flow_control	flow_control = { 0, { 0, 0 } };
+        boolean_t inactive_throttled = FALSE;
+	boolean_t try_failed;
+	mach_timespec_t	ts;
+	unsigned	int msecs = 0;
+	vm_object_t	object;
+	vm_object_t	last_object_tried;
+	uint32_t	catch_up_count = 0;
+	uint32_t	inactive_reclaim_run;
+	boolean_t	forced_reclaim;
+	boolean_t	exceeded_burst_throttle;
+	boolean_t	grab_anonymous = FALSE;
+	boolean_t	force_anonymous = FALSE;
+	int		anons_grabbed = 0;
+	int		page_prev_state = 0;
+	int		cache_evict_throttle = 0;
+	uint32_t	vm_pageout_inactive_external_forced_reactivate_limit = 0;
+	int		force_purge = 0;
+#define	DELAY_SPECULATIVE_AGE	1000
+	int		delay_speculative_age = 0;
+
+#if VM_PRESSURE_EVENTS
+	vm_pressure_level_t pressure_level;
+#endif /* VM_PRESSURE_EVENTS */
+
+	VM_DEBUG_CONSTANT_EVENT(vm_pageout_scan, VM_PAGEOUT_SCAN, DBG_FUNC_START,
+		       vm_pageout_speculative_clean, vm_pageout_inactive_clean,
+		       vm_pageout_inactive_dirty_internal, vm_pageout_inactive_dirty_external);
+
+	flow_control.state = FCS_IDLE;
+	iq = &vm_pageout_queue_internal;
+	eq = &vm_pageout_queue_external;
+	sq = &vm_page_queue_speculative[VM_PAGE_SPECULATIVE_AGED_Q];
+
+
+        XPR(XPR_VM_PAGEOUT, "vm_pageout_scan\n", 0, 0, 0, 0, 0);
+
+        
+	vm_page_lock_queues();
+	delayed_unlock = 1;	/* must be nonzero if Qs are locked, 0 if unlocked */
+
+	/*
+	 *	Calculate the max number of referenced pages on the inactive
+	 *	queue that we will reactivate.
+	 */
+	reactivated_this_call = 0;
+	reactivate_limit = VM_PAGE_REACTIVATE_LIMIT(vm_page_active_count +
+						    vm_page_inactive_count);
+	inactive_reclaim_run = 0;
+
+	vm_pageout_inactive_external_forced_reactivate_limit = vm_page_active_count + vm_page_inactive_count;
+
+	/*
+	 *	We want to gradually dribble pages from the active queue
+	 *	to the inactive queue.  If we let the inactive queue get
+	 *	very small, and then suddenly dump many pages into it,
+	 *	those pages won't get a sufficient chance to be referenced
+	 *	before we start taking them from the inactive queue.
+	 *
+	 *	We must limit the rate at which we send pages to the pagers
+	 *	so that we don't tie up too many pages in the I/O queues.
+	 *	We implement a throttling mechanism using the laundry count
+	 * 	to limit the number of pages outstanding to the default
+	 *	and external pagers.  We can bypass the throttles and look
+	 *	for clean pages if the pageout queues don't drain in a timely
+	 *	fashion since this may indicate that the pageout paths are
+	 *	stalled waiting for memory, which only we can provide.
+	 */
+
+
+Restart:
+	assert(delayed_unlock!=0);
+	
+	/*
+	 *	Recalculate vm_page_inactivate_target.
+	 */
+	vm_page_inactive_target = VM_PAGE_INACTIVE_TARGET(vm_page_active_count +
+							  vm_page_inactive_count +
+							  vm_page_speculative_count);
+
+	vm_page_anonymous_min = vm_page_inactive_target / 20;
+
+
+	/*
+	 * don't want to wake the pageout_scan thread up everytime we fall below
+	 * the targets... set a low water mark at 0.25% below the target
+	 */
+	vm_page_inactive_min = vm_page_inactive_target - (vm_page_inactive_target / 400);
+
+	if (vm_page_speculative_percentage > 50)
+		vm_page_speculative_percentage = 50;
+	else if (vm_page_speculative_percentage <= 0)
+		vm_page_speculative_percentage = 1;
+
+	vm_page_speculative_target = VM_PAGE_SPECULATIVE_TARGET(vm_page_active_count +
+								vm_page_inactive_count);
+
+	object = NULL;
+	last_object_tried = NULL;
+	try_failed = FALSE;
+	
+	if ((vm_page_inactive_count + vm_page_speculative_count) < VM_PAGE_INACTIVE_HEALTHY_LIMIT(vm_page_active_count))
+	        catch_up_count = vm_page_inactive_count + vm_page_speculative_count;
+	else
+	        catch_up_count = 0;
+		    
+	for (;;) {
+		vm_page_t m;
+
+		DTRACE_VM2(rev, int, 1, (uint64_t *), NULL);
+
+		assert(delayed_unlock);
+
+		if (vm_upl_wait_for_pages < 0)
+			vm_upl_wait_for_pages = 0;
+
+		delayed_unlock_limit = VM_PAGEOUT_DELAYED_UNLOCK_LIMIT + vm_upl_wait_for_pages;
+
+		if (delayed_unlock_limit > VM_PAGEOUT_DELAYED_UNLOCK_LIMIT_MAX)
+			delayed_unlock_limit = VM_PAGEOUT_DELAYED_UNLOCK_LIMIT_MAX;
+
+		/*
+		 * Move pages from active to inactive if we're below the target
+		 */
+		/* if we are trying to make clean, we need to make sure we actually have inactive - mj */
+		if ((vm_page_inactive_count + vm_page_speculative_count) >= vm_page_inactive_target)
+			goto done_moving_active_pages;
+
+		if (object != NULL) {
+			vm_object_unlock(object);
+			object = NULL;
+			vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+		}
+		/*
+		 * Don't sweep through active queue more than the throttle
+		 * which should be kept relatively low
+		 */
+		active_burst_count = MIN(vm_pageout_burst_active_throttle, vm_page_active_count);
+
+		VM_DEBUG_EVENT(vm_pageout_balance, VM_PAGEOUT_BALANCE, DBG_FUNC_START,
+			       vm_pageout_inactive, vm_pageout_inactive_used, vm_page_free_count, local_freed);
+
+		VM_DEBUG_EVENT(vm_pageout_balance, VM_PAGEOUT_BALANCE, DBG_FUNC_NONE,
+			       vm_pageout_speculative_clean, vm_pageout_inactive_clean,
+			       vm_pageout_inactive_dirty_internal, vm_pageout_inactive_dirty_external);
+		memoryshot(VM_PAGEOUT_BALANCE, DBG_FUNC_START);
+
+
+		while (!queue_empty(&vm_page_queue_active) && active_burst_count--) {
+
+			vm_pageout_active++;
+
+			m = (vm_page_t) queue_first(&vm_page_queue_active);
+
+			assert(m->active && !m->inactive);
+			assert(!m->laundry);
+			assert(m->object != kernel_object);
+			assert(m->phys_page != vm_page_guard_addr);
+
+			DTRACE_VM2(scan, int, 1, (uint64_t *), NULL);
+
+			/*
+			 * by not passing in a pmap_flush_context we will forgo any TLB flushing, local or otherwise...
+			 *
+			 * a TLB flush isn't really needed here since at worst we'll miss the reference bit being
+			 * updated in the PTE if a remote processor still has this mapping cached in its TLB when the
+			 * new reference happens. If no futher references happen on the page after that remote TLB flushes
+			 * we'll see a clean, non-referenced page when it eventually gets pulled out of the inactive queue
+			 * by pageout_scan, which is just fine since the last reference would have happened quite far
+			 * in the past (TLB caches don't hang around for very long), and of course could just as easily
+			 * have happened before we moved the page
+			 */
+			pmap_clear_refmod_options(m->phys_page, VM_MEM_REFERENCED, PMAP_OPTIONS_NOFLUSH, (void *)NULL);
+
+			/*
+			 * The page might be absent or busy,
+			 * but vm_page_deactivate can handle that.
+			 * FALSE indicates that we don't want a H/W clear reference
+			 */
+			vm_page_deactivate_internal(m, FALSE);
+
+			if (delayed_unlock++ > delayed_unlock_limit) {
+
+				if (local_freeq) {
+					vm_page_unlock_queues();
+					
+					VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_START,
+						       vm_page_free_count, local_freed, delayed_unlock_limit, 1);
+
+					vm_page_free_list(local_freeq, TRUE);
+						
+					VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_END,
+						       vm_page_free_count, 0, 0, 1);
+
+					local_freeq = NULL;
+					local_freed = 0;
+					vm_page_lock_queues();
+				} else {
+					lck_mtx_yield(&vm_page_queue_lock);
+				}
+				
+				delayed_unlock = 1;
+
+				/*
+				 * continue the while loop processing
+				 * the active queue... need to hold
+				 * the page queues lock
+				 */
+			}
+		}
+
+		VM_DEBUG_EVENT(vm_pageout_balance, VM_PAGEOUT_BALANCE, DBG_FUNC_END,
+			       vm_page_active_count, vm_page_inactive_count, vm_page_speculative_count, vm_page_inactive_target);
+		memoryshot(VM_PAGEOUT_BALANCE, DBG_FUNC_END);
+
+		/**********************************************************************
+		 * above this point we're playing with the active queue
+		 * below this point we're playing with the throttling mechanisms
+		 * and the inactive queue
+		 **********************************************************************/
+
+done_moving_active_pages:
+
+		if (vm_page_free_count + local_freed >= vm_page_free_target) {
+			if (object != NULL) {
+			        vm_object_unlock(object);
+				object = NULL;
+			}
+			vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+
+			vm_page_unlock_queues();
+
+			if (local_freeq) {
+					
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_START,
+					       vm_page_free_count, local_freed, delayed_unlock_limit, 2);
+
+				vm_page_free_list(local_freeq, TRUE);
+					
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_END,
+					       vm_page_free_count, local_freed, 0, 2);
+
+				local_freeq = NULL;
+				local_freed = 0;
+			}
+			vm_consider_waking_compactor_swapper();
+
+			vm_page_lock_queues();
+
+			/*
+			 * make sure the pageout I/O threads are running
+			 * throttled in case there are still requests 
+			 * in the laundry... since we have met our targets
+			 * we don't need the laundry to be cleaned in a timely
+			 * fashion... so let's avoid interfering with foreground
+			 * activity
+			 */
+			vm_pageout_adjust_io_throttles(iq, eq, TRUE);
+
+			/*
+			 * recalculate vm_page_inactivate_target
+			 */
+			vm_page_inactive_target = VM_PAGE_INACTIVE_TARGET(vm_page_active_count +
+									  vm_page_inactive_count +
+									  vm_page_speculative_count);
+			if (((vm_page_inactive_count + vm_page_speculative_count) < vm_page_inactive_target) &&
+			    !queue_empty(&vm_page_queue_active)) {
+				/*
+				 * inactive target still not met... keep going
+				 * until we get the queues balanced...
+				 */
+			        continue;
+			}
+		        lck_mtx_lock(&vm_page_queue_free_lock);
+
+			if ((vm_page_free_count >= vm_page_free_target) &&
+			    (vm_page_free_wanted == 0) && (vm_page_free_wanted_privileged == 0)) {
+				/*
+				 * done - we have met our target *and*
+				 * there is no one waiting for a page.
+				 */
+return_from_scan:
+				assert(vm_pageout_scan_wants_object == VM_OBJECT_NULL);
+
+				VM_DEBUG_CONSTANT_EVENT(vm_pageout_scan, VM_PAGEOUT_SCAN, DBG_FUNC_NONE,
+					       vm_pageout_inactive, vm_pageout_inactive_used, 0, 0);
+				VM_DEBUG_CONSTANT_EVENT(vm_pageout_scan, VM_PAGEOUT_SCAN, DBG_FUNC_END,
+					       vm_pageout_speculative_clean, vm_pageout_inactive_clean,
+					       vm_pageout_inactive_dirty_internal, vm_pageout_inactive_dirty_external);
+
+				return;
+			}
+			lck_mtx_unlock(&vm_page_queue_free_lock);
+		}
+		
+		/*
+		 * Before anything, we check if we have any ripe volatile 
+		 * objects around. If so, try to purge the first object.
+		 * If the purge fails, fall through to reclaim a page instead.
+		 * If the purge succeeds, go back to the top and reevalute
+		 * the new memory situation.
+		 */
+		
+		assert (available_for_purge>=0);
+		force_purge = 0; /* no force-purging */
+
+#if VM_PRESSURE_EVENTS
+		pressure_level = memorystatus_vm_pressure_level;
+
+		if (pressure_level > kVMPressureNormal) {
+
+			if (pressure_level >= kVMPressureCritical) {
+				force_purge = memorystatus_purge_on_critical;
+			} else if (pressure_level >= kVMPressureUrgent) {
+				force_purge = memorystatus_purge_on_urgent;
+			} else if (pressure_level >= kVMPressureWarning) {
+				force_purge = memorystatus_purge_on_warning;
+			}
+		}
+#endif /* VM_PRESSURE_EVENTS */
+
+		if (available_for_purge || force_purge) {
+
+		        if (object != NULL) {
+			        vm_object_unlock(object);
+				object = NULL;
+			}
+
+			memoryshot(VM_PAGEOUT_PURGEONE, DBG_FUNC_START);
+
+			VM_DEBUG_EVENT(vm_pageout_purgeone, VM_PAGEOUT_PURGEONE, DBG_FUNC_START, vm_page_free_count, 0, 0, 0);
+			if (vm_purgeable_object_purge_one(force_purge, C_DONT_BLOCK)) {
+				vm_pageout_purged_objects++;
+				VM_DEBUG_EVENT(vm_pageout_purgeone, VM_PAGEOUT_PURGEONE, DBG_FUNC_END, vm_page_free_count, 0, 0, 0);
+				memoryshot(VM_PAGEOUT_PURGEONE, DBG_FUNC_END);
+				continue;
+			}
+			VM_DEBUG_EVENT(vm_pageout_purgeone, VM_PAGEOUT_PURGEONE, DBG_FUNC_END, 0, 0, 0, -1);
+			memoryshot(VM_PAGEOUT_PURGEONE, DBG_FUNC_END);
+		}
+
+		if (queue_empty(&sq->age_q) && vm_page_speculative_count) {
+		        /*
+			 * try to pull pages from the aging bins...
+			 * see vm_page.h for an explanation of how
+			 * this mechanism works
+			 */
+		        struct vm_speculative_age_q	*aq;
+			boolean_t	can_steal = FALSE;
+			int num_scanned_queues;
+		       
+			aq = &vm_page_queue_speculative[speculative_steal_index];
+
+			num_scanned_queues = 0;
+			while (queue_empty(&aq->age_q) &&
+			       num_scanned_queues++ != VM_PAGE_MAX_SPECULATIVE_AGE_Q) {
+
+			        speculative_steal_index++;
+
+				if (speculative_steal_index > VM_PAGE_MAX_SPECULATIVE_AGE_Q)
+				        speculative_steal_index = VM_PAGE_MIN_SPECULATIVE_AGE_Q;
+				
+				aq = &vm_page_queue_speculative[speculative_steal_index];
+			}
+
+			if (num_scanned_queues == VM_PAGE_MAX_SPECULATIVE_AGE_Q + 1) {
+				/*
+				 * XXX We've scanned all the speculative
+				 * queues but still haven't found one
+				 * that is not empty, even though
+				 * vm_page_speculative_count is not 0.
+				 *
+				 * report the anomaly...
+				 */
+				printf("vm_pageout_scan: "
+				       "all speculative queues empty "
+				       "but count=%d.  Re-adjusting.\n",
+				       vm_page_speculative_count);
+				if (vm_page_speculative_count > vm_page_speculative_count_drift_max)
+					vm_page_speculative_count_drift_max = vm_page_speculative_count;
+				vm_page_speculative_count_drifts++;
+#if 6553678
+				Debugger("vm_pageout_scan: no speculative pages");
+#endif
+				/* readjust... */
+				vm_page_speculative_count = 0;
+				/* ... and continue */
+				continue;
+			}
+
+			if (vm_page_speculative_count > vm_page_speculative_target)
+			        can_steal = TRUE;
+			else {
+				if (!delay_speculative_age) {
+					mach_timespec_t	ts_fully_aged;
+
+					ts_fully_aged.tv_sec = (VM_PAGE_MAX_SPECULATIVE_AGE_Q * vm_page_speculative_q_age_ms) / 1000;
+					ts_fully_aged.tv_nsec = ((VM_PAGE_MAX_SPECULATIVE_AGE_Q * vm_page_speculative_q_age_ms) % 1000)
+						* 1000 * NSEC_PER_USEC;
+
+					ADD_MACH_TIMESPEC(&ts_fully_aged, &aq->age_ts);
+
+					clock_sec_t sec;
+					clock_nsec_t nsec;
+					clock_get_system_nanotime(&sec, &nsec);
+					ts.tv_sec = (unsigned int) sec;
+					ts.tv_nsec = nsec;
+
+					if (CMP_MACH_TIMESPEC(&ts, &ts_fully_aged) >= 0)
+						can_steal = TRUE;
+					else
+						delay_speculative_age++;
+				} else {
+					delay_speculative_age++;
+					if (delay_speculative_age == DELAY_SPECULATIVE_AGE)
+						delay_speculative_age = 0;
+				}
+			}
+			if (can_steal == TRUE)
+				vm_page_speculate_ageit(aq);
+		}
+		if (queue_empty(&sq->age_q) && cache_evict_throttle == 0) {
+			int 	pages_evicted;
+
+		        if (object != NULL) {
+			        vm_object_unlock(object);
+				object = NULL;
+			}
+			pages_evicted = vm_object_cache_evict(100, 10);
+
+			if (pages_evicted) {
+
+				vm_pageout_cache_evicted += pages_evicted;
+
+				VM_DEBUG_EVENT(vm_pageout_cache_evict, VM_PAGEOUT_CACHE_EVICT, DBG_FUNC_NONE,
+					       vm_page_free_count, pages_evicted, vm_pageout_cache_evicted, 0);
+				memoryshot(VM_PAGEOUT_CACHE_EVICT, DBG_FUNC_NONE);
+
+				/*
+				 * we just freed up to 100 pages,
+				 * so go back to the top of the main loop
+				 * and re-evaulate the memory situation
+				 */
+				continue;
+			} else
+				cache_evict_throttle = 100;
+		}
+		if  (cache_evict_throttle)
+			cache_evict_throttle--;
+
+#if CONFIG_JETSAM
+		/*
+		 * don't let the filecache_min fall below 15% of available memory
+		 * on systems with an active compressor that isn't nearing its
+		 * limits w/r to accepting new data
+		 *
+		 * on systems w/o the compressor/swapper, the filecache is always
+		 * a very large percentage of the AVAILABLE_NON_COMPRESSED_MEMORY
+		 * since most (if not all) of the anonymous pages are in the
+		 * throttled queue (which isn't counted as available) which
+		 * effectively disables this filter
+		 */
+		if (vm_compressor_low_on_space())
+			vm_page_filecache_min = 0;
+		else
+			vm_page_filecache_min = (AVAILABLE_NON_COMPRESSED_MEMORY / 7);
+#else
+                /*
+		 * don't let the filecache_min fall below 33% of available memory...
+		 */
+		vm_page_filecache_min = (AVAILABLE_NON_COMPRESSED_MEMORY / 3);
+#endif
+
+		exceeded_burst_throttle = FALSE;
+		/*
+		 * Sometimes we have to pause:
+		 *	1) No inactive pages - nothing to do.
+		 *	2) Loop control - no acceptable pages found on the inactive queue
+		 *         within the last vm_pageout_burst_inactive_throttle iterations
+		 *	3) Flow control - default pageout queue is full
+		 */
+		if (queue_empty(&vm_page_queue_inactive) && queue_empty(&vm_page_queue_anonymous) && queue_empty(&sq->age_q)) {
+		        vm_pageout_scan_empty_throttle++;
+			msecs = vm_pageout_empty_wait;
+			goto vm_pageout_scan_delay;
+
+		} else if (inactive_burst_count >= 
+			   MIN(vm_pageout_burst_inactive_throttle,
+			       (vm_page_inactive_count +
+				vm_page_speculative_count))) {
+		        vm_pageout_scan_burst_throttle++;
+			msecs = vm_pageout_burst_wait;
+
+			exceeded_burst_throttle = TRUE;
+			goto vm_pageout_scan_delay;
+
+		} else if (vm_page_free_count > (vm_page_free_reserved / 4) &&
+			   VM_PAGEOUT_SCAN_NEEDS_TO_THROTTLE()) {
+		        vm_pageout_scan_swap_throttle++;
+			msecs = vm_pageout_swap_wait;
+			goto vm_pageout_scan_delay;
+
+		} else if (VM_PAGE_Q_THROTTLED(iq) && 
+				  VM_DYNAMIC_PAGING_ENABLED(memory_manager_default)) {
+			clock_sec_t sec;
+			clock_nsec_t nsec;
+
+		        switch (flow_control.state) {
+
+			case FCS_IDLE:
+				if ((vm_page_free_count + local_freed) < vm_page_free_target) {
+
+					if (object != NULL) {
+						vm_object_unlock(object);
+						object = NULL;
+					}
+					vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+
+					vm_page_unlock_queues();
+
+					if (local_freeq) {
+
+						VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_START,
+							       vm_page_free_count, local_freed, delayed_unlock_limit, 3);
+
+						vm_page_free_list(local_freeq, TRUE);
+							
+						VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_END,
+							       vm_page_free_count, local_freed, 0, 3);
+
+						local_freeq = NULL;
+						local_freed = 0;
+					}
+					thread_yield_internal(1);
+
+					vm_page_lock_queues();
+
+					if (!VM_PAGE_Q_THROTTLED(iq)) {
+						vm_pageout_scan_yield_unthrottled++;
+						continue;
+					}
+					if (vm_page_pageable_external_count > vm_page_filecache_min && !queue_empty(&vm_page_queue_inactive)) {
+						anons_grabbed = ANONS_GRABBED_LIMIT;
+						vm_pageout_scan_throttle_deferred++;
+						goto consider_inactive;
+					}
+					if (((vm_page_inactive_count + vm_page_speculative_count) < vm_page_inactive_target) && vm_page_active_count)
+						continue;
+				}
+reset_deadlock_timer:
+			        ts.tv_sec = vm_pageout_deadlock_wait / 1000;
+				ts.tv_nsec = (vm_pageout_deadlock_wait % 1000) * 1000 * NSEC_PER_USEC;
+			        clock_get_system_nanotime(&sec, &nsec);
+				flow_control.ts.tv_sec = (unsigned int) sec;
+				flow_control.ts.tv_nsec = nsec;
+				ADD_MACH_TIMESPEC(&flow_control.ts, &ts);
+				
+				flow_control.state = FCS_DELAYED;
+				msecs = vm_pageout_deadlock_wait;
+
+				break;
+					
+			case FCS_DELAYED:
+			        clock_get_system_nanotime(&sec, &nsec);
+				ts.tv_sec = (unsigned int) sec;
+				ts.tv_nsec = nsec;
+
+				if (CMP_MACH_TIMESPEC(&ts, &flow_control.ts) >= 0) {
+				        /*
+					 * the pageout thread for the default pager is potentially
+					 * deadlocked since the 
+					 * default pager queue has been throttled for more than the
+					 * allowable time... we need to move some clean pages or dirty
+					 * pages belonging to the external pagers if they aren't throttled
+					 * vm_page_free_wanted represents the number of threads currently
+					 * blocked waiting for pages... we'll move one page for each of
+					 * these plus a fixed amount to break the logjam... once we're done
+					 * moving this number of pages, we'll re-enter the FSC_DELAYED state
+					 * with a new timeout target since we have no way of knowing 
+					 * whether we've broken the deadlock except through observation
+					 * of the queue associated with the default pager... we need to
+					 * stop moving pages and allow the system to run to see what
+					 * state it settles into.
+					 */
+				        vm_pageout_deadlock_target = vm_pageout_deadlock_relief + vm_page_free_wanted + vm_page_free_wanted_privileged;
+					vm_pageout_scan_deadlock_detected++;
+					flow_control.state = FCS_DEADLOCK_DETECTED;
+					thread_wakeup((event_t) &vm_pageout_garbage_collect);
+					goto consider_inactive;
+				}
+				/*
+				 * just resniff instead of trying
+				 * to compute a new delay time... we're going to be
+				 * awakened immediately upon a laundry completion,
+				 * so we won't wait any longer than necessary
+				 */
+				msecs = vm_pageout_idle_wait;
+				break;
+
+			case FCS_DEADLOCK_DETECTED:
+			        if (vm_pageout_deadlock_target)
+				        goto consider_inactive;
+				goto reset_deadlock_timer;
+
+			}
+vm_pageout_scan_delay:
+			if (object != NULL) {
+			        vm_object_unlock(object);
+				object = NULL;
+			}
+			vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+
+			vm_page_unlock_queues();
+
+			if (local_freeq) {
+
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_START,
+					       vm_page_free_count, local_freed, delayed_unlock_limit, 3);
+
+				vm_page_free_list(local_freeq, TRUE);
+					
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_END,
+					       vm_page_free_count, local_freed, 0, 3);
+
+				local_freeq = NULL;
+				local_freed = 0;
+			}
+			vm_consider_waking_compactor_swapper();
+
+			vm_page_lock_queues();
+
+			if (flow_control.state == FCS_DELAYED &&
+			    !VM_PAGE_Q_THROTTLED(iq)) {
+				flow_control.state = FCS_IDLE;
+				goto consider_inactive;
+			}
+			
+			if (vm_page_free_count >= vm_page_free_target) {
+				/*
+				 * we're here because
+				 *  1) someone else freed up some pages while we had
+				 *     the queues unlocked above
+				 * and we've hit one of the 3 conditions that 
+				 * cause us to pause the pageout scan thread
+				 *
+				 * since we already have enough free pages,
+				 * let's avoid stalling and return normally
+				 *
+				 * before we return, make sure the pageout I/O threads
+				 * are running throttled in case there are still requests 
+				 * in the laundry... since we have enough free pages
+				 * we don't need the laundry to be cleaned in a timely
+				 * fashion... so let's avoid interfering with foreground
+				 * activity
+				 *
+				 * we don't want to hold vm_page_queue_free_lock when
+				 * calling vm_pageout_adjust_io_throttles (since it
+				 * may cause other locks to be taken), we do the intitial
+				 * check outside of the lock.  Once we take the lock,
+				 * we recheck the condition since it may have changed.
+				 * if it has, no problem, we will make the threads
+				 * non-throttled before actually blocking
+				 */
+				vm_pageout_adjust_io_throttles(iq, eq, TRUE);
+			}
+			lck_mtx_lock(&vm_page_queue_free_lock);
+
+			if (vm_page_free_count >= vm_page_free_target &&
+			    (vm_page_free_wanted == 0) && (vm_page_free_wanted_privileged == 0)) {
+				goto return_from_scan;
+			}
+			lck_mtx_unlock(&vm_page_queue_free_lock);
+			
+			if ((vm_page_free_count + vm_page_cleaned_count) < vm_page_free_target) {
+				/*
+				 * we're most likely about to block due to one of
+				 * the 3 conditions that cause vm_pageout_scan to
+				 * not be able to make forward progress w/r
+				 * to providing new pages to the free queue,
+				 * so unthrottle the I/O threads in case we
+				 * have laundry to be cleaned... it needs
+				 * to be completed ASAP.
+				 *
+				 * even if we don't block, we want the io threads
+				 * running unthrottled since the sum of free +
+				 * clean pages is still under our free target
+				 */
+				vm_pageout_adjust_io_throttles(iq, eq, FALSE);
+			}
+			if (vm_page_cleaned_count > 0 && exceeded_burst_throttle == FALSE) {
+				/*
+				 * if we get here we're below our free target and
+				 * we're stalling due to a full laundry queue or
+				 * we don't have any inactive pages other then
+				 * those in the clean queue...
+				 * however, we have pages on the clean queue that
+				 * can be moved to the free queue, so let's not
+				 * stall the pageout scan
+				 */
+				flow_control.state = FCS_IDLE;
+				goto consider_inactive;
+			}
+			VM_CHECK_MEMORYSTATUS;
+
+			if (flow_control.state != FCS_IDLE)
+				vm_pageout_scan_throttle++;
+			iq->pgo_throttled = TRUE;
+
+			assert_wait_timeout((event_t) &iq->pgo_laundry, THREAD_INTERRUPTIBLE, msecs, 1000*NSEC_PER_USEC);
+			counter(c_vm_pageout_scan_block++);
+
+			vm_page_unlock_queues();
+
+			assert(vm_pageout_scan_wants_object == VM_OBJECT_NULL);
+
+			VM_DEBUG_EVENT(vm_pageout_thread_block, VM_PAGEOUT_THREAD_BLOCK, DBG_FUNC_START, 
+				       iq->pgo_laundry, iq->pgo_maxlaundry, msecs, 0);
+			memoryshot(VM_PAGEOUT_THREAD_BLOCK, DBG_FUNC_START);
+
+			thread_block(THREAD_CONTINUE_NULL);
+
+			VM_DEBUG_EVENT(vm_pageout_thread_block, VM_PAGEOUT_THREAD_BLOCK, DBG_FUNC_END,
+				       iq->pgo_laundry, iq->pgo_maxlaundry, msecs, 0);
+			memoryshot(VM_PAGEOUT_THREAD_BLOCK, DBG_FUNC_END);
+
+			vm_page_lock_queues();
+			delayed_unlock = 1;
+
+			iq->pgo_throttled = FALSE;
+
+			if (loop_count >= vm_page_inactive_count)
+				loop_count = 0;
+			inactive_burst_count = 0;
+
+			goto Restart;
+			/*NOTREACHED*/
+		}
+
+
+		flow_control.state = FCS_IDLE;
+consider_inactive:
+		vm_pageout_inactive_external_forced_reactivate_limit = MIN((vm_page_active_count + vm_page_inactive_count), 
+									    vm_pageout_inactive_external_forced_reactivate_limit);
+		loop_count++;
+		inactive_burst_count++;
+		vm_pageout_inactive++;
+
+
+		/*
+		 * Choose a victim.
+		 */
+		while (1) {
+			uint32_t	inactive_external_count;
+
+			m = NULL;
+			
+			if (VM_DYNAMIC_PAGING_ENABLED(memory_manager_default)) {
+				assert(vm_page_throttled_count == 0);
+				assert(queue_empty(&vm_page_queue_throttled));
+			}
+			/*
+			 * The most eligible pages are ones we paged in speculatively,
+			 * but which have not yet been touched.
+			 */
+			if (!queue_empty(&sq->age_q) && force_anonymous == FALSE) {
+				m = (vm_page_t) queue_first(&sq->age_q);
+
+				page_prev_state = PAGE_STATE_SPECULATIVE;
+
+				break;
+			}
+			/*
+			 * Try a clean-queue inactive page.
+			 */
+			if (!queue_empty(&vm_page_queue_cleaned)) {
+				m = (vm_page_t) queue_first(&vm_page_queue_cleaned);
+                    
+				page_prev_state = PAGE_STATE_CLEAN;
+                    
+				break;
+			}
+
+			grab_anonymous = (vm_page_anonymous_count > vm_page_anonymous_min);
+			inactive_external_count = vm_page_inactive_count - vm_page_anonymous_count;
+
+			if ((vm_page_pageable_external_count < vm_page_filecache_min || force_anonymous == TRUE) ||
+			    ((inactive_external_count < vm_page_anonymous_count) && (inactive_external_count < (vm_page_pageable_external_count / 3)))) {
+				grab_anonymous = TRUE;
+				anons_grabbed = 0;
+			}
+
+			if (grab_anonymous == FALSE || anons_grabbed >= ANONS_GRABBED_LIMIT || queue_empty(&vm_page_queue_anonymous)) {
+
+				if ( !queue_empty(&vm_page_queue_inactive) ) {
+					m = (vm_page_t) queue_first(&vm_page_queue_inactive);
+				
+					page_prev_state = PAGE_STATE_INACTIVE;
+					anons_grabbed = 0;
+
+					if (vm_page_pageable_external_count < vm_page_filecache_min) {
+						if ((++reactivated_this_call % 100))
+							goto must_activate_page;
+						/*
+						 * steal 1% of the file backed pages even if
+						 * we are under the limit that has been set
+						 * for a healthy filecache
+						 */
+					}
+					break;
+				}
+			}
+			if ( !queue_empty(&vm_page_queue_anonymous) ) {
+				m = (vm_page_t) queue_first(&vm_page_queue_anonymous);
+
+				page_prev_state = PAGE_STATE_ANONYMOUS;
+				anons_grabbed++;
+
+				break;
+			}
+
+			/*
+			 * if we've gotten here, we have no victim page.
+			 * if making clean, free the local freed list and return.
+			 * if making free, check to see if we've finished balancing the queues
+			 * yet, if we haven't just continue, else panic
+			 */
+			vm_page_unlock_queues();
+				
+			if (object != NULL) {
+				vm_object_unlock(object);
+				object = NULL;
+			}
+			vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+				
+			if (local_freeq) {
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_START,
+					       vm_page_free_count, local_freed, delayed_unlock_limit, 5);
+					
+				vm_page_free_list(local_freeq, TRUE);
+					
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_END,
+					       vm_page_free_count, local_freed, 0, 5);
+					
+				local_freeq = NULL;
+				local_freed = 0;
+			}
+			vm_page_lock_queues();
+			delayed_unlock = 1;
+
+			force_anonymous = FALSE;
+
+			if ((vm_page_inactive_count + vm_page_speculative_count) < vm_page_inactive_target)
+				goto Restart;
+
+			if (!queue_empty(&sq->age_q))
+				goto Restart;
+
+			panic("vm_pageout: no victim");
+			
+			/* NOTREACHED */
+		}
+		force_anonymous = FALSE;
+		
+		/*
+		 * we just found this page on one of our queues...
+		 * it can't also be on the pageout queue, so safe
+		 * to call vm_page_queues_remove
+		 */
+		assert(!m->pageout_queue);
+
+		vm_page_queues_remove(m);
+
+		assert(!m->laundry);
+		assert(!m->private);
+		assert(!m->fictitious);
+		assert(m->object != kernel_object);
+		assert(m->phys_page != vm_page_guard_addr);
+
+
+		if (page_prev_state != PAGE_STATE_SPECULATIVE)
+			vm_pageout_stats[vm_pageout_stat_now].considered++;
+
+		DTRACE_VM2(scan, int, 1, (uint64_t *), NULL);
+
+		/*
+		 * check to see if we currently are working
+		 * with the same object... if so, we've
+		 * already got the lock
+		 */
+		if (m->object != object) {
+		        /*
+			 * the object associated with candidate page is 
+			 * different from the one we were just working
+			 * with... dump the lock if we still own it
+			 */
+		        if (object != NULL) {
+			        vm_object_unlock(object);
+				object = NULL;
+				vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+			}
+			/*
+			 * Try to lock object; since we've alread got the
+			 * page queues lock, we can only 'try' for this one.
+			 * if the 'try' fails, we need to do a mutex_pause
+			 * to allow the owner of the object lock a chance to
+			 * run... otherwise, we're likely to trip over this
+			 * object in the same state as we work our way through
+			 * the queue... clumps of pages associated with the same
+			 * object are fairly typical on the inactive and active queues
+			 */
+			if (!vm_object_lock_try_scan(m->object)) {
+				vm_page_t m_want = NULL;
+
+				vm_pageout_inactive_nolock++;
+
+				if (page_prev_state == PAGE_STATE_CLEAN)
+					vm_pageout_cleaned_nolock++;
+
+				if (page_prev_state == PAGE_STATE_SPECULATIVE)
+					page_prev_state = PAGE_STATE_INACTIVE_FIRST;
+
+				pmap_clear_reference(m->phys_page);
+				m->reference = FALSE;
+
+				/*
+				 * m->object must be stable since we hold the page queues lock...
+				 * we can update the scan_collisions field sans the object lock
+				 * since it is a separate field and this is the only spot that does
+				 * a read-modify-write operation and it is never executed concurrently...
+				 * we can asynchronously set this field to 0 when creating a UPL, so it
+				 * is possible for the value to be a bit non-determistic, but that's ok
+				 * since it's only used as a hint
+				 */
+				m->object->scan_collisions = 1;
+
+				if ( !queue_empty(&sq->age_q) )
+					m_want = (vm_page_t) queue_first(&sq->age_q);
+				else if ( !queue_empty(&vm_page_queue_cleaned))
+					m_want = (vm_page_t) queue_first(&vm_page_queue_cleaned);
+				else if (anons_grabbed >= ANONS_GRABBED_LIMIT || queue_empty(&vm_page_queue_anonymous))
+					m_want = (vm_page_t) queue_first(&vm_page_queue_inactive);
+				else if ( !queue_empty(&vm_page_queue_anonymous))
+					m_want = (vm_page_t) queue_first(&vm_page_queue_anonymous);
+
+				/*
+				 * this is the next object we're going to be interested in
+				 * try to make sure its available after the mutex_yield
+				 * returns control
+				 */
+				if (m_want)
+					vm_pageout_scan_wants_object = m_want->object;
+
+				/*
+				 * force us to dump any collected free pages
+				 * and to pause before moving on
+				 */
+				try_failed = TRUE;
+
+				goto requeue_page;
+			}
+			object = m->object;
+			vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+
+			try_failed = FALSE;
+		}
+		if (catch_up_count)
+		        catch_up_count--;
+
+		if (m->busy) {
+			if (m->encrypted_cleaning) {
+				/*
+				 * ENCRYPTED SWAP:
+				 * if this page has already been picked up as
+				 * part of a page-out cluster, it will be busy 
+				 * because it is being encrypted (see
+				 * vm_object_upl_request()).  But we still
+				 * want to demote it from "clean-in-place"
+				 * (aka "adjacent") to "clean-and-free" (aka
+				 * "target"), so let's ignore its "busy" bit
+				 * here and proceed to check for "cleaning" a
+				 * little bit below...
+				 *
+				 * CAUTION CAUTION:
+				 * A "busy" page should still be left alone for
+				 * most purposes, so we have to be very careful
+				 * not to process that page too much.
+				 */
+				assert(m->cleaning);
+				goto consider_inactive_page;
+			}
+
+			/*
+			 *	Somebody is already playing with this page.
+			 *	Put it back on the appropriate queue
+			 *
+			 */
+			vm_pageout_inactive_busy++;
+
+			if (page_prev_state == PAGE_STATE_CLEAN)
+				vm_pageout_cleaned_busy++;
+			
+requeue_page:
+			switch (page_prev_state) {
+
+			case PAGE_STATE_SPECULATIVE:
+			case PAGE_STATE_ANONYMOUS:
+			case PAGE_STATE_CLEAN:
+			case PAGE_STATE_INACTIVE:
+				vm_page_enqueue_inactive(m, FALSE);
+				break;
+
+			case PAGE_STATE_INACTIVE_FIRST:
+				vm_page_enqueue_inactive(m, TRUE);
+				break;
+			}
+			goto done_with_inactivepage;
+		}
+
+
+		/*
+		 *	If it's absent, in error or the object is no longer alive,
+		 *	we can reclaim the page... in the no longer alive case,
+		 *	there are 2 states the page can be in that preclude us
+		 *	from reclaiming it - busy or cleaning - that we've already
+		 *	dealt with
+		 */
+		if (m->absent || m->error || !object->alive) {
+
+			if (m->absent)
+				vm_pageout_inactive_absent++;
+			else if (!object->alive)
+				vm_pageout_inactive_notalive++;
+			else
+				vm_pageout_inactive_error++;
+reclaim_page:			
+			if (vm_pageout_deadlock_target) {
+				vm_pageout_scan_inactive_throttle_success++;
+			        vm_pageout_deadlock_target--;
+			}
+
+			DTRACE_VM2(dfree, int, 1, (uint64_t *), NULL);
+
+			if (object->internal) {
+				DTRACE_VM2(anonfree, int, 1, (uint64_t *), NULL);
+			} else {
+				DTRACE_VM2(fsfree, int, 1, (uint64_t *), NULL);
+			}
+			assert(!m->cleaning);
+			assert(!m->laundry);
+
+			m->busy = TRUE;
+
+			/*
+			 * remove page from object here since we're already
+			 * behind the object lock... defer the rest of the work
+			 * we'd normally do in vm_page_free_prepare_object
+			 * until 'vm_page_free_list' is called
+			 */
+			if (m->tabled)
+				vm_page_remove(m, TRUE);
+
+			assert(m->pageq.next == NULL &&
+			       m->pageq.prev == NULL);
+			m->pageq.next = (queue_entry_t)local_freeq;
+			local_freeq = m;
+			local_freed++;
+			
+			if (page_prev_state == PAGE_STATE_SPECULATIVE)
+				vm_pageout_freed_from_speculative++;
+			else if (page_prev_state == PAGE_STATE_CLEAN)
+				vm_pageout_freed_from_cleaned++;
+			else
+				vm_pageout_freed_from_inactive_clean++;
+
+			if (page_prev_state != PAGE_STATE_SPECULATIVE)
+				vm_pageout_stats[vm_pageout_stat_now].reclaimed++;
+
+			inactive_burst_count = 0;
+			goto done_with_inactivepage;
+		}
+		/*
+		 * If the object is empty, the page must be reclaimed even
+		 * if dirty or used.
+		 * If the page belongs to a volatile object, we stick it back
+		 * on.
+		 */
+		if (object->copy == VM_OBJECT_NULL) {
+			if (object->purgable == VM_PURGABLE_EMPTY) {
+				if (m->pmapped == TRUE) {
+					/* unmap the page */
+					refmod_state = pmap_disconnect(m->phys_page);
+					if (refmod_state & VM_MEM_MODIFIED) {
+						SET_PAGE_DIRTY(m, FALSE);
+					}
+				}
+				if (m->dirty || m->precious) {
+					/* we saved the cost of cleaning this page ! */
+					vm_page_purged_count++;
+				}
+				goto reclaim_page;
+			}
+
+			if (COMPRESSED_PAGER_IS_ACTIVE) {
+				/*
+				 * With the VM compressor, the cost of
+				 * reclaiming a page is much lower (no I/O),
+				 * so if we find a "volatile" page, it's better
+				 * to let it get compressed rather than letting
+				 * it occupy a full page until it gets purged.
+				 * So no need to check for "volatile" here.
+				 */
+			} else if (object->purgable == VM_PURGABLE_VOLATILE) {
+				/*
+				 * Avoid cleaning a "volatile" page which might
+				 * be purged soon.
+				 */
+
+				/* if it's wired, we can't put it on our queue */
+				assert(!VM_PAGE_WIRED(m));
+
+				/* just stick it back on! */
+				reactivated_this_call++;
+
+				if (page_prev_state == PAGE_STATE_CLEAN)
+					vm_pageout_cleaned_volatile_reactivated++;
+
+				goto reactivate_page;
+			}
+		}
+
+consider_inactive_page:
+		if (m->busy) {
+			/*
+			 * CAUTION CAUTION:
+			 * A "busy" page should always be left alone, except...
+			 */
+			if (m->cleaning && m->encrypted_cleaning) {
+				/*
+				 * ENCRYPTED_SWAP:
+				 * We could get here with a "busy" page 
+				 * if it's being encrypted during a
+				 * "clean-in-place" operation.  We'll deal
+				 * with it right away by testing if it has been
+				 * referenced and either reactivating it or
+				 * promoting it from "clean-in-place" to
+				 * "clean-and-free".
+				 */
+			} else {
+				panic("\"busy\" page considered for pageout\n");
+			}
+		}
+
+		/*
+		 *	If it's being used, reactivate.
+		 *	(Fictitious pages are either busy or absent.)
+		 *	First, update the reference and dirty bits
+		 *	to make sure the page is unreferenced.
+		 */
+		refmod_state = -1;
+
+		if (m->reference == FALSE && m->pmapped == TRUE) {
+		        refmod_state = pmap_get_refmod(m->phys_page);
+		  
+		        if (refmod_state & VM_MEM_REFERENCED)
+			        m->reference = TRUE;
+		        if (refmod_state & VM_MEM_MODIFIED) {
+				SET_PAGE_DIRTY(m, FALSE);
+			}
+		}
+		
+		/*
+		 *   if (m->cleaning && !m->pageout)
+		 *	If already cleaning this page in place and it hasn't
+		 *	been recently referenced, just pull off the queue.
+		 *	We can leave the page mapped, and upl_commit_range
+		 *	will put it on the clean queue.
+		 *
+		 *	note: if m->encrypted_cleaning == TRUE, then
+		 *		m->cleaning == TRUE
+		 *	and we'll handle it here
+		 *
+		 *   if (m->pageout && !m->cleaning)
+		 *	an msync INVALIDATE is in progress...
+		 *	this page has been marked for destruction
+		 * 	after it has been cleaned,
+		 * 	but not yet gathered into a UPL
+		 *	where 'cleaning' will be set...
+		 *	just leave it off the paging queues
+		 *
+		 *   if (m->pageout && m->clenaing)
+		 *	an msync INVALIDATE is in progress
+		 *	and the UPL has already gathered this page...
+		 *	just leave it off the paging queues
+		 */
+		
+		/*
+		 * page with m->pageout and still on the queues means that an
+		 * MS_INVALIDATE is in progress on this page... leave it alone
+		 */
+		if (m->pageout) {
+			goto done_with_inactivepage;
+		}
+		
+		/* if cleaning, reactivate if referenced.  otherwise, just pull off queue */
+		if (m->cleaning) {
+			if (m->reference == TRUE) {
+				reactivated_this_call++;
+				goto reactivate_page;
+			} else {
+				goto done_with_inactivepage;
+			}
+		}
+
+                if (m->reference || m->dirty) {
+                        /* deal with a rogue "reusable" page */
+                        VM_PAGEOUT_SCAN_HANDLE_REUSABLE_PAGE(m);
+                }
+
+		if (!m->no_cache &&
+		    (m->reference ||
+		     (m->xpmapped && !object->internal && (vm_page_xpmapped_external_count < (vm_page_external_count / 4))))) {
+			/*
+			 * The page we pulled off the inactive list has
+			 * been referenced.  It is possible for other
+			 * processors to be touching pages faster than we
+			 * can clear the referenced bit and traverse the
+			 * inactive queue, so we limit the number of
+			 * reactivations.
+			 */
+			if (++reactivated_this_call >= reactivate_limit) {
+				vm_pageout_reactivation_limit_exceeded++;
+			} else if (catch_up_count) {
+				vm_pageout_catch_ups++;
+			} else if (++inactive_reclaim_run >= VM_PAGEOUT_INACTIVE_FORCE_RECLAIM) {
+				vm_pageout_inactive_force_reclaim++;
+			} else {
+				uint32_t isinuse;
+
+				if (page_prev_state == PAGE_STATE_CLEAN)
+					vm_pageout_cleaned_reference_reactivated++;
+				
+reactivate_page:
+				if ( !object->internal && object->pager != MEMORY_OBJECT_NULL &&
+				     vnode_pager_get_isinuse(object->pager, &isinuse) == KERN_SUCCESS && !isinuse) {
+					/*
+					 * no explict mappings of this object exist
+					 * and it's not open via the filesystem
+					 */
+					vm_page_deactivate(m);
+					vm_pageout_inactive_deactivated++;
+				} else {
+must_activate_page:
+					/*
+					 * The page was/is being used, so put back on active list.
+					 */
+					vm_page_activate(m);
+					VM_STAT_INCR(reactivations);
+					inactive_burst_count = 0;
+				}
+				
+				if (page_prev_state == PAGE_STATE_CLEAN)
+					vm_pageout_cleaned_reactivated++;
+
+				vm_pageout_inactive_used++;
+
+                                goto done_with_inactivepage;
+			}
+			/* 
+			 * Make sure we call pmap_get_refmod() if it
+			 * wasn't already called just above, to update
+			 * the dirty bit.
+			 */
+			if ((refmod_state == -1) && !m->dirty && m->pmapped) {
+				refmod_state = pmap_get_refmod(m->phys_page);
+				if (refmod_state & VM_MEM_MODIFIED) {
+					SET_PAGE_DIRTY(m, FALSE);
+				}
+			}
+			forced_reclaim = TRUE;
+		} else {
+			forced_reclaim = FALSE;
+		}
+
+                XPR(XPR_VM_PAGEOUT,
+                "vm_pageout_scan, replace object 0x%X offset 0x%X page 0x%X\n",
+                object, m->offset, m, 0,0);
+
+		/*
+		 * we've got a candidate page to steal...
+		 *
+		 * m->dirty is up to date courtesy of the
+		 * preceding check for m->reference... if 
+		 * we get here, then m->reference had to be
+		 * FALSE (or possibly "reactivate_limit" was
+                 * exceeded), but in either case we called
+                 * pmap_get_refmod() and updated both
+                 * m->reference and m->dirty
+		 *
+		 * if it's dirty or precious we need to
+		 * see if the target queue is throtttled
+		 * it if is, we need to skip over it by moving it back
+		 * to the end of the inactive queue
+		 */
+
+		inactive_throttled = FALSE;
+
+		if (m->dirty || m->precious) {
+		        if (object->internal) {
+				if (VM_PAGE_Q_THROTTLED(iq))
+				        inactive_throttled = TRUE;
+			} else if (VM_PAGE_Q_THROTTLED(eq)) {
+				inactive_throttled = TRUE;
+			}
+		}
+throttle_inactive:
+		if (!VM_DYNAMIC_PAGING_ENABLED(memory_manager_default) &&
+		    object->internal && m->dirty &&
+		    (object->purgable == VM_PURGABLE_DENY ||
+		     object->purgable == VM_PURGABLE_NONVOLATILE ||
+		     object->purgable == VM_PURGABLE_VOLATILE)) {
+			vm_page_check_pageable_safe(m);
+			queue_enter(&vm_page_queue_throttled, m,
+				    vm_page_t, pageq);
+			m->throttled = TRUE;
+			vm_page_throttled_count++;
+
+			vm_pageout_scan_reclaimed_throttled++;
+
+			inactive_burst_count = 0;
+			goto done_with_inactivepage;
+		}
+		if (inactive_throttled == TRUE) {
+
+			if (object->internal == FALSE) {
+                                /*
+				 * we need to break up the following potential deadlock case...
+				 *  a) The external pageout thread is stuck on the truncate lock for a file that is being extended i.e. written.
+				 *  b) The thread doing the writing is waiting for pages while holding the truncate lock
+				 *  c) Most of the pages in the inactive queue belong to this file.
+				 *
+				 * we are potentially in this deadlock because...
+				 *  a) the external pageout queue is throttled
+				 *  b) we're done with the active queue and moved on to the inactive queue
+				 *  c) we've got a dirty external page
+				 *
+				 * since we don't know the reason for the external pageout queue being throttled we
+				 * must suspect that we are deadlocked, so move the current page onto the active queue
+				 * in an effort to cause a page from the active queue to 'age' to the inactive queue
+				 *
+				 * if we don't have jetsam configured (i.e. we have a dynamic pager), set
+				 * 'force_anonymous' to TRUE to cause us to grab a page from the cleaned/anonymous
+				 * pool the next time we select a victim page... if we can make enough new free pages,
+				 * the deadlock will break, the external pageout queue will empty and it will no longer
+				 * be throttled
+				 *
+				 * if we have jestam configured, keep a count of the pages reactivated this way so
+				 * that we can try to find clean pages in the active/inactive queues before
+				 * deciding to jetsam a process
+				 */
+				vm_pageout_scan_inactive_throttled_external++;
+
+				vm_page_check_pageable_safe(m);
+				queue_enter(&vm_page_queue_active, m, vm_page_t, pageq);
+				m->active = TRUE;
+				vm_page_active_count++;
+				vm_page_pageable_external_count++;
+
+				vm_pageout_adjust_io_throttles(iq, eq, FALSE);
+
+#if CONFIG_MEMORYSTATUS && CONFIG_JETSAM
+				vm_pageout_inactive_external_forced_reactivate_limit--;
+
+				if (vm_pageout_inactive_external_forced_reactivate_limit <= 0) {
+					vm_pageout_inactive_external_forced_reactivate_limit = vm_page_active_count + vm_page_inactive_count;
+					/*
+					 * Possible deadlock scenario so request jetsam action
+					 */
+					assert(object);
+					vm_object_unlock(object);
+					object = VM_OBJECT_NULL;
+					vm_page_unlock_queues();
+					
+					VM_DEBUG_CONSTANT_EVENT(vm_pageout_jetsam, VM_PAGEOUT_JETSAM, DBG_FUNC_START,
+    					       vm_page_active_count, vm_page_inactive_count, vm_page_free_count, vm_page_free_count);
+
+                                        /* Kill first suitable process */
+					if (memorystatus_kill_on_VM_page_shortage(FALSE) == FALSE) {
+						panic("vm_pageout_scan: Jetsam request failed\n");	
+					}
+					
+					VM_DEBUG_CONSTANT_EVENT(vm_pageout_jetsam, VM_PAGEOUT_JETSAM, DBG_FUNC_END, 0, 0, 0, 0);
+
+					vm_pageout_inactive_external_forced_jetsam_count++;
+					vm_page_lock_queues();	
+					delayed_unlock = 1;
+				}
+#else /* CONFIG_MEMORYSTATUS && CONFIG_JETSAM */
+				force_anonymous = TRUE;
+#endif
+				inactive_burst_count = 0;
+				goto done_with_inactivepage;
+			} else {
+				if (page_prev_state == PAGE_STATE_SPECULATIVE)
+					page_prev_state = PAGE_STATE_INACTIVE;
+
+				vm_pageout_scan_inactive_throttled_internal++;
+
+				goto must_activate_page;
+			}
+		}
+
+		/*
+		 * we've got a page that we can steal...
+		 * eliminate all mappings and make sure
+		 * we have the up-to-date modified state
+		 *
+		 * if we need to do a pmap_disconnect then we
+		 * need to re-evaluate m->dirty since the pmap_disconnect
+		 * provides the true state atomically... the 
+		 * page was still mapped up to the pmap_disconnect
+		 * and may have been dirtied at the last microsecond
+		 *
+		 * Note that if 'pmapped' is FALSE then the page is not
+		 * and has not been in any map, so there is no point calling
+		 * pmap_disconnect().  m->dirty could have been set in anticipation
+		 * of likely usage of the page.
+		 */
+		if (m->pmapped == TRUE) {
+			int pmap_options;
+
+			/*
+			 * Don't count this page as going into the compressor
+			 * if any of these are true:
+			 * 1) We have the dynamic pager i.e. no compressed pager
+			 * 2) Freezer enabled device with a freezer file to
+			 *    hold the app data i.e. no compressed pager
+			 * 3) Freezer enabled device with compressed pager
+			 *    backend (exclusive use) i.e. most of the VM system
+			 *    (including vm_pageout_scan) has no knowledge of
+			 *    the compressor
+			 * 4) This page belongs to a file and hence will not be
+			 *    sent into the compressor
+			 */
+			if (DEFAULT_PAGER_IS_ACTIVE ||
+			    DEFAULT_FREEZER_IS_ACTIVE ||
+			    DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPLESS ||
+			    object->internal == FALSE) {
+				pmap_options = 0;
+			} else if (m->dirty || m->precious) {
+				/*
+				 * VM knows that this page is dirty (or
+				 * precious) and needs to be compressed
+				 * rather than freed.
+				 * Tell the pmap layer to count this page
+				 * as "compressed".
+				 */
+				pmap_options = PMAP_OPTIONS_COMPRESSOR;
+			} else {
+				/*
+				 * VM does not know if the page needs to
+				 * be preserved but the pmap layer might tell
+				 * us if any mapping has "modified" it.
+				 * Let's the pmap layer to count this page
+				 * as compressed if and only if it has been
+				 * modified.
+				 */
+				pmap_options =
+					PMAP_OPTIONS_COMPRESSOR_IFF_MODIFIED;
+			}
+			refmod_state = pmap_disconnect_options(m->phys_page,
+							       pmap_options,
+							       NULL);
+			if (refmod_state & VM_MEM_MODIFIED) {
+				SET_PAGE_DIRTY(m, FALSE);
+			}
+		}
+		/*
+		 * reset our count of pages that have been reclaimed 
+		 * since the last page was 'stolen'
+		 */
+		inactive_reclaim_run = 0;
+
+		/*
+		 *	If it's clean and not precious, we can free the page.
+		 */
+		if (!m->dirty && !m->precious) {
+
+			if (page_prev_state == PAGE_STATE_SPECULATIVE)
+				vm_pageout_speculative_clean++;
+			else {
+				if (page_prev_state == PAGE_STATE_ANONYMOUS)
+					vm_pageout_inactive_anonymous++;
+				else if (page_prev_state == PAGE_STATE_CLEAN)
+					vm_pageout_cleaned_reclaimed++;
+
+				vm_pageout_inactive_clean++;
+			}
+
+			/*
+			 * OK, at this point we have found a page we are going to free.
+			 */
+#if CONFIG_PHANTOM_CACHE
+			if (!object->internal)
+				vm_phantom_cache_add_ghost(m);
+#endif
+			goto reclaim_page;
+		}
+
+		/*
+		 * The page may have been dirtied since the last check
+		 * for a throttled target queue (which may have been skipped
+		 * if the page was clean then).  With the dirty page
+		 * disconnected here, we can make one final check.
+		 */
+		if (object->internal) {
+			if (VM_PAGE_Q_THROTTLED(iq))
+				inactive_throttled = TRUE;
+		} else if (VM_PAGE_Q_THROTTLED(eq)) {
+			inactive_throttled = TRUE;
+		}
+
+		if (inactive_throttled == TRUE)
+			goto throttle_inactive;
+	
+#if VM_PRESSURE_EVENTS
+#if CONFIG_JETSAM
+
+		/*
+		 * If Jetsam is enabled, then the sending
+		 * of memory pressure notifications is handled
+		 * from the same thread that takes care of high-water
+		 * and other jetsams i.e. the memorystatus_thread.
+		 */
+
+#else /* CONFIG_JETSAM */
+		
+		vm_pressure_response();
+
+#endif /* CONFIG_JETSAM */
+#endif /* VM_PRESSURE_EVENTS */
+		
+		if (page_prev_state == PAGE_STATE_ANONYMOUS)
+			vm_pageout_inactive_anonymous++;
+		if (object->internal)
+			vm_pageout_inactive_dirty_internal++;
+		else
+			vm_pageout_inactive_dirty_external++;
+
+		/*
+		 * do NOT set the pageout bit!
+		 * sure, we might need free pages, but this page is going to take time to become free 
+		 * anyway, so we may as well put it on the clean queue first and take it from there later
+		 * if necessary.  that way, we'll ensure we don't free up too much. -mj
+		 */
+		vm_pageout_cluster(m, FALSE, FALSE, FALSE);
+
+done_with_inactivepage:
+
+		if (delayed_unlock++ > delayed_unlock_limit || try_failed == TRUE) {
+			boolean_t	need_delay = TRUE;
+
+		        if (object != NULL) {
+				vm_pageout_scan_wants_object = VM_OBJECT_NULL;
+			        vm_object_unlock(object);
+				object = NULL;
+			}
+			vm_page_unlock_queues();
+
+		        if (local_freeq) {
+
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_START,
+					       vm_page_free_count, local_freed, delayed_unlock_limit, 4);
+					
+				vm_page_free_list(local_freeq, TRUE);
+				
+				VM_DEBUG_EVENT(vm_pageout_freelist, VM_PAGEOUT_FREELIST, DBG_FUNC_END,
+					       vm_page_free_count, local_freed, 0, 4);
+
+				local_freeq = NULL;
+				local_freed = 0;
+				need_delay = FALSE;
+			}
+			vm_consider_waking_compactor_swapper();
+
+			vm_page_lock_queues();
+
+			if (need_delay == TRUE)
+				lck_mtx_yield(&vm_page_queue_lock);
+
+			delayed_unlock = 1;
+		}
+		vm_pageout_considered_page++;
+
+		/*
+		 * back to top of pageout scan loop
+		 */
+	}
+}
+
+
+int vm_page_free_count_init;
+
+void
+vm_page_free_reserve(
+	int pages)
+{
+	int		free_after_reserve;
+
+	if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
+
+		if ((vm_page_free_reserved + pages + COMPRESSOR_FREE_RESERVED_LIMIT) >= (VM_PAGE_FREE_RESERVED_LIMIT + COMPRESSOR_FREE_RESERVED_LIMIT))
+			vm_page_free_reserved = VM_PAGE_FREE_RESERVED_LIMIT + COMPRESSOR_FREE_RESERVED_LIMIT;
+		else
+			vm_page_free_reserved += (pages + COMPRESSOR_FREE_RESERVED_LIMIT);
+
+	} else {
+		if ((vm_page_free_reserved + pages) >= VM_PAGE_FREE_RESERVED_LIMIT)
+			vm_page_free_reserved = VM_PAGE_FREE_RESERVED_LIMIT;
+		else
+			vm_page_free_reserved += pages;
+	}
+	free_after_reserve = vm_page_free_count_init - vm_page_free_reserved;
+
+	vm_page_free_min = vm_page_free_reserved +
+		VM_PAGE_FREE_MIN(free_after_reserve);
+
+	if (vm_page_free_min > VM_PAGE_FREE_MIN_LIMIT)
+	        vm_page_free_min = VM_PAGE_FREE_MIN_LIMIT;
+
+	vm_page_free_target = vm_page_free_reserved +
+		VM_PAGE_FREE_TARGET(free_after_reserve);
+
+	if (vm_page_free_target > VM_PAGE_FREE_TARGET_LIMIT)
+	        vm_page_free_target = VM_PAGE_FREE_TARGET_LIMIT;
+
+	if (vm_page_free_target < vm_page_free_min + 5)
+		vm_page_free_target = vm_page_free_min + 5;
+
+	vm_page_throttle_limit = vm_page_free_target - (vm_page_free_target / 2);
+}
+
+/*
+ *	vm_pageout is the high level pageout daemon.
+ */
+
+void
+vm_pageout_continue(void)
+{
+	DTRACE_VM2(pgrrun, int, 1, (uint64_t *), NULL);
+	vm_pageout_scan_event_counter++;
+
+	vm_pageout_scan();
+	/*
+	 * we hold both the vm_page_queue_free_lock
+	 * and the vm_page_queues_lock at this point
+	 */
+	assert(vm_page_free_wanted == 0);
+	assert(vm_page_free_wanted_privileged == 0);
+	assert_wait((event_t) &vm_page_free_wanted, THREAD_UNINT);
+
+	lck_mtx_unlock(&vm_page_queue_free_lock);
+	vm_page_unlock_queues();
+
+	counter(c_vm_pageout_block++);
+	thread_block((thread_continue_t)vm_pageout_continue);
+	/*NOTREACHED*/
+}
+
+
+#ifdef FAKE_DEADLOCK
+
+#define FAKE_COUNT	5000
+
+int internal_count = 0;
+int fake_deadlock = 0;
+
+#endif
+
+static void
+vm_pageout_iothread_continue(struct vm_pageout_queue *q)
+{
+	vm_page_t	m = NULL;
+	vm_object_t	object;
+	vm_object_offset_t offset;
+	memory_object_t	pager;
+	thread_t	self = current_thread();
+
+	if ((vm_pageout_internal_iothread != THREAD_NULL)
+	    && (self == vm_pageout_external_iothread )
+	    && (self->options & TH_OPT_VMPRIV))
+		self->options &= ~TH_OPT_VMPRIV;
+
+	vm_page_lockspin_queues();
+
+        while ( !queue_empty(&q->pgo_pending) ) {
+
+		   q->pgo_busy = TRUE;
+		   queue_remove_first(&q->pgo_pending, m, vm_page_t, pageq);
+		   if (m->object->object_slid) {
+			   panic("slid page %p not allowed on this path\n", m);
+		   }
+		   VM_PAGE_CHECK(m);
+		   m->pageout_queue = FALSE;
+		   m->pageq.next = NULL;
+		   m->pageq.prev = NULL;
+
+		   /*
+		    * grab a snapshot of the object and offset this
+		    * page is tabled in so that we can relookup this
+		    * page after we've taken the object lock - these
+		    * fields are stable while we hold the page queues lock
+		    * but as soon as we drop it, there is nothing to keep
+		    * this page in this object... we hold an activity_in_progress
+		    * on this object which will keep it from terminating
+		    */
+		   object = m->object;
+		   offset = m->offset;
+
+		   vm_page_unlock_queues();
+
+#ifdef FAKE_DEADLOCK
+		   if (q == &vm_pageout_queue_internal) {
+		           vm_offset_t addr;
+			   int	pg_count;
+
+			   internal_count++;
+
+			   if ((internal_count == FAKE_COUNT)) {
+
+				   pg_count = vm_page_free_count + vm_page_free_reserved;
+
+			           if (kmem_alloc(kernel_map, &addr, PAGE_SIZE * pg_count) == KERN_SUCCESS) {
+				           kmem_free(kernel_map, addr, PAGE_SIZE * pg_count);
+				   }
+				   internal_count = 0;
+				   fake_deadlock++;
+			   }
+		   }
+#endif
+		   vm_object_lock(object);
+
+		   m = vm_page_lookup(object, offset);
+
+		   if (m == NULL ||
+		       m->busy || m->cleaning || m->pageout_queue || !m->laundry) {
+			   /*
+			    * it's either the same page that someone else has
+			    * started cleaning (or it's finished cleaning or
+			    * been put back on the pageout queue), or
+			    * the page has been freed or we have found a
+			    * new page at this offset... in all of these cases
+			    * we merely need to release the activity_in_progress
+			    * we took when we put the page on the pageout queue
+			    */
+			   vm_object_activity_end(object);
+			   vm_object_unlock(object);
+
+			   vm_page_lockspin_queues();
+			   continue;
+		   }
+		   if (!object->pager_initialized) {
+
+			   /*
+			    *	If there is no memory object for the page, create
+			    *	one and hand it to the default pager.
+			    */
+
+			   if (!object->pager_initialized)
+			           vm_object_collapse(object,
+						      (vm_object_offset_t) 0,
+						      TRUE);
+			   if (!object->pager_initialized)
+			           vm_object_pager_create(object);
+			   if (!object->pager_initialized) {
+			           /*
+				    *	Still no pager for the object.
+				    *	Reactivate the page.
+				    *
+				    *	Should only happen if there is no
+				    *	default pager.
+				    */
+				   m->pageout = FALSE;
+
+			           vm_page_lockspin_queues();
+
+				   vm_pageout_throttle_up(m);
+				   vm_page_activate(m);
+				   vm_pageout_dirty_no_pager++;
+
+				   vm_page_unlock_queues();
+
+				   /*
+				    *	And we are done with it.
+				    */
+			           vm_object_activity_end(object);
+				   vm_object_unlock(object);
+
+				   vm_page_lockspin_queues();
+				   continue;
+			   }
+		   }
+		   pager = object->pager;
+
+	           if (pager == MEMORY_OBJECT_NULL) {
+		           /*
+			    * This pager has been destroyed by either
+			    * memory_object_destroy or vm_object_destroy, and
+			    * so there is nowhere for the page to go.
+			    */
+			   if (m->pageout) {
+				   /*
+				    * Just free the page... VM_PAGE_FREE takes
+				    * care of cleaning up all the state...
+				    * including doing the vm_pageout_throttle_up
+				    */
+				   VM_PAGE_FREE(m);
+			   } else {
+			           vm_page_lockspin_queues();
+
+				   vm_pageout_throttle_up(m);
+				   vm_page_activate(m);
+				   
+				   vm_page_unlock_queues();
+
+				   /*
+				    *	And we are done with it.
+				    */
+			   }
+			   vm_object_activity_end(object);
+			   vm_object_unlock(object);
+
+			   vm_page_lockspin_queues();
+			   continue;
+		   }
+#if 0
+		   /*
+		    * we don't hold the page queue lock
+		    * so this check isn't safe to make
+		    */
+		   VM_PAGE_CHECK(m);
+#endif
+		   /*
+		    * give back the activity_in_progress reference we
+		    * took when we queued up this page and replace it
+		    * it with a paging_in_progress reference that will
+                    * also hold the paging offset from changing and
+                    * prevent the object from terminating
+		    */
+		   vm_object_activity_end(object);
+		   vm_object_paging_begin(object);
+		   vm_object_unlock(object);
+
+                   /*
+		    * Send the data to the pager.
+		    * any pageout clustering happens there
+		    */
+		   memory_object_data_return(pager,
+					     m->offset + object->paging_offset,
+					     PAGE_SIZE,
+					     NULL,
+					     NULL,
+					     FALSE,
+					     FALSE,
+					     0);
+
+		   vm_object_lock(object);
+		   vm_object_paging_end(object);
+		   vm_object_unlock(object);
+
+		   vm_pageout_io_throttle();
+
+		   vm_page_lockspin_queues();
+	}
+	q->pgo_busy = FALSE;
+	q->pgo_idle = TRUE;
+
+	assert_wait((event_t) &q->pgo_pending, THREAD_UNINT);
+	vm_page_unlock_queues();
+
+	thread_block_parameter((thread_continue_t)vm_pageout_iothread_continue, (void *) q);
+	/*NOTREACHED*/
+}
+
+
+static void
+vm_pageout_iothread_external_continue(struct vm_pageout_queue *q)
+{
+	vm_page_t	m = NULL;
+	vm_object_t	object;
+	vm_object_offset_t offset;
+	memory_object_t	pager;
+
+
+	if (vm_pageout_internal_iothread != THREAD_NULL)
+		current_thread()->options &= ~TH_OPT_VMPRIV;
+
+	vm_page_lockspin_queues();
+
+        while ( !queue_empty(&q->pgo_pending) ) {
+
+		   q->pgo_busy = TRUE;
+		   queue_remove_first(&q->pgo_pending, m, vm_page_t, pageq);
+		   if (m->object->object_slid) {
+			   panic("slid page %p not allowed on this path\n", m);
+		   }
+		   VM_PAGE_CHECK(m);
+		   m->pageout_queue = FALSE;
+		   m->pageq.next = NULL;
+		   m->pageq.prev = NULL;
+
+		   /*
+		    * grab a snapshot of the object and offset this
+		    * page is tabled in so that we can relookup this
+		    * page after we've taken the object lock - these
+		    * fields are stable while we hold the page queues lock
+		    * but as soon as we drop it, there is nothing to keep
+		    * this page in this object... we hold an activity_in_progress
+		    * on this object which will keep it from terminating
+		    */
+		   object = m->object;
+		   offset = m->offset;
+
+		   vm_page_unlock_queues();
+
+		   vm_object_lock(object);
+
+		   m = vm_page_lookup(object, offset);
+
+		   if (m == NULL ||
+		       m->busy || m->cleaning || m->pageout_queue || !m->laundry) {
+			   /*
+			    * it's either the same page that someone else has
+			    * started cleaning (or it's finished cleaning or
+			    * been put back on the pageout queue), or
+			    * the page has been freed or we have found a
+			    * new page at this offset... in all of these cases
+			    * we merely need to release the activity_in_progress
+			    * we took when we put the page on the pageout queue
+			    */
+			   vm_object_activity_end(object);
+			   vm_object_unlock(object);
+
+			   vm_page_lockspin_queues();
+			   continue;
+		   }
+		   pager = object->pager;
+
+	           if (pager == MEMORY_OBJECT_NULL) {
+		           /*
+			    * This pager has been destroyed by either
+			    * memory_object_destroy or vm_object_destroy, and
+			    * so there is nowhere for the page to go.
+			    */
+			   if (m->pageout) {
+				   /*
+				    * Just free the page... VM_PAGE_FREE takes
+				    * care of cleaning up all the state...
+				    * including doing the vm_pageout_throttle_up
+				    */
+				   VM_PAGE_FREE(m);
+			   } else {
+			           vm_page_lockspin_queues();
+
+				   vm_pageout_throttle_up(m);
+				   vm_page_activate(m);
+				   
+				   vm_page_unlock_queues();
+
+				   /*
+				    *	And we are done with it.
+				    */
+			   }
+			   vm_object_activity_end(object);
+			   vm_object_unlock(object);
+
+			   vm_page_lockspin_queues();
+			   continue;
+		   }
+#if 0
+		   /*
+		    * we don't hold the page queue lock
+		    * so this check isn't safe to make
+		    */
+		   VM_PAGE_CHECK(m);
+#endif
+		   /*
+		    * give back the activity_in_progress reference we
+		    * took when we queued up this page and replace it
+		    * it with a paging_in_progress reference that will
+                    * also hold the paging offset from changing and
+                    * prevent the object from terminating
+		    */
+		   vm_object_activity_end(object);
+		   vm_object_paging_begin(object);
+		   vm_object_unlock(object);
+
+                   /*
+		    * Send the data to the pager.
+		    * any pageout clustering happens there
+		    */
+		   memory_object_data_return(pager,
+					     m->offset + object->paging_offset,
+					     PAGE_SIZE,
+					     NULL,
+					     NULL,
+					     FALSE,
+					     FALSE,
+					     0);
+
+		   vm_object_lock(object);
+		   vm_object_paging_end(object);
+		   vm_object_unlock(object);
+
+		   vm_pageout_io_throttle();
+
+		   vm_page_lockspin_queues();
+	}
+	q->pgo_busy = FALSE;
+	q->pgo_idle = TRUE;
+
+	assert_wait((event_t) &q->pgo_pending, THREAD_UNINT);
+	vm_page_unlock_queues();
+
+	thread_block_parameter((thread_continue_t)vm_pageout_iothread_external_continue, (void *) q);
+	/*NOTREACHED*/
+}
+
+
+uint32_t	vm_compressor_failed;
+
+#define		MAX_FREE_BATCH		32
+
+static void
+vm_pageout_iothread_internal_continue(struct cq *cq)
+{
+	struct vm_pageout_queue *q;
+	vm_page_t	m = NULL;
+	boolean_t	pgo_draining;
+	vm_page_t   local_q;
+	int	    local_cnt;
+	vm_page_t   local_freeq = NULL;
+	int         local_freed = 0;
+	int	    local_batch_size;
+
+
+	KERNEL_DEBUG(0xe040000c | DBG_FUNC_END, 0, 0, 0, 0, 0);
+
+	q = cq->q;
+	local_batch_size = q->pgo_maxlaundry / (vm_compressor_thread_count * 2);
+
+#if RECORD_THE_COMPRESSED_DATA
+	if (q->pgo_laundry)
+		c_compressed_record_init();
+#endif
+	while (TRUE) {
+		int	pages_left_on_q = 0;
+
+		local_cnt = 0;
+		local_q = NULL;
+
+		KERNEL_DEBUG(0xe0400014 | DBG_FUNC_START, 0, 0, 0, 0, 0);
+	
+		vm_page_lock_queues();
+
+		KERNEL_DEBUG(0xe0400014 | DBG_FUNC_END, 0, 0, 0, 0, 0);
+
+		KERNEL_DEBUG(0xe0400018 | DBG_FUNC_START, q->pgo_laundry, 0, 0, 0, 0);
+
+		while ( !queue_empty(&q->pgo_pending) && local_cnt <  local_batch_size) {
+
+			queue_remove_first(&q->pgo_pending, m, vm_page_t, pageq);
+
+			VM_PAGE_CHECK(m);
+
+			m->pageout_queue = FALSE;
+			m->pageq.prev = NULL;
+
+			m->pageq.next = (queue_entry_t)local_q;
+			local_q = m;
+			local_cnt++;
+		}
+		if (local_q == NULL)
+			break;
+
+		q->pgo_busy = TRUE;
+
+		if ((pgo_draining = q->pgo_draining) == FALSE) {
+			vm_pageout_throttle_up_batch(q, local_cnt);
+			pages_left_on_q = q->pgo_laundry;
+		} else
+			pages_left_on_q = q->pgo_laundry - local_cnt;
+
+		vm_page_unlock_queues();
+
+#if !RECORD_THE_COMPRESSED_DATA
+		if (pages_left_on_q >= local_batch_size && cq->id < (vm_compressor_thread_count - 1)) 
+			thread_wakeup((event_t) ((uintptr_t)&q->pgo_pending + cq->id + 1));
+#endif
+		KERNEL_DEBUG(0xe0400018 | DBG_FUNC_END, q->pgo_laundry, 0, 0, 0, 0);
+
+		while (local_q) {
+
+			KERNEL_DEBUG(0xe0400024 | DBG_FUNC_START, local_cnt, 0, 0, 0, 0);
+
+			m = local_q;
+			local_q = (vm_page_t)m->pageq.next;
+			m->pageq.next = NULL;
+
+			if (vm_pageout_compress_page(&cq->current_chead, cq->scratch_buf, m, FALSE) == KERN_SUCCESS) {
+
+				m->pageq.next = (queue_entry_t)local_freeq;
+				local_freeq = m;
+				local_freed++;
+
+				if (local_freed >= MAX_FREE_BATCH) {
+
+					vm_page_free_list(local_freeq, TRUE);
+					local_freeq = NULL;
+					local_freed = 0;
+				}
+			}
+#if !CONFIG_JETSAM
+			while (vm_page_free_count < COMPRESSOR_FREE_RESERVED_LIMIT) {
+				kern_return_t	wait_result;
+				int		need_wakeup = 0;
+
+				if (local_freeq) {
+					vm_page_free_list(local_freeq, TRUE);
+
+					local_freeq = NULL;
+					local_freed = 0;
+
+					continue;
+				}
+				lck_mtx_lock_spin(&vm_page_queue_free_lock);
+
+				if (vm_page_free_count < COMPRESSOR_FREE_RESERVED_LIMIT) {
+
+					if (vm_page_free_wanted_privileged++ == 0)
+						need_wakeup = 1;
+					wait_result = assert_wait((event_t)&vm_page_free_wanted_privileged, THREAD_UNINT);
+
+					lck_mtx_unlock(&vm_page_queue_free_lock);
+
+					if (need_wakeup)
+						thread_wakeup((event_t)&vm_page_free_wanted);
+
+					if (wait_result == THREAD_WAITING)
+
+						thread_block(THREAD_CONTINUE_NULL);
+				} else
+					lck_mtx_unlock(&vm_page_queue_free_lock);
+			}
+#endif
+		}
+		if (local_freeq) {
+			vm_page_free_list(local_freeq, TRUE);
+				
+			local_freeq = NULL;
+			local_freed = 0;
+		}
+		if (pgo_draining == TRUE) {
+			vm_page_lockspin_queues();
+			vm_pageout_throttle_up_batch(q, local_cnt);
+			vm_page_unlock_queues();
+		}
+	}
+	KERNEL_DEBUG(0xe040000c | DBG_FUNC_START, 0, 0, 0, 0, 0);
+
+	/*
+	 * queue lock is held and our q is empty
+	 */
+	q->pgo_busy = FALSE;
+	q->pgo_idle = TRUE;
+
+	assert_wait((event_t) ((uintptr_t)&q->pgo_pending + cq->id), THREAD_UNINT);
+	vm_page_unlock_queues();
+
+	KERNEL_DEBUG(0xe0400018 | DBG_FUNC_END, 0, 0, 0, 0, 0);
+
+	thread_block_parameter((thread_continue_t)vm_pageout_iothread_internal_continue, (void *) cq);
+	/*NOTREACHED*/
+}
+
+
+
+static void
+vm_pageout_immediate(vm_page_t m, boolean_t object_locked_by_caller)
+{
+	assert(vm_pageout_immediate_scratch_buf);
+
+	if (vm_pageout_compress_page(&vm_pageout_immediate_chead, vm_pageout_immediate_scratch_buf, m, object_locked_by_caller) == KERN_SUCCESS) {
+
+		vm_page_free_prepare_object(m, TRUE);
+		vm_page_release(m);
+	}
+}
+
+
+kern_return_t
+vm_pageout_compress_page(void **current_chead, char *scratch_buf, vm_page_t m, boolean_t object_locked_by_caller) 
+{
+	vm_object_t	object;
+	memory_object_t	pager;
+	int		compressed_count_delta;
+	kern_return_t	retval;
+
+	if (m->object->object_slid) {
+		panic("slid page %p not allowed on this path\n", m);
+	}
+
+	object = m->object;
+	pager = object->pager;
+
+	if (object_locked_by_caller == FALSE && (!object->pager_initialized || pager == MEMORY_OBJECT_NULL))  {
+				
+		KERNEL_DEBUG(0xe0400010 | DBG_FUNC_START, object, pager, 0, 0, 0);
+
+		vm_object_lock(object);
+
+		/*
+		 * If there is no memory object for the page, create
+		 * one and hand it to the compression pager.
+		 */
+
+		if (!object->pager_initialized)
+			vm_object_collapse(object, (vm_object_offset_t) 0, TRUE);
+		if (!object->pager_initialized)
+			vm_object_compressor_pager_create(object);
+
+		if (!object->pager_initialized) {
+			/*
+			 * Still no pager for the object.
+			 * Reactivate the page.
+			 *
+			 * Should only happen if there is no
+			 * compression pager
+			 */
+			m->pageout = FALSE;
+			m->laundry = FALSE;
+			PAGE_WAKEUP_DONE(m);
+
+			vm_page_lockspin_queues();
+			vm_page_activate(m);
+			vm_pageout_dirty_no_pager++;
+			vm_page_unlock_queues();
+					
+			/*
+			 *	And we are done with it.
+			 */
+			vm_object_activity_end(object);
+			vm_object_unlock(object);
+
+			return KERN_FAILURE;
+		}
+		pager = object->pager;
+
+		if (pager == MEMORY_OBJECT_NULL) {
+			/*
+			 * This pager has been destroyed by either
+			 * memory_object_destroy or vm_object_destroy, and
+			 * so there is nowhere for the page to go.
+			 */
+			if (m->pageout) {
+				/*
+				 * Just free the page... VM_PAGE_FREE takes
+				 * care of cleaning up all the state...
+				 * including doing the vm_pageout_throttle_up
+				 */
+				VM_PAGE_FREE(m);
+			} else {
+				m->laundry = FALSE;
+				PAGE_WAKEUP_DONE(m);
+
+				vm_page_lockspin_queues();
+				vm_page_activate(m);
+				vm_page_unlock_queues();
+
+				/*
+				 *	And we are done with it.
+				 */
+			}
+			vm_object_activity_end(object);
+			vm_object_unlock(object);
+
+			return KERN_FAILURE;
+		}
+		vm_object_unlock(object);
+				
+		KERNEL_DEBUG(0xe0400010 | DBG_FUNC_END, object, pager, 0, 0, 0);
+	}
+	assert(object->pager_initialized && pager != MEMORY_OBJECT_NULL);
+
+	if (object_locked_by_caller == FALSE)
+		assert(object->activity_in_progress > 0);
+
+	retval = vm_compressor_pager_put(
+		pager,
+		m->offset + object->paging_offset,
+		m->phys_page,
+		current_chead,
+		scratch_buf,
+		&compressed_count_delta);
+
+	if (object_locked_by_caller == FALSE) {
+		vm_object_lock(object);
+
+		assert(object->activity_in_progress > 0);
+		assert(m->object == object);
+	}
+
+	vm_compressor_pager_count(pager,
+				  compressed_count_delta,
+				  FALSE, /* shared_lock */
+				  object);
+
+	m->laundry = FALSE;
+	m->pageout = FALSE;
+
+	if (retval == KERN_SUCCESS) {
+		/*
+		 * If the object is purgeable, its owner's
+		 * purgeable ledgers will be updated in
+		 * vm_page_remove() but the page still
+		 * contributes to the owner's memory footprint,
+		 * so account for it as such.
+		 */
+		if (object->purgable != VM_PURGABLE_DENY &&
+		    object->vo_purgeable_owner != NULL) {
+			/* one more compressed purgeable page */
+			vm_purgeable_compressed_update(object,
+						       +1);
+		}
+		VM_STAT_INCR(compressions);
+			
+		if (m->tabled)
+			vm_page_remove(m, TRUE);
+
+	} else {
+		PAGE_WAKEUP_DONE(m);
+
+		vm_page_lockspin_queues();
+
+		vm_page_activate(m);
+		vm_compressor_failed++;
+
+		vm_page_unlock_queues();
+	}
+	if (object_locked_by_caller == FALSE) {
+		vm_object_activity_end(object);
+		vm_object_unlock(object);
+	}
+	return retval;
+}
+
+
+static void
+vm_pageout_adjust_io_throttles(struct vm_pageout_queue *iq, struct vm_pageout_queue *eq, boolean_t req_lowpriority)
+{
+	uint32_t 	policy;
+	boolean_t	set_iq = FALSE;
+	boolean_t	set_eq = FALSE;
+	
+	if (hibernate_cleaning_in_progress == TRUE)
+		req_lowpriority = FALSE;
+
+	if ((DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE) && iq->pgo_inited == TRUE && iq->pgo_lowpriority != req_lowpriority)
+		set_iq = TRUE;
+
+	if (eq->pgo_inited == TRUE && eq->pgo_lowpriority != req_lowpriority)
+		set_eq = TRUE;
+	
+	if (set_iq == TRUE || set_eq == TRUE) {
+
+		vm_page_unlock_queues();
+
+		if (req_lowpriority == TRUE) {
+			policy = THROTTLE_LEVEL_PAGEOUT_THROTTLED;
+			DTRACE_VM(laundrythrottle);
+		} else {
+			policy = THROTTLE_LEVEL_PAGEOUT_UNTHROTTLED;
+			DTRACE_VM(laundryunthrottle);
+		}
+		if (set_iq == TRUE) {
+			proc_set_task_policy_thread(kernel_task, iq->pgo_tid, TASK_POLICY_EXTERNAL, TASK_POLICY_IO, policy);
+
+			iq->pgo_lowpriority = req_lowpriority;
+		}
+		if (set_eq == TRUE) {
+			proc_set_task_policy_thread(kernel_task, eq->pgo_tid, TASK_POLICY_EXTERNAL, TASK_POLICY_IO, policy);
+
+			eq->pgo_lowpriority = req_lowpriority;
+		}
+		vm_page_lock_queues();
+	}
+}
+
+
+static void
+vm_pageout_iothread_external(void)
+{
+	thread_t	self = current_thread();
+
+	self->options |= TH_OPT_VMPRIV;
+
+	DTRACE_VM2(laundrythrottle, int, 1, (uint64_t *), NULL);	
+
+	proc_set_task_policy_thread(kernel_task, self->thread_id, TASK_POLICY_EXTERNAL,
+	                            TASK_POLICY_IO, THROTTLE_LEVEL_PAGEOUT_THROTTLED);
+
+	vm_page_lock_queues();
+
+	vm_pageout_queue_external.pgo_tid = self->thread_id;
+	vm_pageout_queue_external.pgo_lowpriority = TRUE;
+	vm_pageout_queue_external.pgo_inited = TRUE;
+
+	vm_page_unlock_queues();
+
+	if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE)
+		vm_pageout_iothread_external_continue(&vm_pageout_queue_external);
+	else
+		vm_pageout_iothread_continue(&vm_pageout_queue_external);
+
+	/*NOTREACHED*/
+}
+
+
+static void
+vm_pageout_iothread_internal(struct cq *cq)
+{
+	thread_t	self = current_thread();
+
+	self->options |= TH_OPT_VMPRIV;
+
+	if (DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE) {
+		DTRACE_VM2(laundrythrottle, int, 1, (uint64_t *), NULL);	
+
+		proc_set_task_policy_thread(kernel_task, self->thread_id, TASK_POLICY_EXTERNAL,
+		                            TASK_POLICY_IO, THROTTLE_LEVEL_PAGEOUT_THROTTLED);
+	}
+	vm_page_lock_queues();
+
+	vm_pageout_queue_internal.pgo_tid = self->thread_id;
+	vm_pageout_queue_internal.pgo_lowpriority = TRUE;
+	vm_pageout_queue_internal.pgo_inited = TRUE;
+
+	vm_page_unlock_queues();
+
+	if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
+
+		if (vm_restricted_to_single_processor == TRUE)
+			thread_vm_bind_group_add();
+
+		vm_pageout_iothread_internal_continue(cq);
+	} else
+		vm_pageout_iothread_continue(&vm_pageout_queue_internal);
+
+	/*NOTREACHED*/
+}
+
+kern_return_t
+vm_set_buffer_cleanup_callout(boolean_t (*func)(int)) 
+{
+	if (OSCompareAndSwapPtr(NULL, func, (void * volatile *) &consider_buffer_cache_collect)) {
+		return KERN_SUCCESS;
+	} else {
+		return KERN_FAILURE; /* Already set */
+	}
+}
+
+extern boolean_t	memorystatus_manual_testing_on;
+extern unsigned int 	memorystatus_level;
+
+
+#if VM_PRESSURE_EVENTS
+
+boolean_t vm_pressure_events_enabled = FALSE;
+
+void
+vm_pressure_response(void)
+{
+
+	vm_pressure_level_t	old_level = kVMPressureNormal;
+	int			new_level = -1;
+
+	uint64_t		available_memory = 0;
+
+	if (vm_pressure_events_enabled == FALSE)
+		return;
+
+
+	available_memory = (((uint64_t) AVAILABLE_NON_COMPRESSED_MEMORY) * 100);
+
+
+	memorystatus_level = (unsigned int) (available_memory / atop_64(max_mem));
+
+	if (memorystatus_manual_testing_on) {
+		return;
+	}
+	
+	old_level = memorystatus_vm_pressure_level;
+
+	switch (memorystatus_vm_pressure_level) {
+
+		case kVMPressureNormal:
+		{
+			if (VM_PRESSURE_WARNING_TO_CRITICAL()) {
+				new_level = kVMPressureCritical;
+			}  else if (VM_PRESSURE_NORMAL_TO_WARNING()) {
+				new_level = kVMPressureWarning;
+			}
+			break;
+		}
+
+		case kVMPressureWarning:
+		case kVMPressureUrgent:
+		{
+			if (VM_PRESSURE_WARNING_TO_NORMAL()) {
+				new_level = kVMPressureNormal;
+			}  else if (VM_PRESSURE_WARNING_TO_CRITICAL()) {
+				new_level = kVMPressureCritical;
+			}
+			break;
+		}
+
+		case kVMPressureCritical:
+		{
+			if (VM_PRESSURE_WARNING_TO_NORMAL()) {
+				new_level = kVMPressureNormal;
+			}  else if (VM_PRESSURE_CRITICAL_TO_WARNING()) {
+				new_level = kVMPressureWarning;
+			}
+			break;
+		}
+
+		default:
+			return;
+	}
+		
+	if (new_level != -1) {
+		memorystatus_vm_pressure_level = (vm_pressure_level_t) new_level;
+
+		if ((memorystatus_vm_pressure_level != kVMPressureNormal) || (old_level != new_level)) {
+			if (vm_pressure_thread_running == FALSE) {
+				thread_wakeup(&vm_pressure_thread);
+			}
+
+			if (old_level != new_level) {
+				thread_wakeup(&vm_pressure_changed);
+			}
+		}
+	}
+
+}
+#endif /* VM_PRESSURE_EVENTS */
+
+kern_return_t
+mach_vm_pressure_level_monitor(__unused boolean_t wait_for_pressure, __unused unsigned int *pressure_level) {
+
+#if   !VM_PRESSURE_EVENTS
+	
+	return KERN_FAILURE;
+
+#else /* VM_PRESSURE_EVENTS */
+
+	kern_return_t	kr = KERN_SUCCESS;
+
+	if (pressure_level != NULL) {
+
+		vm_pressure_level_t	old_level = memorystatus_vm_pressure_level;
+
+		if (wait_for_pressure == TRUE) {
+			wait_result_t		wr = 0;
+
+			while (old_level == *pressure_level) {
+				wr = assert_wait((event_t) &vm_pressure_changed,
+						 THREAD_INTERRUPTIBLE);
+				if (wr == THREAD_WAITING) {
+					wr = thread_block(THREAD_CONTINUE_NULL);
+				}
+				if (wr == THREAD_INTERRUPTED) {
+					return KERN_ABORTED;
+				}
+				if (wr == THREAD_AWAKENED) {
+					
+					old_level = memorystatus_vm_pressure_level;
+
+					if (old_level != *pressure_level) {
+						break;
+					}
+				}
+			}
+		}
+
+		*pressure_level = old_level;
+		kr = KERN_SUCCESS;
+	} else {
+		kr = KERN_INVALID_ARGUMENT;
+	}
+
+	return kr;
+#endif /* VM_PRESSURE_EVENTS */
+}
+
+#if VM_PRESSURE_EVENTS
+void
+vm_pressure_thread(void) {
+	static boolean_t thread_initialized = FALSE;
+
+	if (thread_initialized == TRUE) {
+		vm_pressure_thread_running = TRUE;
+		consider_vm_pressure_events();
+		vm_pressure_thread_running = FALSE;
+	}
+
+	thread_initialized = TRUE;
+	assert_wait((event_t) &vm_pressure_thread, THREAD_UNINT);
+	thread_block((thread_continue_t)vm_pressure_thread);
+}
+#endif /* VM_PRESSURE_EVENTS */
+
+
+uint32_t vm_pageout_considered_page_last = 0;
+
+/*
+ * called once per-second via "compute_averages"
+ */
+void
+compute_pageout_gc_throttle()
+{
+	if (vm_pageout_considered_page != vm_pageout_considered_page_last) {
+
+		vm_pageout_considered_page_last = vm_pageout_considered_page;
+
+		thread_wakeup((event_t) &vm_pageout_garbage_collect);
+	}
+}
+
+
+static void
+vm_pageout_garbage_collect(int collect)
+{
+
+	if (collect) {
+		boolean_t buf_large_zfree = FALSE;
+		boolean_t first_try = TRUE;
+
+		stack_collect();
+
+		consider_machine_collect();
+		m_drain();
+
+		do {
+			if (consider_buffer_cache_collect != NULL) {
+				buf_large_zfree = (*consider_buffer_cache_collect)(0);
+			}
+			if (first_try == TRUE || buf_large_zfree == TRUE) {
+				/*
+				 * consider_zone_gc should be last, because the other operations
+				 * might return memory to zones.
+				 */
+				consider_zone_gc(buf_large_zfree);
+			}
+			first_try = FALSE;
+
+		} while (buf_large_zfree == TRUE && vm_page_free_count < vm_page_free_target);
+
+		consider_machine_adjust();
+	}
+	assert_wait((event_t) &vm_pageout_garbage_collect, THREAD_UNINT);
+
+	thread_block_parameter((thread_continue_t) vm_pageout_garbage_collect, (void *)1);
+	/*NOTREACHED*/
+}
+
+
+void	vm_pageout_reinit_tuneables(void);
+
+void
+vm_pageout_reinit_tuneables(void)
+{
+
+	vm_compressor_minorcompact_threshold_divisor = 18;
+	vm_compressor_majorcompact_threshold_divisor = 22;
+	vm_compressor_unthrottle_threshold_divisor = 32;
+}
+
+
+#if VM_PAGE_BUCKETS_CHECK
+#if VM_PAGE_FAKE_BUCKETS
+extern vm_map_offset_t vm_page_fake_buckets_start, vm_page_fake_buckets_end;
+#endif /* VM_PAGE_FAKE_BUCKETS */
+#endif /* VM_PAGE_BUCKETS_CHECK */
+
+#define FBDP_TEST_COLLAPSE_COMPRESSOR 0
+#if FBDP_TEST_COLLAPSE_COMPRESSOR
+extern boolean_t vm_object_collapse_compressor_allowed;
+#include <IOKit/IOLib.h>
+#endif /* FBDP_TEST_COLLAPSE_COMPRESSOR */
+
+#define FBDP_TEST_WIRE_AND_EXTRACT 0
+#if FBDP_TEST_WIRE_AND_EXTRACT
+extern ledger_template_t	task_ledger_template;
+#include <mach/mach_vm.h>
+extern ppnum_t vm_map_get_phys_page(vm_map_t map,
+				    vm_offset_t offset);
+#endif /* FBDP_TEST_WIRE_AND_EXTRACT */
+
+
+void
+vm_set_restrictions()
+{
+	host_basic_info_data_t hinfo;
+	mach_msg_type_number_t count = HOST_BASIC_INFO_COUNT;
+
+#define BSD_HOST 1
+	host_info((host_t)BSD_HOST, HOST_BASIC_INFO, (host_info_t)&hinfo, &count);
+
+	assert(hinfo.max_cpus > 0);
+
+	if (hinfo.max_cpus <= 3) {
+		/*
+		 * on systems with a limited number of CPUS, bind the 
+		 * 4 major threads that can free memory and that tend to use
+		 * a fair bit of CPU under pressured conditions to a single processor.
+		 * This insures that these threads don't hog all of the available CPUs
+		 * (important for camera launch), while allowing them to run independently
+		 * w/r to locks... the 4 threads are
+		 * vm_pageout_scan,  vm_pageout_iothread_internal (compressor), 
+		 * vm_compressor_swap_trigger_thread (minor and major compactions),
+		 * memorystatus_thread (jetsams).
+		 *
+		 * the first time the thread is run, it is responsible for checking the
+		 * state of vm_restricted_to_single_processor, and if TRUE it calls
+		 * thread_bind_master...  someday this should be replaced with a group
+		 * scheduling mechanism and KPI.
+		 */
+		vm_restricted_to_single_processor = TRUE;
+	}
+}
+
+
+void
+vm_pageout(void)
+{
+	thread_t	self = current_thread();
+	thread_t	thread;
+	kern_return_t	result;
+	spl_t		s;
+
+	/*
+	 * Set thread privileges.
+	 */
+	s = splsched();
+
+	thread_lock(self);
+	self->options |= TH_OPT_VMPRIV;
+	sched_set_thread_base_priority(self, BASEPRI_PREEMPT - 1);
+	thread_unlock(self);
+
+	if (!self->reserved_stack)
+		self->reserved_stack = self->kernel_stack;
+
+	if (vm_restricted_to_single_processor == TRUE)
+		thread_vm_bind_group_add();
+
+	splx(s);
+
+	/*
+	 *	Initialize some paging parameters.
+	 */
+
+	if (vm_pageout_swap_wait == 0)
+		vm_pageout_swap_wait = VM_PAGEOUT_SWAP_WAIT;
+
+	if (vm_pageout_idle_wait == 0)
+		vm_pageout_idle_wait = VM_PAGEOUT_IDLE_WAIT;
+
+	if (vm_pageout_burst_wait == 0)
+		vm_pageout_burst_wait = VM_PAGEOUT_BURST_WAIT;
+
+	if (vm_pageout_empty_wait == 0)
+		vm_pageout_empty_wait = VM_PAGEOUT_EMPTY_WAIT;
+
+	if (vm_pageout_deadlock_wait == 0)
+		vm_pageout_deadlock_wait = VM_PAGEOUT_DEADLOCK_WAIT;
+
+	if (vm_pageout_deadlock_relief == 0)
+		vm_pageout_deadlock_relief = VM_PAGEOUT_DEADLOCK_RELIEF;
+
+	if (vm_pageout_inactive_relief == 0)
+		vm_pageout_inactive_relief = VM_PAGEOUT_INACTIVE_RELIEF;
+
+	if (vm_pageout_burst_active_throttle == 0)
+	        vm_pageout_burst_active_throttle = VM_PAGEOUT_BURST_ACTIVE_THROTTLE;
+
+	if (vm_pageout_burst_inactive_throttle == 0)
+	        vm_pageout_burst_inactive_throttle = VM_PAGEOUT_BURST_INACTIVE_THROTTLE;
+
+	/*
+	 * Set kernel task to low backing store privileged 
+	 * status
+	 */
+	task_lock(kernel_task);
+	kernel_task->priv_flags |= VM_BACKING_STORE_PRIV;
+	task_unlock(kernel_task);
+
+	vm_page_free_count_init = vm_page_free_count;
+
+	/*
+	 * even if we've already called vm_page_free_reserve
+	 * call it again here to insure that the targets are
+	 * accurately calculated (it uses vm_page_free_count_init)
+	 * calling it with an arg of 0 will not change the reserve
+	 * but will re-calculate free_min and free_target
+	 */
+	if (vm_page_free_reserved < VM_PAGE_FREE_RESERVED(processor_count)) {
+		vm_page_free_reserve((VM_PAGE_FREE_RESERVED(processor_count)) - vm_page_free_reserved);
+	} else
+		vm_page_free_reserve(0);
+
+
+	queue_init(&vm_pageout_queue_external.pgo_pending);
+	vm_pageout_queue_external.pgo_maxlaundry = VM_PAGE_LAUNDRY_MAX;
+	vm_pageout_queue_external.pgo_laundry = 0;
+	vm_pageout_queue_external.pgo_idle = FALSE;
+	vm_pageout_queue_external.pgo_busy = FALSE;
+	vm_pageout_queue_external.pgo_throttled = FALSE;
+	vm_pageout_queue_external.pgo_draining = FALSE;
+	vm_pageout_queue_external.pgo_lowpriority = FALSE;
+	vm_pageout_queue_external.pgo_tid = -1;
+	vm_pageout_queue_external.pgo_inited = FALSE;
+
+	queue_init(&vm_pageout_queue_internal.pgo_pending);
+	vm_pageout_queue_internal.pgo_maxlaundry = 0;
+	vm_pageout_queue_internal.pgo_laundry = 0;
+	vm_pageout_queue_internal.pgo_idle = FALSE;
+	vm_pageout_queue_internal.pgo_busy = FALSE;
+	vm_pageout_queue_internal.pgo_throttled = FALSE;
+	vm_pageout_queue_internal.pgo_draining = FALSE;
+	vm_pageout_queue_internal.pgo_lowpriority = FALSE;
+	vm_pageout_queue_internal.pgo_tid = -1;
+	vm_pageout_queue_internal.pgo_inited = FALSE;
+
+	/* internal pageout thread started when default pager registered first time */
+	/* external pageout and garbage collection threads started here */
+
+	result = kernel_thread_start_priority((thread_continue_t)vm_pageout_iothread_external, NULL, 
+					      BASEPRI_PREEMPT - 1, 
+					      &vm_pageout_external_iothread);
+	if (result != KERN_SUCCESS)
+		panic("vm_pageout_iothread_external: create failed");
+
+	thread_deallocate(vm_pageout_external_iothread);
+
+	result = kernel_thread_start_priority((thread_continue_t)vm_pageout_garbage_collect, NULL,
+					      BASEPRI_DEFAULT, 
+					      &thread);
+	if (result != KERN_SUCCESS)
+		panic("vm_pageout_garbage_collect: create failed");
+
+	thread_deallocate(thread);
+
+#if VM_PRESSURE_EVENTS
+	result = kernel_thread_start_priority((thread_continue_t)vm_pressure_thread, NULL,
+						BASEPRI_DEFAULT,
+						&thread);
+
+	if (result != KERN_SUCCESS)
+		panic("vm_pressure_thread: create failed");
+
+	thread_deallocate(thread);
+#endif
+
+	vm_object_reaper_init();
+	
+	if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE)
+		vm_compressor_pager_init();
+
+#if VM_PRESSURE_EVENTS
+	vm_pressure_events_enabled = TRUE;
+#endif /* VM_PRESSURE_EVENTS */
+
+#if CONFIG_PHANTOM_CACHE
+	vm_phantom_cache_init();
+#endif
+#if VM_PAGE_BUCKETS_CHECK
+#if VM_PAGE_FAKE_BUCKETS
+	printf("**** DEBUG: protecting fake buckets [0x%llx:0x%llx]\n",
+	       (uint64_t) vm_page_fake_buckets_start,
+	       (uint64_t) vm_page_fake_buckets_end);
+	pmap_protect(kernel_pmap,
+		     vm_page_fake_buckets_start,
+		     vm_page_fake_buckets_end,
+		     VM_PROT_READ);
+//	*(char *) vm_page_fake_buckets_start = 'x';	/* panic! */
+#endif /* VM_PAGE_FAKE_BUCKETS */
+#endif /* VM_PAGE_BUCKETS_CHECK */
+
+#if VM_OBJECT_TRACKING
+	vm_object_tracking_init();
+#endif /* VM_OBJECT_TRACKING */
+
+
+#if FBDP_TEST_COLLAPSE_COMPRESSOR
+	vm_object_size_t	backing_size, top_size;
+	vm_object_t		backing_object, top_object;
+	vm_map_offset_t		backing_offset, top_offset;
+	unsigned char		*backing_address, *top_address;
+	kern_return_t		kr;
+
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR:\n");
+
+	/* create backing object */
+	backing_size = 15 * PAGE_SIZE;
+	backing_object = vm_object_allocate(backing_size);
+	assert(backing_object != VM_OBJECT_NULL);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: created backing object %p\n",
+		backing_object);
+	/* map backing object */
+	backing_offset = 0;
+	kr = vm_map_enter(kernel_map, &backing_offset, backing_size, 0,
+			  VM_FLAGS_ANYWHERE, backing_object, 0, FALSE,
+			  VM_PROT_DEFAULT, VM_PROT_DEFAULT, VM_INHERIT_DEFAULT);
+	assert(kr == KERN_SUCCESS);
+	backing_address = (unsigned char *) backing_offset;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "mapped backing object %p at 0x%llx\n",
+	       backing_object, (uint64_t) backing_offset);
+	/* populate with pages to be compressed in backing object */
+	backing_address[0x1*PAGE_SIZE] = 0xB1;
+	backing_address[0x4*PAGE_SIZE] = 0xB4;
+	backing_address[0x7*PAGE_SIZE] = 0xB7;
+	backing_address[0xa*PAGE_SIZE] = 0xBA;
+	backing_address[0xd*PAGE_SIZE] = 0xBD;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "populated pages to be compressed in "
+	       "backing_object %p\n", backing_object);
+	/* compress backing object */
+	vm_object_pageout(backing_object);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: compressing backing_object %p\n",
+	       backing_object);
+	/* wait for all the pages to be gone */
+	while (*(volatile int *)&backing_object->resident_page_count != 0)
+		IODelay(10);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: backing_object %p compressed\n",
+	       backing_object);
+	/* populate with pages to be resident in backing object */
+	backing_address[0x0*PAGE_SIZE] = 0xB0;
+	backing_address[0x3*PAGE_SIZE] = 0xB3;
+	backing_address[0x6*PAGE_SIZE] = 0xB6;
+	backing_address[0x9*PAGE_SIZE] = 0xB9;
+	backing_address[0xc*PAGE_SIZE] = 0xBC;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "populated pages to be resident in "
+	       "backing_object %p\n", backing_object);
+	/* leave the other pages absent */
+	/* mess with the paging_offset of the backing_object */
+	assert(backing_object->paging_offset == 0);
+	backing_object->paging_offset = 0x3000;
+
+	/* create top object */
+	top_size = 9 * PAGE_SIZE;
+	top_object = vm_object_allocate(top_size);
+	assert(top_object != VM_OBJECT_NULL);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: created top object %p\n",
+		top_object);
+	/* map top object */
+	top_offset = 0;
+	kr = vm_map_enter(kernel_map, &top_offset, top_size, 0,
+			  VM_FLAGS_ANYWHERE, top_object, 0, FALSE,
+			  VM_PROT_DEFAULT, VM_PROT_DEFAULT, VM_INHERIT_DEFAULT);
+	assert(kr == KERN_SUCCESS);
+	top_address = (unsigned char *) top_offset;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "mapped top object %p at 0x%llx\n",
+	       top_object, (uint64_t) top_offset);
+	/* populate with pages to be compressed in top object */
+	top_address[0x3*PAGE_SIZE] = 0xA3;
+	top_address[0x4*PAGE_SIZE] = 0xA4;
+	top_address[0x5*PAGE_SIZE] = 0xA5;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "populated pages to be compressed in "
+	       "top_object %p\n", top_object);
+	/* compress top object */
+	vm_object_pageout(top_object);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: compressing top_object %p\n",
+	       top_object);
+	/* wait for all the pages to be gone */
+	while (top_object->resident_page_count != 0);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: top_object %p compressed\n",
+	       top_object);
+	/* populate with pages to be resident in top object */
+	top_address[0x0*PAGE_SIZE] = 0xA0;
+	top_address[0x1*PAGE_SIZE] = 0xA1;
+	top_address[0x2*PAGE_SIZE] = 0xA2;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "populated pages to be resident in "
+	       "top_object %p\n", top_object);
+	/* leave the other pages absent */
+	
+	/* link the 2 objects */
+	vm_object_reference(backing_object);
+	top_object->shadow = backing_object;
+	top_object->vo_shadow_offset = 0x3000;
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: linked %p and %p\n",
+	       top_object, backing_object);
+
+	/* unmap backing object */
+	vm_map_remove(kernel_map,
+		      backing_offset,
+		      backing_offset + backing_size,
+		      0);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+	       "unmapped backing_object %p [0x%llx:0x%llx]\n",
+	       backing_object,
+	       (uint64_t) backing_offset,
+	       (uint64_t) (backing_offset + backing_size));
+
+	/* collapse */
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: collapsing %p\n", top_object);
+	vm_object_lock(top_object);
+	vm_object_collapse(top_object, 0, FALSE);
+	vm_object_unlock(top_object);
+	printf("FBDP_TEST_COLLAPSE_COMPRESSOR: collapsed %p\n", top_object);
+
+	/* did it work? */
+	if (top_object->shadow != VM_OBJECT_NULL) {
+		printf("FBDP_TEST_COLLAPSE_COMPRESSOR: not collapsed\n");
+		printf("FBDP_TEST_COLLAPSE_COMPRESSOR: FAIL\n");
+		if (vm_object_collapse_compressor_allowed) {
+			panic("FBDP_TEST_COLLAPSE_COMPRESSOR: FAIL\n");
+		}
+	} else {
+		/* check the contents of the mapping */
+		unsigned char expect[9] =
+			{ 0xA0, 0xA1, 0xA2,	/* resident in top */
+			  0xA3, 0xA4, 0xA5,	/* compressed in top */
+			  0xB9,	/* resident in backing + shadow_offset */
+			  0xBD,	/* compressed in backing + shadow_offset + paging_offset */
+			  0x00 };		/* absent in both */
+		unsigned char actual[9];
+		unsigned int i, errors;
+
+		errors = 0;
+		for (i = 0; i < sizeof (actual); i++) {
+			actual[i] = (unsigned char) top_address[i*PAGE_SIZE];
+			if (actual[i] != expect[i]) {
+				errors++;
+			}
+		}
+		printf("FBDP_TEST_COLLAPSE_COMPRESSOR: "
+		       "actual [%x %x %x %x %x %x %x %x %x] "
+		       "expect [%x %x %x %x %x %x %x %x %x] "
+		       "%d errors\n",
+		       actual[0], actual[1], actual[2], actual[3],
+		       actual[4], actual[5], actual[6], actual[7],
+		       actual[8],
+		       expect[0], expect[1], expect[2], expect[3],
+		       expect[4], expect[5], expect[6], expect[7],
+		       expect[8],
+		       errors);
+		if (errors) {
+			panic("FBDP_TEST_COLLAPSE_COMPRESSOR: FAIL\n"); 
+		} else {
+			printf("FBDP_TEST_COLLAPSE_COMPRESSOR: PASS\n");
+		}
+	}
+#endif /* FBDP_TEST_COLLAPSE_COMPRESSOR */
+
+#if FBDP_TEST_WIRE_AND_EXTRACT
+	ledger_t		ledger;
+	vm_map_t		user_map, wire_map;
+	mach_vm_address_t	user_addr, wire_addr;
+	mach_vm_size_t		user_size, wire_size;
+	mach_vm_offset_t	cur_offset;
+	vm_prot_t		cur_prot, max_prot;
+	ppnum_t			user_ppnum, wire_ppnum;
+	kern_return_t		kr;
+
+	ledger = ledger_instantiate(task_ledger_template,
+				    LEDGER_CREATE_ACTIVE_ENTRIES);
+	user_map = vm_map_create(pmap_create(ledger, 0, PMAP_CREATE_64BIT),
+				 0x100000000ULL,
+				 0x200000000ULL,
+				 TRUE);
+	wire_map = vm_map_create(NULL,
+				 0x100000000ULL,
+				 0x200000000ULL,
+				 TRUE);
+	user_addr = 0;
+	user_size = 0x10000;
+	kr = mach_vm_allocate(user_map,
+			      &user_addr,
+			      user_size,
+			      VM_FLAGS_ANYWHERE);
+	assert(kr == KERN_SUCCESS);
+	wire_addr = 0;
+	wire_size = user_size;
+	kr = mach_vm_remap(wire_map,
+			   &wire_addr,
+			   wire_size,
+			   0,
+			   VM_FLAGS_ANYWHERE,
+			   user_map,
+			   user_addr,
+			   FALSE,
+			   &cur_prot,
+			   &max_prot,
+			   VM_INHERIT_NONE);
+	assert(kr == KERN_SUCCESS);
+	for (cur_offset = 0;
+	     cur_offset < wire_size;
+	     cur_offset += PAGE_SIZE) {
+		kr = vm_map_wire_and_extract(wire_map,
+					     wire_addr + cur_offset,
+					     VM_PROT_DEFAULT | VM_PROT_MEMORY_TAG_MAKE(VM_KERN_MEMORY_OSFMK)),
+					     TRUE,
+					     &wire_ppnum);
+		assert(kr == KERN_SUCCESS);
+		user_ppnum = vm_map_get_phys_page(user_map,
+						  user_addr + cur_offset);
+		printf("FBDP_TEST_WIRE_AND_EXTRACT: kr=0x%x "
+		       "user[%p:0x%llx:0x%x] wire[%p:0x%llx:0x%x]\n",
+		       kr,
+		       user_map, user_addr + cur_offset, user_ppnum,
+		       wire_map, wire_addr + cur_offset, wire_ppnum);
+		if (kr != KERN_SUCCESS ||
+		    wire_ppnum == 0 ||
+		    wire_ppnum != user_ppnum) {
+			panic("FBDP_TEST_WIRE_AND_EXTRACT: FAIL\n");
+		}
+	}
+	cur_offset -= PAGE_SIZE;
+	kr = vm_map_wire_and_extract(wire_map,
+				     wire_addr + cur_offset,
+				     VM_PROT_DEFAULT,
+				     TRUE,
+				     &wire_ppnum);
+	assert(kr == KERN_SUCCESS);
+	printf("FBDP_TEST_WIRE_AND_EXTRACT: re-wire kr=0x%x "
+	       "user[%p:0x%llx:0x%x] wire[%p:0x%llx:0x%x]\n",
+	       kr,
+	       user_map, user_addr + cur_offset, user_ppnum,
+	       wire_map, wire_addr + cur_offset, wire_ppnum);
+	if (kr != KERN_SUCCESS ||
+	    wire_ppnum == 0 ||
+	    wire_ppnum != user_ppnum) {
+		panic("FBDP_TEST_WIRE_AND_EXTRACT: FAIL\n");
+	}
+	
+	printf("FBDP_TEST_WIRE_AND_EXTRACT: PASS\n");
+#endif /* FBDP_TEST_WIRE_AND_EXTRACT */
+
+	vm_pageout_continue();
+
+	/*
+	 * Unreached code!
+	 *
+	 * The vm_pageout_continue() call above never returns, so the code below is never
+	 * executed.  We take advantage of this to declare several DTrace VM related probe
+	 * points that our kernel doesn't have an analog for.  These are probe points that
+	 * exist in Solaris and are in the DTrace documentation, so people may have written
+	 * scripts that use them.  Declaring the probe points here means their scripts will
+	 * compile and execute which we want for portability of the scripts, but since this
+	 * section of code is never reached, the probe points will simply never fire.  Yes,
+	 * this is basically a hack.  The problem is the DTrace probe points were chosen with
+	 * Solaris specific VM events in mind, not portability to different VM implementations.
+	 */
+
+	DTRACE_VM2(execfree, int, 1, (uint64_t *), NULL);
+	DTRACE_VM2(execpgin, int, 1, (uint64_t *), NULL);
+	DTRACE_VM2(execpgout, int, 1, (uint64_t *), NULL);
+	DTRACE_VM2(pgswapin, int, 1, (uint64_t *), NULL);
+	DTRACE_VM2(pgswapout, int, 1, (uint64_t *), NULL);
+	DTRACE_VM2(swapin, int, 1, (uint64_t *), NULL);
+	DTRACE_VM2(swapout, int, 1, (uint64_t *), NULL);
+	/*NOTREACHED*/
+}
+
+
+
+int vm_compressor_thread_count = 2;
+
+kern_return_t
+vm_pageout_internal_start(void)
+{
+	kern_return_t	result;
+	int		i;
+	host_basic_info_data_t hinfo;
+	int		thread_count;
+
+
+	if (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
+		mach_msg_type_number_t count = HOST_BASIC_INFO_COUNT;
+#define BSD_HOST 1
+		host_info((host_t)BSD_HOST, HOST_BASIC_INFO, (host_info_t)&hinfo, &count);
+
+		assert(hinfo.max_cpus > 0);
+
+		if (vm_compressor_thread_count >= hinfo.max_cpus)
+			vm_compressor_thread_count = hinfo.max_cpus - 1;
+		if (vm_compressor_thread_count <= 0)
+			vm_compressor_thread_count = 1;
+		else if (vm_compressor_thread_count > MAX_COMPRESSOR_THREAD_COUNT)
+			vm_compressor_thread_count = MAX_COMPRESSOR_THREAD_COUNT;
+
+		if (vm_compressor_immediate_preferred == TRUE) {
+			vm_pageout_immediate_chead = NULL;
+			vm_pageout_immediate_scratch_buf = kalloc(COMPRESSOR_SCRATCH_BUF_SIZE);
+
+			vm_compressor_thread_count = 1;
+		}
+		thread_count = vm_compressor_thread_count;
+
+		vm_pageout_queue_internal.pgo_maxlaundry = (vm_compressor_thread_count * 4) * VM_PAGE_LAUNDRY_MAX;
+	} else {
+		vm_compressor_thread_count = 0;
+		thread_count = 1;
+		vm_pageout_queue_internal.pgo_maxlaundry = VM_PAGE_LAUNDRY_MAX;
+	}
+
+	for (i = 0; i < vm_compressor_thread_count; i++) {
+		ciq[i].id = i;
+		ciq[i].q = &vm_pageout_queue_internal;
+		ciq[i].current_chead = NULL;
+		ciq[i].scratch_buf = kalloc(COMPRESSOR_SCRATCH_BUF_SIZE);
+	}		
+	for (i = 0; i < thread_count; i++) {
+		result = kernel_thread_start_priority((thread_continue_t)vm_pageout_iothread_internal, (void *)&ciq[i], BASEPRI_PREEMPT - 1, &vm_pageout_internal_iothread);
+
+		if (result == KERN_SUCCESS)
+			thread_deallocate(vm_pageout_internal_iothread);
+		else
+			break;
+	}
+	return result;
+}
+
+#if CONFIG_IOSCHED
+/*
+ * To support I/O Expedite for compressed files we mark the upls with special flags.
+ * The way decmpfs works is that we create a big upl which marks all the pages needed to
+ * represent the compressed file as busy. We tag this upl with the flag UPL_DECMP_REQ. Decmpfs
+ * then issues smaller I/Os for compressed I/Os, deflates them and puts the data into the pages
+ * being held in the big original UPL. We mark each of these smaller UPLs with the flag
+ * UPL_DECMP_REAL_IO. Any outstanding real I/O UPL is tracked by the big req upl using the
+ * decmp_io_upl field (in the upl structure). This link is protected in the forward direction
+ * by the req upl lock (the reverse link doesnt need synch. since we never inspect this link
+ * unless the real I/O upl is being destroyed).
+ */
+
+
+static void
+upl_set_decmp_info(upl_t upl, upl_t src_upl)
+{
+        assert((src_upl->flags & UPL_DECMP_REQ) != 0);
+
+        upl_lock(src_upl);
+        if (src_upl->decmp_io_upl) {
+                /*
+                 * If there is already an alive real I/O UPL, ignore this new UPL.
+                 * This case should rarely happen and even if it does, it just means
+                 * that we might issue a spurious expedite which the driver is expected
+                 * to handle.
+                 */ 
+                upl_unlock(src_upl);
+                return;
+        }
+        src_upl->decmp_io_upl = (void *)upl;
+        src_upl->ref_count++;
+
+        upl->flags |= UPL_DECMP_REAL_IO;
+        upl->decmp_io_upl = (void *)src_upl;
+	upl_unlock(src_upl);
+}
+#endif /* CONFIG_IOSCHED */  
+
+#if UPL_DEBUG
+int	upl_debug_enabled = 1;
+#else
+int	upl_debug_enabled = 0;
+#endif
+
+static upl_t
+upl_create(int type, int flags, upl_size_t size)
+{
+	upl_t	upl;
+	vm_size_t	page_field_size = 0;
+	int	upl_flags = 0;
+	vm_size_t	upl_size  = sizeof(struct upl);
+
+	size = round_page_32(size);
+
+	if (type & UPL_CREATE_LITE) {
+		page_field_size = (atop(size) + 7) >> 3;
+		page_field_size = (page_field_size + 3) & 0xFFFFFFFC;
+
+		upl_flags |= UPL_LITE;
+	}
+	if (type & UPL_CREATE_INTERNAL) {
+		upl_size += sizeof(struct upl_page_info) * atop(size);
+
+		upl_flags |= UPL_INTERNAL;
+	}
+	upl = (upl_t)kalloc(upl_size + page_field_size);
+
+	if (page_field_size)
+	        bzero((char *)upl + upl_size, page_field_size);
+
+	upl->flags = upl_flags | flags;
+	upl->src_object = NULL;
+	upl->kaddr = (vm_offset_t)0;
+	upl->size = 0;
+	upl->map_object = NULL;
+	upl->ref_count = 1;
+	upl->ext_ref_count = 0;
+	upl->highest_page = 0;
+	upl_lock_init(upl);
+	upl->vector_upl = NULL;
+	upl->associated_upl = NULL;
+#if CONFIG_IOSCHED
+	if (type & UPL_CREATE_IO_TRACKING) {
+		upl->upl_priority = proc_get_effective_thread_policy(current_thread(), TASK_POLICY_IO);
+	}
+	
+	upl->upl_reprio_info = 0;
+	upl->decmp_io_upl = 0;
+	if ((type & UPL_CREATE_INTERNAL) && (type & UPL_CREATE_EXPEDITE_SUP)) {
+		/* Only support expedite on internal UPLs */
+		thread_t        curthread = current_thread();
+		upl->upl_reprio_info = (uint64_t *)kalloc(sizeof(uint64_t) * atop(size));
+		bzero(upl->upl_reprio_info, (sizeof(uint64_t) * atop(size)));
+		upl->flags |= UPL_EXPEDITE_SUPPORTED;
+		if (curthread->decmp_upl != NULL) 
+			upl_set_decmp_info(upl, curthread->decmp_upl);
+	}
+#endif
+#if CONFIG_IOSCHED || UPL_DEBUG
+	if ((type & UPL_CREATE_IO_TRACKING) || upl_debug_enabled) {
+		upl->upl_creator = current_thread();
+		upl->uplq.next = 0;
+		upl->uplq.prev = 0;
+		upl->flags |= UPL_TRACKED_BY_OBJECT;
+	}
+#endif
+
+#if UPL_DEBUG
+	upl->ubc_alias1 = 0;
+	upl->ubc_alias2 = 0;
+
+	upl->upl_state = 0;
+	upl->upl_commit_index = 0;
+	bzero(&upl->upl_commit_records[0], sizeof(upl->upl_commit_records));
+
+	(void) OSBacktrace(&upl->upl_create_retaddr[0], UPL_DEBUG_STACK_FRAMES);
+#endif /* UPL_DEBUG */
+
+	return(upl);
+}
+
+static void
+upl_destroy(upl_t upl)
+{
+	int	page_field_size;  /* bit field in word size buf */
+        int	size;
+
+	if (upl->ext_ref_count) {
+		panic("upl(%p) ext_ref_count", upl);
+	}
+
+#if CONFIG_IOSCHED
+        if ((upl->flags & UPL_DECMP_REAL_IO) && upl->decmp_io_upl) {
+                upl_t src_upl;
+                src_upl = upl->decmp_io_upl;
+                assert((src_upl->flags & UPL_DECMP_REQ) != 0);
+                upl_lock(src_upl);
+                src_upl->decmp_io_upl = NULL;
+                upl_unlock(src_upl);
+                upl_deallocate(src_upl);
+        }
+#endif /* CONFIG_IOSCHED */
+
+#if CONFIG_IOSCHED || UPL_DEBUG
+	if ((upl->flags & UPL_TRACKED_BY_OBJECT) && !(upl->flags & UPL_VECTOR)) {
+		vm_object_t	object;
+
+		if (upl->flags & UPL_SHADOWED) {
+			object = upl->map_object->shadow;
+		} else {
+			object = upl->map_object;
+		}
+
+		vm_object_lock(object);
+		queue_remove(&object->uplq, upl, upl_t, uplq);
+		vm_object_activity_end(object);
+		vm_object_collapse(object, 0, TRUE);
+		vm_object_unlock(object);
+	}
+#endif
+	/*
+	 * drop a reference on the map_object whether or
+	 * not a pageout object is inserted
+	 */
+	if (upl->flags & UPL_SHADOWED)
+		vm_object_deallocate(upl->map_object);
+
+        if (upl->flags & UPL_DEVICE_MEMORY)
+	        size = PAGE_SIZE;
+	else
+	        size = upl->size;
+	page_field_size = 0;
+
+	if (upl->flags & UPL_LITE) {
+		page_field_size = ((size/PAGE_SIZE) + 7) >> 3;
+		page_field_size = (page_field_size + 3) & 0xFFFFFFFC;
+	}
+	upl_lock_destroy(upl);
+	upl->vector_upl = (vector_upl_t) 0xfeedbeef;
+
+#if CONFIG_IOSCHED
+	if (upl->flags & UPL_EXPEDITE_SUPPORTED)
+		kfree(upl->upl_reprio_info, sizeof(uint64_t) * (size/PAGE_SIZE));
+#endif
+
+	if (upl->flags & UPL_INTERNAL) {
+		kfree(upl,
+		      sizeof(struct upl) + 
+		      (sizeof(struct upl_page_info) * (size/PAGE_SIZE))
+		      + page_field_size);
+	} else {
+		kfree(upl, sizeof(struct upl) + page_field_size);
+	}
+}
+
+void
+upl_deallocate(upl_t upl)
+{
+	upl_lock(upl);
+	if (--upl->ref_count == 0) {
+		if(vector_upl_is_valid(upl))
+			vector_upl_deallocate(upl);
+		upl_unlock(upl);	
+		upl_destroy(upl);
+	}
+	else
+		upl_unlock(upl);
+}
+
+#if CONFIG_IOSCHED
+void
+upl_mark_decmp(upl_t upl)
+{
+	if (upl->flags & UPL_TRACKED_BY_OBJECT) {
+		upl->flags |= UPL_DECMP_REQ;
+		upl->upl_creator->decmp_upl = (void *)upl;
+	}	
+}
+
+void
+upl_unmark_decmp(upl_t upl)
+{
+	if(upl && (upl->flags & UPL_DECMP_REQ)) {
+		upl->upl_creator->decmp_upl = NULL;
+	}
+} 
+
+#endif /* CONFIG_IOSCHED */
+
+#define VM_PAGE_Q_BACKING_UP(q)		\
+        ((q)->pgo_laundry >= (((q)->pgo_maxlaundry * 8) / 10))
+
+boolean_t must_throttle_writes(void);
+
+boolean_t
+must_throttle_writes()
+{
+	if (VM_PAGE_Q_BACKING_UP(&vm_pageout_queue_external) &&
+	    vm_page_pageable_external_count > (AVAILABLE_NON_COMPRESSED_MEMORY * 6) / 10)
+		return (TRUE);
+
+	return (FALSE);
+}
+
+
+#if DEVELOPMENT || DEBUG
+/*/*
+ * Statistics about UPL enforcement of copy-on-write obligations.
+ */
+unsigned long upl_cow = 0;
+unsigned long upl_cow_again = 0;
+unsigned long upl_cow_pages = 0;
+unsigned long upl_cow_again_pages = 0;
+
+unsigned long iopl_cow = 0;
+unsigned long iopl_cow_pages = 0;
+#endif
+
+/*  
+ *	Routine:	vm_object_upl_request 
+ *	Purpose:	
+ *		Cause the population of a portion of a vm_object.
+ *		Depending on the nature of the request, the pages
+ *		returned may be contain valid data or be uninitialized.
+ *		A page list structure, listing the physical pages
+ *		will be returned upon request.
+ *		This function is called by the file system or any other
+ *		supplier of backing store to a pager.
+ *		IMPORTANT NOTE: The caller must still respect the relationship
+ *		between the vm_object and its backing memory object.  The
+ *		caller MUST NOT substitute changes in the backing file
+ *		without first doing a memory_object_lock_request on the 
+ *		target range unless it is know that the pages are not
+ *		shared with another entity at the pager level.
+ *		Copy_in_to:
+ *			if a page list structure is present
+ *			return the mapped physical pages, where a
+ *			page is not present, return a non-initialized
+ *			one.  If the no_sync bit is turned on, don't
+ *			call the pager unlock to synchronize with other
+ *			possible copies of the page. Leave pages busy
+ *			in the original object, if a page list structure
+ *			was specified.  When a commit of the page list
+ *			pages is done, the dirty bit will be set for each one.
+ *		Copy_out_from:
+ *			If a page list structure is present, return
+ *			all mapped pages.  Where a page does not exist
+ *			map a zero filled one. Leave pages busy in
+ *			the original object.  If a page list structure
+ *			is not specified, this call is a no-op. 
+ *
+ *		Note:  access of default pager objects has a rather interesting
+ *		twist.  The caller of this routine, presumably the file system
+ *		page cache handling code, will never actually make a request
+ *		against a default pager backed object.  Only the default
+ *		pager will make requests on backing store related vm_objects
+ *		In this way the default pager can maintain the relationship
+ *		between backing store files (abstract memory objects) and 
+ *		the vm_objects (cache objects), they support.
+ *
+ */
+
+__private_extern__ kern_return_t
+vm_object_upl_request(
+	vm_object_t		object,
+	vm_object_offset_t	offset,
+	upl_size_t		size,
+	upl_t			*upl_ptr,
+	upl_page_info_array_t	user_page_list,
+	unsigned int		*page_list_count,
+	upl_control_flags_t	cntrl_flags)
+{
+	vm_page_t		dst_page = VM_PAGE_NULL;
+	vm_object_offset_t	dst_offset;
+	upl_size_t		xfer_size;
+	unsigned int		size_in_pages;
+	boolean_t		dirty;
+	boolean_t		hw_dirty;
+	upl_t			upl = NULL;
+	unsigned int		entry;
+#if MACH_CLUSTER_STATS
+	boolean_t		encountered_lrp = FALSE;
+#endif
+	vm_page_t		alias_page = NULL;
+        int			refmod_state = 0;
+	wpl_array_t 		lite_list = NULL;
+	vm_object_t		last_copy_object;
+	struct	vm_page_delayed_work	dw_array[DEFAULT_DELAYED_WORK_LIMIT];
+	struct	vm_page_delayed_work	*dwp;
+	int			dw_count;
+	int			dw_limit;
+	int 			io_tracking_flag = 0;
+
+	if (cntrl_flags & ~UPL_VALID_FLAGS) {
+		/*
+		 * For forward compatibility's sake,
+		 * reject any unknown flag.
+		 */
+		return KERN_INVALID_VALUE;
+	}
+	if ( (!object->internal) && (object->paging_offset != 0) )
+		panic("vm_object_upl_request: external object with non-zero paging offset\n");
+	if (object->phys_contiguous)
+	        panic("vm_object_upl_request: contiguous object specified\n");
+
+
+	if (size > MAX_UPL_SIZE_BYTES)
+		size = MAX_UPL_SIZE_BYTES;
+
+	if ( (cntrl_flags & UPL_SET_INTERNAL) && page_list_count != NULL)
+	        *page_list_count = MAX_UPL_SIZE_BYTES >> PAGE_SHIFT;
+
+#if CONFIG_IOSCHED || UPL_DEBUG
+	if (object->io_tracking || upl_debug_enabled)
+		io_tracking_flag |= UPL_CREATE_IO_TRACKING;
+#endif
+#if CONFIG_IOSCHED
+	if (object->io_tracking)
+		io_tracking_flag |= UPL_CREATE_EXPEDITE_SUP;
+#endif
+
+	if (cntrl_flags & UPL_SET_INTERNAL) {
+	        if (cntrl_flags & UPL_SET_LITE) {
+
+			upl = upl_create(UPL_CREATE_INTERNAL | UPL_CREATE_LITE | io_tracking_flag, 0, size);
+
+			user_page_list = (upl_page_info_t *) (((uintptr_t)upl) + sizeof(struct upl));
+			lite_list = (wpl_array_t)
+					(((uintptr_t)user_page_list) + 
+					((size/PAGE_SIZE) * sizeof(upl_page_info_t)));
+			if (size == 0) {
+				user_page_list = NULL;
+				lite_list = NULL;
+			}
+		} else {
+		        upl = upl_create(UPL_CREATE_INTERNAL | io_tracking_flag, 0, size);
+
+			user_page_list = (upl_page_info_t *) (((uintptr_t)upl) + sizeof(struct upl));
+			if (size == 0) {
+				user_page_list = NULL;
+			}
+		}
+	} else {
+	        if (cntrl_flags & UPL_SET_LITE) {
+
+			upl = upl_create(UPL_CREATE_EXTERNAL | UPL_CREATE_LITE | io_tracking_flag, 0, size);
+
+			lite_list = (wpl_array_t) (((uintptr_t)upl) + sizeof(struct upl));
+			if (size == 0) {
+				lite_list = NULL;
+			}
+		} else {
+		        upl = upl_create(UPL_CREATE_EXTERNAL | io_tracking_flag, 0, size);
+		}
+	}
+	*upl_ptr = upl;
+	
+	if (user_page_list)
+	        user_page_list[0].device = FALSE;
+
+	if (cntrl_flags & UPL_SET_LITE) {
+	        upl->map_object = object;
+	} else {
+	        upl->map_object = vm_object_allocate(size);
+		/*
+		 * No neeed to lock the new object: nobody else knows
+		 * about it yet, so it's all ours so far.
+		 */
+		upl->map_object->shadow = object;
+		upl->map_object->pageout = TRUE;
+		upl->map_object->can_persist = FALSE;
+		upl->map_object->copy_strategy = MEMORY_OBJECT_COPY_NONE;
+		upl->map_object->vo_shadow_offset = offset;
+		upl->map_object->wimg_bits = object->wimg_bits;
+
+		VM_PAGE_GRAB_FICTITIOUS(alias_page);
+
+		upl->flags |= UPL_SHADOWED;
+	}
+	/*
+	 * ENCRYPTED SWAP:
+	 * Just mark the UPL as "encrypted" here.
+	 * We'll actually encrypt the pages later,
+	 * in upl_encrypt(), when the caller has
+	 * selected which pages need to go to swap.
+	 */
+	if (cntrl_flags & UPL_ENCRYPT)
+		upl->flags |= UPL_ENCRYPTED;
+
+	if (cntrl_flags & UPL_FOR_PAGEOUT)
+		upl->flags |= UPL_PAGEOUT;
+
+	vm_object_lock(object);
+	vm_object_activity_begin(object);
+
+	/*
+	 * we can lock in the paging_offset once paging_in_progress is set
+	 */
+	upl->size = size;
+	upl->offset = offset + object->paging_offset;
+
+#if CONFIG_IOSCHED || UPL_DEBUG
+	if (object->io_tracking || upl_debug_enabled) {
+		vm_object_activity_begin(object);
+		queue_enter(&object->uplq, upl, upl_t, uplq);
+	}
+#endif
+	if ((cntrl_flags & UPL_WILL_MODIFY) && object->copy != VM_OBJECT_NULL) {
+		/*
+		 * Honor copy-on-write obligations
+		 *
+		 * The caller is gathering these pages and
+		 * might modify their contents.  We need to
+		 * make sure that the copy object has its own
+		 * private copies of these pages before we let
+		 * the caller modify them.
+		 */
+		vm_object_update(object,
+				 offset,
+				 size,
+				 NULL,
+				 NULL,
+				 FALSE,	/* should_return */
+				 MEMORY_OBJECT_COPY_SYNC,
+				 VM_PROT_NO_CHANGE);
+#if DEVELOPMENT || DEBUG
+		upl_cow++;
+		upl_cow_pages += size >> PAGE_SHIFT;
+#endif
+	}
+	/*
+	 * remember which copy object we synchronized with
+	 */
+	last_copy_object = object->copy;
+	entry = 0;
+
+	xfer_size = size;
+	dst_offset = offset;
+	size_in_pages = size / PAGE_SIZE;
+
+	dwp = &dw_array[0];
+	dw_count = 0;
+	dw_limit = DELAYED_WORK_LIMIT(DEFAULT_DELAYED_WORK_LIMIT);
+
+	if (vm_page_free_count > (vm_page_free_target + size_in_pages) ||
+	    object->resident_page_count < ((MAX_UPL_SIZE_BYTES * 2) >> PAGE_SHIFT))
+		object->scan_collisions = 0;
+
+	if ((cntrl_flags & UPL_WILL_MODIFY) && must_throttle_writes() == TRUE) {
+		boolean_t	isSSD = FALSE;
+
+		vnode_pager_get_isSSD(object->pager, &isSSD);
+		vm_object_unlock(object);
+		
+		OSAddAtomic(size_in_pages, &vm_upl_wait_for_pages);
+
+		if (isSSD == TRUE)
+			delay(1000 * size_in_pages);
+		else
+			delay(5000 * size_in_pages);
+		OSAddAtomic(-size_in_pages, &vm_upl_wait_for_pages);
+
+		vm_object_lock(object);
+	}
+
+	while (xfer_size) {
+
+		dwp->dw_mask = 0;
+
+		if ((alias_page == NULL) && !(cntrl_flags & UPL_SET_LITE)) {
+			vm_object_unlock(object);
+			VM_PAGE_GRAB_FICTITIOUS(alias_page);
+			vm_object_lock(object);
+		}
+		if (cntrl_flags & UPL_COPYOUT_FROM) {
+		        upl->flags |= UPL_PAGE_SYNC_DONE;
+
+			if ( ((dst_page = vm_page_lookup(object, dst_offset)) == VM_PAGE_NULL) ||
+				dst_page->fictitious ||
+				dst_page->absent ||
+				dst_page->error ||
+			        dst_page->cleaning ||
+			        (VM_PAGE_WIRED(dst_page))) {
+				
+				if (user_page_list)
+					user_page_list[entry].phys_addr = 0;
+
+				goto try_next_page;
+			}
+			/*
+			 * grab this up front...
+			 * a high percentange of the time we're going to
+			 * need the hardware modification state a bit later
+			 * anyway... so we can eliminate an extra call into
+			 * the pmap layer by grabbing it here and recording it
+			 */
+			if (dst_page->pmapped)
+			        refmod_state = pmap_get_refmod(dst_page->phys_page);
+			else
+			        refmod_state = 0;
+
+			if ( (refmod_state & VM_MEM_REFERENCED) && dst_page->inactive ) {
+			        /*
+				 * page is on inactive list and referenced...
+				 * reactivate it now... this gets it out of the
+				 * way of vm_pageout_scan which would have to
+				 * reactivate it upon tripping over it
+				 */
+				dwp->dw_mask |= DW_vm_page_activate;
+			}
+			if (cntrl_flags & UPL_RET_ONLY_DIRTY) {
+			        /*
+				 * we're only asking for DIRTY pages to be returned
+				 */
+			        if (dst_page->laundry || !(cntrl_flags & UPL_FOR_PAGEOUT)) {
+				        /*
+					 * if we were the page stolen by vm_pageout_scan to be
+					 * cleaned (as opposed to a buddy being clustered in 
+					 * or this request is not being driven by a PAGEOUT cluster
+					 * then we only need to check for the page being dirty or
+					 * precious to decide whether to return it
+					 */
+				        if (dst_page->dirty || dst_page->precious || (refmod_state & VM_MEM_MODIFIED))
+					        goto check_busy;
+					goto dont_return;
+				}
+				/*
+				 * this is a request for a PAGEOUT cluster and this page
+				 * is merely along for the ride as a 'buddy'... not only
+				 * does it have to be dirty to be returned, but it also
+				 * can't have been referenced recently...
+				 */
+				if ( (hibernate_cleaning_in_progress == TRUE ||
+				      (!((refmod_state & VM_MEM_REFERENCED) || dst_page->reference) || dst_page->throttled)) && 
+				      ((refmod_state & VM_MEM_MODIFIED) || dst_page->dirty || dst_page->precious) ) {
+				        goto check_busy;
+				}
+dont_return:
+				/*
+				 * if we reach here, we're not to return
+				 * the page... go on to the next one
+				 */
+				if (dst_page->laundry == TRUE) {
+					/*
+					 * if we get here, the page is not 'cleaning' (filtered out above).
+					 * since it has been referenced, remove it from the laundry
+					 * so we don't pay the cost of an I/O to clean a page
+					 * we're just going to take back
+					 */
+					vm_page_lockspin_queues();
+
+					vm_pageout_steal_laundry(dst_page, TRUE);
+					vm_page_activate(dst_page);
+					
+					vm_page_unlock_queues();
+				}
+				if (user_page_list)
+				        user_page_list[entry].phys_addr = 0;
+
+				goto try_next_page;
+			}
+check_busy:			
+			if (dst_page->busy) {
+			        if (cntrl_flags & UPL_NOBLOCK) {	
+			        if (user_page_list)
+					        user_page_list[entry].phys_addr = 0;
+
+					goto try_next_page;
+				}
+				/*
+				 * someone else is playing with the
+				 * page.  We will have to wait.
+				 */
+				PAGE_SLEEP(object, dst_page, THREAD_UNINT);
+
+				continue;
+			}
+			/*
+			 * ENCRYPTED SWAP:
+			 * The caller is gathering this page and might
+			 * access its contents later on.  Decrypt the
+			 * page before adding it to the UPL, so that
+			 * the caller never sees encrypted data.
+			 */
+			if (! (cntrl_flags & UPL_ENCRYPT) && dst_page->encrypted) {
+			        int  was_busy;
+
+				/*
+				 * save the current state of busy
+				 * mark page as busy while decrypt
+				 * is in progress since it will drop
+				 * the object lock...
+				 */
+				was_busy = dst_page->busy;
+				dst_page->busy = TRUE;
+
+				vm_page_decrypt(dst_page, 0);
+				vm_page_decrypt_for_upl_counter++;
+				/*
+				 * restore to original busy state
+				 */
+				dst_page->busy = was_busy;
+			}
+			if (dst_page->pageout_queue == TRUE) {
+
+				vm_page_lockspin_queues();
+
+				if (dst_page->pageout_queue == TRUE) {
+					/*
+					 * we've buddied up a page for a clustered pageout
+					 * that has already been moved to the pageout
+					 * queue by pageout_scan... we need to remove
+					 * it from the queue and drop the laundry count
+					 * on that queue
+					 */
+					vm_pageout_throttle_up(dst_page);
+				}
+				vm_page_unlock_queues();
+			}
+#if MACH_CLUSTER_STATS
+			/*
+			 * pageout statistics gathering.  count
+			 * all the pages we will page out that
+			 * were not counted in the initial
+			 * vm_pageout_scan work
+			 */
+			if (dst_page->pageout)
+			        encountered_lrp = TRUE;
+			if ((dst_page->dirty ||	(dst_page->object->internal && dst_page->precious))) {
+			        if (encountered_lrp)
+				        CLUSTER_STAT(pages_at_higher_offsets++;)
+				else
+				        CLUSTER_STAT(pages_at_lower_offsets++;)
+			}
+#endif
+			hw_dirty = refmod_state & VM_MEM_MODIFIED;
+			dirty = hw_dirty ? TRUE : dst_page->dirty;
+
+			if (dst_page->phys_page > upl->highest_page)
+			        upl->highest_page = dst_page->phys_page;
+
+			assert (!pmap_is_noencrypt(dst_page->phys_page));
+
+			if (cntrl_flags & UPL_SET_LITE) {
+				unsigned int	pg_num;
+
+				pg_num = (unsigned int) ((dst_offset-offset)/PAGE_SIZE);
+				assert(pg_num == (dst_offset-offset)/PAGE_SIZE);
+				lite_list[pg_num>>5] |= 1 << (pg_num & 31);
+
+				if (hw_dirty)
+				        pmap_clear_modify(dst_page->phys_page);
+
+				/*
+				 * Mark original page as cleaning 
+				 * in place.
+				 */
+				dst_page->cleaning = TRUE;
+				dst_page->precious = FALSE;
+			} else {
+			        /*
+				 * use pageclean setup, it is more
+				 * convenient even for the pageout
+				 * cases here
+				 */
+			        vm_object_lock(upl->map_object);
+				vm_pageclean_setup(dst_page, alias_page, upl->map_object, size - xfer_size);
+				vm_object_unlock(upl->map_object);
+
+				alias_page->absent = FALSE;
+				alias_page = NULL;
+			}
+#if     MACH_PAGEMAP
+			/*
+			 * Record that this page has been 
+			 * written out
+			 */
+			vm_external_state_set(object->existence_map, dst_page->offset);
+#endif  /*MACH_PAGEMAP*/
+			if (dirty) {
+				SET_PAGE_DIRTY(dst_page, FALSE);
+			} else {
+				dst_page->dirty = FALSE;
+			}
+
+			if (!dirty)
+				dst_page->precious = TRUE;
+
+			if ( (cntrl_flags & UPL_ENCRYPT) ) {
+			        /*
+				 * ENCRYPTED SWAP:
+				 * We want to deny access to the target page
+				 * because its contents are about to be
+				 * encrypted and the user would be very
+				 * confused to see encrypted data instead
+				 * of their data.
+				 * We also set "encrypted_cleaning" to allow
+				 * vm_pageout_scan() to demote that page
+				 * from "adjacent/clean-in-place" to
+				 * "target/clean-and-free" if it bumps into
+				 * this page during its scanning while we're
+				 * still processing this cluster.
+				 */
+			        dst_page->busy = TRUE;
+				dst_page->encrypted_cleaning = TRUE;
+			}
+			if ( !(cntrl_flags & UPL_CLEAN_IN_PLACE) ) {
+				if ( !VM_PAGE_WIRED(dst_page))
+					dst_page->pageout = TRUE;
+			}
+		} else {
+			if ((cntrl_flags & UPL_WILL_MODIFY) && object->copy != last_copy_object) {
+				/*
+				 * Honor copy-on-write obligations
+				 *
+				 * The copy object has changed since we
+				 * last synchronized for copy-on-write.
+				 * Another copy object might have been
+				 * inserted while we released the object's
+				 * lock.  Since someone could have seen the
+				 * original contents of the remaining pages
+				 * through that new object, we have to
+				 * synchronize with it again for the remaining
+				 * pages only.  The previous pages are "busy"
+				 * so they can not be seen through the new
+				 * mapping.  The new mapping will see our
+				 * upcoming changes for those previous pages,
+				 * but that's OK since they couldn't see what
+				 * was there before.  It's just a race anyway
+				 * and there's no guarantee of consistency or
+				 * atomicity.  We just don't want new mappings
+				 * to see both the *before* and *after* pages.
+				 */
+				if (object->copy != VM_OBJECT_NULL) {
+					vm_object_update(
+						object,
+						dst_offset,/* current offset */
+						xfer_size, /* remaining size */
+						NULL,
+						NULL,
+						FALSE,	   /* should_return */
+						MEMORY_OBJECT_COPY_SYNC,
+						VM_PROT_NO_CHANGE);
+
+#if DEVELOPMENT || DEBUG
+					upl_cow_again++;
+					upl_cow_again_pages += xfer_size >> PAGE_SHIFT;
+#endif
+				}
+				/*
+				 * remember the copy object we synced with
+				 */
+				last_copy_object = object->copy;
+			}
+			dst_page = vm_page_lookup(object, dst_offset);
+			
+			if (dst_page != VM_PAGE_NULL) {
+
+				if ((cntrl_flags & UPL_RET_ONLY_ABSENT)) {
+					/*
+					 * skip over pages already present in the cache
+					 */
+					if (user_page_list)
+						user_page_list[entry].phys_addr = 0;
+
+					goto try_next_page;
+				}
+				if (dst_page->fictitious) {
+					panic("need corner case for fictitious page");
+				}
+
+				if (dst_page->busy || dst_page->cleaning) {
+					/*
+					 * someone else is playing with the
+					 * page.  We will have to wait.
+					 */
+					PAGE_SLEEP(object, dst_page, THREAD_UNINT);
+
+					continue;
+				}
+				if (dst_page->laundry) {
+					dst_page->pageout = FALSE;
+
+					vm_pageout_steal_laundry(dst_page, FALSE);
+				}
+			} else {
+				if (object->private) {
+					/* 
+					 * This is a nasty wrinkle for users 
+					 * of upl who encounter device or 
+					 * private memory however, it is 
+					 * unavoidable, only a fault can
+					 * resolve the actual backing
+					 * physical page by asking the
+					 * backing device.
+					 */
+					if (user_page_list)
+						user_page_list[entry].phys_addr = 0;
+
+					goto try_next_page;
+				}
+				if (object->scan_collisions) {
+					/*
+					 * the pageout_scan thread is trying to steal
+					 * pages from this object, but has run into our
+					 * lock... grab 2 pages from the head of the object...
+					 * the first is freed on behalf of pageout_scan, the
+					 * 2nd is for our own use... we use vm_object_page_grab
+					 * in both cases to avoid taking pages from the free
+					 * list since we are under memory pressure and our
+					 * lock on this object is getting in the way of
+					 * relieving it
+					 */
+					dst_page = vm_object_page_grab(object);
+
+					if (dst_page != VM_PAGE_NULL)
+						vm_page_release(dst_page);
+
+					dst_page = vm_object_page_grab(object);
+				}
+				if (dst_page == VM_PAGE_NULL) {
+					/*
+					 * need to allocate a page
+					 */
+					dst_page = vm_page_grab();
+				}
+				if (dst_page == VM_PAGE_NULL) {
+				        if ( (cntrl_flags & (UPL_RET_ONLY_ABSENT | UPL_NOBLOCK)) == (UPL_RET_ONLY_ABSENT | UPL_NOBLOCK)) {
+					       /*
+						* we don't want to stall waiting for pages to come onto the free list
+						* while we're already holding absent pages in this UPL
+						* the caller will deal with the empty slots
+						*/
+					        if (user_page_list)
+						        user_page_list[entry].phys_addr = 0;
+
+						goto try_next_page;
+					}
+				        /*
+					 * no pages available... wait
+					 * then try again for the same
+					 * offset...
+					 */
+					vm_object_unlock(object);
+					
+					OSAddAtomic(size_in_pages, &vm_upl_wait_for_pages);
+
+					VM_DEBUG_EVENT(vm_upl_page_wait, VM_UPL_PAGE_WAIT, DBG_FUNC_START, vm_upl_wait_for_pages, 0, 0, 0);
+
+					VM_PAGE_WAIT();
+					OSAddAtomic(-size_in_pages, &vm_upl_wait_for_pages);
+
+					VM_DEBUG_EVENT(vm_upl_page_wait, VM_UPL_PAGE_WAIT, DBG_FUNC_END, vm_upl_wait_for_pages, 0, 0, 0);
+
+					vm_object_lock(object);
+
+					continue;
+				}
+				vm_page_insert(dst_page, object, dst_offset);
+
+				dst_page->absent = TRUE;
+				dst_page->busy = FALSE;
+
+				if (cntrl_flags & UPL_RET_ONLY_ABSENT) {
+				        /*
+					 * if UPL_RET_ONLY_ABSENT was specified,
+					 * than we're definitely setting up a
+					 * upl for a clustered read/pagein 
+					 * operation... mark the pages as clustered
+					 * so upl_commit_range can put them on the
+					 * speculative list
+					 */
+				        dst_page->clustered = TRUE;
+
+					if ( !(cntrl_flags & UPL_FILE_IO))
+						VM_STAT_INCR(pageins);
+				}
+			}
+			/*
+			 * ENCRYPTED SWAP:
+			 */
+			if (cntrl_flags & UPL_ENCRYPT) {
+				/*
+				 * The page is going to be encrypted when we
+				 * get it from the pager, so mark it so.
+				 */
+				dst_page->encrypted = TRUE;
+			} else {
+				/*
+				 * Otherwise, the page will not contain
+				 * encrypted data.
+				 */
+				dst_page->encrypted = FALSE;
+			}
+			dst_page->overwriting = TRUE;
+
+			if (dst_page->pmapped) {
+			        if ( !(cntrl_flags & UPL_FILE_IO))
+				        /*
+					 * eliminate all mappings from the
+					 * original object and its prodigy
+					 */
+				        refmod_state = pmap_disconnect(dst_page->phys_page);
+				else
+				        refmod_state = pmap_get_refmod(dst_page->phys_page);
+			} else
+			        refmod_state = 0;
+
+			hw_dirty = refmod_state & VM_MEM_MODIFIED;
+			dirty = hw_dirty ? TRUE : dst_page->dirty;
+
+			if (cntrl_flags & UPL_SET_LITE) {
+				unsigned int	pg_num;
+
+				pg_num = (unsigned int) ((dst_offset-offset)/PAGE_SIZE);
+				assert(pg_num == (dst_offset-offset)/PAGE_SIZE);
+				lite_list[pg_num>>5] |= 1 << (pg_num & 31);
+
+				if (hw_dirty)
+				        pmap_clear_modify(dst_page->phys_page);
+
+				/*
+				 * Mark original page as cleaning 
+				 * in place.
+				 */
+				dst_page->cleaning = TRUE;
+				dst_page->precious = FALSE;
+			} else {
+				/*
+				 * use pageclean setup, it is more
+				 * convenient even for the pageout
+				 * cases here
+				 */
+			        vm_object_lock(upl->map_object);
+				vm_pageclean_setup(dst_page, alias_page, upl->map_object, size - xfer_size);
+			        vm_object_unlock(upl->map_object);
+
+				alias_page->absent = FALSE;
+				alias_page = NULL;
+			}
+
+			if (cntrl_flags & UPL_REQUEST_SET_DIRTY) {
+				upl->flags &= ~UPL_CLEAR_DIRTY;
+				upl->flags |= UPL_SET_DIRTY;
+				dirty = TRUE;
+				upl->flags |= UPL_SET_DIRTY;
+			} else if (cntrl_flags & UPL_CLEAN_IN_PLACE) {
+				/*
+				 * clean in place for read implies
+				 * that a write will be done on all
+				 * the pages that are dirty before
+				 * a upl commit is done.  The caller
+				 * is obligated to preserve the
+				 * contents of all pages marked dirty
+				 */
+				upl->flags |= UPL_CLEAR_DIRTY;
+			}
+			dst_page->dirty = dirty;
+
+			if (!dirty)
+				dst_page->precious = TRUE;
+
+			if ( !VM_PAGE_WIRED(dst_page)) {
+			        /*
+				 * deny access to the target page while
+				 * it is being worked on
+				 */
+				dst_page->busy = TRUE;
+			} else
+				dwp->dw_mask |= DW_vm_page_wire;
+
+			/*
+			 * We might be about to satisfy a fault which has been
+			 * requested. So no need for the "restart" bit.
+			 */
+			dst_page->restart = FALSE;
+			if (!dst_page->absent && !(cntrl_flags & UPL_WILL_MODIFY)) {
+			        /*
+				 * expect the page to be used
+				 */
+				dwp->dw_mask |= DW_set_reference;
+			}
+			if (cntrl_flags & UPL_PRECIOUS) {
+				if (dst_page->object->internal) {
+					SET_PAGE_DIRTY(dst_page, FALSE);
+					dst_page->precious = FALSE;
+				} else {
+					dst_page->precious = TRUE;
+				}
+			} else {
+				dst_page->precious = FALSE;
+			}
+		}
+		if (dst_page->busy)
+			upl->flags |= UPL_HAS_BUSY;
+
+		if (dst_page->phys_page > upl->highest_page)
+		        upl->highest_page = dst_page->phys_page;
+		assert (!pmap_is_noencrypt(dst_page->phys_page));
+		if (user_page_list) {
+			user_page_list[entry].phys_addr = dst_page->phys_page;
+			user_page_list[entry].pageout	= dst_page->pageout;
+			user_page_list[entry].absent	= dst_page->absent;
+			user_page_list[entry].dirty	= dst_page->dirty;
+			user_page_list[entry].precious	= dst_page->precious;
+			user_page_list[entry].device	= FALSE;
+			user_page_list[entry].needed    = FALSE;
+			if (dst_page->clustered == TRUE)
+			        user_page_list[entry].speculative = dst_page->speculative;
+			else
+			        user_page_list[entry].speculative = FALSE;
+			user_page_list[entry].cs_validated = dst_page->cs_validated;
+			user_page_list[entry].cs_tainted = dst_page->cs_tainted;
+			user_page_list[entry].cs_nx = dst_page->cs_nx;
+			user_page_list[entry].mark      = FALSE;
+		}
+	        /*
+		 * if UPL_RET_ONLY_ABSENT is set, then
+		 * we are working with a fresh page and we've
+		 * just set the clustered flag on it to
+		 * indicate that it was drug in as part of a
+		 * speculative cluster... so leave it alone
+		 */
+		if ( !(cntrl_flags & UPL_RET_ONLY_ABSENT)) {
+		        /*
+			 * someone is explicitly grabbing this page...
+			 * update clustered and speculative state
+			 * 
+			 */
+			if (dst_page->clustered)
+				VM_PAGE_CONSUME_CLUSTERED(dst_page);
+		}
+try_next_page:
+		if (dwp->dw_mask) {
+			if (dwp->dw_mask & DW_vm_page_activate)
+				VM_STAT_INCR(reactivations);
+
+			VM_PAGE_ADD_DELAYED_WORK(dwp, dst_page, dw_count);
+
+			if (dw_count >= dw_limit) {
+				vm_page_do_delayed_work(object, UPL_MEMORY_TAG(cntrl_flags), &dw_array[0], dw_count);
+
+				dwp = &dw_array[0];
+				dw_count = 0;
+			}
+		}
+		entry++;
+		dst_offset += PAGE_SIZE_64;
+		xfer_size -= PAGE_SIZE;
+	}
+	if (dw_count)
+		vm_page_do_delayed_work(object, UPL_MEMORY_TAG(cntrl_flags), &dw_array[0], dw_count);
+
+	if (alias_page != NULL) {
+		VM_PAGE_FREE(alias_page);
+	}
+
+	if (page_list_count != NULL) {
+	        if (upl->flags & UPL_INTERNAL)
+			*page_list_count = 0;
+		else if (*page_list_count > entry)
+			*page_list_count = entry;
+	}
+#if UPL_DEBUG
+	upl->upl_state = 1;
+#endif
+	vm_object_unlock(object);
+
+	return KERN_SUCCESS;
+}
+
+/*  
+ *	Routine:	vm_object_super_upl_request
+ *	Purpose:	
+ *		Cause the population of a portion of a vm_object
+ *		in much the same way as memory_object_upl_request.
+ *		Depending on the nature of the request, the pages
+ *		returned may be contain valid data or be uninitialized.
+ *		However, the region may be expanded up to the super
+ *		cluster size provided.
+ */
+
+__private_extern__ kern_return_t
+vm_object_super_upl_request(
+	vm_object_t object,
+	vm_object_offset_t	offset,
+	upl_size_t		size,
+	upl_size_t		super_cluster,
+	upl_t			*upl,
+	upl_page_info_t		*user_page_list,
+	unsigned int		*page_list_count,
+	upl_control_flags_t	cntrl_flags)
+{
+	if (object->paging_offset > offset  || ((cntrl_flags & UPL_VECTOR)==UPL_VECTOR))
+		return KERN_FAILURE;
+
+	assert(object->paging_in_progress);
+	offset = offset - object->paging_offset;
+
+	if (super_cluster > size) {
+
+		vm_object_offset_t	base_offset;
+		upl_size_t		super_size;
+		vm_object_size_t	super_size_64;
+
+		base_offset = (offset & ~((vm_object_offset_t) super_cluster - 1));
+		super_size = (offset + size) > (base_offset + super_cluster) ? super_cluster<<1 : super_cluster;
+		super_size_64 = ((base_offset + super_size) > object->vo_size) ? (object->vo_size - base_offset) : super_size;
+		super_size = (upl_size_t) super_size_64;
+		assert(super_size == super_size_64);
+
+		if (offset > (base_offset + super_size)) {
+		        panic("vm_object_super_upl_request: Missed target pageout"
+			      " %#llx,%#llx, %#x, %#x, %#x, %#llx\n",
+			      offset, base_offset, super_size, super_cluster,
+			      size, object->paging_offset);
+		}
+		/*
+		 * apparently there is a case where the vm requests a
+		 * page to be written out who's offset is beyond the
+		 * object size
+		 */
+		if ((offset + size) > (base_offset + super_size)) {
+		        super_size_64 = (offset + size) - base_offset;
+			super_size = (upl_size_t) super_size_64;
+			assert(super_size == super_size_64);
+		}
+
+		offset = base_offset;
+		size = super_size;
+	}
+	return vm_object_upl_request(object, offset, size, upl, user_page_list, page_list_count, cntrl_flags);
+}
+
+
+kern_return_t
+vm_map_create_upl(
+	vm_map_t		map,
+	vm_map_address_t	offset,
+	upl_size_t		*upl_size,
+	upl_t			*upl,
+	upl_page_info_array_t	page_list,
+	unsigned int		*count,
+	upl_control_flags_t	*flags)
+{
+	vm_map_entry_t		entry;
+	upl_control_flags_t	caller_flags;
+	int			force_data_sync;
+	int			sync_cow_data;
+	vm_object_t		local_object;
+	vm_map_offset_t		local_offset;
+	vm_map_offset_t		local_start;
+	kern_return_t		ret;
+
+	caller_flags = *flags;
+
+	if (caller_flags & ~UPL_VALID_FLAGS) {
+		/*
+		 * For forward compatibility's sake,
+		 * reject any unknown flag.
+		 */
+		return KERN_INVALID_VALUE;
+	}
+	force_data_sync = (caller_flags & UPL_FORCE_DATA_SYNC);
+	sync_cow_data = !(caller_flags & UPL_COPYOUT_FROM);
+
+	if (upl == NULL)
+		return KERN_INVALID_ARGUMENT;
+
+REDISCOVER_ENTRY:
+	vm_map_lock_read(map);
+
+	if (!vm_map_lookup_entry(map, offset, &entry)) {
+		vm_map_unlock_read(map);
+		return KERN_FAILURE;
+	}
+
+	if ((entry->vme_end - offset) < *upl_size) {
+		*upl_size = (upl_size_t) (entry->vme_end - offset);
+		assert(*upl_size == entry->vme_end - offset);
+	}
+
+	if (caller_flags & UPL_QUERY_OBJECT_TYPE) {
+		*flags = 0;
+
+		if (!entry->is_sub_map &&
+		    VME_OBJECT(entry) != VM_OBJECT_NULL) {
+			if (VME_OBJECT(entry)->private)
+				*flags = UPL_DEV_MEMORY;
+
+			if (VME_OBJECT(entry)->phys_contiguous)
+				*flags |= UPL_PHYS_CONTIG;
+		}
+		vm_map_unlock_read(map);
+		return KERN_SUCCESS;
+	}
+
+	if (entry->is_sub_map) {
+		vm_map_t	submap;
+
+		submap = VME_SUBMAP(entry);
+		local_start = entry->vme_start;
+		local_offset = VME_OFFSET(entry);
+
+		vm_map_reference(submap);
+		vm_map_unlock_read(map);
+
+		ret = vm_map_create_upl(submap, 
+					local_offset + (offset - local_start), 
+					upl_size, upl, page_list, count, flags);
+		vm_map_deallocate(submap);
+
+		return ret;
+	}
+
+	if (VME_OBJECT(entry) == VM_OBJECT_NULL ||
+	    !VME_OBJECT(entry)->phys_contiguous) {
+		if (*upl_size > MAX_UPL_SIZE_BYTES)
+			*upl_size = MAX_UPL_SIZE_BYTES;
+	}
+
+	/*
+	 *      Create an object if necessary.
+	 */
+	if (VME_OBJECT(entry) == VM_OBJECT_NULL) {
+
+		if (vm_map_lock_read_to_write(map))
+			goto REDISCOVER_ENTRY;
+
+		VME_OBJECT_SET(entry,
+			       vm_object_allocate((vm_size_t)
+						  (entry->vme_end -
+						   entry->vme_start)));
+		VME_OFFSET_SET(entry, 0);
+
+		vm_map_lock_write_to_read(map);
+	}
+
+	if (!(caller_flags & UPL_COPYOUT_FROM) &&
+	    !(entry->protection & VM_PROT_WRITE)) {
+		vm_map_unlock_read(map);
+		return KERN_PROTECTION_FAILURE;
+	}
+
+	local_object = VME_OBJECT(entry);
+	assert(local_object != VM_OBJECT_NULL);
+
+	if (*upl_size != 0 &&
+	    local_object->vo_size > *upl_size && /* partial UPL */
+	    entry->wired_count == 0 && /* No COW for entries that are wired */
+	    (map->pmap != kernel_pmap) && /* alias checks */
+	    (vm_map_entry_should_cow_for_true_share(entry) /* case 1 */
+	     ||
+	     (!entry->needs_copy &&	/* case 2 */
+	      local_object->internal &&
+	      (local_object->copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC) &&
+	      local_object->ref_count > 1))) {
+		vm_prot_t	prot;
+
+		/*
+		 * Case 1:
+		 * Set up the targeted range for copy-on-write to avoid
+		 * applying true_share/copy_delay to the entire object.
+		 *
+		 * Case 2:
+		 * This map entry covers only part of an internal
+		 * object.  There could be other map entries covering
+		 * other areas of this object and some of these map
+		 * entries could be marked as "needs_copy", which
+		 * assumes that the object is COPY_SYMMETRIC.
+		 * To avoid marking this object as COPY_DELAY and
+		 * "true_share", let's shadow it and mark the new
+		 * (smaller) object as "true_share" and COPY_DELAY.
+		 */
+
+		if (vm_map_lock_read_to_write(map)) {
+			goto REDISCOVER_ENTRY;
+		}
+		vm_map_lock_assert_exclusive(map);
+		assert(VME_OBJECT(entry) == local_object);
+
+		vm_map_clip_start(map,
+				  entry,
+				  vm_map_trunc_page(offset,
+						    VM_MAP_PAGE_MASK(map)));
+		vm_map_clip_end(map,
+				entry,
+				vm_map_round_page(offset + *upl_size,
+						  VM_MAP_PAGE_MASK(map)));
+		if ((entry->vme_end - offset) < *upl_size) {
+			*upl_size = (upl_size_t) (entry->vme_end - offset);
+			assert(*upl_size == entry->vme_end - offset);
+		}
+
+		prot = entry->protection & ~VM_PROT_WRITE;
+		if (override_nx(map, VME_ALIAS(entry)) && prot)
+			prot |= VM_PROT_EXECUTE;
+		vm_object_pmap_protect(local_object,
+				       VME_OFFSET(entry),
+				       entry->vme_end - entry->vme_start,
+				       ((entry->is_shared ||
+					 map->mapped_in_other_pmaps)
+					? PMAP_NULL
+					: map->pmap),
+				       entry->vme_start,
+				       prot);
+
+		assert(entry->wired_count == 0);
+
+		/*
+		 * Lock the VM object and re-check its status: if it's mapped
+		 * in another address space, we could still be racing with
+		 * another thread holding that other VM map exclusively.
+		 */
+		vm_object_lock(local_object);
+		if (local_object->true_share) {
+			/* object is already in proper state: no COW needed */
+			assert(local_object->copy_strategy !=
+			       MEMORY_OBJECT_COPY_SYMMETRIC);
+		} else {
+			/* not true_share: ask for copy-on-write below */
+			assert(local_object->copy_strategy ==
+			       MEMORY_OBJECT_COPY_SYMMETRIC);
+			entry->needs_copy = TRUE;
+		}
+		vm_object_unlock(local_object);
+
+		vm_map_lock_write_to_read(map);
+	}
+
+	if (entry->needs_copy)  {
+		/*
+		 * Honor copy-on-write for COPY_SYMMETRIC
+		 * strategy.
+		 */
+		vm_map_t		local_map;
+		vm_object_t		object;
+		vm_object_offset_t	new_offset;
+		vm_prot_t		prot;
+		boolean_t		wired;
+		vm_map_version_t	version;
+		vm_map_t		real_map;
+		vm_prot_t		fault_type;
+
+		local_map = map;
+
+		if (caller_flags & UPL_COPYOUT_FROM) {
+			fault_type = VM_PROT_READ | VM_PROT_COPY;
+			vm_counters.create_upl_extra_cow++;
+			vm_counters.create_upl_extra_cow_pages +=
+				(entry->vme_end - entry->vme_start) / PAGE_SIZE;
+		} else {
+			fault_type = VM_PROT_WRITE;
+		}
+		if (vm_map_lookup_locked(&local_map,
+					 offset, fault_type,
+					 OBJECT_LOCK_EXCLUSIVE,
+					 &version, &object,
+					 &new_offset, &prot, &wired,
+					 NULL,
+					 &real_map) != KERN_SUCCESS) {
+			if (fault_type == VM_PROT_WRITE) {
+				vm_counters.create_upl_lookup_failure_write++;
+			} else {
+				vm_counters.create_upl_lookup_failure_copy++;
+			}
+			vm_map_unlock_read(local_map);
+			return KERN_FAILURE;
+		}
+		if (real_map != map)
+			vm_map_unlock(real_map);
+		vm_map_unlock_read(local_map);
+
+		vm_object_unlock(object);
+
+		goto REDISCOVER_ENTRY;
+	}
+
+	if (sync_cow_data &&
+	    (VME_OBJECT(entry)->shadow ||
+	     VME_OBJECT(entry)->copy)) {
+		local_object = VME_OBJECT(entry);
+		local_start = entry->vme_start;
+		local_offset = VME_OFFSET(entry);
+
+		vm_object_reference(local_object);
+		vm_map_unlock_read(map);
+
+		if (local_object->shadow && local_object->copy) {
+			vm_object_lock_request(local_object->shadow,
+					       ((vm_object_offset_t)
+						((offset - local_start) +
+						 local_offset) +
+						local_object->vo_shadow_offset),
+					       *upl_size, FALSE, 
+					       MEMORY_OBJECT_DATA_SYNC,
+					       VM_PROT_NO_CHANGE);
+		}
+		sync_cow_data = FALSE;
+		vm_object_deallocate(local_object);
+
+		goto REDISCOVER_ENTRY;
+	}
+	if (force_data_sync) {
+		local_object = VME_OBJECT(entry);
+		local_start = entry->vme_start;
+		local_offset = VME_OFFSET(entry);
+
+		vm_object_reference(local_object);
+		vm_map_unlock_read(map);
+
+		vm_object_lock_request(local_object,
+				       ((vm_object_offset_t)
+					((offset - local_start) +
+					 local_offset)),
+				       (vm_object_size_t)*upl_size,
+				       FALSE, 
+				       MEMORY_OBJECT_DATA_SYNC,
+				       VM_PROT_NO_CHANGE);
+
+		force_data_sync = FALSE;
+		vm_object_deallocate(local_object);
+
+		goto REDISCOVER_ENTRY;
+	}
+	if (VME_OBJECT(entry)->private)
+		*flags = UPL_DEV_MEMORY;
+	else
+		*flags = 0;
+
+	if (VME_OBJECT(entry)->phys_contiguous)
+		*flags |= UPL_PHYS_CONTIG;
+
+	local_object = VME_OBJECT(entry);
+	local_offset = VME_OFFSET(entry);
+	local_start = entry->vme_start;
+
+	vm_object_lock(local_object);
+
+	/*
+	 * Ensure that this object is "true_share" and "copy_delay" now,
+	 * while we're still holding the VM map lock.  After we unlock the map,
+	 * anything could happen to that mapping, including some copy-on-write
+	 * activity.  We need to make sure that the IOPL will point at the
+	 * same memory as the mapping.
+	 */
+	if (local_object->true_share) {
+		assert(local_object->copy_strategy !=
+		       MEMORY_OBJECT_COPY_SYMMETRIC);
+	} else if (local_object != kernel_object &&
+		   local_object != compressor_object &&
+		   !local_object->phys_contiguous) {
+#if VM_OBJECT_TRACKING_OP_TRUESHARE
+		if (!local_object->true_share &&
+		    vm_object_tracking_inited) {
+			void *bt[VM_OBJECT_TRACKING_BTDEPTH];
+			int num = 0;
+			num = OSBacktrace(bt,
+					  VM_OBJECT_TRACKING_BTDEPTH);
+			btlog_add_entry(vm_object_tracking_btlog,
+					local_object,
+					VM_OBJECT_TRACKING_OP_TRUESHARE,
+					bt,
+					num);
+		}
+#endif /* VM_OBJECT_TRACKING_OP_TRUESHARE */
+		local_object->true_share = TRUE;
+		if (local_object->copy_strategy ==
+		    MEMORY_OBJECT_COPY_SYMMETRIC) {
+			local_object->copy_strategy = MEMORY_OBJECT_COPY_DELAY;
+		}
+	}
+
+	vm_object_reference_locked(local_object);
+	vm_object_unlock(local_object);
+
+	vm_map_unlock_read(map);
+
+	ret = vm_object_iopl_request(local_object, 
+				     ((vm_object_offset_t)
+				      ((offset - local_start) + local_offset)),
+				     *upl_size,
+				     upl,
+				     page_list,
+				     count,
+				     caller_flags);
+	vm_object_deallocate(local_object);
+
+	return ret;
+}
+
+/*
+ * Internal routine to enter a UPL into a VM map.
+ * 
+ * JMM - This should just be doable through the standard
+ * vm_map_enter() API.
+ */
+kern_return_t
+vm_map_enter_upl(
+	vm_map_t		map, 
+	upl_t			upl, 
+	vm_map_offset_t		*dst_addr)
+{
+	vm_map_size_t	 	size;
+	vm_object_offset_t 	offset;
+	vm_map_offset_t		addr;
+	vm_page_t		m;
+	kern_return_t		kr;
+	int			isVectorUPL = 0, curr_upl=0;
+	upl_t			vector_upl = NULL;
+	vm_offset_t		vector_upl_dst_addr = 0;
+	vm_map_t		vector_upl_submap = NULL;
+	upl_offset_t 		subupl_offset = 0;
+	upl_size_t		subupl_size = 0;
+
+	if (upl == UPL_NULL)
+		return KERN_INVALID_ARGUMENT;
+
+	if((isVectorUPL = vector_upl_is_valid(upl))) {
+		int mapped=0,valid_upls=0;
+		vector_upl = upl;
+
+		upl_lock(vector_upl);
+		for(curr_upl=0; curr_upl < MAX_VECTOR_UPL_ELEMENTS; curr_upl++) {
+			upl =  vector_upl_subupl_byindex(vector_upl, curr_upl );
+			if(upl == NULL)
+				continue;
+			valid_upls++;
+			if (UPL_PAGE_LIST_MAPPED & upl->flags)
+				mapped++;
+		}
+
+		if(mapped) { 
+			if(mapped != valid_upls)
+				panic("Only %d of the %d sub-upls within the Vector UPL are alread mapped\n", mapped, valid_upls);
+			else {
+				upl_unlock(vector_upl);
+				return KERN_FAILURE;
+			}
+		}
+
+		kr = kmem_suballoc(map, &vector_upl_dst_addr, vector_upl->size, FALSE, VM_FLAGS_ANYWHERE, &vector_upl_submap);
+		if( kr != KERN_SUCCESS )
+			panic("Vector UPL submap allocation failed\n");
+		map = vector_upl_submap;
+		vector_upl_set_submap(vector_upl, vector_upl_submap, vector_upl_dst_addr);
+		curr_upl=0;
+	}
+	else
+		upl_lock(upl);
+
+process_upl_to_enter:
+	if(isVectorUPL){
+		if(curr_upl == MAX_VECTOR_UPL_ELEMENTS) {
+			*dst_addr = vector_upl_dst_addr;
+			upl_unlock(vector_upl);
+			return KERN_SUCCESS;
+		}
+		upl =  vector_upl_subupl_byindex(vector_upl, curr_upl++ );
+		if(upl == NULL)
+			goto process_upl_to_enter;
+
+		vector_upl_get_iostate(vector_upl, upl, &subupl_offset, &subupl_size);
+		*dst_addr = (vm_map_offset_t)(vector_upl_dst_addr + (vm_map_offset_t)subupl_offset);
+	} else {
+		/*
+		 * check to see if already mapped
+		 */
+		if (UPL_PAGE_LIST_MAPPED & upl->flags) {
+			upl_unlock(upl);
+			return KERN_FAILURE;
+		}
+	}
+	if ((!(upl->flags & UPL_SHADOWED)) &&
+	    ((upl->flags & UPL_HAS_BUSY) ||
+	     !((upl->flags & (UPL_DEVICE_MEMORY | UPL_IO_WIRE)) || (upl->map_object->phys_contiguous)))) {
+
+		vm_object_t 		object;
+		vm_page_t		alias_page;
+		vm_object_offset_t	new_offset;
+		unsigned int		pg_num;
+		wpl_array_t 		lite_list;
+
+		if (upl->flags & UPL_INTERNAL) {
+			lite_list = (wpl_array_t) 
+				((((uintptr_t)upl) + sizeof(struct upl))
+				 + ((upl->size/PAGE_SIZE) * sizeof(upl_page_info_t)));
+		} else {
+		        lite_list = (wpl_array_t)(((uintptr_t)upl) + sizeof(struct upl));
+		}
+		object = upl->map_object;
+		upl->map_object = vm_object_allocate(upl->size);
+
+		vm_object_lock(upl->map_object);
+
+		upl->map_object->shadow = object;
+		upl->map_object->pageout = TRUE;
+		upl->map_object->can_persist = FALSE;
+		upl->map_object->copy_strategy = MEMORY_OBJECT_COPY_NONE;
+		upl->map_object->vo_shadow_offset = upl->offset - object->paging_offset;
+		upl->map_object->wimg_bits = object->wimg_bits;
+		offset = upl->map_object->vo_shadow_offset;
+		new_offset = 0;
+		size = upl->size;
+
+		upl->flags |= UPL_SHADOWED;
+
+		while (size) {
+			pg_num = (unsigned int) (new_offset / PAGE_SIZE);
+			assert(pg_num == new_offset / PAGE_SIZE);
+
+			if (lite_list[pg_num>>5] & (1 << (pg_num & 31))) {
+
+				VM_PAGE_GRAB_FICTITIOUS(alias_page);
+
+				vm_object_lock(object);
+
+				m = vm_page_lookup(object, offset);
+				if (m == VM_PAGE_NULL) {
+				        panic("vm_upl_map: page missing\n");
+				}
+
+				/*
+				 * Convert the fictitious page to a private 
+				 * shadow of the real page.
+				 */
+				assert(alias_page->fictitious);
+				alias_page->fictitious = FALSE;
+				alias_page->private = TRUE;
+				alias_page->pageout = TRUE;
+				/*
+				 * since m is a page in the upl it must
+				 * already be wired or BUSY, so it's
+				 * safe to assign the underlying physical
+				 * page to the alias
+				 */
+				alias_page->phys_page = m->phys_page;
+
+			        vm_object_unlock(object);
+
+				vm_page_lockspin_queues();
+				vm_page_wire(alias_page, VM_KERN_MEMORY_NONE, TRUE);
+				vm_page_unlock_queues();
+				
+				/*
+				 * ENCRYPTED SWAP:
+				 * The virtual page ("m") has to be wired in some way
+				 * here or its physical page ("m->phys_page") could
+				 * be recycled at any time.
+				 * Assuming this is enforced by the caller, we can't
+				 * get an encrypted page here.  Since the encryption
+				 * key depends on the VM page's "pager" object and
+				 * the "paging_offset", we couldn't handle 2 pageable
+				 * VM pages (with different pagers and paging_offsets)
+				 * sharing the same physical page:  we could end up
+				 * encrypting with one key (via one VM page) and
+				 * decrypting with another key (via the alias VM page).
+				 */
+				ASSERT_PAGE_DECRYPTED(m);
+
+				vm_page_insert_wired(alias_page, upl->map_object, new_offset, VM_KERN_MEMORY_NONE);
+
+				assert(!alias_page->wanted);
+				alias_page->busy = FALSE;
+				alias_page->absent = FALSE;
+			}
+			size -= PAGE_SIZE;
+			offset += PAGE_SIZE_64;
+			new_offset += PAGE_SIZE_64;
+		}
+		vm_object_unlock(upl->map_object);
+	}
+	if (upl->flags & UPL_SHADOWED)
+	        offset = 0;
+	else
+	        offset = upl->offset - upl->map_object->paging_offset;
+
+	size = upl->size;
+	
+	vm_object_reference(upl->map_object);
+
+	if(!isVectorUPL) {
+		*dst_addr = 0;
+		/*
+	 	* NEED A UPL_MAP ALIAS
+	 	*/
+		kr = vm_map_enter(map, dst_addr, (vm_map_size_t)size, (vm_map_offset_t) 0,
+				  VM_FLAGS_ANYWHERE | VM_MAKE_TAG(VM_KERN_MEMORY_OSFMK), 
+				  upl->map_object, offset, FALSE,
+				  VM_PROT_DEFAULT, VM_PROT_ALL, VM_INHERIT_DEFAULT);
+
+		if (kr != KERN_SUCCESS) {
+			upl_unlock(upl);
+			return(kr);
+		}
+	}
+	else {
+		kr = vm_map_enter(map, dst_addr, (vm_map_size_t)size, (vm_map_offset_t) 0,
+				  VM_FLAGS_FIXED | VM_MAKE_TAG(VM_KERN_MEMORY_OSFMK),
+				  upl->map_object, offset, FALSE,
+				  VM_PROT_DEFAULT, VM_PROT_ALL, VM_INHERIT_DEFAULT);
+		if(kr)
+			panic("vm_map_enter failed for a Vector UPL\n");
+	}
+	vm_object_lock(upl->map_object);
+
+	for (addr = *dst_addr; size > 0; size -= PAGE_SIZE, addr += PAGE_SIZE) {
+		m = vm_page_lookup(upl->map_object, offset);
+
+		if (m) {
+			m->pmapped = TRUE;
+
+			/* CODE SIGNING ENFORCEMENT: page has been wpmapped, 
+			 * but only in kernel space. If this was on a user map,
+			 * we'd have to set the wpmapped bit. */
+			/* m->wpmapped = TRUE; */
+			assert(map->pmap == kernel_pmap);
+	
+			PMAP_ENTER(map->pmap, addr, m, VM_PROT_DEFAULT, VM_PROT_NONE, 0, TRUE);
+		}
+		offset += PAGE_SIZE_64;
+	}
+	vm_object_unlock(upl->map_object);
+
+	/*
+	 * hold a reference for the mapping
+	 */
+	upl->ref_count++;
+	upl->flags |= UPL_PAGE_LIST_MAPPED;
+	upl->kaddr = (vm_offset_t) *dst_addr;
+	assert(upl->kaddr == *dst_addr);
+	
+	if(isVectorUPL)
+		goto process_upl_to_enter;
+
+	upl_unlock(upl);
+
+	return KERN_SUCCESS;
+}
+	
+/*
+ * Internal routine to remove a UPL mapping from a VM map.
+ *
+ * XXX - This should just be doable through a standard
+ * vm_map_remove() operation.  Otherwise, implicit clean-up
+ * of the target map won't be able to correctly remove
+ * these (and release the reference on the UPL).  Having
+ * to do this means we can't map these into user-space
+ * maps yet.
+ */
+kern_return_t
+vm_map_remove_upl(
+	vm_map_t	map, 
+	upl_t		upl)
+{
+	vm_address_t	addr;
+	upl_size_t	size;
+	int		isVectorUPL = 0, curr_upl = 0;
+	upl_t		vector_upl = NULL;
+
+	if (upl == UPL_NULL)
+		return KERN_INVALID_ARGUMENT;
+
+	if((isVectorUPL = vector_upl_is_valid(upl))) {
+		int 	unmapped=0, valid_upls=0;
+		vector_upl = upl;
+		upl_lock(vector_upl);
+		for(curr_upl=0; curr_upl < MAX_VECTOR_UPL_ELEMENTS; curr_upl++) {
+			upl =  vector_upl_subupl_byindex(vector_upl, curr_upl );
+			if(upl == NULL)
+				continue;
+			valid_upls++;
+			if (!(UPL_PAGE_LIST_MAPPED & upl->flags))
+				unmapped++;
+		}
+
+		if(unmapped) {
+			if(unmapped != valid_upls)
+				panic("%d of the %d sub-upls within the Vector UPL is/are not mapped\n", unmapped, valid_upls);
+			else {
+				upl_unlock(vector_upl);
+				return KERN_FAILURE;
+			}
+		}
+		curr_upl=0;
+	}
+	else
+		upl_lock(upl);
+
+process_upl_to_remove:
+	if(isVectorUPL) {
+		if(curr_upl == MAX_VECTOR_UPL_ELEMENTS) {
+			vm_map_t v_upl_submap;
+			vm_offset_t v_upl_submap_dst_addr;
+			vector_upl_get_submap(vector_upl, &v_upl_submap, &v_upl_submap_dst_addr);
+
+			vm_map_remove(map, v_upl_submap_dst_addr, v_upl_submap_dst_addr + vector_upl->size, VM_MAP_NO_FLAGS);
+			vm_map_deallocate(v_upl_submap);
+			upl_unlock(vector_upl);
+			return KERN_SUCCESS;
+		}
+
+		upl =  vector_upl_subupl_byindex(vector_upl, curr_upl++ );
+		if(upl == NULL)
+			goto process_upl_to_remove;	
+	}
+
+	if (upl->flags & UPL_PAGE_LIST_MAPPED) {
+		addr = upl->kaddr;
+		size = upl->size;
+
+		assert(upl->ref_count > 1);
+		upl->ref_count--;		/* removing mapping ref */
+
+		upl->flags &= ~UPL_PAGE_LIST_MAPPED;
+		upl->kaddr = (vm_offset_t) 0;
+		
+		if(!isVectorUPL) {
+			upl_unlock(upl);
+		
+			vm_map_remove(
+				map,
+				vm_map_trunc_page(addr,
+						  VM_MAP_PAGE_MASK(map)),
+				vm_map_round_page(addr + size,
+						  VM_MAP_PAGE_MASK(map)),
+				VM_MAP_NO_FLAGS);
+		
+			return KERN_SUCCESS;
+		}
+		else {
+			/*
+			* If it's a Vectored UPL, we'll be removing the entire
+			* submap anyways, so no need to remove individual UPL
+			* element mappings from within the submap
+			*/	
+			goto process_upl_to_remove;
+		}
+	}
+	upl_unlock(upl);
+
+	return KERN_FAILURE;
+}
+
+kern_return_t
+upl_commit_range(
+	upl_t			upl, 
+	upl_offset_t		offset, 
+	upl_size_t		size,
+	int			flags,
+	upl_page_info_t		*page_list,
+	mach_msg_type_number_t	count,
+	boolean_t		*empty) 
+{
+	upl_size_t		xfer_size, subupl_size = size;
+	vm_object_t		shadow_object;
+	vm_object_t		object;
+	vm_object_offset_t	target_offset;
+	upl_offset_t		subupl_offset = offset;
+	int			entry;
+	wpl_array_t 		lite_list;
+	int			occupied;
+	int			clear_refmod = 0;
+	int			pgpgout_count = 0;
+	struct	vm_page_delayed_work	dw_array[DEFAULT_DELAYED_WORK_LIMIT];
+	struct	vm_page_delayed_work	*dwp;
+	int			dw_count;
+	int			dw_limit;
+	int			isVectorUPL = 0;
+	upl_t			vector_upl = NULL;
+	boolean_t		should_be_throttled = FALSE;
+
+	vm_page_t		nxt_page = VM_PAGE_NULL;
+	int			fast_path_possible = 0;
+	int			fast_path_full_commit = 0;
+	int			throttle_page = 0;
+	int			unwired_count = 0;
+	int			local_queue_count = 0;
+	queue_head_t		local_queue;
+
+	*empty = FALSE;
+
+	if (upl == UPL_NULL)
+		return KERN_INVALID_ARGUMENT;
+
+	if (count == 0)
+		page_list = NULL;
+
+	if((isVectorUPL = vector_upl_is_valid(upl))) {
+		vector_upl = upl;
+		upl_lock(vector_upl);
+	}
+	else
+		upl_lock(upl);
+
+process_upl_to_commit:
+
+	if(isVectorUPL) {
+		size = subupl_size;
+		offset = subupl_offset;
+		if(size == 0) {
+			upl_unlock(vector_upl);
+			return KERN_SUCCESS;
+		}
+		upl =  vector_upl_subupl_byoffset(vector_upl, &offset, &size);
+		if(upl == NULL) {
+			upl_unlock(vector_upl);
+			return KERN_FAILURE;
+		}
+		page_list = UPL_GET_INTERNAL_PAGE_LIST_SIMPLE(upl);
+		subupl_size -= size;
+		subupl_offset += size;
+	}
+
+#if UPL_DEBUG
+	if (upl->upl_commit_index < UPL_DEBUG_COMMIT_RECORDS) {
+		(void) OSBacktrace(&upl->upl_commit_records[upl->upl_commit_index].c_retaddr[0], UPL_DEBUG_STACK_FRAMES);
+		
+		upl->upl_commit_records[upl->upl_commit_index].c_beg = offset;
+		upl->upl_commit_records[upl->upl_commit_index].c_end = (offset + size);
+
+		upl->upl_commit_index++;
+	}
+#endif
+	if (upl->flags & UPL_DEVICE_MEMORY)
+		xfer_size = 0;
+	else if ((offset + size) <= upl->size)
+	        xfer_size = size;
+	else {
+		if(!isVectorUPL)
+			upl_unlock(upl);
+		else {
+			upl_unlock(vector_upl);
+		}
+		return KERN_FAILURE;
+	}
+	if (upl->flags & UPL_SET_DIRTY)
+		flags |= UPL_COMMIT_SET_DIRTY;
+	if (upl->flags & UPL_CLEAR_DIRTY)
+	        flags |= UPL_COMMIT_CLEAR_DIRTY;
+
+	if (upl->flags & UPL_INTERNAL)
+		lite_list = (wpl_array_t) ((((uintptr_t)upl) + sizeof(struct upl))
+					   + ((upl->size/PAGE_SIZE) * sizeof(upl_page_info_t)));
+	else
+		lite_list = (wpl_array_t) (((uintptr_t)upl) + sizeof(struct upl));
+
+	object = upl->map_object;
+
+	if (upl->flags & UPL_SHADOWED) {
+	        vm_object_lock(object);
+		shadow_object = object->shadow;
+	} else {
+		shadow_object = object;
+	}
+	entry = offset/PAGE_SIZE;
+	target_offset = (vm_object_offset_t)offset;
+
+	assert(!(target_offset & PAGE_MASK));
+	assert(!(xfer_size & PAGE_MASK));
+
+	if (upl->flags & UPL_KERNEL_OBJECT)
+		vm_object_lock_shared(shadow_object);
+	else
+		vm_object_lock(shadow_object);
+
+	if (upl->flags & UPL_ACCESS_BLOCKED) {
+		assert(shadow_object->blocked_access);
+		shadow_object->blocked_access = FALSE;
+		vm_object_wakeup(object, VM_OBJECT_EVENT_UNBLOCKED);
+	}
+
+	if (shadow_object->code_signed) {
+		/*
+		 * CODE SIGNING:
+		 * If the object is code-signed, do not let this UPL tell
+		 * us if the pages are valid or not.  Let the pages be
+		 * validated by VM the normal way (when they get mapped or
+		 * copied).
+		 */
+		flags &= ~UPL_COMMIT_CS_VALIDATED;
+	}
+	if (! page_list) {
+		/*
+		 * No page list to get the code-signing info from !?
+		 */
+		flags &= ~UPL_COMMIT_CS_VALIDATED;
+	}
+	if (!VM_DYNAMIC_PAGING_ENABLED(memory_manager_default) && shadow_object->internal)
+		should_be_throttled = TRUE;
+
+	dwp = &dw_array[0];
+	dw_count = 0;
+	dw_limit = DELAYED_WORK_LIMIT(DEFAULT_DELAYED_WORK_LIMIT);
+
+	if ((upl->flags & UPL_IO_WIRE) &&
+	    !(flags & UPL_COMMIT_FREE_ABSENT) &&
+	    !isVectorUPL &&
+	    shadow_object->purgable != VM_PURGABLE_VOLATILE &&
+	    shadow_object->purgable != VM_PURGABLE_EMPTY) {
+
+		if (!queue_empty(&shadow_object->memq)) {
+			queue_init(&local_queue);
+			if (size == shadow_object->vo_size) {
+				nxt_page = (vm_page_t)queue_first(&shadow_object->memq);
+				fast_path_full_commit = 1;
+			}
+			fast_path_possible = 1;
+
+			if (!VM_DYNAMIC_PAGING_ENABLED(memory_manager_default) && shadow_object->internal &&
+			    (shadow_object->purgable == VM_PURGABLE_DENY ||
+			     shadow_object->purgable == VM_PURGABLE_NONVOLATILE ||
+			     shadow_object->purgable == VM_PURGABLE_VOLATILE)) {
+				throttle_page = 1;
+			}
+		}
+	}
+
+	while (xfer_size) {
+		vm_page_t	t, m;
+
+		dwp->dw_mask = 0;
+		clear_refmod = 0;
+
+		m = VM_PAGE_NULL;
+
+		if (upl->flags & UPL_LITE) {
+			unsigned int	pg_num;
+
+			if (nxt_page != VM_PAGE_NULL) {
+				m = nxt_page;
+				nxt_page = (vm_page_t)queue_next(&nxt_page->listq);
+				target_offset = m->offset;
+			}
+			pg_num = (unsigned int) (target_offset/PAGE_SIZE);
+			assert(pg_num == target_offset/PAGE_SIZE);
+
+			if (lite_list[pg_num>>5] & (1 << (pg_num & 31))) {
+			        lite_list[pg_num>>5] &= ~(1 << (pg_num & 31));
+
+				if (!(upl->flags & UPL_KERNEL_OBJECT) && m == VM_PAGE_NULL)
+					m = vm_page_lookup(shadow_object, target_offset + (upl->offset - shadow_object->paging_offset));
+			} else
+				m = NULL;
+		}
+		if (upl->flags & UPL_SHADOWED) {
+			if ((t = vm_page_lookup(object, target_offset))	!= VM_PAGE_NULL) {
+
+				t->pageout = FALSE;
+
+				VM_PAGE_FREE(t);
+
+				if (!(upl->flags & UPL_KERNEL_OBJECT) && m == VM_PAGE_NULL)
+					m = vm_page_lookup(shadow_object, target_offset + object->vo_shadow_offset);
+			}
+		}
+		if (m == VM_PAGE_NULL)
+			goto commit_next_page;
+
+		if (m->compressor) {
+			assert(m->busy);
+
+			dwp->dw_mask |= (DW_clear_busy | DW_PAGE_WAKEUP);
+			goto commit_next_page;
+		}
+
+		if (flags & UPL_COMMIT_CS_VALIDATED) {
+			/*
+			 * CODE SIGNING:
+			 * Set the code signing bits according to
+			 * what the UPL says they should be.
+			 */
+			m->cs_validated = page_list[entry].cs_validated;
+			m->cs_tainted = page_list[entry].cs_tainted;
+			m->cs_nx = page_list[entry].cs_nx;
+		}
+		if (flags & UPL_COMMIT_WRITTEN_BY_KERNEL)
+			m->written_by_kernel = TRUE;
+
+		if (upl->flags & UPL_IO_WIRE) {
+
+			if (page_list)
+				page_list[entry].phys_addr = 0;
+
+			if (flags & UPL_COMMIT_SET_DIRTY) {
+				SET_PAGE_DIRTY(m, FALSE);
+			} else if (flags & UPL_COMMIT_CLEAR_DIRTY) {
+				m->dirty = FALSE;
+
+				if (! (flags & UPL_COMMIT_CS_VALIDATED) &&
+				    m->cs_validated && !m->cs_tainted) {
+					/*
+					 * CODE SIGNING:
+					 * This page is no longer dirty
+					 * but could have been modified,
+					 * so it will need to be
+					 * re-validated.
+					 */
+					if (m->slid) {
+						panic("upl_commit_range(%p): page %p was slid\n",
+						      upl, m);
+					}
+					assert(!m->slid);
+					m->cs_validated = FALSE;
+#if DEVELOPMENT || DEBUG
+					vm_cs_validated_resets++;
+#endif
+					pmap_disconnect(m->phys_page);
+				}
+				clear_refmod |= VM_MEM_MODIFIED;
+			}
+			if (upl->flags & UPL_ACCESS_BLOCKED) {
+				/*
+				 * We blocked access to the pages in this UPL.
+				 * Clear the "busy" bit and wake up any waiter
+				 * for this page.
+				 */
+				dwp->dw_mask |= (DW_clear_busy | DW_PAGE_WAKEUP);
+			}
+			if (fast_path_possible) {
+				assert(m->object->purgable != VM_PURGABLE_EMPTY);
+				assert(m->object->purgable != VM_PURGABLE_VOLATILE);
+				if (m->absent) {
+					assert(m->wire_count == 0);
+					assert(m->busy);
+
+					m->absent = FALSE;
+					dwp->dw_mask |= (DW_clear_busy | DW_PAGE_WAKEUP);
+				} else {
+					if (m->wire_count == 0)
+						panic("wire_count == 0, m = %p, obj = %p\n", m, shadow_object);
+
+					/*
+					 * XXX FBDP need to update some other
+					 * counters here (purgeable_wired_count)
+					 * (ledgers), ...
+					 */
+					assert(m->wire_count);
+					m->wire_count--;
+
+					if (m->wire_count == 0)
+						unwired_count++;
+				}
+				if (m->wire_count == 0) {
+					queue_enter(&local_queue, m, vm_page_t, pageq);
+					local_queue_count++;
+
+					if (throttle_page) {
+						m->throttled = TRUE;
+					} else {
+						if (flags & UPL_COMMIT_INACTIVATE)
+							m->inactive = TRUE;
+						else
+							m->active = TRUE;
+					}
+				}
+			} else {
+				if (flags & UPL_COMMIT_INACTIVATE) {
+					dwp->dw_mask |= DW_vm_page_deactivate_internal;
+					clear_refmod |= VM_MEM_REFERENCED;
+				}
+				if (m->absent) {
+					if (flags & UPL_COMMIT_FREE_ABSENT)
+						dwp->dw_mask |= DW_vm_page_free;
+					else {
+						m->absent = FALSE;
+						dwp->dw_mask |= (DW_clear_busy | DW_PAGE_WAKEUP);
+
+						if ( !(dwp->dw_mask & DW_vm_page_deactivate_internal))
+							dwp->dw_mask |= DW_vm_page_activate;
+					}
+				} else
+					dwp->dw_mask |= DW_vm_page_unwire;
+			}
+			goto commit_next_page;
+		}
+		assert(!m->compressor);
+
+		if (page_list)
+			page_list[entry].phys_addr = 0;
+
+		/*
+		 * make sure to clear the hardware
+		 * modify or reference bits before
+		 * releasing the BUSY bit on this page
+		 * otherwise we risk losing a legitimate
+		 * change of state
+		 */
+		if (flags & UPL_COMMIT_CLEAR_DIRTY) {
+			m->dirty = FALSE;
+
+			clear_refmod |= VM_MEM_MODIFIED;
+		}
+		if (m->laundry)
+			dwp->dw_mask |= DW_vm_pageout_throttle_up;
+
+		if (VM_PAGE_WIRED(m))
+			m->pageout = FALSE;
+		
+		if (! (flags & UPL_COMMIT_CS_VALIDATED) &&
+		    m->cs_validated && !m->cs_tainted) {
+			/*
+			 * CODE SIGNING:
+			 * This page is no longer dirty
+			 * but could have been modified,
+			 * so it will need to be
+			 * re-validated.
+			 */
+			if (m->slid) {
+				panic("upl_commit_range(%p): page %p was slid\n",
+				      upl, m);
+			}
+			assert(!m->slid);
+			m->cs_validated = FALSE;
+#if DEVELOPMENT || DEBUG
+			vm_cs_validated_resets++;
+#endif
+			pmap_disconnect(m->phys_page);
+		}
+		if (m->overwriting) {
+			/*
+			 * the (COPY_OUT_FROM == FALSE) request_page_list case
+			 */
+			if (m->busy) {
+#if CONFIG_PHANTOM_CACHE
+				if (m->absent && !m->object->internal)
+					dwp->dw_mask |= DW_vm_phantom_cache_update;
+#endif
+				m->absent = FALSE;
+
+				dwp->dw_mask |= DW_clear_busy;
+			} else {
+				/*
+				 * alternate (COPY_OUT_FROM == FALSE) page_list case
+				 * Occurs when the original page was wired
+				 * at the time of the list request
+				 */
+				assert(VM_PAGE_WIRED(m));
+
+				dwp->dw_mask |= DW_vm_page_unwire; /* reactivates */
+			}
+			m->overwriting = FALSE;
+		}
+		if (m->encrypted_cleaning == TRUE) {
+			m->encrypted_cleaning = FALSE;
+
+			dwp->dw_mask |= DW_clear_busy | DW_PAGE_WAKEUP;
+		}
+		m->cleaning = FALSE;
+
+		if (m->pageout) {
+			/* 
+			 * With the clean queue enabled, UPL_PAGEOUT should
+			 * no longer set the pageout bit. It's pages now go 
+			 * to the clean queue.
+			 */
+			assert(!(flags & UPL_PAGEOUT));
+
+			m->pageout = FALSE;
+#if MACH_CLUSTER_STATS
+			if (m->wanted) vm_pageout_target_collisions++;
+#endif
+			if ((flags & UPL_COMMIT_SET_DIRTY) ||
+			    (m->pmapped && (pmap_disconnect(m->phys_page) & VM_MEM_MODIFIED))) {
+				/*
+				 * page was re-dirtied after we started
+				 * the pageout... reactivate it since 
+				 * we don't know whether the on-disk
+				 * copy matches what is now in memory
+				 */
+				SET_PAGE_DIRTY(m, FALSE);
+				
+				dwp->dw_mask |= DW_vm_page_activate | DW_PAGE_WAKEUP;
+
+				if (upl->flags & UPL_PAGEOUT) {
+					CLUSTER_STAT(vm_pageout_target_page_dirtied++;)
+					VM_STAT_INCR(reactivations);
+					DTRACE_VM2(pgrec, int, 1, (uint64_t *), NULL);
+				}
+			} else {
+				/*
+				 * page has been successfully cleaned
+				 * go ahead and free it for other use
+				 */
+				if (m->object->internal) {
+					DTRACE_VM2(anonpgout, int, 1, (uint64_t *), NULL);
+				} else {
+					DTRACE_VM2(fspgout, int, 1, (uint64_t *), NULL);
+				}
+				m->dirty = FALSE;
+				m->busy = TRUE;
+
+				dwp->dw_mask |= DW_vm_page_free;
+			}
+			goto commit_next_page;
+		}
+#if MACH_CLUSTER_STATS
+		if (m->wpmapped)
+			m->dirty = pmap_is_modified(m->phys_page);
+
+		if (m->dirty)   vm_pageout_cluster_dirtied++;
+		else            vm_pageout_cluster_cleaned++;
+		if (m->wanted)  vm_pageout_cluster_collisions++;
+#endif
+		/*
+		 * It is a part of the semantic of COPYOUT_FROM
+		 * UPLs that a commit implies cache sync
+		 * between the vm page and the backing store
+		 * this can be used to strip the precious bit
+		 * as well as clean
+		 */
+		if ((upl->flags & UPL_PAGE_SYNC_DONE) || (flags & UPL_COMMIT_CLEAR_PRECIOUS))
+			m->precious = FALSE;
+
+		if (flags & UPL_COMMIT_SET_DIRTY) {
+			SET_PAGE_DIRTY(m, FALSE);
+		} else {
+			m->dirty = FALSE;
+		}
+
+		/* with the clean queue on, move *all* cleaned pages to the clean queue */
+		if (hibernate_cleaning_in_progress == FALSE && !m->dirty && (upl->flags & UPL_PAGEOUT)) {
+			pgpgout_count++;
+
+			VM_STAT_INCR(pageouts);
+			DTRACE_VM2(pgout, int, 1, (uint64_t *), NULL);
+
+			dwp->dw_mask |= DW_enqueue_cleaned;
+			vm_pageout_enqueued_cleaned_from_inactive_dirty++;
+		} else if (should_be_throttled == TRUE && !m->active && !m->inactive && !m->speculative && !m->throttled) {
+			/*
+			 * page coming back in from being 'frozen'...
+			 * it was dirty before it was frozen, so keep it so
+			 * the vm_page_activate will notice that it really belongs
+			 * on the throttle queue and put it there
+			 */
+			SET_PAGE_DIRTY(m, FALSE);
+			dwp->dw_mask |= DW_vm_page_activate;
+
+		} else {
+			if ((flags & UPL_COMMIT_INACTIVATE) && !m->clustered && !m->speculative) {
+				dwp->dw_mask |= DW_vm_page_deactivate_internal;
+				clear_refmod |= VM_MEM_REFERENCED;
+			} else if (!m->active && !m->inactive && !m->speculative) {
+
+				if (m->clustered || (flags & UPL_COMMIT_SPECULATE))
+					dwp->dw_mask |= DW_vm_page_speculate;
+				else if (m->reference)
+					dwp->dw_mask |= DW_vm_page_activate;
+				else {
+					dwp->dw_mask |= DW_vm_page_deactivate_internal;
+					clear_refmod |= VM_MEM_REFERENCED;
+				}
+			}
+		}
+		if (upl->flags & UPL_ACCESS_BLOCKED) {
+			/*
+			 * We blocked access to the pages in this URL.
+			 * Clear the "busy" bit on this page before we
+			 * wake up any waiter.
+			 */
+			dwp->dw_mask |= DW_clear_busy;
+		}
+		/*
+		 * Wakeup any thread waiting for the page to be un-cleaning.
+		 */
+		dwp->dw_mask |= DW_PAGE_WAKEUP;
+
+commit_next_page:
+		if (clear_refmod)
+			pmap_clear_refmod(m->phys_page, clear_refmod);
+
+		target_offset += PAGE_SIZE_64;
+		xfer_size -= PAGE_SIZE;
+		entry++;
+
+		if (dwp->dw_mask) {
+			if (dwp->dw_mask & ~(DW_clear_busy | DW_PAGE_WAKEUP)) {
+				VM_PAGE_ADD_DELAYED_WORK(dwp, m, dw_count);
+
+				if (dw_count >= dw_limit) {
+					vm_page_do_delayed_work(shadow_object, VM_KERN_MEMORY_NONE, &dw_array[0], dw_count);
+			
+					dwp = &dw_array[0];
+					dw_count = 0;
+				}
+			} else {
+				if (dwp->dw_mask & DW_clear_busy)
+					m->busy = FALSE;
+
+				if (dwp->dw_mask & DW_PAGE_WAKEUP)
+					PAGE_WAKEUP(m);
+			}
+		}
+	}
+	if (dw_count)
+		vm_page_do_delayed_work(shadow_object, VM_KERN_MEMORY_NONE, &dw_array[0], dw_count);
+
+	if (fast_path_possible) {
+
+		assert(shadow_object->purgable != VM_PURGABLE_VOLATILE);
+		assert(shadow_object->purgable != VM_PURGABLE_EMPTY);
+
+		if (local_queue_count || unwired_count) {
+
+			if (local_queue_count) {
+				vm_page_t	first_local, last_local;
+				vm_page_t	first_target;
+				queue_head_t	*target_queue;
+
+				if (throttle_page)
+					target_queue = &vm_page_queue_throttled;
+				else {
+					if (flags & UPL_COMMIT_INACTIVATE) {
+						if (shadow_object->internal)
+							target_queue = &vm_page_queue_anonymous;
+						else
+							target_queue = &vm_page_queue_inactive;
+					} else
+						target_queue = &vm_page_queue_active;
+				}
+				/*
+				 * Transfer the entire local queue to a regular LRU page queues.
+				 */
+				first_local = (vm_page_t) queue_first(&local_queue);
+				last_local = (vm_page_t) queue_last(&local_queue);
+
+				vm_page_lockspin_queues();
+
+				first_target = (vm_page_t) queue_first(target_queue);
+
+				if (queue_empty(target_queue))
+					queue_last(target_queue) = (queue_entry_t) last_local;
+				else
+					queue_prev(&first_target->pageq) = (queue_entry_t) last_local;
+
+				queue_first(target_queue) = (queue_entry_t) first_local;
+				queue_prev(&first_local->pageq) = (queue_entry_t) target_queue;
+				queue_next(&last_local->pageq) = (queue_entry_t) first_target;
+
+				/*
+				 * Adjust the global page counts.
+				 */
+				if (throttle_page) {
+					vm_page_throttled_count += local_queue_count;
+				} else {
+					if (flags & UPL_COMMIT_INACTIVATE) {
+						if (shadow_object->internal)
+							vm_page_anonymous_count += local_queue_count;
+						vm_page_inactive_count += local_queue_count;
+
+						token_new_pagecount += local_queue_count;
+					} else
+						vm_page_active_count += local_queue_count;
+
+					if (shadow_object->internal)
+						vm_page_pageable_internal_count += local_queue_count;
+					else
+						vm_page_pageable_external_count += local_queue_count;
+				}
+			} else {
+				vm_page_lockspin_queues();
+			}
+			if (unwired_count) {	
+				vm_page_wire_count -= unwired_count;
+				VM_CHECK_MEMORYSTATUS;
+			}
+			vm_page_unlock_queues();
+
+			shadow_object->wired_page_count -= unwired_count;
+
+			if (!shadow_object->wired_page_count) {
+			    VM_OBJECT_UNWIRED(shadow_object);
+			}
+		}
+	}
+	occupied = 1;
+
+	if (upl->flags & UPL_DEVICE_MEMORY)  {
+		occupied = 0;
+	} else if (upl->flags & UPL_LITE) {
+		int	pg_num;
+		int	i;
+
+		occupied = 0;
+
+		if (!fast_path_full_commit) {
+			pg_num = upl->size/PAGE_SIZE;
+			pg_num = (pg_num + 31) >> 5;
+
+			for (i = 0; i < pg_num; i++) {
+				if (lite_list[i] != 0) {
+					occupied = 1;
+					break;
+				}
+			}
+		}
+	} else {
+		if (queue_empty(&upl->map_object->memq))
+			occupied = 0;
+	}
+	if (occupied == 0) {
+		/*
+		 * If this UPL element belongs to a Vector UPL and is
+		 * empty, then this is the right function to deallocate
+		 * it. So go ahead set the *empty variable. The flag
+		 * UPL_COMMIT_NOTIFY_EMPTY, from the caller's point of view
+		 * should be considered relevant for the Vector UPL and not
+		 * the internal UPLs.
+		 */
+		if ((upl->flags & UPL_COMMIT_NOTIFY_EMPTY) || isVectorUPL)
+			*empty = TRUE;
+
+		if (object == shadow_object && !(upl->flags & UPL_KERNEL_OBJECT)) {
+		        /*
+			 * this is not a paging object
+			 * so we need to drop the paging reference
+			 * that was taken when we created the UPL
+			 * against this object
+			 */
+			vm_object_activity_end(shadow_object);
+			vm_object_collapse(shadow_object, 0, TRUE);
+		} else {
+		         /*
+			  * we dontated the paging reference to
+			  * the map object... vm_pageout_object_terminate
+			  * will drop this reference
+			  */
+		}
+	}
+	vm_object_unlock(shadow_object);
+	if (object != shadow_object)
+	        vm_object_unlock(object);
+	
+	if(!isVectorUPL)
+		upl_unlock(upl);
+	else {
+		/* 
+		 * If we completed our operations on an UPL that is
+		 * part of a Vectored UPL and if empty is TRUE, then
+		 * we should go ahead and deallocate this UPL element. 
+		 * Then we check if this was the last of the UPL elements
+		 * within that Vectored UPL. If so, set empty to TRUE
+		 * so that in ubc_upl_commit_range or ubc_upl_commit, we
+		 * can go ahead and deallocate the Vector UPL too.
+		 */
+		if(*empty==TRUE) {
+			*empty = vector_upl_set_subupl(vector_upl, upl, 0);
+			upl_deallocate(upl);
+		}
+		goto process_upl_to_commit;
+	}
+
+	if (pgpgout_count) {
+		DTRACE_VM2(pgpgout, int, pgpgout_count, (uint64_t *), NULL);
+	}
+
+	return KERN_SUCCESS;
+}
+
+kern_return_t
+upl_abort_range(
+	upl_t			upl, 
+	upl_offset_t		offset, 
+	upl_size_t		size,
+	int			error,
+	boolean_t		*empty) 
+{
+	upl_page_info_t		*user_page_list = NULL;
+	upl_size_t		xfer_size, subupl_size = size;
+	vm_object_t		shadow_object;
+	vm_object_t		object;
+	vm_object_offset_t	target_offset;
+	upl_offset_t		subupl_offset = offset;
+	int			entry;
+	wpl_array_t 	 	lite_list;
+	int			occupied;
+	struct	vm_page_delayed_work	dw_array[DEFAULT_DELAYED_WORK_LIMIT];
+	struct	vm_page_delayed_work	*dwp;
+	int			dw_count;
+	int			dw_limit;
+	int			isVectorUPL = 0;
+	upl_t			vector_upl = NULL;
+
+	*empty = FALSE;
+
+	if (upl == UPL_NULL)
+		return KERN_INVALID_ARGUMENT;
+
+	if ( (upl->flags & UPL_IO_WIRE) && !(error & UPL_ABORT_DUMP_PAGES) )
+		return upl_commit_range(upl, offset, size, UPL_COMMIT_FREE_ABSENT, NULL, 0, empty);
+
+	if((isVectorUPL = vector_upl_is_valid(upl))) {
+		vector_upl = upl;
+		upl_lock(vector_upl);
+	}
+	else
+		upl_lock(upl);
+
+process_upl_to_abort:
+	if(isVectorUPL) {
+		size = subupl_size;
+		offset = subupl_offset;
+		if(size == 0) {
+			upl_unlock(vector_upl);
+			return KERN_SUCCESS;
+		}
+		upl =  vector_upl_subupl_byoffset(vector_upl, &offset, &size);
+		if(upl == NULL) {
+			upl_unlock(vector_upl);
+			return KERN_FAILURE;
+		}
+		subupl_size -= size;
+		subupl_offset += size;
+	}
+
+	*empty = FALSE;
+
+#if UPL_DEBUG
+	if (upl->upl_commit_index < UPL_DEBUG_COMMIT_RECORDS) {
+		(void) OSBacktrace(&upl->upl_commit_records[upl->upl_commit_index].c_retaddr[0], UPL_DEBUG_STACK_FRAMES);
+		
+		upl->upl_commit_records[upl->upl_commit_index].c_beg = offset;
+		upl->upl_commit_records[upl->upl_commit_index].c_end = (offset + size);
+		upl->upl_commit_records[upl->upl_commit_index].c_aborted = 1;
+
+		upl->upl_commit_index++;
+	}
+#endif
+	if (upl->flags & UPL_DEVICE_MEMORY)
+		xfer_size = 0;
+	else if ((offset + size) <= upl->size)
+	        xfer_size = size;
+	else {
+		if(!isVectorUPL)
+			upl_unlock(upl);
+		else {
+			upl_unlock(vector_upl);
+		}
+
+		return KERN_FAILURE;
+	}
+	if (upl->flags & UPL_INTERNAL) {
+		lite_list = (wpl_array_t) 
+			((((uintptr_t)upl) + sizeof(struct upl))
+			+ ((upl->size/PAGE_SIZE) * sizeof(upl_page_info_t)));
+
+		user_page_list = (upl_page_info_t *) (((uintptr_t)upl) + sizeof(struct upl));
+	} else {
+		lite_list = (wpl_array_t) 
+			(((uintptr_t)upl) + sizeof(struct upl));
+	}
+	object = upl->map_object;
+
+	if (upl->flags & UPL_SHADOWED) {
+	        vm_object_lock(object);
+		shadow_object = object->shadow;
+	} else
+		shadow_object = object;
+
+	entry = offset/PAGE_SIZE;
+	target_offset = (vm_object_offset_t)offset;
+
+	assert(!(target_offset & PAGE_MASK));
+	assert(!(xfer_size & PAGE_MASK));
+
+	if (upl->flags & UPL_KERNEL_OBJECT)
+		vm_object_lock_shared(shadow_object);
+	else
+		vm_object_lock(shadow_object);
+
+	if (upl->flags & UPL_ACCESS_BLOCKED) {
+		assert(shadow_object->blocked_access);
+		shadow_object->blocked_access = FALSE;
+		vm_object_wakeup(object, VM_OBJECT_EVENT_UNBLOCKED);
+	}
+
+	dwp = &dw_array[0];
+	dw_count = 0;
+	dw_limit = DELAYED_WORK_LIMIT(DEFAULT_DELAYED_WORK_LIMIT);
+
+	if ((error & UPL_ABORT_DUMP_PAGES) && (upl->flags & UPL_KERNEL_OBJECT))
+		panic("upl_abort_range: kernel_object being DUMPED");
+
+	while (xfer_size) {
+		vm_page_t	t, m;
+		unsigned int	pg_num;
+		boolean_t	needed;
+
+		pg_num = (unsigned int) (target_offset/PAGE_SIZE);
+		assert(pg_num == target_offset/PAGE_SIZE);
+
+		needed = FALSE;
+
+		if (user_page_list)
+			needed = user_page_list[pg_num].needed;
+
+		dwp->dw_mask = 0;
+		m = VM_PAGE_NULL;
+
+		if (upl->flags & UPL_LITE) {
+
+			if (lite_list[pg_num>>5] & (1 << (pg_num & 31))) {
+				lite_list[pg_num>>5] &= ~(1 << (pg_num & 31));
+
+				if ( !(upl->flags & UPL_KERNEL_OBJECT))
+					m = vm_page_lookup(shadow_object, target_offset +
+							   (upl->offset - shadow_object->paging_offset));
+			}
+		}
+		if (upl->flags & UPL_SHADOWED) {
+		        if ((t = vm_page_lookup(object, target_offset))	!= VM_PAGE_NULL) {
+			        t->pageout = FALSE;
+
+				VM_PAGE_FREE(t);
+
+				if (m == VM_PAGE_NULL)
+					m = vm_page_lookup(shadow_object, target_offset + object->vo_shadow_offset);
+			}
+		}
+		if ((upl->flags & UPL_KERNEL_OBJECT))
+			goto abort_next_page;
+
+		if (m != VM_PAGE_NULL) {
+
+			assert(!m->compressor);
+
+			if (m->absent) {
+			        boolean_t must_free = TRUE;
+
+				/*
+				 * COPYOUT = FALSE case
+				 * check for error conditions which must
+				 * be passed back to the pages customer
+				 */
+				if (error & UPL_ABORT_RESTART) {
+					m->restart = TRUE;
+					m->absent = FALSE;
+					m->unusual = TRUE;
+					must_free = FALSE;
+				} else if (error & UPL_ABORT_UNAVAILABLE) {
+					m->restart = FALSE;
+					m->unusual = TRUE;
+					must_free = FALSE;
+				} else if (error & UPL_ABORT_ERROR) {
+					m->restart = FALSE;
+					m->absent = FALSE;
+					m->error = TRUE;
+					m->unusual = TRUE;
+					must_free = FALSE;
+				}
+				if (m->clustered && needed == FALSE) {
+					/*
+					 * This page was a part of a speculative
+					 * read-ahead initiated by the kernel
+					 * itself.  No one is expecting this
+					 * page and no one will clean up its
+					 * error state if it ever becomes valid
+					 * in the future.
+					 * We have to free it here.
+					 */
+					must_free = TRUE;
+				}
+
+				/*
+				 * ENCRYPTED SWAP:
+				 * If the page was already encrypted,
+				 * we don't really need to decrypt it
+				 * now.  It will get decrypted later,
+				 * on demand, as soon as someone needs
+				 * to access its contents.
+				 */
+
+				m->cleaning = FALSE;
+				m->encrypted_cleaning = FALSE;
+
+				if (m->overwriting && !m->busy) {
+					/*
+					 * this shouldn't happen since
+					 * this is an 'absent' page, but
+					 * it doesn't hurt to check for
+					 * the 'alternate' method of 
+					 * stabilizing the page...
+					 * we will mark 'busy' to be cleared
+					 * in the following code which will
+					 * take care of the primary stabilzation
+					 * method (i.e. setting 'busy' to TRUE)
+					 */
+					dwp->dw_mask |= DW_vm_page_unwire;
+				}
+				m->overwriting = FALSE;
+
+				dwp->dw_mask |= (DW_clear_busy | DW_PAGE_WAKEUP);
+
+				if (must_free == TRUE)
+					dwp->dw_mask |= DW_vm_page_free;
+				else
+					dwp->dw_mask |= DW_vm_page_activate;
+			} else {
+			        /*                          
+				 * Handle the trusted pager throttle.
+				 */                     
+			        if (m->laundry)
+					dwp->dw_mask |= DW_vm_pageout_throttle_up;
+
+				if (upl->flags & UPL_ACCESS_BLOCKED) {
+					/*
+					 * We blocked access to the pages in this UPL.
+					 * Clear the "busy" bit and wake up any waiter
+					 * for this page.
+					 */
+					dwp->dw_mask |= DW_clear_busy;
+				}
+				if (m->overwriting) {
+					if (m->busy)
+						dwp->dw_mask |= DW_clear_busy;
+					else {
+						/*
+						 * deal with the 'alternate' method
+						 * of stabilizing the page...
+						 * we will either free the page
+						 * or mark 'busy' to be cleared
+						 * in the following code which will
+						 * take care of the primary stabilzation
+						 * method (i.e. setting 'busy' to TRUE)
+						 */
+						dwp->dw_mask |= DW_vm_page_unwire;
+					}
+					m->overwriting = FALSE;
+				}
+				if (m->encrypted_cleaning == TRUE) {
+					m->encrypted_cleaning = FALSE;
+
+					dwp->dw_mask |= DW_clear_busy;
+				}
+				m->pageout = FALSE;
+				m->cleaning = FALSE;
+#if	MACH_PAGEMAP
+				vm_external_state_clr(m->object->existence_map, m->offset);
+#endif	/* MACH_PAGEMAP */
+				if (error & UPL_ABORT_DUMP_PAGES) {
+					pmap_disconnect(m->phys_page);
+
+					dwp->dw_mask |= DW_vm_page_free;
+				} else {
+					if (!(dwp->dw_mask & DW_vm_page_unwire)) {
+						if (error & UPL_ABORT_REFERENCE) {
+							/*
+							 * we've been told to explictly
+							 * reference this page... for 
+							 * file I/O, this is done by
+							 * implementing an LRU on the inactive q
+							 */
+							dwp->dw_mask |= DW_vm_page_lru;
+
+						} else if (!m->active && !m->inactive && !m->speculative)
+							dwp->dw_mask |= DW_vm_page_deactivate_internal;
+					}
+					dwp->dw_mask |= DW_PAGE_WAKEUP;
+				}
+			}
+		}
+abort_next_page:
+		target_offset += PAGE_SIZE_64;
+		xfer_size -= PAGE_SIZE;
+		entry++;
+
+		if (dwp->dw_mask) {
+			if (dwp->dw_mask & ~(DW_clear_busy | DW_PAGE_WAKEUP)) {
+				VM_PAGE_ADD_DELAYED_WORK(dwp, m, dw_count);
+
+				if (dw_count >= dw_limit) {
+					vm_page_do_delayed_work(shadow_object, VM_KERN_MEMORY_NONE, &dw_array[0], dw_count);
+				
+					dwp = &dw_array[0];
+					dw_count = 0;
+				}
+			} else {
+				if (dwp->dw_mask & DW_clear_busy)
+					m->busy = FALSE;
+
+				if (dwp->dw_mask & DW_PAGE_WAKEUP)
+					PAGE_WAKEUP(m);
+			}
+		}
+	}
+	if (dw_count)
+		vm_page_do_delayed_work(shadow_object, VM_KERN_MEMORY_NONE, &dw_array[0], dw_count);
+
+	occupied = 1;
+
+	if (upl->flags & UPL_DEVICE_MEMORY)  {
+		occupied = 0;
+	} else if (upl->flags & UPL_LITE) {
+		int	pg_num;
+		int	i;
+
+		pg_num = upl->size/PAGE_SIZE;
+		pg_num = (pg_num + 31) >> 5;
+		occupied = 0;
+
+		for (i = 0; i < pg_num; i++) {
+			if (lite_list[i] != 0) {
+				occupied = 1;
+				break;
+			}
+		}
+	} else {
+		if (queue_empty(&upl->map_object->memq))
+			occupied = 0;
+	}
+	if (occupied == 0) {
+		/*
+		 * If this UPL element belongs to a Vector UPL and is
+		 * empty, then this is the right function to deallocate
+		 * it. So go ahead set the *empty variable. The flag
+		 * UPL_COMMIT_NOTIFY_EMPTY, from the caller's point of view
+		 * should be considered relevant for the Vector UPL and
+		 * not the internal UPLs.
+		 */
+		if ((upl->flags & UPL_COMMIT_NOTIFY_EMPTY) || isVectorUPL)
+			*empty = TRUE;
+
+		if (object == shadow_object && !(upl->flags & UPL_KERNEL_OBJECT)) {
+		        /*
+			 * this is not a paging object
+			 * so we need to drop the paging reference
+			 * that was taken when we created the UPL
+			 * against this object
+			 */
+			vm_object_activity_end(shadow_object);
+			vm_object_collapse(shadow_object, 0, TRUE);
+		} else {
+		         /*
+			  * we dontated the paging reference to
+			  * the map object... vm_pageout_object_terminate
+			  * will drop this reference
+			  */
+		}
+	}
+	vm_object_unlock(shadow_object);
+	if (object != shadow_object)
+	        vm_object_unlock(object);
+	
+	if(!isVectorUPL)
+		upl_unlock(upl);
+	else {
+		/* 
+		* If we completed our operations on an UPL that is
+	 	* part of a Vectored UPL and if empty is TRUE, then
+	 	* we should go ahead and deallocate this UPL element. 
+	 	* Then we check if this was the last of the UPL elements
+	 	* within that Vectored UPL. If so, set empty to TRUE
+	 	* so that in ubc_upl_abort_range or ubc_upl_abort, we
+	 	* can go ahead and deallocate the Vector UPL too.
+	 	*/
+		if(*empty == TRUE) {
+			*empty = vector_upl_set_subupl(vector_upl, upl,0);
+			upl_deallocate(upl);
+		}
+		goto process_upl_to_abort;
+	}
+
+	return KERN_SUCCESS;
+}
+
+
+kern_return_t
+upl_abort(
+	upl_t	upl,
+	int	error)
+{
+	boolean_t	empty;
+
+	return upl_abort_range(upl, 0, upl->size, error, &empty);
+}
+
+
+/* an option on commit should be wire */
+kern_return_t
+upl_commit(
+	upl_t			upl,
+	upl_page_info_t		*page_list,
+	mach_msg_type_number_t	count)
+{
+	boolean_t	empty;
+
+	return upl_commit_range(upl, 0, upl->size, 0, page_list, count, &empty);
+}
+
+
+void
+iopl_valid_data(
+	upl_t	upl)
+{
+	vm_object_t	object;
+	vm_offset_t	offset;
+	vm_page_t	m, nxt_page = VM_PAGE_NULL;
+	upl_size_t	size;
+	int		wired_count = 0;
+
+	if (upl == NULL)
+		panic("iopl_valid_data: NULL upl");
+	if (vector_upl_is_valid(upl))
+		panic("iopl_valid_data: vector upl");
+	if ((upl->flags & (UPL_DEVICE_MEMORY|UPL_SHADOWED|UPL_ACCESS_BLOCKED|UPL_IO_WIRE|UPL_INTERNAL)) != UPL_IO_WIRE)
+		panic("iopl_valid_data: unsupported upl, flags = %x", upl->flags);
+
+	object = upl->map_object;
+
+	if (object == kernel_object || object == compressor_object)
+		panic("iopl_valid_data: object == kernel or compressor");
+
+	if (object->purgable == VM_PURGABLE_VOLATILE)
+		panic("iopl_valid_data: object == VM_PURGABLE_VOLATILE");
+
+	size = upl->size;
+
+	vm_object_lock(object);
+
+	if (object->vo_size == size && object->resident_page_count == (size / PAGE_SIZE))
+		nxt_page = (vm_page_t)queue_first(&object->memq);
+	else
+		offset = 0 + upl->offset - object->paging_offset;
+
+	while (size) {
+
+		if (nxt_page != VM_PAGE_NULL) {
+			m = nxt_page;
+			nxt_page = (vm_page_t)queue_next(&nxt_page->listq);
+		} else {
+			m = vm_page_lookup(object, offset);
+			offset += PAGE_SIZE;
+
+			if (m == VM_PAGE_NULL)
+				panic("iopl_valid_data: missing expected page at offset %lx", (long)offset);
+		}
+		if (m->busy) {
+			if (!m->absent)
+				panic("iopl_valid_data: busy page w/o absent");
+
+			if (m->pageq.next || m->pageq.prev)
+				panic("iopl_valid_data: busy+absent page on page queue");
+
+			m->absent = FALSE;
+			m->dirty = TRUE;
+			m->wire_count++;
+			wired_count++;
+			
+			PAGE_WAKEUP_DONE(m);
+		}
+		size -= PAGE_SIZE;
+	}
+	if (wired_count) {
+
+		if (!object->wired_page_count) {
+		    VM_OBJECT_WIRED(object);
+		}
+		object->wired_page_count += wired_count;
+
+		vm_page_lockspin_queues();
+		vm_page_wire_count += wired_count;
+		vm_page_unlock_queues();
+	}
+	vm_object_unlock(object);
+}
+
+void
+vm_object_set_pmap_cache_attr(
+		vm_object_t		object,
+		upl_page_info_array_t	user_page_list,
+		unsigned int		num_pages,
+		boolean_t		batch_pmap_op)
+{
+	unsigned int    cache_attr = 0;
+
+	cache_attr = object->wimg_bits & VM_WIMG_MASK;
+	assert(user_page_list);
+	if (cache_attr != VM_WIMG_USE_DEFAULT) {
+		PMAP_BATCH_SET_CACHE_ATTR(object, user_page_list, cache_attr, num_pages, batch_pmap_op);
+	}
+}
+
+
+boolean_t	vm_object_iopl_wire_full(vm_object_t, upl_t, upl_page_info_array_t, wpl_array_t, upl_control_flags_t);
+kern_return_t	vm_object_iopl_wire_empty(vm_object_t, upl_t, upl_page_info_array_t, wpl_array_t, upl_control_flags_t, vm_object_offset_t *, int);
+
+
+
+boolean_t
+vm_object_iopl_wire_full(vm_object_t object, upl_t upl, upl_page_info_array_t user_page_list,
+			    wpl_array_t lite_list, upl_control_flags_t cntrl_flags)
+{
+	vm_page_t	dst_page;
+	vm_tag_t        tag;
+	unsigned int	entry;
+	int		page_count;
+	int		delayed_unlock = 0;
+	boolean_t	retval = TRUE;
+
+	vm_object_lock_assert_exclusive(object);
+	assert(object->purgable != VM_PURGABLE_VOLATILE);
+	assert(object->purgable != VM_PURGABLE_EMPTY);
+	assert(object->pager == NULL);
+	assert(object->copy == NULL);
+	assert(object->shadow == NULL);
+
+	tag = UPL_MEMORY_TAG(cntrl_flags);
+	page_count = object->resident_page_count;
+	dst_page = (vm_page_t)queue_first(&object->memq);
+
+	vm_page_lock_queues();
+
+	while (page_count--) {
+
+		if (dst_page->busy ||
+		    dst_page->fictitious ||
+		    dst_page->absent ||
+		    dst_page->error ||
+		    dst_page->cleaning ||
+		    dst_page->restart ||
+		    dst_page->encrypted ||
+		    dst_page->laundry) {
+			retval = FALSE;
+			goto done;
+		}
+		if ((cntrl_flags & UPL_REQUEST_FORCE_COHERENCY) && dst_page->written_by_kernel == TRUE) {
+			retval = FALSE;
+			goto done;
+		}
+		dst_page->reference = TRUE;
+
+		vm_page_wire(dst_page, tag, FALSE);
+
+		if (!(cntrl_flags & UPL_COPYOUT_FROM)) {
+			SET_PAGE_DIRTY(dst_page, FALSE);
+		}
+		entry = (unsigned int)(dst_page->offset / PAGE_SIZE);
+		assert(entry >= 0 && entry < object->resident_page_count);
+		lite_list[entry>>5] |= 1 << (entry & 31);
+		
+		if (dst_page->phys_page > upl->highest_page)
+			upl->highest_page = dst_page->phys_page;
+
+		if (user_page_list) {
+			user_page_list[entry].phys_addr = dst_page->phys_page;
+			user_page_list[entry].absent    = dst_page->absent;
+			user_page_list[entry].dirty     = dst_page->dirty;
+			user_page_list[entry].pageout   = dst_page->pageout;;
+			user_page_list[entry].precious  = dst_page->precious;
+			user_page_list[entry].device    = FALSE;
+			user_page_list[entry].speculative = FALSE;
+			user_page_list[entry].cs_validated = FALSE;
+			user_page_list[entry].cs_tainted = FALSE;
+			user_page_list[entry].cs_nx	= FALSE;
+			user_page_list[entry].needed    = FALSE;
+			user_page_list[entry].mark      = FALSE;
+		}
+		if (delayed_unlock++ > 256) {
+			delayed_unlock = 0;
+			lck_mtx_yield(&vm_page_queue_lock);
+
+			VM_CHECK_MEMORYSTATUS;
+		}
+		dst_page = (vm_page_t)queue_next(&dst_page->listq);
+	}
+done:
+	vm_page_unlock_queues();
+
+	VM_CHECK_MEMORYSTATUS;
+
+	return (retval);
+}
+
+
+kern_return_t
+vm_object_iopl_wire_empty(vm_object_t object, upl_t upl, upl_page_info_array_t user_page_list,
+			     wpl_array_t lite_list, upl_control_flags_t cntrl_flags, vm_object_offset_t *dst_offset, int page_count)
+{
+	vm_page_t	dst_page;
+	vm_tag_t        tag;
+	boolean_t	no_zero_fill = FALSE;
+	int		interruptible;
+	int		pages_wired = 0;
+	int		pages_inserted = 0;
+	int		entry = 0;
+	uint64_t	delayed_ledger_update = 0;
+	kern_return_t	ret = KERN_SUCCESS;
+
+	vm_object_lock_assert_exclusive(object);
+	assert(object->purgable != VM_PURGABLE_VOLATILE);
+	assert(object->purgable != VM_PURGABLE_EMPTY);
+	assert(object->pager == NULL);
+	assert(object->copy == NULL);
+	assert(object->shadow == NULL);
+
+	if (cntrl_flags & UPL_SET_INTERRUPTIBLE)
+		interruptible = THREAD_ABORTSAFE;
+	else
+		interruptible = THREAD_UNINT;
+
+	if (cntrl_flags & (UPL_NOZEROFILL | UPL_NOZEROFILLIO))
+	        no_zero_fill = TRUE;
+
+	tag = UPL_MEMORY_TAG(cntrl_flags);
+
+	while (page_count--) {
+			
+		while ( (dst_page = vm_page_grab()) == VM_PAGE_NULL) {
+
+			OSAddAtomic(page_count, &vm_upl_wait_for_pages);
+
+			VM_DEBUG_EVENT(vm_iopl_page_wait, VM_IOPL_PAGE_WAIT, DBG_FUNC_START, vm_upl_wait_for_pages, 0, 0, 0);
+
+			if (vm_page_wait(interruptible) == FALSE) {
+				/*
+				 * interrupted case
+				 */
+				OSAddAtomic(-page_count, &vm_upl_wait_for_pages);
+
+				VM_DEBUG_EVENT(vm_iopl_page_wait, VM_IOPL_PAGE_WAIT, DBG_FUNC_END, vm_upl_wait_for_pages, 0, 0, -1);
+				
+				ret = MACH_SEND_INTERRUPTED;
+				goto done;
+			}
+			OSAddAtomic(-page_count, &vm_upl_wait_for_pages);
+
+			VM_DEBUG_EVENT(vm_iopl_page_wait, VM_IOPL_PAGE_WAIT, DBG_FUNC_END, vm_upl_wait_for_pages, 0, 0, 0);
+		}
+		if (no_zero_fill == FALSE)
+			vm_page_zero_fill(dst_page);
+		else
+			dst_page->absent = TRUE;
+
+		dst_page->reference = TRUE;
+
+		if (!(cntrl_flags & UPL_COPYOUT_FROM)) {
+			SET_PAGE_DIRTY(dst_page, FALSE);	
+		}
+		if (dst_page->absent == FALSE) {
+			dst_page->wire_count++;
+			pages_wired++;
+			PAGE_WAKEUP_DONE(dst_page);
+		}
+		pages_inserted++;
+
+		vm_page_insert_internal(dst_page, object, *dst_offset, tag, FALSE, TRUE, TRUE, TRUE, &delayed_ledger_update);
+
+		lite_list[entry>>5] |= 1 << (entry & 31);
+		
+		if (dst_page->phys_page > upl->highest_page)
+			upl->highest_page = dst_page->phys_page;
+
+		if (user_page_list) {
+			user_page_list[entry].phys_addr	= dst_page->phys_page;
+			user_page_list[entry].absent	= dst_page->absent;
+			user_page_list[entry].dirty 	= dst_page->dirty;
+			user_page_list[entry].pageout	= FALSE;
+			user_page_list[entry].precious	= FALSE;
+			user_page_list[entry].device 	= FALSE;
+			user_page_list[entry].speculative = FALSE;
+			user_page_list[entry].cs_validated = FALSE;
+			user_page_list[entry].cs_tainted = FALSE;
+			user_page_list[entry].cs_nx     = FALSE;
+			user_page_list[entry].needed    = FALSE;
+			user_page_list[entry].mark      = FALSE;
+		}
+		entry++;
+		*dst_offset += PAGE_SIZE_64;
+	}
+done:
+	if (pages_wired) {
+		vm_page_lockspin_queues();
+		vm_page_wire_count += pages_wired;
+		vm_page_unlock_queues();
+	}
+	if (pages_inserted) {
+		if (object->internal) {
+			OSAddAtomic(pages_inserted, &vm_page_internal_count);
+		} else {
+			OSAddAtomic(pages_inserted, &vm_page_external_count);
+		}
+	}
+	if (delayed_ledger_update) {
+		task_t		owner;
+
+		owner = object->vo_purgeable_owner;
+		assert(owner);
+
+		/* more non-volatile bytes */
+		ledger_credit(owner->ledger,
+			      task_ledgers.purgeable_nonvolatile,
+			      delayed_ledger_update);
+		/* more footprint */
+		ledger_credit(owner->ledger,
+			      task_ledgers.phys_footprint,
+			      delayed_ledger_update);
+	}
+	return (ret);
+}
+
+
+unsigned int vm_object_iopl_request_sleep_for_cleaning = 0;
+
+
+kern_return_t
+vm_object_iopl_request(
+	vm_object_t		object,
+	vm_object_offset_t	offset,
+	upl_size_t		size,
+	upl_t			*upl_ptr,
+	upl_page_info_array_t	user_page_list,
+	unsigned int		*page_list_count,
+	upl_control_flags_t	cntrl_flags)
+{
+	vm_page_t		dst_page;
+	vm_object_offset_t	dst_offset;
+	upl_size_t		xfer_size;
+	upl_t			upl = NULL;
+	unsigned int		entry;
+	wpl_array_t 		lite_list = NULL;
+	int			no_zero_fill = FALSE;
+	unsigned int		size_in_pages;
+	u_int32_t		psize;
+	kern_return_t		ret;
+	vm_prot_t		prot;
+	struct vm_object_fault_info fault_info;
+	struct	vm_page_delayed_work	dw_array[DEFAULT_DELAYED_WORK_LIMIT];
+	struct	vm_page_delayed_work	*dwp;
+	int			dw_count;
+	int			dw_limit;
+	int			dw_index;
+	boolean_t		caller_lookup;
+	int			io_tracking_flag = 0;
+	int			interruptible;
+
+	boolean_t		set_cache_attr_needed = FALSE;
+	boolean_t		free_wired_pages = FALSE;
+	boolean_t		fast_path_empty_req = FALSE;
+	boolean_t		fast_path_full_req = FALSE;
+
+	if (cntrl_flags & ~UPL_VALID_FLAGS) {
+		/*
+		 * For forward compatibility's sake,
+		 * reject any unknown flag.
+		 */
+		return KERN_INVALID_VALUE;
+	}
+	if (vm_lopage_needed == FALSE)
+	        cntrl_flags &= ~UPL_NEED_32BIT_ADDR;
+
+	if (cntrl_flags & UPL_NEED_32BIT_ADDR) {
+	        if ( (cntrl_flags & (UPL_SET_IO_WIRE | UPL_SET_LITE)) != (UPL_SET_IO_WIRE | UPL_SET_LITE))
+		        return KERN_INVALID_VALUE;
+
+		if (object->phys_contiguous) {
+		        if ((offset + object->vo_shadow_offset) >= (vm_object_offset_t)max_valid_dma_address)
+			        return KERN_INVALID_ADDRESS;
+	      
+			if (((offset + object->vo_shadow_offset) + size) >= (vm_object_offset_t)max_valid_dma_address)
+			        return KERN_INVALID_ADDRESS;
+		}
+	}
+
+	if (cntrl_flags & UPL_ENCRYPT) {
+		/*
+		 * ENCRYPTED SWAP:
+		 * The paging path doesn't use this interface,
+		 * so we don't support the UPL_ENCRYPT flag
+		 * here.  We won't encrypt the pages.
+		 */
+		assert(! (cntrl_flags & UPL_ENCRYPT));
+	}
+	if (cntrl_flags & (UPL_NOZEROFILL | UPL_NOZEROFILLIO))
+	        no_zero_fill = TRUE;
+
+	if (cntrl_flags & UPL_COPYOUT_FROM)
+		prot = VM_PROT_READ;
+	else
+		prot = VM_PROT_READ | VM_PROT_WRITE;
+
+	if ((!object->internal) && (object->paging_offset != 0))
+		panic("vm_object_iopl_request: external object with non-zero paging offset\n");
+
+#if CONFIG_IOSCHED || UPL_DEBUG
+	if ((object->io_tracking && object != kernel_object) || upl_debug_enabled)
+		io_tracking_flag |= UPL_CREATE_IO_TRACKING;
+#endif
+
+#if CONFIG_IOSCHED
+	if (object->io_tracking) {
+		/* Check if we're dealing with the kernel object. We do not support expedite on kernel object UPLs */
+		if (object != kernel_object)
+			io_tracking_flag |= UPL_CREATE_EXPEDITE_SUP;
+	}
+#endif
+
+	if (object->phys_contiguous)
+	        psize = PAGE_SIZE;
+	else
+	        psize = size;
+
+	if (cntrl_flags & UPL_SET_INTERNAL) {
+	        upl = upl_create(UPL_CREATE_INTERNAL | UPL_CREATE_LITE | io_tracking_flag, UPL_IO_WIRE, psize);
+
+		user_page_list = (upl_page_info_t *) (((uintptr_t)upl) + sizeof(struct upl));
+		lite_list = (wpl_array_t) (((uintptr_t)user_page_list) +
+					   ((psize / PAGE_SIZE) * sizeof(upl_page_info_t)));
+		if (size == 0) {
+			user_page_list = NULL;
+			lite_list = NULL;
+		}
+	} else {
+	        upl = upl_create(UPL_CREATE_LITE | io_tracking_flag, UPL_IO_WIRE, psize);
+
+		lite_list = (wpl_array_t) (((uintptr_t)upl) + sizeof(struct upl));
+		if (size == 0) {
+			lite_list = NULL;
+		}
+	}
+	if (user_page_list)
+	        user_page_list[0].device = FALSE;
+	*upl_ptr = upl;
+
+	upl->map_object = object;
+	upl->size = size;
+
+	size_in_pages = size / PAGE_SIZE;
+
+	if (object == kernel_object &&
+	    !(cntrl_flags & (UPL_NEED_32BIT_ADDR | UPL_BLOCK_ACCESS))) {
+		upl->flags |= UPL_KERNEL_OBJECT;
+#if UPL_DEBUG
+		vm_object_lock(object);
+#else
+		vm_object_lock_shared(object);
+#endif
+	} else {
+		vm_object_lock(object);
+		vm_object_activity_begin(object);
+	}
+	/*
+	 * paging in progress also protects the paging_offset
+	 */
+	upl->offset = offset + object->paging_offset;
+
+	if (cntrl_flags & UPL_BLOCK_ACCESS) {
+		/*
+		 * The user requested that access to the pages in this UPL
+		 * be blocked until the UPL is commited or aborted.
+		 */
+		upl->flags |= UPL_ACCESS_BLOCKED;
+	}
+
+#if CONFIG_IOSCHED || UPL_DEBUG
+	if (upl->flags & UPL_TRACKED_BY_OBJECT) {
+		vm_object_activity_begin(object);
+		queue_enter(&object->uplq, upl, upl_t, uplq);
+	}
+#endif
+
+	if (object->phys_contiguous) {
+
+		if (upl->flags & UPL_ACCESS_BLOCKED) {
+			assert(!object->blocked_access);
+			object->blocked_access = TRUE;
+		}
+
+		vm_object_unlock(object);
+
+		/*
+		 * don't need any shadow mappings for this one
+		 * since it is already I/O memory
+		 */
+		upl->flags |= UPL_DEVICE_MEMORY;
+
+		upl->highest_page = (ppnum_t) ((offset + object->vo_shadow_offset + size - 1)>>PAGE_SHIFT);
+
+		if (user_page_list) {
+		        user_page_list[0].phys_addr = (ppnum_t) ((offset + object->vo_shadow_offset)>>PAGE_SHIFT);
+			user_page_list[0].device = TRUE;
+		}
+		if (page_list_count != NULL) {
+		        if (upl->flags & UPL_INTERNAL)
+			        *page_list_count = 0;
+			else
+			        *page_list_count = 1;
+		}
+		return KERN_SUCCESS;
+	}
+	if (object != kernel_object && object != compressor_object) {
+		/*
+		 * Protect user space from future COW operations
+		 */
+#if VM_OBJECT_TRACKING_OP_TRUESHARE
+		if (!object->true_share &&
+		    vm_object_tracking_inited) {
+			void *bt[VM_OBJECT_TRACKING_BTDEPTH];
+			int num = 0;
+
+			num = OSBacktrace(bt,
+					  VM_OBJECT_TRACKING_BTDEPTH);
+			btlog_add_entry(vm_object_tracking_btlog,
+					object,
+					VM_OBJECT_TRACKING_OP_TRUESHARE,
+					bt,
+					num);
+		}
+#endif /* VM_OBJECT_TRACKING_OP_TRUESHARE */
+
+		object->true_share = TRUE;
+
+		if (object->copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC)
+			object->copy_strategy = MEMORY_OBJECT_COPY_DELAY;
+	}
+
+	if (!(cntrl_flags & UPL_COPYOUT_FROM) &&
+	    object->copy != VM_OBJECT_NULL) {
+		/*
+		 * Honor copy-on-write obligations
+		 *
+		 * The caller is gathering these pages and
+		 * might modify their contents.  We need to
+		 * make sure that the copy object has its own
+		 * private copies of these pages before we let
+		 * the caller modify them.
+		 *
+		 * NOTE: someone else could map the original object
+		 * after we've done this copy-on-write here, and they
+		 * could then see an inconsistent picture of the memory
+		 * while it's being modified via the UPL.  To prevent this,
+		 * we would have to block access to these pages until the
+		 * UPL is released.  We could use the UPL_BLOCK_ACCESS
+		 * code path for that...
+		 */
+		vm_object_update(object,
+				 offset,
+				 size,
+				 NULL,
+				 NULL,
+				 FALSE,	/* should_return */
+				 MEMORY_OBJECT_COPY_SYNC,
+				 VM_PROT_NO_CHANGE);
+#if DEVELOPMENT || DEBUG
+		iopl_cow++;
+		iopl_cow_pages += size >> PAGE_SHIFT;
+#endif
+	}
+	if (!(cntrl_flags & (UPL_NEED_32BIT_ADDR | UPL_BLOCK_ACCESS)) &&
+	    object->purgable != VM_PURGABLE_VOLATILE &&
+	    object->purgable != VM_PURGABLE_EMPTY &&
+	    object->copy == NULL &&
+	    size == object->vo_size &&
+	    offset == 0 &&
+	    object->shadow == NULL &&
+	    object->pager == NULL)
+	{
+		if (object->resident_page_count == size_in_pages)
+		{
+			assert(object != compressor_object);
+			assert(object != kernel_object);
+			fast_path_full_req = TRUE;
+		}
+		else if (object->resident_page_count == 0)
+		{
+			assert(object != compressor_object);
+			assert(object != kernel_object);
+			fast_path_empty_req = TRUE;
+			set_cache_attr_needed = TRUE;
+		}
+	}
+
+	if (cntrl_flags & UPL_SET_INTERRUPTIBLE)
+		interruptible = THREAD_ABORTSAFE;
+	else
+		interruptible = THREAD_UNINT;
+
+	entry = 0;
+
+	xfer_size = size;
+	dst_offset = offset;
+	dw_count = 0;
+
+	if (fast_path_full_req) {
+
+		if (vm_object_iopl_wire_full(object, upl, user_page_list, lite_list, cntrl_flags) == TRUE)
+			goto finish;
+		/*
+		 * we couldn't complete the processing of this request on the fast path
+		 * so fall through to the slow path and finish up
+		 */
+
+	} else if (fast_path_empty_req) {
+
+		if (cntrl_flags & UPL_REQUEST_NO_FAULT) {
+			ret = KERN_MEMORY_ERROR;
+			goto return_err;
+		}
+		ret = vm_object_iopl_wire_empty(object, upl, user_page_list, lite_list, cntrl_flags, &dst_offset, size_in_pages);
+		
+		if (ret) {
+			free_wired_pages = TRUE;
+			goto return_err;
+		}
+		goto finish;
+	}
+
+	fault_info.behavior = VM_BEHAVIOR_SEQUENTIAL;
+	fault_info.user_tag  = 0;
+	fault_info.lo_offset = offset;
+	fault_info.hi_offset = offset + xfer_size;
+	fault_info.no_cache  = FALSE;
+	fault_info.stealth = FALSE;
+	fault_info.io_sync = FALSE;
+	fault_info.cs_bypass = FALSE;
+	fault_info.mark_zf_absent = TRUE;
+	fault_info.interruptible = interruptible;
+	fault_info.batch_pmap_op = TRUE;
+
+	dwp = &dw_array[0];
+	dw_limit = DELAYED_WORK_LIMIT(DEFAULT_DELAYED_WORK_LIMIT);
+
+	while (xfer_size) {
+	        vm_fault_return_t	result;
+
+		dwp->dw_mask = 0;
+
+		if (fast_path_full_req) {
+			/*
+			 * if we get here, it means that we ran into a page
+			 * state we couldn't handle in the fast path and
+			 * bailed out to the slow path... since the order
+			 * we look at pages is different between the 2 paths,
+			 * the following check is needed to determine whether
+			 * this page was already processed in the fast path
+			 */
+			if (lite_list[entry>>5] & (1 << (entry & 31)))
+				goto skip_page;
+		}
+		dst_page = vm_page_lookup(object, dst_offset);
+
+		/*
+		 * ENCRYPTED SWAP:
+		 * If the page is encrypted, we need to decrypt it,
+		 * so force a soft page fault.
+		 */
+		if (dst_page == VM_PAGE_NULL ||
+		    dst_page->busy ||
+		    dst_page->encrypted ||
+		    dst_page->error || 
+		    dst_page->restart ||
+		    dst_page->absent ||
+		    dst_page->fictitious) {
+
+		   if (object == kernel_object)
+			   panic("vm_object_iopl_request: missing/bad page in kernel object\n");
+		   if (object == compressor_object)
+			   panic("vm_object_iopl_request: missing/bad page in compressor object\n");
+
+		   if (cntrl_flags & UPL_REQUEST_NO_FAULT) {
+			   ret = KERN_MEMORY_ERROR;
+			   goto return_err;
+		   }
+		   set_cache_attr_needed = TRUE;
+
+                   /*
+		    * We just looked up the page and the result remains valid
+		    * until the object lock is release, so send it to
+		    * vm_fault_page() (as "dst_page"), to avoid having to
+		    * look it up again there.
+		    */
+		   caller_lookup = TRUE;
+
+		   do {
+			vm_page_t	top_page;
+			kern_return_t	error_code;
+
+			fault_info.cluster_size = xfer_size;
+
+			vm_object_paging_begin(object);
+
+			result = vm_fault_page(object, dst_offset,
+					       prot | VM_PROT_WRITE, FALSE,
+					       caller_lookup,
+					       &prot, &dst_page, &top_page,
+					       (int *)0,
+					       &error_code, no_zero_fill,
+					       FALSE, &fault_info);
+
+                        /* our lookup is no longer valid at this point */
+			caller_lookup = FALSE;
+
+			switch (result) {
+
+			case VM_FAULT_SUCCESS:
+
+				if ( !dst_page->absent) {
+					PAGE_WAKEUP_DONE(dst_page);
+				} else {
+					/*
+					 * we only get back an absent page if we
+					 * requested that it not be zero-filled
+					 * because we are about to fill it via I/O
+					 * 
+					 * absent pages should be left BUSY
+					 * to prevent them from being faulted
+					 * into an address space before we've
+					 * had a chance to complete the I/O on
+					 * them since they may contain info that
+					 * shouldn't be seen by the faulting task
+					 */
+				}
+				/*
+				 *	Release paging references and
+				 *	top-level placeholder page, if any.
+				 */
+				if (top_page != VM_PAGE_NULL) {
+					vm_object_t local_object;
+
+					local_object = top_page->object;
+
+					if (top_page->object != dst_page->object) {
+						vm_object_lock(local_object);
+						VM_PAGE_FREE(top_page);
+						vm_object_paging_end(local_object);
+						vm_object_unlock(local_object);
+					} else {
+						VM_PAGE_FREE(top_page);
+						vm_object_paging_end(local_object);
+					}
+				}
+				vm_object_paging_end(object);
+				break;
+			
+			case VM_FAULT_RETRY:
+				vm_object_lock(object);
+				break;
+
+			case VM_FAULT_MEMORY_SHORTAGE:
+				OSAddAtomic((size_in_pages - entry), &vm_upl_wait_for_pages);
+
+				VM_DEBUG_EVENT(vm_iopl_page_wait, VM_IOPL_PAGE_WAIT, DBG_FUNC_START, vm_upl_wait_for_pages, 0, 0, 0);
+
+				if (vm_page_wait(interruptible)) {
+					OSAddAtomic(-(size_in_pages - entry), &vm_upl_wait_for_pages);
+
+					VM_DEBUG_EVENT(vm_iopl_page_wait, VM_IOPL_PAGE_WAIT, DBG_FUNC_END, vm_upl_wait_for_pages, 0, 0, 0);
+					vm_object_lock(object);
+
+					break;
+				}
+				OSAddAtomic(-(size_in_pages - entry), &vm_upl_wait_for_pages);
+
+				VM_DEBUG_EVENT(vm_iopl_page_wait, VM_IOPL_PAGE_WAIT, DBG_FUNC_END, vm_upl_wait_for_pages, 0, 0, -1);
+
+				/* fall thru */
+
+			case VM_FAULT_INTERRUPTED:
+				error_code = MACH_SEND_INTERRUPTED;
+			case VM_FAULT_MEMORY_ERROR:
+			memory_error:
+				ret = (error_code ? error_code:	KERN_MEMORY_ERROR);
+
+				vm_object_lock(object);
+				goto return_err;
+
+			case VM_FAULT_SUCCESS_NO_VM_PAGE:
+				/* success but no page: fail */
+				vm_object_paging_end(object);
+				vm_object_unlock(object);
+				goto memory_error;
+
+			default:
+				panic("vm_object_iopl_request: unexpected error"
+				      " 0x%x from vm_fault_page()\n", result);
+			}
+		   } while (result != VM_FAULT_SUCCESS);
+
+		}
+		if (upl->flags & UPL_KERNEL_OBJECT)
+			goto record_phys_addr;
+
+		if (dst_page->compressor) {
+			dst_page->busy = TRUE;
+			goto record_phys_addr;
+		}
+
+		if (dst_page->cleaning) {
+			/*
+			 * Someone else is cleaning this page in place.
+			 * In theory, we should be able to  proceed and use this
+			 * page but they'll probably end up clearing the "busy"
+			 * bit on it in upl_commit_range() but they didn't set
+			 * it, so they would clear our "busy" bit and open
+			 * us to race conditions.
+			 * We'd better wait for the cleaning to complete and
+			 * then try again.
+			 */
+			vm_object_iopl_request_sleep_for_cleaning++;
+			PAGE_SLEEP(object, dst_page, THREAD_UNINT);
+			continue;
+		}
+		if (dst_page->laundry) {
+			dst_page->pageout = FALSE;
+			
+			vm_pageout_steal_laundry(dst_page, FALSE);
+		}			
+		if ( (cntrl_flags & UPL_NEED_32BIT_ADDR) &&
+		     dst_page->phys_page >= (max_valid_dma_address >> PAGE_SHIFT) ) {
+		        vm_page_t	low_page;
+			int 		refmod;
+
+			/*
+			 * support devices that can't DMA above 32 bits
+			 * by substituting pages from a pool of low address
+			 * memory for any pages we find above the 4G mark
+			 * can't substitute if the page is already wired because
+			 * we don't know whether that physical address has been
+			 * handed out to some other 64 bit capable DMA device to use
+			 */
+			if (VM_PAGE_WIRED(dst_page)) {
+			        ret = KERN_PROTECTION_FAILURE;
+				goto return_err;
+			}
+			low_page = vm_page_grablo();
+
+			if (low_page == VM_PAGE_NULL) {
+			        ret = KERN_RESOURCE_SHORTAGE;
+				goto return_err;
+			}
+			/*
+			 * from here until the vm_page_replace completes
+			 * we musn't drop the object lock... we don't
+			 * want anyone refaulting this page in and using
+			 * it after we disconnect it... we want the fault
+			 * to find the new page being substituted.
+			 */
+			if (dst_page->pmapped)
+			        refmod = pmap_disconnect(dst_page->phys_page);
+			else
+			        refmod = 0;
+
+			if (!dst_page->absent)
+				vm_page_copy(dst_page, low_page);
+		  
+			low_page->reference = dst_page->reference;
+			low_page->dirty     = dst_page->dirty;
+			low_page->absent    = dst_page->absent;
+
+			if (refmod & VM_MEM_REFERENCED)
+			        low_page->reference = TRUE;
+			if (refmod & VM_MEM_MODIFIED) {
+			        SET_PAGE_DIRTY(low_page, FALSE);
+			}
+
+			vm_page_replace(low_page, object, dst_offset);
+
+			dst_page = low_page;
+			/*
+			 * vm_page_grablo returned the page marked
+			 * BUSY... we don't need a PAGE_WAKEUP_DONE
+			 * here, because we've never dropped the object lock
+			 */
+			if ( !dst_page->absent)
+				dst_page->busy = FALSE;
+		}
+		if ( !dst_page->busy)
+			dwp->dw_mask |= DW_vm_page_wire;
+
+		if (cntrl_flags & UPL_BLOCK_ACCESS) {
+			/*
+			 * Mark the page "busy" to block any future page fault
+			 * on this page in addition to wiring it.
+			 * We'll also remove the mapping
+			 * of all these pages before leaving this routine.
+			 */
+			assert(!dst_page->fictitious);
+			dst_page->busy = TRUE;
+		}
+		/*
+		 * expect the page to be used
+		 * page queues lock must be held to set 'reference'
+		 */
+		dwp->dw_mask |= DW_set_reference;
+
+   		if (!(cntrl_flags & UPL_COPYOUT_FROM)) {
+			SET_PAGE_DIRTY(dst_page, TRUE);	
+		}
+		if ((cntrl_flags & UPL_REQUEST_FORCE_COHERENCY) && dst_page->written_by_kernel == TRUE) {
+			pmap_sync_page_attributes_phys(dst_page->phys_page);
+			dst_page->written_by_kernel = FALSE;
+		}
+
+record_phys_addr:
+		if (dst_page->busy)
+			upl->flags |= UPL_HAS_BUSY;
+
+		lite_list[entry>>5] |= 1 << (entry & 31);
+
+		if (dst_page->phys_page > upl->highest_page)
+		        upl->highest_page = dst_page->phys_page;
+
+		if (user_page_list) {
+			user_page_list[entry].phys_addr	= dst_page->phys_page;
+			user_page_list[entry].pageout	= dst_page->pageout;
+			user_page_list[entry].absent	= dst_page->absent;
+			user_page_list[entry].dirty 	= dst_page->dirty;
+			user_page_list[entry].precious	= dst_page->precious;
+			user_page_list[entry].device 	= FALSE;
+			user_page_list[entry].needed    = FALSE;
+			if (dst_page->clustered == TRUE)
+			        user_page_list[entry].speculative = dst_page->speculative;
+			else
+			        user_page_list[entry].speculative = FALSE;
+			user_page_list[entry].cs_validated = dst_page->cs_validated;
+			user_page_list[entry].cs_tainted = dst_page->cs_tainted;
+			user_page_list[entry].cs_nx = dst_page->cs_nx;
+			user_page_list[entry].mark      = FALSE;
+		}
+		if (object != kernel_object && object != compressor_object) {
+			/*
+			 * someone is explicitly grabbing this page...
+			 * update clustered and speculative state
+			 * 
+			 */
+			if (dst_page->clustered)
+				VM_PAGE_CONSUME_CLUSTERED(dst_page);
+		}
+skip_page:
+		entry++;
+		dst_offset += PAGE_SIZE_64;
+		xfer_size -= PAGE_SIZE;
+
+		if (dwp->dw_mask) {
+			VM_PAGE_ADD_DELAYED_WORK(dwp, dst_page, dw_count);
+
+			if (dw_count >= dw_limit) {
+				vm_page_do_delayed_work(object, UPL_MEMORY_TAG(cntrl_flags), &dw_array[0], dw_count);
+				
+				dwp = &dw_array[0];
+				dw_count = 0;
+			}
+		}
+	}
+	assert(entry == size_in_pages);
+
+	if (dw_count)
+		vm_page_do_delayed_work(object, UPL_MEMORY_TAG(cntrl_flags), &dw_array[0], dw_count);
+finish:
+	if (user_page_list && set_cache_attr_needed == TRUE)
+		vm_object_set_pmap_cache_attr(object, user_page_list, size_in_pages, TRUE);
+
+	if (page_list_count != NULL) {
+	        if (upl->flags & UPL_INTERNAL)
+			*page_list_count = 0;
+		else if (*page_list_count > size_in_pages)
+			*page_list_count = size_in_pages;
+	}
+	vm_object_unlock(object);
+
+	if (cntrl_flags & UPL_BLOCK_ACCESS) {
+		/*
+		 * We've marked all the pages "busy" so that future
+		 * page faults will block.
+		 * Now remove the mapping for these pages, so that they
+		 * can't be accessed without causing a page fault.
+		 */
+		vm_object_pmap_protect(object, offset, (vm_object_size_t)size,
+				       PMAP_NULL, 0, VM_PROT_NONE);
+		assert(!object->blocked_access);
+		object->blocked_access = TRUE;
+	}
+
+	return KERN_SUCCESS;
+
+return_err:
+	dw_index = 0;
+
+	for (; offset < dst_offset; offset += PAGE_SIZE) {
+		boolean_t need_unwire;
+
+	        dst_page = vm_page_lookup(object, offset);
+
+		if (dst_page == VM_PAGE_NULL)
+		        panic("vm_object_iopl_request: Wired page missing. \n");
+
+		/*
+		 * if we've already processed this page in an earlier 
+		 * dw_do_work, we need to undo the wiring... we will
+		 * leave the dirty and reference bits on if they
+		 * were set, since we don't have a good way of knowing
+		 * what the previous state was and we won't get here
+		 * under any normal circumstances...  we will always
+		 * clear BUSY and wakeup any waiters via vm_page_free
+		 * or PAGE_WAKEUP_DONE
+		 */
+		need_unwire = TRUE;
+
+		if (dw_count) {
+			if (dw_array[dw_index].dw_m == dst_page) {
+				/*
+				 * still in the deferred work list
+				 * which means we haven't yet called
+				 * vm_page_wire on this page
+				 */
+				need_unwire = FALSE;
+
+				dw_index++;
+				dw_count--;
+			}
+		}
+		vm_page_lock_queues();
+
+		if (dst_page->absent || free_wired_pages == TRUE) {
+			vm_page_free(dst_page);
+
+			need_unwire = FALSE;
+		} else {
+			if (need_unwire == TRUE)
+				vm_page_unwire(dst_page, TRUE);
+
+			PAGE_WAKEUP_DONE(dst_page);
+		}
+		vm_page_unlock_queues();
+
+		if (need_unwire == TRUE)
+			VM_STAT_INCR(reactivations);
+	}
+#if UPL_DEBUG
+	upl->upl_state = 2;
+#endif
+	if (! (upl->flags & UPL_KERNEL_OBJECT)) {
+		vm_object_activity_end(object);
+		vm_object_collapse(object, 0, TRUE);
+	}
+	vm_object_unlock(object);
+	upl_destroy(upl);
+
+	return ret;
+}
+
+kern_return_t
+upl_transpose(
+	upl_t		upl1,
+	upl_t		upl2)
+{
+	kern_return_t		retval;
+	boolean_t		upls_locked;
+	vm_object_t		object1, object2;
+
+	if (upl1 == UPL_NULL || upl2 == UPL_NULL || upl1 == upl2  || ((upl1->flags & UPL_VECTOR)==UPL_VECTOR)  || ((upl2->flags & UPL_VECTOR)==UPL_VECTOR)) {
+		return KERN_INVALID_ARGUMENT;
+	}
+	
+	upls_locked = FALSE;
+
+	/*
+	 * Since we need to lock both UPLs at the same time,
+	 * avoid deadlocks by always taking locks in the same order.
+	 */
+	if (upl1 < upl2) {
+		upl_lock(upl1);
+		upl_lock(upl2);
+	} else {
+		upl_lock(upl2);
+		upl_lock(upl1);
+	}
+	upls_locked = TRUE;	/* the UPLs will need to be unlocked */
+
+	object1 = upl1->map_object;
+	object2 = upl2->map_object;
+
+	if (upl1->offset != 0 || upl2->offset != 0 ||
+	    upl1->size != upl2->size) {
+		/*
+		 * We deal only with full objects, not subsets.
+		 * That's because we exchange the entire backing store info
+		 * for the objects: pager, resident pages, etc...  We can't do
+		 * only part of it.
+		 */
+		retval = KERN_INVALID_VALUE;
+		goto done;
+	}
+
+	/*
+	 * Tranpose the VM objects' backing store.
+	 */
+	retval = vm_object_transpose(object1, object2,
+				     (vm_object_size_t) upl1->size);
+
+	if (retval == KERN_SUCCESS) {
+		/*
+		 * Make each UPL point to the correct VM object, i.e. the
+		 * object holding the pages that the UPL refers to...
+		 */
+#if CONFIG_IOSCHED || UPL_DEBUG
+		if ((upl1->flags & UPL_TRACKED_BY_OBJECT) || (upl2->flags & UPL_TRACKED_BY_OBJECT)) {
+			vm_object_lock(object1);
+			vm_object_lock(object2);
+		}
+		if (upl1->flags & UPL_TRACKED_BY_OBJECT)
+			queue_remove(&object1->uplq, upl1, upl_t, uplq);
+		if (upl2->flags & UPL_TRACKED_BY_OBJECT)
+			queue_remove(&object2->uplq, upl2, upl_t, uplq);
+#endif
+		upl1->map_object = object2;
+		upl2->map_object = object1;
+
+#if CONFIG_IOSCHED || UPL_DEBUG
+		if (upl1->flags & UPL_TRACKED_BY_OBJECT)
+			queue_enter(&object2->uplq, upl1, upl_t, uplq);
+		if (upl2->flags & UPL_TRACKED_BY_OBJECT)
+			queue_enter(&object1->uplq, upl2, upl_t, uplq);
+		if ((upl1->flags & UPL_TRACKED_BY_OBJECT) || (upl2->flags & UPL_TRACKED_BY_OBJECT)) {
+			vm_object_unlock(object2);
+			vm_object_unlock(object1);
+		}
+#endif
+	}
+
+done:
+	/*
+	 * Cleanup.
+	 */
+	if (upls_locked) {
+		upl_unlock(upl1);
+		upl_unlock(upl2);
+		upls_locked = FALSE;
+	}
+
+	return retval;
+}
+
+void
+upl_range_needed(
+	upl_t		upl,
+	int		index,
+	int		count)
+{
+	upl_page_info_t	*user_page_list;
+	int		size_in_pages;
+
+	if ( !(upl->flags & UPL_INTERNAL) || count <= 0)
+		return;
+
+	size_in_pages = upl->size / PAGE_SIZE;
+
+	user_page_list = (upl_page_info_t *) (((uintptr_t)upl) + sizeof(struct upl));
+
+	while (count-- && index < size_in_pages)
+		user_page_list[index++].needed = TRUE;
+}
+
+
+/*
+ * ENCRYPTED SWAP:
+ *
+ * Rationale:  the user might have some encrypted data on disk (via
+ * FileVault or any other mechanism).  That data is then decrypted in
+ * memory, which is safe as long as the machine is secure.  But that
+ * decrypted data in memory could be paged out to disk by the default
+ * pager.  The data would then be stored on disk in clear (not encrypted)
+ * and it could be accessed by anyone who gets physical access to the
+ * disk (if the laptop or the disk gets stolen for example).  This weakens
+ * the security offered by FileVault.
+ *
+ * Solution:  the default pager will optionally request that all the
+ * pages it gathers for pageout be encrypted, via the UPL interfaces,
+ * before it sends this UPL to disk via the vnode_pageout() path.
+ * 
+ * Notes:
+ * 
+ * To avoid disrupting the VM LRU algorithms, we want to keep the
+ * clean-in-place mechanisms, which allow us to send some extra pages to 
+ * swap (clustering) without actually removing them from the user's
+ * address space.  We don't want the user to unknowingly access encrypted
+ * data, so we have to actually remove the encrypted pages from the page
+ * table.  When the user accesses the data, the hardware will fail to
+ * locate the virtual page in its page table and will trigger a page
+ * fault.  We can then decrypt the page and enter it in the page table
+ * again.  Whenever we allow the user to access the contents of a page,
+ * we have to make sure it's not encrypted.
+ *
+ * 
+ */
+/*
+ * ENCRYPTED SWAP:
+ * Reserve of virtual addresses in the kernel address space.
+ * We need to map the physical pages in the kernel, so that we
+ * can call the encryption/decryption routines with a kernel
+ * virtual address.  We keep this pool of pre-allocated kernel
+ * virtual addresses so that we don't have to scan the kernel's
+ * virtaul address space each time we need to encrypt or decrypt
+ * a physical page.
+ * It would be nice to be able to encrypt and decrypt in physical
+ * mode but that might not always be more efficient...
+ */
+decl_simple_lock_data(,vm_paging_lock)
+#define VM_PAGING_NUM_PAGES	64
+vm_map_offset_t vm_paging_base_address = 0;
+boolean_t	vm_paging_page_inuse[VM_PAGING_NUM_PAGES] = { FALSE, };
+int		vm_paging_max_index = 0;
+int		vm_paging_page_waiter = 0;
+int		vm_paging_page_waiter_total = 0;
+unsigned long	vm_paging_no_kernel_page = 0;
+unsigned long	vm_paging_objects_mapped = 0;
+unsigned long	vm_paging_pages_mapped = 0;
+unsigned long	vm_paging_objects_mapped_slow = 0;
+unsigned long	vm_paging_pages_mapped_slow = 0;
+
+void
+vm_paging_map_init(void)
+{
+	kern_return_t	kr;
+	vm_map_offset_t	page_map_offset;
+	vm_map_entry_t	map_entry;
+
+	assert(vm_paging_base_address == 0);
+
+	/*
+	 * Initialize our pool of pre-allocated kernel
+	 * virtual addresses.
+	 */
+	page_map_offset = 0;
+	kr = vm_map_find_space(kernel_map,
+			       &page_map_offset,
+			       VM_PAGING_NUM_PAGES * PAGE_SIZE,
+			       0,
+			       0,
+			       &map_entry);
+	if (kr != KERN_SUCCESS) {
+		panic("vm_paging_map_init: kernel_map full\n");
+	}
+	VME_OBJECT_SET(map_entry, kernel_object);
+	VME_OFFSET_SET(map_entry, page_map_offset);
+	map_entry->protection = VM_PROT_NONE;
+	map_entry->max_protection = VM_PROT_NONE;
+	map_entry->permanent = TRUE;
+	vm_object_reference(kernel_object);
+	vm_map_unlock(kernel_map);
+
+	assert(vm_paging_base_address == 0);
+	vm_paging_base_address = page_map_offset;
+}
+
+/*
+ * ENCRYPTED SWAP:
+ * vm_paging_map_object:
+ *	Maps part of a VM object's pages in the kernel
+ * 	virtual address space, using the pre-allocated
+ *	kernel virtual addresses, if possible.
+ * Context:
+ * 	The VM object is locked.  This lock will get
+ * 	dropped and re-acquired though, so the caller
+ * 	must make sure the VM object is kept alive
+ *	(by holding a VM map that has a reference
+ * 	on it, for example, or taking an extra reference).
+ * 	The page should also be kept busy to prevent
+ *	it from being reclaimed.
+ */
+kern_return_t
+vm_paging_map_object(
+	vm_page_t		page,
+	vm_object_t		object,
+	vm_object_offset_t	offset,
+	vm_prot_t		protection,
+	boolean_t		can_unlock_object,
+	vm_map_size_t		*size,		/* IN/OUT */
+	vm_map_offset_t		*address,	/* OUT */
+	boolean_t		*need_unmap)	/* OUT */
+{
+	kern_return_t		kr;
+	vm_map_offset_t		page_map_offset;
+	vm_map_size_t		map_size;
+	vm_object_offset_t	object_offset;
+	int			i;
+
+	if (page != VM_PAGE_NULL && *size == PAGE_SIZE) {
+		/* use permanent 1-to-1 kernel mapping of physical memory ? */
+#if __x86_64__
+		*address = (vm_map_offset_t)
+			PHYSMAP_PTOV((pmap_paddr_t)page->phys_page <<
+				     PAGE_SHIFT);
+		*need_unmap = FALSE;
+		return KERN_SUCCESS;
+#else
+#warn "vm_paging_map_object: no 1-to-1 kernel mapping of physical memory..."
+#endif
+
+		assert(page->busy);
+		/*
+		 * Use one of the pre-allocated kernel virtual addresses
+		 * and just enter the VM page in the kernel address space
+		 * at that virtual address.
+		 */
+		simple_lock(&vm_paging_lock);
+
+		/*
+		 * Try and find an available kernel virtual address
+		 * from our pre-allocated pool.
+		 */
+		page_map_offset = 0;
+		for (;;) {
+			for (i = 0; i < VM_PAGING_NUM_PAGES; i++) {
+				if (vm_paging_page_inuse[i] == FALSE) {
+					page_map_offset =
+						vm_paging_base_address +
+						(i * PAGE_SIZE);
+					break;
+				}
+			}
+			if (page_map_offset != 0) {
+				/* found a space to map our page ! */
+				break;
+			}
+
+			if (can_unlock_object) {
+				/*
+				 * If we can afford to unlock the VM object,
+				 * let's take the slow path now...
+				 */
+				break;
+			}
+			/*
+			 * We can't afford to unlock the VM object, so
+			 * let's wait for a space to become available...
+			 */
+			vm_paging_page_waiter_total++;
+			vm_paging_page_waiter++;
+			kr = assert_wait((event_t)&vm_paging_page_waiter, THREAD_UNINT);
+			if (kr == THREAD_WAITING) {
+				simple_unlock(&vm_paging_lock);
+				kr = thread_block(THREAD_CONTINUE_NULL);
+				simple_lock(&vm_paging_lock);
+			}
+			vm_paging_page_waiter--;
+			/* ... and try again */
+		}
+
+		if (page_map_offset != 0) {
+			/*
+			 * We found a kernel virtual address;
+			 * map the physical page to that virtual address.
+			 */
+			if (i > vm_paging_max_index) {
+				vm_paging_max_index = i;
+			}
+			vm_paging_page_inuse[i] = TRUE;
+			simple_unlock(&vm_paging_lock);
+
+			page->pmapped = TRUE;
+
+			/*
+			 * Keep the VM object locked over the PMAP_ENTER
+			 * and the actual use of the page by the kernel,
+			 * or this pmap mapping might get undone by a 
+			 * vm_object_pmap_protect() call...
+			 */
+			PMAP_ENTER(kernel_pmap,
+				   page_map_offset,
+				   page,
+				   protection,
+				   VM_PROT_NONE,
+				   0,
+				   TRUE);
+			vm_paging_objects_mapped++;
+			vm_paging_pages_mapped++; 
+			*address = page_map_offset;
+			*need_unmap = TRUE;
+
+			/* all done and mapped, ready to use ! */
+			return KERN_SUCCESS;
+		}
+
+		/*
+		 * We ran out of pre-allocated kernel virtual
+		 * addresses.  Just map the page in the kernel
+		 * the slow and regular way.
+		 */
+		vm_paging_no_kernel_page++;
+		simple_unlock(&vm_paging_lock);
+	}
+
+	if (! can_unlock_object) {
+		*address = 0;
+		*size = 0;
+		*need_unmap = FALSE;
+		return KERN_NOT_SUPPORTED;
+	}
+
+	object_offset = vm_object_trunc_page(offset);
+	map_size = vm_map_round_page(*size,
+				     VM_MAP_PAGE_MASK(kernel_map));
+
+	/*
+	 * Try and map the required range of the object
+	 * in the kernel_map
+	 */
+
+	vm_object_reference_locked(object);	/* for the map entry */
+	vm_object_unlock(object);
+
+	kr = vm_map_enter(kernel_map,
+			  address,
+			  map_size,
+			  0,
+			  VM_FLAGS_ANYWHERE,
+			  object,
+			  object_offset,
+			  FALSE,
+			  protection,
+			  VM_PROT_ALL,
+			  VM_INHERIT_NONE);
+	if (kr != KERN_SUCCESS) {
+		*address = 0;
+		*size = 0;
+		*need_unmap = FALSE;
+		vm_object_deallocate(object);	/* for the map entry */
+		vm_object_lock(object);
+		return kr;
+	}
+
+	*size = map_size;
+
+	/*
+	 * Enter the mapped pages in the page table now.
+	 */
+	vm_object_lock(object);
+	/*
+	 * VM object must be kept locked from before PMAP_ENTER()
+	 * until after the kernel is done accessing the page(s).
+	 * Otherwise, the pmap mappings in the kernel could be
+	 * undone by a call to vm_object_pmap_protect().
+	 */
+
+	for (page_map_offset = 0;
+	     map_size != 0;
+	     map_size -= PAGE_SIZE_64, page_map_offset += PAGE_SIZE_64) {
+
+		page = vm_page_lookup(object, offset + page_map_offset);
+		if (page == VM_PAGE_NULL) {
+			printf("vm_paging_map_object: no page !?");
+			vm_object_unlock(object);
+			kr = vm_map_remove(kernel_map, *address, *size,
+					   VM_MAP_NO_FLAGS);
+			assert(kr == KERN_SUCCESS);
+			*address = 0;
+			*size = 0;
+			*need_unmap = FALSE;
+			vm_object_lock(object);
+			return KERN_MEMORY_ERROR;
+		}
+		page->pmapped = TRUE;
+
+		//assert(pmap_verify_free(page->phys_page));
+		PMAP_ENTER(kernel_pmap,
+			   *address + page_map_offset,
+			   page,
+			   protection,
+			   VM_PROT_NONE,
+			   0,
+			   TRUE);
+	}
+			   
+	vm_paging_objects_mapped_slow++;
+	vm_paging_pages_mapped_slow += (unsigned long) (map_size / PAGE_SIZE_64);
+
+	*need_unmap = TRUE;
+
+	return KERN_SUCCESS;
+}
+
+/*
+ * ENCRYPTED SWAP:
+ * vm_paging_unmap_object:
+ *	Unmaps part of a VM object's pages from the kernel
+ * 	virtual address space.
+ * Context:
+ * 	The VM object is locked.  This lock will get
+ * 	dropped and re-acquired though.
+ */
+void
+vm_paging_unmap_object(
+	vm_object_t	object,
+	vm_map_offset_t	start,
+	vm_map_offset_t	end)
+{
+	kern_return_t	kr;
+	int		i;
+
+	if ((vm_paging_base_address == 0) ||
+	    (start < vm_paging_base_address) ||
+	    (end > (vm_paging_base_address
+		     + (VM_PAGING_NUM_PAGES * PAGE_SIZE)))) {
+		/*
+		 * We didn't use our pre-allocated pool of
+		 * kernel virtual address.  Deallocate the
+		 * virtual memory.
+		 */
+		if (object != VM_OBJECT_NULL) {
+			vm_object_unlock(object);
+		}
+		kr = vm_map_remove(kernel_map, start, end, VM_MAP_NO_FLAGS);
+		if (object != VM_OBJECT_NULL) {
+			vm_object_lock(object);
+		}
+		assert(kr == KERN_SUCCESS);
+	} else {
+		/*
+		 * We used a kernel virtual address from our
+		 * pre-allocated pool.  Put it back in the pool
+		 * for next time.
+		 */
+		assert(end - start == PAGE_SIZE);
+		i = (int) ((start - vm_paging_base_address) >> PAGE_SHIFT);
+		assert(i >= 0 && i < VM_PAGING_NUM_PAGES);
+
+		/* undo the pmap mapping */
+		pmap_remove(kernel_pmap, start, end);
+
+		simple_lock(&vm_paging_lock);
+		vm_paging_page_inuse[i] = FALSE;
+		if (vm_paging_page_waiter) {
+			thread_wakeup(&vm_paging_page_waiter);
+		}
+		simple_unlock(&vm_paging_lock);
+	}
+}
+
+#if ENCRYPTED_SWAP
+/*
+ * Encryption data.
+ * "iv" is the "initial vector".  Ideally, we want to
+ * have a different one for each page we encrypt, so that
+ * crackers can't find encryption patterns too easily.
+ */
+#define SWAP_CRYPT_AES_KEY_SIZE	128	/* XXX 192 and 256 don't work ! */
+boolean_t		swap_crypt_ctx_initialized = FALSE;
+uint32_t 		swap_crypt_key[8]; /* big enough for a 256 key */
+aes_ctx			swap_crypt_ctx;
+const unsigned char	swap_crypt_null_iv[AES_BLOCK_SIZE] = {0xa, };
+
+#if DEBUG
+boolean_t		swap_crypt_ctx_tested = FALSE;
+unsigned char swap_crypt_test_page_ref[4096] __attribute__((aligned(4096)));
+unsigned char swap_crypt_test_page_encrypt[4096] __attribute__((aligned(4096)));
+unsigned char swap_crypt_test_page_decrypt[4096] __attribute__((aligned(4096)));
+#endif /* DEBUG */
+
+/*
+ * Initialize the encryption context: key and key size.
+ */
+void swap_crypt_ctx_initialize(void); /* forward */
+void
+swap_crypt_ctx_initialize(void)
+{
+	unsigned int	i;
+
+	/*
+	 * No need for locking to protect swap_crypt_ctx_initialized
+	 * because the first use of encryption will come from the
+	 * pageout thread (we won't pagein before there's been a pageout)
+	 * and there's only one pageout thread.
+	 */
+	if (swap_crypt_ctx_initialized == FALSE) {
+		for (i = 0;
+		     i < (sizeof (swap_crypt_key) /
+			  sizeof (swap_crypt_key[0]));
+		     i++) {
+			swap_crypt_key[i] = random();
+		}
+		aes_encrypt_key((const unsigned char *) swap_crypt_key,
+				SWAP_CRYPT_AES_KEY_SIZE,
+				&swap_crypt_ctx.encrypt);
+		aes_decrypt_key((const unsigned char *) swap_crypt_key,
+				SWAP_CRYPT_AES_KEY_SIZE,
+				&swap_crypt_ctx.decrypt);
+		swap_crypt_ctx_initialized = TRUE;
+	}
+
+#if DEBUG
+	/*
+	 * Validate the encryption algorithms.
+	 */
+	if (swap_crypt_ctx_tested == FALSE) {
+		/* initialize */
+		for (i = 0; i < 4096; i++) {
+			swap_crypt_test_page_ref[i] = (char) i;
+		}
+		/* encrypt */
+		aes_encrypt_cbc(swap_crypt_test_page_ref,
+				swap_crypt_null_iv,
+				PAGE_SIZE / AES_BLOCK_SIZE,
+				swap_crypt_test_page_encrypt,
+				&swap_crypt_ctx.encrypt);
+		/* decrypt */
+		aes_decrypt_cbc(swap_crypt_test_page_encrypt,
+				swap_crypt_null_iv,
+				PAGE_SIZE / AES_BLOCK_SIZE,
+				swap_crypt_test_page_decrypt,
+				&swap_crypt_ctx.decrypt);
+		/* compare result with original */
+		for (i = 0; i < 4096; i ++) {
+			if (swap_crypt_test_page_decrypt[i] !=
+			    swap_crypt_test_page_ref[i]) {
+				panic("encryption test failed");
+			}
+		}
+
+		/* encrypt again */
+		aes_encrypt_cbc(swap_crypt_test_page_decrypt,
+				swap_crypt_null_iv,
+				PAGE_SIZE / AES_BLOCK_SIZE,
+				swap_crypt_test_page_decrypt,
+				&swap_crypt_ctx.encrypt);
+		/* decrypt in place */
+		aes_decrypt_cbc(swap_crypt_test_page_decrypt,
+				swap_crypt_null_iv,
+				PAGE_SIZE / AES_BLOCK_SIZE,
+				swap_crypt_test_page_decrypt,
+				&swap_crypt_ctx.decrypt);
+		for (i = 0; i < 4096; i ++) {
+			if (swap_crypt_test_page_decrypt[i] !=
+			    swap_crypt_test_page_ref[i]) {
+				panic("in place encryption test failed");
+			}
+		}
+
+		swap_crypt_ctx_tested = TRUE;
+	}
+#endif /* DEBUG */
+}
+
+/*
+ * ENCRYPTED SWAP:
+ * vm_page_encrypt:
+ * 	Encrypt the given page, for secure paging.
+ * 	The page might already be mapped at kernel virtual
+ * 	address "kernel_mapping_offset".  Otherwise, we need
+ * 	to map it.
+ * 
+ * Context:
+ * 	The page's object is locked, but this lock will be released
+ * 	and re-acquired.
+ * 	The page is busy and not accessible by users (not entered in any pmap).
+ */
+void
+vm_page_encrypt(
+	vm_page_t	page,
+	vm_map_offset_t	kernel_mapping_offset)
+{
+	kern_return_t		kr;
+	vm_map_size_t		kernel_mapping_size;
+	boolean_t		kernel_mapping_needs_unmap;
+	vm_offset_t		kernel_vaddr;
+	union {
+		unsigned char	aes_iv[AES_BLOCK_SIZE];
+		struct {
+			memory_object_t		pager_object;
+			vm_object_offset_t	paging_offset;
+		} vm;
+	} encrypt_iv;
+
+	if (! vm_pages_encrypted) {
+		vm_pages_encrypted = TRUE;
+	}
+
+	assert(page->busy);
+	
+	if (page->encrypted) {
+		/*
+		 * Already encrypted: no need to do it again.
+		 */
+		vm_page_encrypt_already_encrypted_counter++;
+		return;
+	}
+	assert(page->dirty || page->precious);
+
+	ASSERT_PAGE_DECRYPTED(page);
+
+	/*
+	 * Take a paging-in-progress reference to keep the object
+	 * alive even if we have to unlock it (in vm_paging_map_object()
+	 * for example)...
+	 */
+	vm_object_paging_begin(page->object);
+
+	if (kernel_mapping_offset == 0) {
+		/*
+		 * The page hasn't already been mapped in kernel space
+		 * by the caller.  Map it now, so that we can access
+		 * its contents and encrypt them.
+		 */
+		kernel_mapping_size = PAGE_SIZE;
+		kernel_mapping_needs_unmap = FALSE;
+		kr = vm_paging_map_object(page,
+					  page->object,
+					  page->offset,
+					  VM_PROT_READ | VM_PROT_WRITE,
+					  FALSE,
+					  &kernel_mapping_size,
+					  &kernel_mapping_offset,
+					  &kernel_mapping_needs_unmap);
+		if (kr != KERN_SUCCESS) {
+			panic("vm_page_encrypt: "
+			      "could not map page in kernel: 0x%x\n",
+			      kr);
+		}
+	} else {
+		kernel_mapping_size = 0;
+		kernel_mapping_needs_unmap = FALSE;
+	}
+	kernel_vaddr = CAST_DOWN(vm_offset_t, kernel_mapping_offset);
+
+	if (swap_crypt_ctx_initialized == FALSE) {
+		swap_crypt_ctx_initialize();
+	}
+	assert(swap_crypt_ctx_initialized);
+
+	/*
+	 * Prepare an "initial vector" for the encryption.
+	 * We use the "pager" and the "paging_offset" for that
+	 * page to obfuscate the encrypted data a bit more and
+	 * prevent crackers from finding patterns that they could
+	 * use to break the key.
+	 */
+	bzero(&encrypt_iv.aes_iv[0], sizeof (encrypt_iv.aes_iv));
+	encrypt_iv.vm.pager_object = page->object->pager;
+	encrypt_iv.vm.paging_offset =
+		page->object->paging_offset + page->offset;
+
+	/* encrypt the "initial vector" */
+	aes_encrypt_cbc((const unsigned char *) &encrypt_iv.aes_iv[0],
+			swap_crypt_null_iv,
+			1,
+			&encrypt_iv.aes_iv[0],
+			&swap_crypt_ctx.encrypt);
+		  
+	/*
+	 * Encrypt the page.
+	 */
+	aes_encrypt_cbc((const unsigned char *) kernel_vaddr,
+			&encrypt_iv.aes_iv[0],
+			PAGE_SIZE / AES_BLOCK_SIZE,
+			(unsigned char *) kernel_vaddr,
+			&swap_crypt_ctx.encrypt);
+
+	vm_page_encrypt_counter++;
+
+	/*
+	 * Unmap the page from the kernel's address space,
+	 * if we had to map it ourselves.  Otherwise, let
+	 * the caller undo the mapping if needed.
+	 */
+	if (kernel_mapping_needs_unmap) {
+		vm_paging_unmap_object(page->object,
+				       kernel_mapping_offset,
+				       kernel_mapping_offset + kernel_mapping_size);
+	}
+
+	/*
+	 * Clear the "reference" and "modified" bits.
+	 * This should clean up any impact the encryption had
+	 * on them.
+	 * The page was kept busy and disconnected from all pmaps,
+	 * so it can't have been referenced or modified from user
+	 * space.
+	 * The software bits will be reset later after the I/O
+	 * has completed (in upl_commit_range()).
+	 */
+	pmap_clear_refmod(page->phys_page, VM_MEM_REFERENCED | VM_MEM_MODIFIED);
+
+	page->encrypted = TRUE;
+
+	vm_object_paging_end(page->object);
+}
+
+/*
+ * ENCRYPTED SWAP:
+ * vm_page_decrypt:
+ * 	Decrypt the given page.
+ * 	The page might already be mapped at kernel virtual
+ * 	address "kernel_mapping_offset".  Otherwise, we need
+ * 	to map it.
+ *
+ * Context:
+ *	The page's VM object is locked but will be unlocked and relocked.
+ * 	The page is busy and not accessible by users (not entered in any pmap).
+ */
+void
+vm_page_decrypt(
+	vm_page_t	page,
+	vm_map_offset_t	kernel_mapping_offset)
+{
+	kern_return_t		kr;
+	vm_map_size_t		kernel_mapping_size;
+	vm_offset_t		kernel_vaddr;
+	boolean_t		kernel_mapping_needs_unmap;
+	union {
+		unsigned char	aes_iv[AES_BLOCK_SIZE];
+		struct {
+			memory_object_t		pager_object;
+			vm_object_offset_t	paging_offset;
+		} vm;
+	} decrypt_iv;
+	boolean_t		was_dirty;
+
+	assert(page->busy);
+	assert(page->encrypted);
+
+	was_dirty = page->dirty;
+
+	/*
+	 * Take a paging-in-progress reference to keep the object
+	 * alive even if we have to unlock it (in vm_paging_map_object()
+	 * for example)...
+	 */
+	vm_object_paging_begin(page->object);
+
+	if (kernel_mapping_offset == 0) {
+		/*
+		 * The page hasn't already been mapped in kernel space
+		 * by the caller.  Map it now, so that we can access
+		 * its contents and decrypt them.
+		 */
+		kernel_mapping_size = PAGE_SIZE;
+		kernel_mapping_needs_unmap = FALSE;
+		kr = vm_paging_map_object(page,
+					  page->object,
+					  page->offset,
+					  VM_PROT_READ | VM_PROT_WRITE,
+					  FALSE,
+					  &kernel_mapping_size,
+					  &kernel_mapping_offset,
+					  &kernel_mapping_needs_unmap);
+		if (kr != KERN_SUCCESS) {
+			panic("vm_page_decrypt: "
+			      "could not map page in kernel: 0x%x\n",
+			      kr);
+		}
+	} else {
+		kernel_mapping_size = 0;
+		kernel_mapping_needs_unmap = FALSE;
+	}
+	kernel_vaddr = CAST_DOWN(vm_offset_t, kernel_mapping_offset);
+
+	assert(swap_crypt_ctx_initialized);
+
+	/*
+	 * Prepare an "initial vector" for the decryption.
+	 * It has to be the same as the "initial vector" we
+	 * used to encrypt that page.
+	 */
+	bzero(&decrypt_iv.aes_iv[0], sizeof (decrypt_iv.aes_iv));
+	decrypt_iv.vm.pager_object = page->object->pager;
+	decrypt_iv.vm.paging_offset =
+		page->object->paging_offset + page->offset;
+
+	/* encrypt the "initial vector" */
+	aes_encrypt_cbc((const unsigned char *) &decrypt_iv.aes_iv[0],
+			swap_crypt_null_iv,
+			1,
+			&decrypt_iv.aes_iv[0],
+			&swap_crypt_ctx.encrypt);
+
+	/*
+	 * Decrypt the page.
+	 */
+	aes_decrypt_cbc((const unsigned char *) kernel_vaddr,
+			&decrypt_iv.aes_iv[0],
+			PAGE_SIZE / AES_BLOCK_SIZE,
+			(unsigned char *) kernel_vaddr,
+			&swap_crypt_ctx.decrypt);
+	vm_page_decrypt_counter++;
+
+	/*
+	 * Unmap the page from the kernel's address space,
+	 * if we had to map it ourselves.  Otherwise, let
+	 * the caller undo the mapping if needed.
+	 */
+	if (kernel_mapping_needs_unmap) {
+		vm_paging_unmap_object(page->object,
+				       kernel_vaddr,
+				       kernel_vaddr + PAGE_SIZE);
+	}
+
+	if (was_dirty) {
+		/*
+		 * The pager did not specify that the page would be
+		 * clean when it got paged in, so let's not clean it here
+		 * either.
+		 */
+	} else {
+		/*
+		 * After decryption, the page is actually still clean.
+		 * It was encrypted as part of paging, which "cleans"
+		 * the "dirty" pages.
+		 * Noone could access it after it was encrypted
+		 * and the decryption doesn't count.
+		 */
+		page->dirty = FALSE;
+		assert (page->cs_validated == FALSE);
+		pmap_clear_refmod(page->phys_page, VM_MEM_MODIFIED | VM_MEM_REFERENCED);
+	}
+	page->encrypted = FALSE;
+
+	/*
+	 * We've just modified the page's contents via the data cache and part
+	 * of the new contents might still be in the cache and not yet in RAM.
+	 * Since the page is now available and might get gathered in a UPL to
+	 * be part of a DMA transfer from a driver that expects the memory to
+	 * be coherent at this point, we have to flush the data cache.
+	 */
+	pmap_sync_page_attributes_phys(page->phys_page);
+	/*
+	 * Since the page is not mapped yet, some code might assume that it
+	 * doesn't need to invalidate the instruction cache when writing to
+	 * that page.  That code relies on "pmapped" being FALSE, so that the
+	 * caches get synchronized when the page is first mapped.
+	 */
+	assert(pmap_verify_free(page->phys_page));
+	page->pmapped = FALSE;
+	page->wpmapped = FALSE;
+
+	vm_object_paging_end(page->object);
+}
+
+#if DEVELOPMENT || DEBUG
+unsigned long upl_encrypt_upls = 0;
+unsigned long upl_encrypt_pages = 0;
+#endif
+
+/*
+ * ENCRYPTED SWAP:
+ *
+ * upl_encrypt:
+ * 	Encrypts all the pages in the UPL, within the specified range.
+ *
+ */
+void
+upl_encrypt(
+	upl_t			upl,
+	upl_offset_t		crypt_offset,
+	upl_size_t		crypt_size)
+{
+	upl_size_t		upl_size, subupl_size=crypt_size;
+	upl_offset_t		offset_in_upl, subupl_offset=crypt_offset;
+	vm_object_t		upl_object;
+	vm_object_offset_t	upl_offset;
+	vm_page_t		page;
+	vm_object_t		shadow_object;
+	vm_object_offset_t	shadow_offset;
+	vm_object_offset_t	paging_offset;
+	vm_object_offset_t	base_offset;
+	int	 		isVectorUPL = 0;
+	upl_t			vector_upl = NULL;
+
+	if((isVectorUPL = vector_upl_is_valid(upl)))
+		vector_upl = upl;
+
+process_upl_to_encrypt:
+	if(isVectorUPL) {
+		crypt_size = subupl_size;
+		crypt_offset = subupl_offset;
+		upl =  vector_upl_subupl_byoffset(vector_upl, &crypt_offset, &crypt_size);
+		if(upl == NULL)
+			panic("upl_encrypt: Accessing a sub-upl that doesn't exist\n");
+		subupl_size -= crypt_size;
+		subupl_offset += crypt_size;
+	}
+
+#if DEVELOPMENT || DEBUG
+	upl_encrypt_upls++;
+	upl_encrypt_pages += crypt_size / PAGE_SIZE;
+#endif
+	upl_object = upl->map_object;
+	upl_offset = upl->offset;
+	upl_size = upl->size;
+
+	vm_object_lock(upl_object);
+
+	/*
+	 * Find the VM object that contains the actual pages.
+	 */
+	if (upl_object->pageout) {
+		shadow_object = upl_object->shadow;
+		/*
+		 * The offset in the shadow object is actually also
+		 * accounted for in upl->offset.  It possibly shouldn't be
+		 * this way, but for now don't account for it twice.
+		 */
+		shadow_offset = 0;
+		assert(upl_object->paging_offset == 0);	/* XXX ? */
+		vm_object_lock(shadow_object);
+	} else {
+		shadow_object = upl_object;
+		shadow_offset = 0;
+	}
+
+	paging_offset = shadow_object->paging_offset;
+	vm_object_paging_begin(shadow_object);
+
+	if (shadow_object != upl_object)
+	        vm_object_unlock(upl_object);
+
+
+	base_offset = shadow_offset;
+	base_offset += upl_offset;
+	base_offset += crypt_offset;
+	base_offset -= paging_offset;
+
+	assert(crypt_offset + crypt_size <= upl_size);
+
+	for (offset_in_upl = 0;
+	     offset_in_upl < crypt_size;
+	     offset_in_upl += PAGE_SIZE) {
+		page = vm_page_lookup(shadow_object,
+				      base_offset + offset_in_upl);
+		if (page == VM_PAGE_NULL) {
+			panic("upl_encrypt: "
+			      "no page for (obj=%p,off=0x%llx+0x%x)!\n",
+			      shadow_object,
+			      base_offset,
+			      offset_in_upl);
+		}
+		/*
+		 * Disconnect the page from all pmaps, so that nobody can
+		 * access it while it's encrypted.  After that point, all
+		 * accesses to this page will cause a page fault and block
+		 * while the page is busy being encrypted.  After the
+		 * encryption completes, any access will cause a
+		 * page fault and the page gets decrypted at that time.
+		 */
+		pmap_disconnect(page->phys_page);
+		vm_page_encrypt(page, 0);
+
+		if (vm_object_lock_avoid(shadow_object)) {
+			/*
+			 * Give vm_pageout_scan() a chance to convert more
+			 * pages from "clean-in-place" to "clean-and-free",
+			 * if it's interested in the same pages we selected
+			 * in this cluster.
+			 */
+			vm_object_unlock(shadow_object);
+			mutex_pause(2);
+			vm_object_lock(shadow_object);
+		}
+	}
+
+	vm_object_paging_end(shadow_object);
+	vm_object_unlock(shadow_object);
+	
+	if(isVectorUPL && subupl_size)
+		goto process_upl_to_encrypt;
+}
+
+#else /* ENCRYPTED_SWAP */
+void
+upl_encrypt(
+	__unused upl_t			upl,
+	__unused upl_offset_t	crypt_offset,
+	__unused upl_size_t	crypt_size)
+{
+}
+
+void
+vm_page_encrypt(
+	__unused vm_page_t		page,
+	__unused vm_map_offset_t	kernel_mapping_offset)
+{
+} 
+
+void
+vm_page_decrypt(
+	__unused vm_page_t		page,
+	__unused vm_map_offset_t	kernel_mapping_offset)
+{
+}
+
+#endif /* ENCRYPTED_SWAP */
+
+/*
+ * page->object must be locked
+ */
+void
+vm_pageout_steal_laundry(vm_page_t page, boolean_t queues_locked)
+{
+	if (!queues_locked) {
+		vm_page_lockspin_queues();
+	}
+
+	/*
+	 * need to drop the laundry count...
+	 * we may also need to remove it
+	 * from the I/O paging queue...
+	 * vm_pageout_throttle_up handles both cases
+	 *
+	 * the laundry and pageout_queue flags are cleared...
+	 */
+	vm_pageout_throttle_up(page);
+
+	vm_page_steal_pageout_page++;
+
+	if (!queues_locked) {
+		vm_page_unlock_queues();
+	}
+}
+
+upl_t
+vector_upl_create(vm_offset_t upl_offset)
+{
+	int	vector_upl_size  = sizeof(struct _vector_upl);
+	int i=0;
+	upl_t	upl;
+	vector_upl_t vector_upl = (vector_upl_t)kalloc(vector_upl_size);
+
+	upl = upl_create(0,UPL_VECTOR,0);
+	upl->vector_upl = vector_upl;
+	upl->offset = upl_offset;
+	vector_upl->size = 0;
+	vector_upl->offset = upl_offset;
+	vector_upl->invalid_upls=0;
+	vector_upl->num_upls=0;
+	vector_upl->pagelist = NULL;
+	
+	for(i=0; i < MAX_VECTOR_UPL_ELEMENTS ; i++) {
+		vector_upl->upl_iostates[i].size = 0;
+		vector_upl->upl_iostates[i].offset = 0;
+		
+	}
+	return upl;
+}
+
+void
+vector_upl_deallocate(upl_t upl)
+{
+	if(upl) {
+		vector_upl_t vector_upl = upl->vector_upl;
+		if(vector_upl) {
+			if(vector_upl->invalid_upls != vector_upl->num_upls)
+				panic("Deallocating non-empty Vectored UPL\n");
+			kfree(vector_upl->pagelist,(sizeof(struct upl_page_info)*(vector_upl->size/PAGE_SIZE)));
+			vector_upl->invalid_upls=0;
+			vector_upl->num_upls = 0;
+			vector_upl->pagelist = NULL;
+			vector_upl->size = 0;
+			vector_upl->offset = 0;
+			kfree(vector_upl, sizeof(struct _vector_upl));
+			vector_upl = (vector_upl_t)0xfeedfeed;
+		}
+		else
+			panic("vector_upl_deallocate was passed a non-vectored upl\n");
+	}
+	else
+		panic("vector_upl_deallocate was passed a NULL upl\n");
+}
+
+boolean_t
+vector_upl_is_valid(upl_t upl)
+{
+	if(upl &&  ((upl->flags & UPL_VECTOR)==UPL_VECTOR)) {
+		vector_upl_t vector_upl = upl->vector_upl;
+		if(vector_upl == NULL || vector_upl == (vector_upl_t)0xfeedfeed || vector_upl == (vector_upl_t)0xfeedbeef)
+			return FALSE;
+		else
+			return TRUE;
+	}
+	return FALSE;
+}
+
+boolean_t
+vector_upl_set_subupl(upl_t upl,upl_t subupl, uint32_t io_size)
+{
+	if(vector_upl_is_valid(upl)) {		
+		vector_upl_t vector_upl = upl->vector_upl;
+		
+		if(vector_upl) {
+			if(subupl) {
+				if(io_size) {
+					if(io_size < PAGE_SIZE)
+						io_size = PAGE_SIZE;
+					subupl->vector_upl = (void*)vector_upl;
+					vector_upl->upl_elems[vector_upl->num_upls++] = subupl;
+					vector_upl->size += io_size;
+					upl->size += io_size;
+				}
+				else {
+					uint32_t i=0,invalid_upls=0;
+					for(i = 0; i < vector_upl->num_upls; i++) {
+						if(vector_upl->upl_elems[i] == subupl)
+							break;
+					}
+					if(i == vector_upl->num_upls)
+						panic("Trying to remove sub-upl when none exists");
+					
+					vector_upl->upl_elems[i] = NULL;
+					invalid_upls = hw_atomic_add(&(vector_upl)->invalid_upls, 1); 
+					if(invalid_upls == vector_upl->num_upls)
+						return TRUE;
+					else 
+						return FALSE;
+				}
+			}
+			else
+				panic("vector_upl_set_subupl was passed a NULL upl element\n");
+		}
+		else
+			panic("vector_upl_set_subupl was passed a non-vectored upl\n");
+	}
+	else
+		panic("vector_upl_set_subupl was passed a NULL upl\n");
+
+	return FALSE;
+}	
+
+void
+vector_upl_set_pagelist(upl_t upl)
+{
+	if(vector_upl_is_valid(upl)) {		
+		uint32_t i=0;
+		vector_upl_t vector_upl = upl->vector_upl;
+
+		if(vector_upl) {
+			vm_offset_t pagelist_size=0, cur_upl_pagelist_size=0;
+
+			vector_upl->pagelist = (upl_page_info_array_t)kalloc(sizeof(struct upl_page_info)*(vector_upl->size/PAGE_SIZE));
+			
+			for(i=0; i < vector_upl->num_upls; i++) {
+				cur_upl_pagelist_size = sizeof(struct upl_page_info) * vector_upl->upl_elems[i]->size/PAGE_SIZE;
+				bcopy(UPL_GET_INTERNAL_PAGE_LIST_SIMPLE(vector_upl->upl_elems[i]), (char*)vector_upl->pagelist + pagelist_size, cur_upl_pagelist_size);
+				pagelist_size += cur_upl_pagelist_size;
+				if(vector_upl->upl_elems[i]->highest_page > upl->highest_page)
+					upl->highest_page = vector_upl->upl_elems[i]->highest_page;
+			}
+			assert( pagelist_size == (sizeof(struct upl_page_info)*(vector_upl->size/PAGE_SIZE)) );
+		}
+		else
+			panic("vector_upl_set_pagelist was passed a non-vectored upl\n");
+	}
+	else
+		panic("vector_upl_set_pagelist was passed a NULL upl\n");
+
+}
+
+upl_t
+vector_upl_subupl_byindex(upl_t upl, uint32_t index)
+{
+	if(vector_upl_is_valid(upl)) {		
+		vector_upl_t vector_upl = upl->vector_upl;
+		if(vector_upl) {
+			if(index < vector_upl->num_upls)
+				return vector_upl->upl_elems[index];
+		}
+		else
+			panic("vector_upl_subupl_byindex was passed a non-vectored upl\n");
+	}
+	return NULL;
+}
+
+upl_t
+vector_upl_subupl_byoffset(upl_t upl, upl_offset_t *upl_offset, upl_size_t *upl_size)
+{
+	if(vector_upl_is_valid(upl)) {		
+		uint32_t i=0;
+		vector_upl_t vector_upl = upl->vector_upl;
+
+		if(vector_upl) {
+			upl_t subupl = NULL;
+			vector_upl_iostates_t subupl_state;
+
+			for(i=0; i < vector_upl->num_upls; i++) {
+				subupl = vector_upl->upl_elems[i];
+				subupl_state = vector_upl->upl_iostates[i];
+				if( *upl_offset <= (subupl_state.offset + subupl_state.size - 1)) {
+					/* We could have been passed an offset/size pair that belongs
+					 * to an UPL element that has already been committed/aborted.
+					 * If so, return NULL.
+					 */
+					if(subupl == NULL)
+						return NULL;
+					if((subupl_state.offset + subupl_state.size) < (*upl_offset + *upl_size)) {
+						*upl_size = (subupl_state.offset + subupl_state.size) - *upl_offset;
+						if(*upl_size > subupl_state.size)
+							*upl_size = subupl_state.size;
+					}
+					if(*upl_offset >= subupl_state.offset)
+						*upl_offset -= subupl_state.offset;
+					else if(i)
+						panic("Vector UPL offset miscalculation\n");
+					return subupl;
+				}	
+			}
+		}
+		else
+			panic("vector_upl_subupl_byoffset was passed a non-vectored UPL\n");
+	}
+	return NULL;
+}
+
+void
+vector_upl_get_submap(upl_t upl, vm_map_t *v_upl_submap, vm_offset_t *submap_dst_addr)
+{
+	*v_upl_submap = NULL;
+
+	if(vector_upl_is_valid(upl)) {		
+		vector_upl_t vector_upl = upl->vector_upl;
+		if(vector_upl) {
+			*v_upl_submap = vector_upl->submap;
+			*submap_dst_addr = vector_upl->submap_dst_addr;
+		}
+		else
+			panic("vector_upl_get_submap was passed a non-vectored UPL\n");
+	}
+	else
+		panic("vector_upl_get_submap was passed a null UPL\n");
+}
+
+void
+vector_upl_set_submap(upl_t upl, vm_map_t submap, vm_offset_t submap_dst_addr)
+{
+	if(vector_upl_is_valid(upl)) {		
+		vector_upl_t vector_upl = upl->vector_upl;
+		if(vector_upl) {
+			vector_upl->submap = submap;
+			vector_upl->submap_dst_addr = submap_dst_addr;
+		}
+		else
+			panic("vector_upl_get_submap was passed a non-vectored UPL\n");
+	}
+	else
+		panic("vector_upl_get_submap was passed a NULL UPL\n");
+}
+
+void
+vector_upl_set_iostate(upl_t upl, upl_t subupl, upl_offset_t offset, upl_size_t size)
+{
+	if(vector_upl_is_valid(upl)) {		
+		uint32_t i = 0;
+		vector_upl_t vector_upl = upl->vector_upl;
+
+		if(vector_upl) {
+			for(i = 0; i < vector_upl->num_upls; i++) {
+				if(vector_upl->upl_elems[i] == subupl)
+					break;
+			}
+			
+			if(i == vector_upl->num_upls)
+				panic("setting sub-upl iostate when none exists");
+
+			vector_upl->upl_iostates[i].offset = offset;
+			if(size < PAGE_SIZE)
+				size = PAGE_SIZE;
+			vector_upl->upl_iostates[i].size = size;
+		}
+		else
+			panic("vector_upl_set_iostate was passed a non-vectored UPL\n");
+	}
+	else
+		panic("vector_upl_set_iostate was passed a NULL UPL\n");
+}
+
+void
+vector_upl_get_iostate(upl_t upl, upl_t subupl, upl_offset_t *offset, upl_size_t *size)
+{
+	if(vector_upl_is_valid(upl)) {		
+		uint32_t i = 0;
+		vector_upl_t vector_upl = upl->vector_upl;
+
+		if(vector_upl) {
+			for(i = 0; i < vector_upl->num_upls; i++) {
+				if(vector_upl->upl_elems[i] == subupl)
+					break;
+			}
+			
+			if(i == vector_upl->num_upls)
+				panic("getting sub-upl iostate when none exists");
+
+			*offset = vector_upl->upl_iostates[i].offset;
+			*size = vector_upl->upl_iostates[i].size;
+		}
+		else
+			panic("vector_upl_get_iostate was passed a non-vectored UPL\n");
+	}
+	else
+		panic("vector_upl_get_iostate was passed a NULL UPL\n");
+}
+
+void
+vector_upl_get_iostate_byindex(upl_t upl, uint32_t index, upl_offset_t *offset, upl_size_t *size)
+{
+	if(vector_upl_is_valid(upl)) {		
+		vector_upl_t vector_upl = upl->vector_upl;
+		if(vector_upl) {
+			if(index < vector_upl->num_upls) {
+				*offset = vector_upl->upl_iostates[index].offset;
+				*size = vector_upl->upl_iostates[index].size;
+			}
+			else
+				*offset = *size = 0;
+		}
+		else
+			panic("vector_upl_get_iostate_byindex was passed a non-vectored UPL\n");
+	}
+	else
+		panic("vector_upl_get_iostate_byindex was passed a NULL UPL\n");
+}
+
+upl_page_info_t *
+upl_get_internal_vectorupl_pagelist(upl_t upl)
+{
+	return ((vector_upl_t)(upl->vector_upl))->pagelist;
+}
+
+void *
+upl_get_internal_vectorupl(upl_t upl)
+{
+	return upl->vector_upl;
+}
+
+vm_size_t
+upl_get_internal_pagelist_offset(void)
+{
+	return sizeof(struct upl);
+}
+
+void
+upl_clear_dirty(
+	upl_t		upl,
+	boolean_t 	value)
+{
+	if (value) {
+		upl->flags |= UPL_CLEAR_DIRTY;
+	} else {
+		upl->flags &= ~UPL_CLEAR_DIRTY;
+	}
+}
+
+void
+upl_set_referenced(
+	upl_t		upl,
+	boolean_t 	value)
+{
+	upl_lock(upl);
+	if (value) {
+		upl->ext_ref_count++;
+	} else {
+		if (!upl->ext_ref_count) {
+			panic("upl_set_referenced not %p\n", upl);
+		}
+		upl->ext_ref_count--;
+	}
+	upl_unlock(upl);
+}
+
+#if CONFIG_IOSCHED
+void
+upl_set_blkno(
+	upl_t		upl,
+	vm_offset_t	upl_offset,
+	int		io_size,
+	int64_t		blkno)
+{
+		int i,j;
+		if ((upl->flags & UPL_EXPEDITE_SUPPORTED) == 0)
+			return;
+			
+		assert(upl->upl_reprio_info != 0);	
+		for(i = (int)(upl_offset / PAGE_SIZE), j = 0; j < io_size; i++, j += PAGE_SIZE) {
+			UPL_SET_REPRIO_INFO(upl, i, blkno, io_size);
+		}
+}
+#endif
+
+boolean_t
+vm_page_is_slideable(vm_page_t m)
+{
+	boolean_t result = FALSE;
+	vm_shared_region_slide_info_t si;
+
+	vm_object_lock_assert_held(m->object);
+
+	/* make sure our page belongs to the one object allowed to do this */
+	if (!m->object->object_slid) {
+		goto done;
+	}
+
+	si = m->object->vo_slide_info;
+	if (si == NULL) {
+		goto done;
+	}
+
+	if(!m->slid && (si->start <= m->offset && si->end > m->offset)) {
+		result = TRUE;
+	}
+
+done:
+	return result;
+}
+
+int vm_page_slide_counter = 0;
+int vm_page_slide_errors = 0;
+kern_return_t
+vm_page_slide(
+	vm_page_t	page,
+	vm_map_offset_t	kernel_mapping_offset)
+{
+	kern_return_t		kr;
+	vm_map_size_t		kernel_mapping_size;
+	boolean_t		kernel_mapping_needs_unmap;
+	vm_offset_t		kernel_vaddr;
+	uint32_t		pageIndex;
+	uint32_t		slide_chunk;
+
+	assert(!page->slid);
+	assert(page->object->object_slid);
+	vm_object_lock_assert_exclusive(page->object);
+
+	if (page->error)
+		return KERN_FAILURE;
+	
+	/*
+	 * Take a paging-in-progress reference to keep the object
+	 * alive even if we have to unlock it (in vm_paging_map_object()
+	 * for example)...
+	 */
+	vm_object_paging_begin(page->object);
+
+	if (kernel_mapping_offset == 0) {
+		/*
+		 * The page hasn't already been mapped in kernel space
+		 * by the caller.  Map it now, so that we can access
+		 * its contents and decrypt them.
+		 */
+		kernel_mapping_size = PAGE_SIZE;
+		kernel_mapping_needs_unmap = FALSE;
+		kr = vm_paging_map_object(page,
+					  page->object,
+					  page->offset,
+					  VM_PROT_READ | VM_PROT_WRITE,
+					  FALSE,
+					  &kernel_mapping_size,
+					  &kernel_mapping_offset,
+					  &kernel_mapping_needs_unmap);
+		if (kr != KERN_SUCCESS) {
+			panic("vm_page_slide: "
+			      "could not map page in kernel: 0x%x\n",
+			      kr);
+		}
+	} else {
+		kernel_mapping_size = 0;
+		kernel_mapping_needs_unmap = FALSE;
+	}
+	kernel_vaddr = CAST_DOWN(vm_offset_t, kernel_mapping_offset);
+
+	/*
+	 * Slide the pointers on the page.
+	 */
+
+	/*assert that slide_file_info.start/end are page-aligned?*/
+
+	assert(!page->slid);
+	assert(page->object->object_slid);
+
+#define PAGE_SIZE_FOR_SR_SLIDE 4096
+	pageIndex = (uint32_t)((page->offset -
+				page->object->vo_slide_info->start) /
+			       PAGE_SIZE_FOR_SR_SLIDE);
+	for (slide_chunk = 0;
+	     slide_chunk < PAGE_SIZE / PAGE_SIZE_FOR_SR_SLIDE;
+	     slide_chunk++) {
+		kr = vm_shared_region_slide_page(page->object->vo_slide_info,
+						 (kernel_vaddr +
+						  (slide_chunk *
+						   PAGE_SIZE_FOR_SR_SLIDE)),
+						 (pageIndex + slide_chunk));
+		if (kr != KERN_SUCCESS) {
+			break;
+		}
+	}
+
+	vm_page_slide_counter++;
+
+	/*
+	 * Unmap the page from the kernel's address space,
+	 */
+	if (kernel_mapping_needs_unmap) {
+		vm_paging_unmap_object(page->object,
+				       kernel_vaddr,
+				       kernel_vaddr + PAGE_SIZE);
+	}
+	
+	page->dirty = FALSE;
+	pmap_clear_refmod(page->phys_page, VM_MEM_MODIFIED | VM_MEM_REFERENCED);
+	
+	if (kr != KERN_SUCCESS || cs_debug > 1) {
+		printf("vm_page_slide(%p): "
+		       "obj %p off 0x%llx mobj %p moff 0x%llx\n",
+		       page,
+		       page->object, page->offset,
+		       page->object->pager,
+		       page->offset + page->object->paging_offset);
+	}
+
+	if (kr == KERN_SUCCESS) {
+		page->slid = TRUE;
+	} else {
+		page->error = TRUE;
+		vm_page_slide_errors++;
+	}
+
+	vm_object_paging_end(page->object);
+
+	return kr;
+}
+
+void inline memoryshot(unsigned int event, unsigned int control)
+{
+	if (vm_debug_events) {
+		KERNEL_DEBUG_CONSTANT1((MACHDBG_CODE(DBG_MACH_VM_PRESSURE, event)) | control,
+					vm_page_active_count, vm_page_inactive_count,
+					vm_page_free_count, vm_page_speculative_count,
+					vm_page_throttled_count);
+	} else {
+		(void) event;
+		(void) control;
+	}
+
+}
+
+#ifdef MACH_BSD
+
+boolean_t  upl_device_page(upl_page_info_t *upl)
+{
+	return(UPL_DEVICE_PAGE(upl));
+}
+boolean_t  upl_page_present(upl_page_info_t *upl, int index)
+{
+	return(UPL_PAGE_PRESENT(upl, index));
+}
+boolean_t  upl_speculative_page(upl_page_info_t *upl, int index)
+{
+	return(UPL_SPECULATIVE_PAGE(upl, index));
+}
+boolean_t  upl_dirty_page(upl_page_info_t *upl, int index)
+{
+	return(UPL_DIRTY_PAGE(upl, index));
+}
+boolean_t  upl_valid_page(upl_page_info_t *upl, int index)
+{
+	return(UPL_VALID_PAGE(upl, index));
+}
+ppnum_t  upl_phys_page(upl_page_info_t *upl, int index)
+{
+	return(UPL_PHYS_PAGE(upl, index));
+}
+
+void upl_page_set_mark(upl_page_info_t *upl, int index, boolean_t v)
+{
+	upl[index].mark = v;
+}
+
+boolean_t upl_page_get_mark(upl_page_info_t *upl, int index)
+{
+	return upl[index].mark;
+}
+
+void
+vm_countdirtypages(void)
+{
+	vm_page_t m;
+	int dpages;
+	int pgopages;
+	int precpages;
+
+
+	dpages=0;
+	pgopages=0;
+	precpages=0;
+
+	vm_page_lock_queues();
+	m = (vm_page_t) queue_first(&vm_page_queue_inactive);
+	do {
+		if (m ==(vm_page_t )0) break;
+
+		if(m->dirty) dpages++;
+		if(m->pageout) pgopages++;
+		if(m->precious) precpages++;
+
+		assert(m->object != kernel_object);
+		m = (vm_page_t) queue_next(&m->pageq);
+		if (m ==(vm_page_t )0) break;
+
+	} while (!queue_end(&vm_page_queue_inactive,(queue_entry_t) m));
+	vm_page_unlock_queues();
+
+	vm_page_lock_queues();
+	m = (vm_page_t) queue_first(&vm_page_queue_throttled);
+	do {
+		if (m ==(vm_page_t )0) break;
+
+		dpages++;
+		assert(m->dirty);
+		assert(!m->pageout);
+		assert(m->object != kernel_object);
+		m = (vm_page_t) queue_next(&m->pageq);
+		if (m ==(vm_page_t )0) break;
+
+	} while (!queue_end(&vm_page_queue_throttled,(queue_entry_t) m));
+	vm_page_unlock_queues();
+
+	vm_page_lock_queues();
+	m = (vm_page_t) queue_first(&vm_page_queue_anonymous);
+	do {
+		if (m ==(vm_page_t )0) break;
+
+		if(m->dirty) dpages++;
+		if(m->pageout) pgopages++;
+		if(m->precious) precpages++;
+
+		assert(m->object != kernel_object);
+		m = (vm_page_t) queue_next(&m->pageq);
+		if (m ==(vm_page_t )0) break;
+
+	} while (!queue_end(&vm_page_queue_anonymous,(queue_entry_t) m));
+	vm_page_unlock_queues();
+
+	printf("IN Q: %d : %d : %d\n", dpages, pgopages, precpages);
+
+	dpages=0;
+	pgopages=0;
+	precpages=0;
+
+	vm_page_lock_queues();
+	m = (vm_page_t) queue_first(&vm_page_queue_active);
+
+	do {
+		if(m == (vm_page_t )0) break;
+		if(m->dirty) dpages++;
+		if(m->pageout) pgopages++;
+		if(m->precious) precpages++;
+
+		assert(m->object != kernel_object);
+		m = (vm_page_t) queue_next(&m->pageq);
+		if(m == (vm_page_t )0) break;
+
+	} while (!queue_end(&vm_page_queue_active,(queue_entry_t) m));
+	vm_page_unlock_queues();
+
+	printf("AC Q: %d : %d : %d\n", dpages, pgopages, precpages);
+
+}
+#endif /* MACH_BSD */
+
+ppnum_t upl_get_highest_page(
+			     upl_t			upl)
+{
+        return upl->highest_page;
+}
+
+upl_size_t upl_get_size(
+			     upl_t			upl)
+{
+        return upl->size;
+}
+
+upl_t upl_associated_upl(upl_t upl)
+{
+	return upl->associated_upl;
+}
+
+void upl_set_associated_upl(upl_t upl, upl_t associated_upl)
+{
+	upl->associated_upl = associated_upl;
+}
+
+#if UPL_DEBUG
+kern_return_t  upl_ubc_alias_set(upl_t upl, uintptr_t alias1, uintptr_t alias2)
+{
+	upl->ubc_alias1 = alias1;
+	upl->ubc_alias2 = alias2;
+	return KERN_SUCCESS;
+}
+int  upl_ubc_alias_get(upl_t upl, uintptr_t * al, uintptr_t * al2)
+{
+	if(al)
+		*al = upl->ubc_alias1;
+	if(al2)
+		*al2 = upl->ubc_alias2;
+	return KERN_SUCCESS;
+}
+#endif /* UPL_DEBUG */
+
+#if VM_PRESSURE_EVENTS
+/*
+ * Upward trajectory.
+ */
+extern boolean_t vm_compressor_low_on_space(void);
+
+boolean_t
+VM_PRESSURE_NORMAL_TO_WARNING(void)	{
+
+	if (DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPLESS) {
+		
+		/* Available pages below our threshold */
+		if (memorystatus_available_pages < memorystatus_available_pages_pressure) {
+			/* No frozen processes to kill */
+			if (memorystatus_frozen_count == 0) {
+				/* Not enough suspended processes available. */
+				if (memorystatus_suspended_count < MEMORYSTATUS_SUSPENDED_THRESHOLD) {
+					return TRUE;
+				}
+			}
+		}
+		return FALSE;
+
+	} else {
+		return ((AVAILABLE_NON_COMPRESSED_MEMORY < VM_PAGE_COMPRESSOR_COMPACT_THRESHOLD) ? 1 : 0);
+	}
+}
+
+boolean_t
+VM_PRESSURE_WARNING_TO_CRITICAL(void) {
+
+	if (DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPLESS) {
+		/* Available pages below our threshold */
+		if (memorystatus_available_pages < memorystatus_available_pages_critical) {
+			return TRUE;
+		}
+		return FALSE;
+	} else {
+		return (vm_compressor_low_on_space() || (AVAILABLE_NON_COMPRESSED_MEMORY < ((12 * VM_PAGE_COMPRESSOR_SWAP_UNTHROTTLE_THRESHOLD) / 10)) ? 1 : 0);
+	}
+}
+
+/*
+ * Downward trajectory.
+ */
+boolean_t
+VM_PRESSURE_WARNING_TO_NORMAL(void) {
+
+	if (DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPLESS) {
+		/* Available pages above our threshold */
+		unsigned int target_threshold = memorystatus_available_pages_pressure + ((15 * memorystatus_available_pages_pressure) / 100);
+		if (memorystatus_available_pages > target_threshold) {
+			return TRUE;
+		}
+		return FALSE;
+	} else {
+		return ((AVAILABLE_NON_COMPRESSED_MEMORY > ((12 * VM_PAGE_COMPRESSOR_COMPACT_THRESHOLD) / 10)) ? 1 : 0);
+	}
+}
+
+boolean_t
+VM_PRESSURE_CRITICAL_TO_WARNING(void) {
+
+	if (DEFAULT_PAGER_IS_ACTIVE || DEFAULT_FREEZER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPLESS) {
+		/* Available pages above our threshold */
+		unsigned int target_threshold = memorystatus_available_pages_critical + ((15 * memorystatus_available_pages_critical) / 100);
+		if (memorystatus_available_pages > target_threshold) {
+			return TRUE;
+		}
+		return FALSE;
+	} else {
+		return ((AVAILABLE_NON_COMPRESSED_MEMORY > ((14 * VM_PAGE_COMPRESSOR_SWAP_UNTHROTTLE_THRESHOLD) / 10)) ? 1 : 0);
+	}
+}
+#endif /* VM_PRESSURE_EVENTS */
+
diff -Nur xnu-3247.1.106/osfmk/x86_64/idt64.s xnu-3247.1.106-AnV/osfmk/x86_64/idt64.s
--- xnu-3247.1.106/osfmk/x86_64/idt64.s	2015-12-06 01:33:11.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/x86_64/idt64.s	2015-12-13 17:08:10.000000000 +0100
@@ -280,8 +280,8 @@
 L_common_dispatch:
 	cld		/* Ensure the direction flag is clear in the kernel */
 	cmpl    $0, EXT(pmap_smap_enabled)(%rip)
-	je	1f
-	clac		/* Clear EFLAGS.AC if SMAP is present/enabled */
+	jmp	1f
+	/* clac */		/* Clear EFLAGS.AC if SMAP is present/enabled */
 1:
 	/*
 	 * On entering the kernel, we don't need to switch cr3
diff -Nur xnu-3247.1.106/osfmk/x86_64/opemu.c xnu-3247.1.106-AnV/osfmk/x86_64/opemu.c
--- xnu-3247.1.106/osfmk/x86_64/opemu.c	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/x86_64/opemu.c	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,1316 @@
+/*   ** SINETEK **
+ * This is called the Opcode Emulator: it traps invalid opcode exceptions
+ *   and modifies the state of the running thread accordingly.
+ * There are two entry points, one for user space exceptions, and another for
+ *   exceptions coming from kernel space.
+ *
+ * STATUS
+ *  . SSE3 is implemented.
+ *  . SSSE3 is implemented.
+ *  . SYSENTER is implemented.
+ *  . SYSEXIT is implemented.
+ *  . RDMSR is implemented.
+ *  . Vector register save and restore is implemented.
+ *
+ * This is a new version of AnV Software based on the AMD SSEPlus project
+ * It runs much more reliable and much faster
+ */
+
+#define __SSEPLUS_NATIVE_SSE2_H__ 1
+#define __SSEPLUS_EMULATION_SSE2_H__ 1
+#define __SSEPLUS_ARITHMETIC_SSE2_H__ 1
+
+#include <stdint.h>
+
+#include "opemu.h"
+#include "opemu_math.h"
+
+#include <SSEPlus/SSEPlus_Base.h>
+#include <SSEPlus/SSEPlus_REF.h>
+
+#ifndef TESTCASE
+#include <kern/sched_prim.h>
+
+#define EMULATION_FAILED -1
+
+// forward declaration for syscall handlers of mach/bsd (32+64 bit);
+extern void mach_call_munger(x86_saved_state_t *state);
+extern void unix_syscall(x86_saved_state_t *);
+extern void mach_call_munger64(x86_saved_state_t *state);
+extern void unix_syscall64(x86_saved_state_t *);
+
+// forward declaration of panic handler for kernel traps;
+extern void panic_trap(x86_saved_state64_t *regs);
+
+// AnV - Implemented i386 version
+#ifdef __x86_64__
+unsigned char opemu_ktrap(x86_saved_state_t *state)
+{
+    x86_saved_state64_t *saved_state = saved_state64(state);
+    
+    uint8_t *code_buffer = (uint8_t *)saved_state->isf.rip;
+    unsigned int bytes_skip = 0;
+    
+    
+    bytes_skip = ssse3_run(code_buffer, state, 1, 1);
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run_a(code_buffer, state, 1, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run_b(code_buffer, state, 1, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run_c(code_buffer, state, 1, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = fisttp_run(code_buffer, state, 1, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = monitor_mwait_run(code_buffer, state, 1, 1);
+    }
+    
+    if(!bytes_skip)
+    {
+        /* since this is ring0, it could be an invalid MSR read.
+         * Instead of crashing the whole machine, report on it and keep running. */
+        if((code_buffer[0]==0x0f) && (code_buffer[1]==0x32))
+        {
+            printf("[MSR] unknown location 0x%016llx\r\n", saved_state->rcx);
+            // best we can do is return 0;
+            saved_state->rdx = saved_state->rax = 0;
+            bytes_skip = 2;
+        }
+    }
+    
+    saved_state->isf.rip += bytes_skip;
+    
+    if(!bytes_skip)
+    {
+        uint8_t *ripptr = (uint8_t *)&(saved_state->isf.rip);
+        printf("invalid kernel opcode (64-bit): ");
+        print_bytes(ripptr, 16);
+        
+        /* Fall through to trap */
+        return 0;
+    }
+
+    return 1;
+}
+#else
+unsigned char opemu_ktrap(x86_saved_state_t *state)
+{
+    x86_saved_state32_t *saved_state = saved_state32(state);
+    uint64_t op = saved_state->eip;
+    uint8_t *code_buffer = (uint8_t*)op ;
+    unsigned int bytes_skip = 0;
+    
+    
+    bytes_skip = ssse3_run(code_buffer, state, 0, 1);
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run_a(code_buffer, state, 0, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run_b(code_buffer, state, 0, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = sse3_run_c(code_buffer, state, 0, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = fisttp_run(code_buffer, state, 0, 1);
+    }
+    
+    if (!bytes_skip)
+    {
+        bytes_skip = monitor_mwait_run(code_buffer, state, 0, 1);
+    }
+    
+    if(!bytes_skip)
+    {
+        /* since this is ring0, it could be an invalid MSR read.
+         * Instead of crashing the whole machine, report on it and keep running. */
+        if(code_buffer[0]==0x0f && code_buffer[1]==0x32)
+        {
+            printf("[MSR] unknown location 0x%016llx\r\n", saved_state->ecx);
+            
+            // best we can do is return 0;
+            saved_state->edx = saved_state->eax = 0;
+            bytes_skip = 2;
+        }
+    }
+    
+    saved_state->eip += bytes_skip;
+    
+    if(!bytes_skip)
+    {
+        uint8_t *eipptr = (uint8_t *)&(saved_state->eip);
+        
+        printf("invalid kernel opcode (32-bit): ");
+        print_bytes(eipptr, 16);
+        
+        /* Fall through to trap */
+        return 0;
+    }
+    
+    return 1;
+}
+#endif
+
+void opemu_utrap(x86_saved_state_t *state)
+{
+    
+    int longmode;
+    
+    unsigned int bytes_skip = 0;
+    vm_offset_t addr;
+    
+    if ((longmode = is_saved_state64(state)))
+    {
+        
+        x86_saved_state64_t *saved_state = saved_state64(state);
+        uint8_t *code_buffer = (uint8_t*)saved_state->isf.rip;
+        
+        addr = saved_state->isf.rip;
+        uint16_t opcode;
+        
+        opcode = *(uint16_t *) addr;
+        
+        x86_saved_state64_t *regs;
+        regs = saved_state64(state);
+        if (opcode == 0x340f)
+        {
+            regs->isf.rip = regs->rdx;
+            regs->isf.rsp = regs->rcx;
+            
+            if((signed int)regs->rax < 0) {
+                //printf("mach call 64\n");
+                mach_call_munger64(state);
+            } else {
+                //printf("unix call 64\n");
+                unix_syscall64(state);
+            }
+            return;
+        }
+        
+        if (opcode == 0x350f)
+        {
+            regs->isf.rip = regs->rdx;
+            regs->isf.rsp = regs->rcx;
+            //if (kernel_trap)
+            //{
+            //  addr = regs->rcx;
+            //  return 0x7FFF;
+            //} else {
+            thread_exception_return();
+            //}
+            return;
+        }
+        
+        bytes_skip = ssse3_run(code_buffer, state, longmode, 0);
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run_a(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run_b(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run_c(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = fisttp_run(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = monitor_mwait_run(code_buffer, state, longmode, 0);
+        }
+        
+        regs->isf.rip += bytes_skip;
+        
+        if(!bytes_skip) {
+            uint8_t *ripptr = (uint8_t *)&(regs->isf.rip);
+            
+            printf("invalid user opcode 64: ");
+            print_bytes(ripptr, 16);
+
+            /* Fall through to trap */
+            return;
+        }
+    } else {
+        x86_saved_state32_t *saved_state = saved_state32(state);
+        uint64_t op = saved_state->eip;
+        uint8_t *code_buffer = (uint8_t*)op;
+        
+        addr = saved_state->eip;
+        uint16_t opcode;
+        
+        opcode = *(uint16_t *) addr;
+        
+        x86_saved_state32_t *regs;
+        regs = saved_state32(state);
+        
+        /*if (opcode == 0x340f)
+        {
+            regs->eip = regs->edx;
+            regs->uesp = regs->ecx;
+            
+            if((signed int)regs->eax < 0) {
+                mach_call_munger(state);
+            } else {
+                unix_syscall(state);
+            }
+            return;
+            
+        }*/
+        
+        if (opcode == 0x350f)
+        {
+            regs->eip = regs->edx;
+            regs->uesp = regs->ecx;
+            
+            thread_exception_return();
+            
+            return;
+        }
+        
+        bytes_skip = ssse3_run(code_buffer, state, longmode, 0);
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run_a(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run_b(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = sse3_run_c(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = fisttp_run(code_buffer, state, longmode, 0);
+        }
+        
+        if (!bytes_skip)
+        {
+            bytes_skip = monitor_mwait_run(code_buffer, state, longmode, 0);
+        }
+        
+        regs->eip += bytes_skip;
+        
+        if(!bytes_skip) {
+            uint8_t *eipptr = (uint8_t *)&(regs->eip);
+            
+            printf("invalid user opcode 32: ");
+            print_bytes(eipptr, 16);
+            
+            /* Fall through to trap */
+            return;
+        }
+        
+    }
+    
+    thread_exception_return();
+    /*** NOTREACHED ***/
+    
+    //EMULATION_FAILED;
+}
+
+/** Runs the sse3 emulator. returns the number of bytes consumed.
+ **/
+int sse3_run_a(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap)
+{
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    ssp_m128 xmmsrc, xmmdst, xmmres;
+    int src_higher = 0, dst_higher = 0;
+    
+    if(*bytep != 0xF2) return 0;
+    
+    bytep++;
+    ins_size++;
+    
+    if(*bytep != 0x0f) return 0;
+    bytep++;
+    ins_size++;
+    
+    uint8_t *modrm = &bytep[1];
+    ins_size += 1;
+    int consumed = fetchoperands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, ins_size);
+    ins_size += consumed;
+
+    switch (*bytep)
+    {
+        case 0x12:
+            //movddup(&xmmsrc,&xmmres);
+            xmmres.d = ssp_movedup_pd_REF(xmmsrc.d);
+            break;
+
+        case 0x7C:
+            //haddps(&xmmsrc,&xmmdst,&xmmres);
+            xmmres.f = ssp_hadd_ps_REF(xmmdst.f, xmmsrc.f);
+            break;
+
+        case 0x7D:
+            //hsubps(&xmmsrc,&xmmdst,&xmmres);
+            xmmres.f = ssp_hsub_ps_REF(xmmdst.f, xmmsrc.f);
+            break;
+
+        case 0xD0:
+            //addsubps(&xmmsrc,&xmmdst,&xmmres);
+            xmmres.f = ssp_addsub_ps_REF(xmmdst.f, xmmsrc.f);
+            break;
+
+        case 0xF0:
+            //lddqu(&xmmsrc,&xmmres);
+            xmmres.i = ssp_lddqu_si128_REF(&xmmsrc.i);
+            break;
+
+        default:
+            return 0;
+    }
+
+    storeresult128(*modrm, dst_higher, xmmres);
+    
+    return ins_size;
+}
+
+int sse3_run_b(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap)
+{
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    ssp_m128 xmmsrc, xmmdst, xmmres;
+    int src_higher = 0, dst_higher = 0;
+    
+    if(*bytep != 0xF3) return 0;
+    
+    bytep++;
+    ins_size++;
+    
+    if(*bytep != 0x0f) return 0;
+    bytep++;
+    ins_size++;
+    
+    uint8_t *modrm = &bytep[1];
+    ins_size += 1;
+    int consumed = fetchoperands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, ins_size);
+    ins_size += consumed;
+
+    switch (*bytep)
+    {
+        case 0x12:
+            //movsldup(&xmmsrc,&xmmres);
+            xmmres.f = ssp_moveldup_ps_REF(xmmsrc.f);
+            break;
+
+        case 0x16:
+            //movshdup(&xmmsrc,&xmmres);
+            xmmres.f = ssp_movehdup_ps_REF(xmmsrc.f);
+            break;
+
+        default:
+            return 0;
+    }
+
+    storeresult128(*modrm, dst_higher, xmmres);
+    
+    return ins_size;
+}
+
+int sse3_run_c(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap)
+{
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    ssp_m128 xmmsrc, xmmdst, xmmres;
+    int src_higher = 0, dst_higher = 0;
+    
+    if(*bytep != 0x66) return 0;
+    
+    bytep++;
+    ins_size++;
+    
+    if(*bytep != 0x0f) return 0;
+    bytep++;
+    ins_size++;
+    
+    uint8_t *modrm = &bytep[1];
+    ins_size += 1;
+    int consumed = fetchoperands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, ins_size);
+    ins_size += consumed;
+
+    switch (*bytep)
+    {
+        case 0x7C:
+            //haddpd(&xmmsrc,&xmmdst,&xmmres);
+            xmmres.d = ssp_hadd_pd_REF(xmmdst.d, xmmsrc.d);
+            break;
+
+        case 0x7D:
+            //hsubpd(&xmmsrc,&xmmdst,&xmmres);
+            xmmres.d = ssp_hsub_pd_REF(xmmdst.d, xmmsrc.d);
+            break;
+
+        case 0xD0:
+            //addsubpd(&xmmsrc,&xmmdst,&xmmres);
+            xmmres.d = ssp_addsub_pd_REF(xmmdst.d, xmmsrc.d);
+            break;
+
+        default:
+            return 0;
+    }
+
+    storeresult128(*modrm, dst_higher, xmmres);
+    
+    return ins_size;
+}
+
+int fisttp_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int __unused kernel_trap)
+{
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    uint8_t base = 0;
+    uint8_t mod = 0;
+    int8_t add = 0;
+    uint8_t modrm = 0;
+    uint64_t address = 0;
+    uint64_t reg_sel[8];
+    
+    if (longmode)
+    {
+        x86_saved_state64_t* r64 = saved_state64(state);
+        reg_sel[0] = r64->rax;
+        reg_sel[1] = r64->rcx;
+        reg_sel[2] = r64->rdx;
+        reg_sel[3] = r64->rbx;
+        reg_sel[4] = r64->isf.rsp;
+        reg_sel[5] = r64->rbp;
+        reg_sel[6] = r64->rsi;
+        reg_sel[7] = r64->rdi;
+    } else {
+        x86_saved_state32_t* r32 = saved_state32(state);
+        reg_sel[0] = r32->eax;
+        reg_sel[1] = r32->ecx;
+        reg_sel[2] = r32->edx;
+        reg_sel[3] = r32->ebx;
+        reg_sel[4] = r32->uesp;
+        reg_sel[5] = r32->ebp;
+        reg_sel[6] = r32->esi;
+        reg_sel[7] = r32->edi;
+    }
+    
+    if (*bytep == 0x66)
+    {
+        bytep++;
+        ins_size++;
+    }
+    
+    switch (*bytep)
+    {
+        case 0xDB:
+        {
+            bytep++;
+            ins_size++;
+            
+            modrm = *bytep;
+            base = modrm & 0x7;
+            mod = (modrm & 0xC0) >> 6;
+            
+            if (mod == 0)
+            {
+                address = reg_sel[base];
+            } else if (mod == 1) {
+                bytep++;
+                ins_size++;
+                
+                add = *bytep;
+                address = reg_sel[base] + add;
+            } else {
+                return 0;
+            }
+            
+            *(int *)address = fisttpl((double *)address);
+
+            ins_size++;
+            
+            return(ins_size);
+        }
+
+        case 0xDD:
+        {
+            bytep++;
+            ins_size++;
+            
+            modrm = *bytep;
+            base = modrm & 0x7;
+            mod = (modrm & 0xC0) >> 6;
+            
+            if (mod == 0)
+            {
+                address = reg_sel[base];
+            } else if (mod == 1) {
+                bytep++;
+                ins_size++;
+                
+                add = *bytep;
+                address = reg_sel[base] + add;
+            } else {
+                return 0;
+            }
+            
+            *(long long *)address = fisttpq((long double *)address);
+            
+            ins_size++;
+            
+            return(ins_size);
+        }
+
+        case 0xDF:
+        {
+            bytep++;
+            ins_size++;
+            
+            modrm = *bytep;
+            base = modrm & 0x7;
+            mod = (modrm & 0xC0) >> 6;
+            
+            if (mod == 0)
+            {
+                address = reg_sel[base];
+            } else if (mod == 1) {
+                bytep++;
+                ins_size++;
+                
+                add = *bytep;
+                address = reg_sel[base] + add;
+            } else {
+                return 0;
+            }
+            
+            *(short *)address = fisttps((float *)address);
+            
+            ins_size++;
+            
+            return(ins_size);
+        }
+    }
+    
+    return 0;
+}
+
+int monitor_mwait_run(uint8_t *instruction, __unused x86_saved_state_t *  state, int __unused longmode, int __unused kernel_trap)
+{
+    uint8_t *bytep = instruction;
+    
+    if (*bytep != 0x0F)
+    {
+        return 0;
+    }
+    
+    bytep++;
+    
+    if (*bytep != 0x01)
+    {
+        return 0;
+    }
+    
+    bytep++;
+    
+    switch(*bytep)
+    {
+        case 0xC8:
+        case 0xC9:
+            return 3;
+    }
+    
+    return 0;
+}
+
+const int palignr_getimm128(const unsigned char *bytep)
+{
+    int rv = 0;
+    uint8_t modrm = bytep[4];
+    
+    if (modrm < 0x40)
+    {
+        rv = (int)bytep[5];
+    } else if (modrm < 0x80) {
+        rv = (int)bytep[6];
+    } else if (modrm < 0xC0) {
+        rv = (int)bytep[9];
+    } else {
+        rv = (int)bytep[5];
+    }
+    
+    return (const int)rv;
+}
+
+const int palignr_getimm64(const unsigned char *bytep)
+{
+    int rv = 0;
+    uint8_t modrm = bytep[3];
+    
+    if (modrm < 0x40)
+    {
+        rv = (int)bytep[4];
+    } else if (modrm < 0x80) {
+        rv = (int)bytep[5];
+    } else if (modrm < 0xC0) {
+        rv = (int)bytep[8];
+    } else {
+        rv = (int)bytep[4];
+    }
+    
+    return (const int)rv;
+}
+
+/** Runs the ssse3 emulator. returns the number of bytes consumed.
+ **/
+int ssse3_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap)
+{
+    // pointer to the current byte we're working on
+    uint8_t *bytep = instruction;
+    int ins_size = 0;
+    int is_128 = 0, src_higher = 0, dst_higher = 0;
+    
+    
+    ssp_m128 xmmsrc, xmmdst, xmmres;
+    ssp_m64 mmsrc,mmdst, mmres;
+    
+    
+    /** We can get a few prefixes, in any order:
+     **  66 throws into 128-bit xmm mode.
+     **  40->4f use higher xmm registers.
+     **/
+    if(*bytep == 0x66) {
+        is_128 = 1;
+        bytep++;
+        ins_size++;
+    }
+    if((*bytep & 0xF0) == 0x40) {
+        if(*bytep & 1) src_higher = 1;
+        if(*bytep & 4) dst_higher = 1;
+        bytep++;
+        ins_size++;
+    }
+    
+    if(*bytep != 0x0f) return 0;
+    bytep++;
+    ins_size++;
+    
+    /* Two SSSE3 instruction prefixes. */
+    if((*bytep == 0x38 && bytep[1] != 0x0f) || (*bytep == 0x3a && bytep[1] == 0x0f)) {
+        uint8_t opcode = bytep[1];
+        uint8_t *modrm = &bytep[2];
+        uint8_t operand;
+        ins_size += 2; // not counting modRM byte or anything after.
+        
+        if(is_128) {
+            int consumed = fetchoperands(modrm, src_higher, dst_higher, &xmmsrc, &xmmdst, longmode, state, kernel_trap, 1, ins_size);
+            operand = bytep[2 + consumed];
+            ins_size += consumed;
+
+            switch(opcode) {
+                case 0x00:
+                    //pshufb128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_shuffle_epi8_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x01:
+                    //phaddw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_hadd_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x02:
+                    //phaddd128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_hadd_epi32_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x03:
+                    //phaddsw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_hadds_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x04:
+                    //pmaddubsw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_maddubs_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x05:
+                    //phsubw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_hsub_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x06:
+                    //phsubd128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_hsub_epi32_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x07:
+                    //phsubsw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_hsubs_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x08:
+                    //psignb128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_sign_epi8_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x09:
+                    //psignw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_sign_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x0A:
+                    //psignd128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_sign_epi32_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x0B:
+                    //pmulhrsw128(&xmmsrc,&xmmdst,&xmmres);
+                    xmmres.i = ssp_mulhrs_epi16_REF(xmmdst.i, xmmsrc.i);
+                    break;
+
+                case 0x0F:
+                    //palignr128(&xmmsrc,&xmmdst,&xmmres,(const int)operand);
+                    xmmres.i = ssp_alignr_epi8_REF(xmmdst.i, xmmsrc.i, palignr_getimm128(bytep));
+                    ins_size++;
+                    break;
+
+                case 0x1C:
+                    //pabsb128(&xmmsrc,&xmmres);
+                    xmmres.i = ssp_abs_epi8_REF(xmmsrc.i);
+                    break;
+
+                case 0x1D:
+                    //pabsw128(&xmmsrc,&xmmres);
+                    xmmres.i = ssp_abs_epi16_REF(xmmsrc.i);
+                    break;
+
+                case 0x1E:
+                    //pabsd128(&xmmsrc,&xmmres);
+                    xmmres.i = ssp_abs_epi32_REF(xmmsrc.i);
+                    break;
+
+                default:
+                    return 0;
+            }
+
+            storeresult128(*modrm, dst_higher, xmmres);
+        } else {
+            int consumed = fetchoperands(modrm, src_higher, dst_higher, &mmsrc, &mmdst, longmode, state, kernel_trap, 0, ins_size);
+            operand = bytep[2 + consumed];
+            ins_size += consumed;
+
+            switch(opcode) {
+                case 0x00:
+                    //pshufb64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_shuffle_pi8_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x01:
+                    //phaddw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_hadd_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x02:
+                    //phaddd64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_hadd_pi32_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x03:
+                    //phaddsw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_hadds_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x04:
+                    //pmaddubsw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_maddubs_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x05:
+                    //phsubw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_hsub_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x06:
+                    //phsubd64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_hsub_pi32_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x07:
+                    //phsubsw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_hsubs_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x08:
+                    //psignb64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_sign_pi8_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x09:
+                    //psignw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_sign_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x0A:
+                    //psignd64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_sign_pi32_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x0B:
+                    //pmulhrsw64(&mmsrc,&mmdst,&mmres);
+                    mmres.m64 = ssp_mulhrs_pi16_REF(mmdst.m64, mmsrc.m64);
+                    break;
+
+                case 0x0F:
+                    //palignr64(&mmsrc,&mmdst,&mmres, (const int)operand);
+                    mmres.m64 = ssp_alignr_pi8_REF(mmdst.m64, mmsrc.m64, palignr_getimm64(bytep));
+                    ins_size++;
+                    break;
+
+                case 0x1C:
+                    //pabsb64(&mmsrc,&mmres);
+                    mmres.m64 = ssp_abs_pi8_REF(mmsrc.m64);
+                    break;
+
+                case 0x1D:
+                    //pabsw64(&mmsrc,&mmres);
+                    mmres.m64 = ssp_abs_pi16_REF(mmsrc.m64);
+                    break;
+
+                case 0x1E:
+                    //pabsd64(&mmsrc,&mmres);
+                    mmres.m64 = ssp_abs_pi32_REF(mmsrc.m64);
+                    break;
+
+                default:
+                    return 0;
+            }
+
+            storeresult64(*modrm, dst_higher, mmres);
+        }
+        
+    } else {
+        // opcode wasn't handled here
+        return 0;
+    }
+    
+    return ins_size;
+}
+
+void print_bytes(uint8_t *from, int size)
+{
+    int i;
+    for(i = 0; i < size; ++i)
+    {
+        printf("%02x ", from[i]);
+    }
+    printf("\n");
+}
+
+/** Fetch SSSE3 operands (except immediate values, which are fetched elsewhere).
+ * We depend on memory copies, which differs depending on whether we're in kernel space
+ * or not. For this reason, need to pass in a lot of information, including the state of
+ * registers.
+ *
+ * The return value is the number of bytes used, including the ModRM byte,
+ * and displacement values, as well as SIB if used.
+ */
+int fetchoperands(uint8_t *ModRM, unsigned int hsrc, unsigned int hdst, void *src, void *dst, unsigned int longmode, x86_saved_state_t *saved_state, int kernel_trap, int size_128, __unused int  ins_size)
+{
+    unsigned int num_src = *ModRM & 0x7;
+    unsigned int num_dst = (*ModRM >> 3) & 0x7;
+    unsigned int mod = *ModRM >> 6;
+    int consumed = 1;
+    
+    if(hsrc) num_src += 8;
+    if(hdst) num_dst += 8;
+    if(size_128) getxmm((ssp_m128*)dst, num_dst);
+    else getmm((ssp_m64*)dst, num_dst);
+    
+    if(mod == 3) {
+        if(size_128) getxmm((ssp_m128*)src, num_src);
+        else getmm((ssp_m64*)src, num_src);
+    } else if ((longmode = is_saved_state64(saved_state))) {
+        uint64_t address;
+        
+        // DST is always an XMM register. decode for SRC.
+        x86_saved_state64_t *r64 = saved_state64(saved_state);
+        __uint64_t reg_sel[8] = {r64->rax, r64->rcx, r64->rdx,
+            r64->rbx, r64->isf.rsp, r64->rbp,
+            r64->rsi, r64->rdi};
+        if(hsrc) printf("opemu error: high reg ssse\n"); // FIXME
+        if(num_src == 4) {
+            // Special case: SIB byte used TODO fix r8-r15? 
+            uint8_t scale = ModRM[1] >> 6;
+            uint8_t base = ModRM[1] & 0x7;
+            uint8_t index = (ModRM[1] >> 3) & 0x7;
+            consumed++;
+            
+            // meaning of SIB depends on mod
+            if(mod == 0) {
+                if(base == 5) printf("opemu error: mod0 disp32 not implemented\n"); // FIXME
+                if(index == 4) address = reg_sel[base];
+                else
+				address = reg_sel[base] + (reg_sel[index] * (1<<scale));
+            } else {
+                if(index == 4) 
+				address = reg_sel[base];
+                else 
+				address = reg_sel[base] + (reg_sel[index] * (1<<scale));
+            }
+        } else {
+            address = reg_sel[num_src];
+        }
+        
+        if((mod == 0) && (num_src == 5)) {
+            // RIP-relative dword displacement
+            // AnV - Warning from cast alignment fix
+            __uint64_t ModRMVal = (__uint64_t)&ModRM[consumed];
+            __int32_t *ModRMCast = (__int32_t *)ModRMVal;
+            address = *(uint32_t*)&r64->isf.rip + *ModRMCast;
+            //printf("opemu adress rip: %llu \n",address);
+            
+            consumed += 4;
+        }
+		if(mod == 1) {
+            // byte displacement
+            address +=(int8_t)ModRM[consumed];
+            //printf("opemu adress byte : %llu \n",address);
+            consumed++;
+        } else if(mod == 2) {
+            // dword displacement. can it be qword?
+            // AnV - Warning from cast alignment fix
+            __uint64_t ModRMVal = (__uint64_t)&ModRM[consumed];
+            __int32_t *ModRMCast = (__int32_t *)ModRMVal;
+            address +=  *ModRMCast;
+            
+            //printf("opemu adress byte : %llu \n",address);
+            consumed += 4;
+        }
+        
+        // address is good now, do read and store operands.
+        if(kernel_trap) {
+            if(size_128) ((ssp_m128*)src)->ui = *(__uint128_t *)address;
+            else ((ssp_m64*)src)->u64 = *(uint64_t *)address;
+        } else {
+            //printf("xnu: da = %llx, rsp=%llx,  rip=%llx\n", address, reg_sel[4], r64->isf.rip);
+            if(size_128) copyin(address, (char*)& ((ssp_m128*)src)->u8, 16);
+            else copyin(address, (char*)& ((ssp_m64*)src)->u8, 8);
+        }
+    }else {
+        // AnV - Implemented 32-bit fetch
+        uint32_t address;
+        
+        // DST is always an XMM register. decode for SRC.
+        x86_saved_state32_t* r32 = saved_state32(saved_state);
+        uint32_t reg_sel[8] = {r32->eax, r32->ecx, r32->edx,
+            r32->ebx, r32->uesp, r32->ebp,
+            r32->esi, r32->edi};
+        if(hsrc) printf("opemu error: high reg ssse\n"); // FIXME
+        if(num_src == 4) {
+            /* Special case: SIB byte used TODO fix r8-r15? */
+            uint8_t scale = ModRM[1] >> 6;
+            uint8_t base = ModRM[1] & 0x7;
+            uint8_t index = (ModRM[1] >> 3) & 0x7;
+            consumed++;
+            
+            // meaning of SIB depends on mod
+            if(mod == 0) {
+                if(base == 5) printf("opemu error: mod0 disp32 not implemented\n"); // FIXME
+                if(index == 4) address = reg_sel[base];
+                else address = reg_sel[base] + (reg_sel[index] * (1<<scale));
+            } else {
+                if(index == 4) address = reg_sel[base];
+                else address = reg_sel[base] + (reg_sel[index] * (1<<scale));
+            }
+        } else {
+            address = reg_sel[num_src];
+        }
+        
+        if((mod == 0) && (num_src == 5)) {
+            // RIP-relative dword displacement
+            // AnV - Warning from cast alignment fix
+            uint64_t ModRMVal = (uint64_t)&ModRM[consumed];
+            int32_t *ModRMCast = (int32_t *)ModRMVal;
+            address = r32->eip + *ModRMCast;
+            
+            //address = r32->eip + *((int32_t*)&ModRM[consumed]);
+            consumed += 4;
+        } if(mod == 1) {
+            // byte displacement
+            //int32_t mods = (int32_t)ModRM[consumed];
+            //int8_t *Mods = (int8_t*)&mods;
+            address += (int8_t)ModRM[consumed];
+            // printf("opemu adress byte : %llu \n",address);
+            consumed++;
+        } else if(mod == 2) {
+            // dword displacement. can it be qword?
+            // AnV - Warning from cast alignment fix
+            uint64_t ModRMVal = (uint64_t)&ModRM[consumed];
+            int32_t *ModRMCast = (int32_t *)ModRMVal;
+            address += *ModRMCast;
+            
+            //address += *((int32_t*)&ModRM[consumed]);
+            consumed += 4;
+        }
+        
+        // address is good now, do read and store operands.
+        uint64_t addr = address;
+        
+        if(kernel_trap) {
+            if(size_128) ((ssp_m128*)src)->ui = *(__uint128_t *)addr;
+            else ((ssp_m64*)src)->u64 = *(uint64_t *)addr;
+        } else {
+            //printf("xnu: da = %llx, rsp=%llx,  rip=%llx\n", address, reg_sel[4], r32->eip);
+            if(size_128) copyin(addr, (char*) &((ssp_m128*)src)->u8, 16);
+            else copyin(addr, (char*) &((ssp_m64*)src)->u64, 8);
+        }
+    }
+    
+    return consumed;
+}
+
+void storeresult128(uint8_t ModRM, unsigned int hdst, ssp_m128 res)
+{
+    unsigned int num_dst = (ModRM >> 3) & 0x7;
+    if(hdst) num_dst += 8;
+    movxmm(&res, num_dst);
+}
+
+void storeresult64(uint8_t ModRM, unsigned int __unused hdst, ssp_m64 res)
+{
+    unsigned int num_dst = (ModRM >> 3) & 0x7;
+    movmm(&res, num_dst);
+}
+
+#endif /* TESTCASE */
+
+/* get value from the xmm register i */
+void getxmm(ssp_m128 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movdqu %%xmm0, %0" : "=m" (v->u64));
+            break;
+        case 1:
+            asm __volatile__ ("movdqu %%xmm1, %0" : "=m" (v->u64));
+            break;
+        case 2:
+            asm __volatile__ ("movdqu %%xmm2, %0" : "=m" (v->u64));
+            break;
+        case 3:
+            asm __volatile__ ("movdqu %%xmm3, %0" : "=m" (v->u64));
+            break;
+        case 4:
+            asm __volatile__ ("movdqu %%xmm4, %0" : "=m" (v->u64));
+            break;
+        case 5:
+            asm __volatile__ ("movdqu %%xmm5, %0" : "=m" (v->u64));
+            break;
+        case 6:
+            asm __volatile__ ("movdqu %%xmm6, %0" : "=m" (v->u64));
+            break;
+        case 7:
+            asm __volatile__ ("movdqu %%xmm7, %0" : "=m" (v->u64));
+            break;
+#ifdef __x86_64__
+        case 8:
+            asm __volatile__ ("movdqu %%xmm8, %0" : "=m" (v->u64));
+            break;
+        case 9:
+            asm __volatile__ ("movdqu %%xmm9, %0" : "=m" (v->u64));
+            break;
+        case 10:
+            asm __volatile__ ("movdqu %%xmm10, %0" : "=m" (v->u64));
+            break;
+        case 11:
+            asm __volatile__ ("movdqu %%xmm11, %0" : "=m" (v->u64));
+            break;
+        case 12:
+            asm __volatile__ ("movdqu %%xmm12, %0" : "=m" (v->u64));
+            break;
+        case 13:
+            asm __volatile__ ("movdqu %%xmm13, %0" : "=m" (v->u64));
+            break;
+        case 14:
+            asm __volatile__ ("movdqu %%xmm14, %0" : "=m" (v->u64));
+            break;
+        case 15:
+            asm __volatile__ ("movdqu %%xmm15, %0" : "=m" (v->u64));
+            break;
+#endif
+    }
+}
+
+/* get value from the mm register i  */
+void getmm(ssp_m64 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movq %%mm0, %0" : "=m" (v->u64));
+            break;
+        case 1:
+            asm __volatile__ ("movq %%mm1, %0" : "=m" (v->u64));
+            break;
+        case 2:
+            asm __volatile__ ("movq %%mm2, %0" : "=m" (v->u64));
+            break;
+        case 3:
+            asm __volatile__ ("movq %%mm3, %0" : "=m" (v->u64));
+            break;
+        case 4:
+            asm __volatile__ ("movq %%mm4, %0" : "=m" (v->u64));
+            break;
+        case 5:
+            asm __volatile__ ("movq %%mm5, %0" : "=m" (v->u64));
+            break;
+        case 6:
+            asm __volatile__ ("movq %%mm6, %0" : "=m" (v->u64));
+            break;
+        case 7:
+            asm __volatile__ ("movq %%mm7, %0" : "=m" (v->u64));
+            break;
+    }
+}
+
+/* move value over to xmm register i */
+void movxmm(ssp_m128 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movdqu %0, %%xmm0" :: "m" (v->ui) );
+            break;
+        case 1:
+            asm __volatile__ ("movdqu %0, %%xmm1" :: "m" (v->ui) );
+            break;
+        case 2:
+            asm __volatile__ ("movdqu %0, %%xmm2" :: "m" (v->ui) );
+            break;
+        case 3:
+            asm __volatile__ ("movdqu %0, %%xmm3" :: "m" (v->ui) );
+            break;
+        case 4:
+            asm __volatile__ ("movdqu %0, %%xmm4" :: "m" (v->ui) );
+            break;
+        case 5:
+            asm __volatile__ ("movdqu %0, %%xmm5" :: "m" (v->ui) );
+            break;
+        case 6:
+            asm __volatile__ ("movdqu %0, %%xmm6" :: "m" (v->ui) );
+            break;
+        case 7:
+            asm __volatile__ ("movdqu %0, %%xmm7" :: "m" (v->ui) );
+            break;
+#ifdef __x86_64__
+        case 8:
+            asm __volatile__ ("movdqu %0, %%xmm8" :: "m" (v->ui) );
+            break;
+        case 9:
+            asm __volatile__ ("movdqu %0, %%xmm9" :: "m" (v->ui) );
+            break;
+        case 10:
+            asm __volatile__ ("movdqu %0, %%xmm10" :: "m" (v->ui) );
+            break;
+        case 11:
+            asm __volatile__ ("movdqu %0, %%xmm11" :: "m" (v->ui) );
+            break;
+        case 12:
+            asm __volatile__ ("movdqu %0, %%xmm12" :: "m" (v->ui) );
+            break;
+        case 13:
+            asm __volatile__ ("movdqu %0, %%xmm13" :: "m" (v->ui) );
+            break;
+        case 14:
+            asm __volatile__ ("movdqu %0, %%xmm14" :: "m" (v->ui) );
+            break;
+        case 15:
+            asm __volatile__ ("movdqu %0, %%xmm15" :: "m" (v->ui) );
+            break;
+#endif
+    }
+}
+
+/* move value over to mm register i */
+void movmm(ssp_m64 *v, unsigned int i)
+{
+    switch(i) {
+        case 0:
+            asm __volatile__ ("movq %0, %%mm0" :: "m" (v->u64) );
+            break;
+        case 1:
+            asm __volatile__ ("movq %0, %%mm1" :: "m" (v->u64) );
+            break;
+        case 2:
+            asm __volatile__ ("movq %0, %%mm2" :: "m" (v->u64) );
+            break;
+        case 3:
+            asm __volatile__ ("movq %0, %%mm3" :: "m" (v->u64) );
+            break;
+        case 4:
+            asm __volatile__ ("movq %0, %%mm4" :: "m" (v->u64) );
+            break;
+        case 5:
+            asm __volatile__ ("movq %0, %%mm5" :: "m" (v->u64) );
+            break;
+        case 6:
+            asm __volatile__ ("movq %0, %%mm6" :: "m" (v->u64) );
+            break;
+        case 7:
+            asm __volatile__ ("movq %0, %%mm7" :: "m" (v->u64) );
+            break;
+    }
+}
+
+short fisttps(float *res)
+{
+    float value = opemu_truncf(*res);
+    __asm__ ("fistps %0" : : "m" (value));
+    *res = value;
+    return (short)(res);
+}
+
+int fisttpl(double *res)
+{
+    double value = opemu_trunc(*res);
+    __asm__ ("fistpl %0" : : "m" (value));
+    *res = value;
+    return (int)res;
+}
+
+long long fisttpq(long double *res)
+{
+    long double value = *res;
+    __asm__ ("fistpq %0" : : "m" (value));
+    *res = value;
+    return (long long)res;
+}
diff -Nur xnu-3247.1.106/osfmk/x86_64/opemu.h xnu-3247.1.106-AnV/osfmk/x86_64/opemu.h
--- xnu-3247.1.106/osfmk/x86_64/opemu.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/x86_64/opemu.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,56 @@
+#ifndef OPEMU_H
+#define OPEMU_H
+#include <stdint.h>
+
+#ifndef TESTCASE
+#include <mach/thread_status.h>
+#endif
+
+#define __SSEPLUS_CPUID_H__ 1
+#define __SSEPLUS_EMULATION_COMPS_SSE3_H__ 1
+
+#include <SSEPlus/SSEPlus_base.h>
+#include <SSEPlus/SSEPlus_REF.h>
+#include <SSEPlus/SSEPlus_SSE2.h>
+
+#ifndef TESTCASE
+/** XNU TRAP HANDLERS **/
+unsigned char opemu_ktrap(x86_saved_state_t *state);
+void opemu_utrap(x86_saved_state_t *state);
+int ssse3_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int);
+int sse3_run_a(uint8_t *instruction, x86_saved_state_t *state, int longmode, int );
+int sse3_run_b(uint8_t *instruction, x86_saved_state_t *state, int longmode, int );
+int sse3_run_c(uint8_t *instruction, x86_saved_state_t *state, int longmode, int );
+//int sysenter_run(uint8_t *instruction, x86_saved_state_t *state, int longmode);
+int sysexit_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int );
+int monitor_mwait_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap);
+int fisttp_run(uint8_t *instruction, x86_saved_state_t *state, int longmode, int kernel_trap);
+
+int fetchoperands(uint8_t *ModRM, unsigned int hsrc, unsigned int hdst, void *src, void *dst,
+                  unsigned int longmode, x86_saved_state_t *regs, int , int size_128, int ins_size);
+void storeresult128(uint8_t ModRM, unsigned int hdst, ssp_m128 res);
+void storeresult64(uint8_t ModRM, unsigned int hdst, ssp_m64 res);
+#endif
+
+void print_bytes(uint8_t *from, int size);
+
+void loadallssp_m128(void);
+void loadallssp_m64(void);
+void storeallssp_m128(void);
+void storeallssp_m64(void);
+
+void getssp_m128(ssp_m128 *v, unsigned int i);
+void getssp_m64(ssp_m64 *v, unsigned int i);
+void movssp_m128(ssp_m128 *v, unsigned int i);
+void movssp_m64(ssp_m64 *v, unsigned int i);
+
+short fisttps(float *res);
+int fisttpl(double *res);
+long long fisttpq(long double *res);
+
+void getxmm(ssp_m128 *v, unsigned int i);
+void getmm(ssp_m64 *v, unsigned int i);
+void movxmm(ssp_m128 *v, unsigned int i);
+void movmm(ssp_m64 *v, unsigned int i);
+
+#endif
diff -Nur xnu-3247.1.106/osfmk/x86_64/opemu_math.c xnu-3247.1.106-AnV/osfmk/x86_64/opemu_math.c
--- xnu-3247.1.106/osfmk/x86_64/opemu_math.c	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/x86_64/opemu_math.c	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,140 @@
+#include <stdint.h>
+#include "opemu_math.h"
+
+/* 32-bit */
+float opemu_truncf(float x)
+{
+        float ret = x;
+        int32_t signbit=0;
+        int32_t w=0;
+        int32_t exponent_less_127=0;
+    
+        GET_FLOAT_WORD(w,ret);
+    
+        /* Extract sign bit. */
+        signbit = w & 0x80000000;
+    
+        /* Extract exponent field. */
+        exponent_less_127 = ((w & 0x7f800000) >> 23) - 127;
+    
+        if (exponent_less_127 < 23)
+            {
+                    if (exponent_less_127 < 0)
+                        {
+                                /* -1 < x < 1, so result is +0 or -0. */
+                                SET_FLOAT_WORD(ret, signbit);
+                            }
+                    else
+                        {
+                                SET_FLOAT_WORD(ret, signbit | (w & ~(0x007fffff >> exponent_less_127)));
+                            }
+                }
+        else
+            {
+                    if (exponent_less_127 == 255)
+                        /* x is NaN or infinite. */
+                            return ret + ret;
+            
+                    /* All bits in the fraction field are relevant. */
+                }
+        return ret;
+    }
+
+/* 64-bit */
+double opemu_trunc(double x)
+{
+        double ret = x;
+        int signbit=0;
+        /* Most significant word, least significant word. */
+        int msw=0;
+        unsigned int lsw=0;
+        int exponent_less_1023=0;
+    
+        EXTRACT_WORDS(msw, lsw, ret);
+    
+        /* Extract sign bit. */
+        signbit = msw & 0x80000000;
+    
+        /* Extract exponent field. */
+        exponent_less_1023 = ((msw & 0x7ff00000) >> 20) - 1023;
+    
+        if (exponent_less_1023 < 20)
+            {
+                    /* All significant digits are in msw. */
+                    if (exponent_less_1023 < 0)
+                        {
+                                /* -1 < x < 1, so result is +0 or -0. */
+                                INSERT_WORDS(ret, signbit, 0);
+                            }
+                    else
+                        {
+                                /* All relevant fraction bits are in msw, so lsw of the result is 0. */
+                                INSERT_WORDS(ret, signbit | (msw & ~(0x000fffff >> exponent_less_1023)), 0);
+                            }
+                }
+        else if (exponent_less_1023 > 51)
+            {
+                    if (exponent_less_1023 == 1024)
+                        {
+                                /* x is infinite, or not a number, so trigger an exception. */
+                                return ret + ret;
+                            }
+                    /* All bits in the fraction fields of the msw and lsw are needed in the result. */
+                }
+        else
+            {
+                    /* All fraction bits in msw are relevant.  Truncate irrelevant
+                               bits from lsw. */
+                    INSERT_WORDS(ret, msw, lsw & ~(0xffffffffu >> (exponent_less_1023 - 20)));
+                }
+        return ret;
+    }
+
+/* 128-bit */
+long double opemu_truncl(long double x)
+{
+        long double ret = x;
+        uint64_t signbit;
+        /* Most significant word, least significant word. */
+        uint64_t msw=0;
+        uint64_t lsw=0;
+        int32_t exponent_less_16383=0;
+    
+        GET_LDOUBLE_WORDS(msw, lsw, ret);
+    
+        /* Extract sign bit. */
+        signbit = msw & 0x8000000000000000ULL;
+    
+        /* Extract exponent field. */
+        exponent_less_16383 = ((msw & 0x7fff000000000000ULL) >> 48) - 16383;
+    
+        if (exponent_less_16383 < 48)
+            {
+                    /* All significant digits are in msw. */
+                    if (exponent_less_16383 < 0)
+                        {
+                                /* -1 < x < 1, so result is +0 or -0. */
+                                SET_LDOUBLE_WORDS(ret, signbit, 0);
+                            } else {
+                                    /* All relevant fraction bits are in msw, so lsw of the result is 0. */
+                                    SET_LDOUBLE_WORDS(ret, msw & ~(0x0000ffffffffffffLL >> exponent_less_16383), 0);
+                                }
+                }
+        else if (exponent_less_16383 > 111)
+            {
+                    if (exponent_less_16383 == 16384)
+                        {
+                                /* x is infinite, or not a number, so trigger an exception. */
+                                return ret + ret;
+                            }
+                    /* All bits in the fraction fields of the msw and lsw are needed in the result. */
+                }
+        else
+            {
+                    /* All fraction bits in msw are relevant.  Truncate irrelevant
+                               bits from lsw. */
+                    SET_LDOUBLE_WORDS(ret, msw, lsw & ~(0xffffffffffffffffULL >> (exponent_less_16383 - 48)));
+                }
+        
+        return ret;
+    }
\ No newline at end of file
diff -Nur xnu-3247.1.106/osfmk/x86_64/opemu_math.h xnu-3247.1.106-AnV/osfmk/x86_64/opemu_math.h
--- xnu-3247.1.106/osfmk/x86_64/opemu_math.h	1970-01-01 01:00:00.000000000 +0100
+++ xnu-3247.1.106-AnV/osfmk/x86_64/opemu_math.h	2015-12-13 17:08:10.000000000 +0100
@@ -0,0 +1,79 @@
+/* Needed unions */
+typedef union
+{
+        float value;
+        uint32_t word;
+    } ieee_float_shape_type;
+
+typedef union
+{
+        double value;
+        struct
+        {
+                uint32_t lsw;
+                uint32_t msw;
+            } parts;
+    } ieee_double_shape_type;
+
+typedef union
+{
+        long double value;
+        struct
+        {
+                uint64_t lsw;
+                uint64_t msw;
+            } parts;
+    } ieee_long_double_shape_type;
+
+/* 32-bit (float) */
+#define GET_FLOAT_WORD(i,d) \
+  do { \
+        ieee_float_shape_type gf_u; \
+        gf_u.value = (d); \
+        (i) = gf_u.word; \
+      } while (0)
+
+#define SET_FLOAT_WORD(d,i) \
+  do { \
+        ieee_float_shape_type sf_u; \
+        sf_u.word = (i); \
+        (d) = sf_u.value; \
+      } while (0)
+
+/* 64-bit (double) */
+#define EXTRACT_WORDS(ix0,ix1,d) \
+  do { \
+        ieee_double_shape_type ew_u; \
+        ew_u.value = (d); \
+        (ix0) = ew_u.parts.msw; \
+        (ix1) = ew_u.parts.lsw; \
+      } while (0)
+
+#define INSERT_WORDS(d,ix0,ix1) \
+  do { \
+        ieee_double_shape_type iw_u; \
+        iw_u.parts.msw = (ix0); \
+        iw_u.parts.lsw = (ix1); \
+        (d) = iw_u.value; \
+      } while (0)
+
+/* 128-bit (long double) */
+#define GET_LDOUBLE_WORDS(ix0,ix1,d) \
+  do { \
+        ieee_long_double_shape_type qw_u; \
+        qw_u.value = (d); \
+        (ix0) = qw_u.parts.msw; \
+        (ix1) = qw_u.parts.lsw; \
+      } while (0)
+
+#define SET_LDOUBLE_WORDS(d,ix0,ix1) \
+  do { \
+        ieee_long_double_shape_type qw_u; \
+        qw_u.parts.msw = (ix0); \
+        qw_u.parts.lsw = (ix1); \
+        (d) = qw_u.value; \
+      } while (0)
+
+float opemu_truncf(float x);
+double opemu_trunc(double x);
+long double opemu_truncl(long double x);
\ No newline at end of file
diff -Nur xnu-3247.1.106/pexpert/i386/pe_kprintf.c xnu-3247.1.106-AnV/pexpert/i386/pe_kprintf.c
--- xnu-3247.1.106/pexpert/i386/pe_kprintf.c	2015-12-06 01:33:12.000000000 +0100
+++ xnu-3247.1.106-AnV/pexpert/i386/pe_kprintf.c	2015-12-13 17:08:10.000000000 +0100
@@ -39,6 +39,8 @@
 #include <machine/pal_routines.h>
 #include <i386/proc_reg.h>
 
+extern void conslog_putc(char);
+
 /* Globals */
 void (*PE_kputc)(char c);
 
@@ -50,6 +52,7 @@
 #else
 unsigned int disable_serial_output = TRUE;
 #endif
+unsigned int enable_conslog_kprintf = FALSE;
 
 decl_simple_lock_data(static, kprintf_lock)
 
@@ -65,7 +68,9 @@
 
 		simple_lock_init(&kprintf_lock, 0);
 
-		if (PE_parse_boot_argn("debug", &boot_arg, sizeof (boot_arg)))
+		if (PE_parse_boot_argn("kprintf", &boot_arg, sizeof(boot_arg)) && boot_arg)
+ 			enable_conslog_kprintf = TRUE;
+ 		else if (PE_parse_boot_argn("debug", &boot_arg, sizeof(boot_arg)))
 			if (boot_arg & DB_KPRT)
 				new_disable_serial_output = FALSE;
 
@@ -75,7 +80,7 @@
 		if (!new_disable_serial_output && (!disable_serial_output || pal_serial_init()))
 			PE_kputc = pal_serial_putc;
 		else
-			PE_kputc = cnputc;
+			PE_kputc = conslog_putc;
 
 		disable_serial_output = new_disable_serial_output;
 	}
@@ -106,7 +111,7 @@
 	va_list   listp;
 	boolean_t state;
 
-	if (!disable_serial_output) {
+	if (enable_conslog_kprintf || !disable_serial_output) {
 		boolean_t early = FALSE;
 		if (rdmsr64(MSR_IA32_GS_BASE) == 0) {
 			early = TRUE;
